{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "730e80aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T03:02:03.733124Z",
     "start_time": "2024-05-15T03:01:52.499161Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import torch\n",
    "from torch.nn import Embedding\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, average_precision_score\n",
    "from sklearn import metrics\n",
    "import torch.utils.data as data\n",
    "from torch.nn import functional as F\n",
    "#from torch_geometric.datasets import Planetoid\n",
    "EPS = 1e-15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07e7def3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T03:02:03.768802Z",
     "start_time": "2024-05-15T03:02:03.752566Z"
    }
   },
   "outputs": [],
   "source": [
    "def computer_prediction(true_edges, recons_edges):\n",
    "    predict_graph = recons_edges\n",
    "    predict_edges = np.array(predict_graph)\n",
    "    \n",
    "    ap = average_precision_score(true_edges, predict_edges)\n",
    "    print(\"AP： \", ap)\n",
    "    fpr, tpr, _ = metrics.roc_curve(true_edges, predict_edges, pos_label=1)\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    print(\"AUC SCORE: \",auc)\n",
    "    \n",
    "    return ap, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0379877b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T03:02:03.801148Z",
     "start_time": "2024-05-15T03:02:03.786955Z"
    }
   },
   "outputs": [],
   "source": [
    "#####################################  测试用  #########################################\n",
    "def predict_edges(lhs, rhs, anchor_emb):\n",
    "    # power = float(alpha + 1) / 2\n",
    "    power = 1.0\n",
    "    \n",
    "    # Calculate the attention score (in nodes) for each src node to each cluster\n",
    "    src_norm_squared = torch.sum((lhs.unsqueeze(1) - anchor_emb)**2, 2)   # Distance from the node to the center of mass\n",
    "    src_numerator = 1.0 / (1.0 + (src_norm_squared / DEC))\n",
    "    src_numerator = src_numerator**power   \n",
    "    src_soft_assignments = (src_numerator.t() / torch.sum(src_numerator, 1)).t()       #soft assignment using t-distribution\n",
    "    \n",
    "    # Calculate the attention score (in nodes) for each dst node to each cluster\n",
    "    dst_norm_squared = torch.sum((rhs.unsqueeze(1) - anchor_emb)**2, 2)   # Distance from the node to the center of mass\n",
    "    dst_numerator = 1.0 / (1.0 + (dst_norm_squared / DEC))\n",
    "    dst_numerator = dst_numerator**power   \n",
    "    dst_soft_assignments = (dst_numerator.t() / torch.sum(dst_numerator, 1)).t()       #soft assignment using t-distribution\n",
    "    \n",
    "    clusters_similar = torch.cosine_similarity(src_soft_assignments, dst_soft_assignments)\n",
    "    \n",
    "#     nodes_distance = torch.norm(lhs[:, None]-rhs, dim=2, p=2)\n",
    "#     nodes_distance = torch.div(nodes_distance, torch.max(nodes_distance))\n",
    "    \n",
    "    distance_U_V = F.pairwise_distance(lhs, rhs)\n",
    "    nodes_distance = torch.div(distance_U_V, torch.max(distance_U_V))\n",
    "#     distance_U_V = 1.0 / (1.0 + (distance_U_V / DEC))\n",
    "#     nodes_distance = distance_U_V**power\n",
    "    \n",
    "    distance_similar = torch.div(beta*nodes_distance, clusters_similar)      \n",
    "    LINK_PROB = torch.exp(-distance_similar)\n",
    "\n",
    "#     LINK_PROB = 0.5*nodes_distance + 0.5*clusters_similar\n",
    "\n",
    "    return LINK_PROB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56d1e28b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T03:02:03.822416Z",
     "start_time": "2024-05-15T03:02:03.808040Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"  Given the cluster centroids, update the node representation  \"\"\"\n",
    "class get_node_emb(nn.Module):\n",
    "    def __init__(self, node_emb, cluster_emb, alpha=1.0):\n",
    "        super(get_node_emb, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.power = float(self.alpha + 1) / 2 \n",
    "        self.nodes_embedding = Parameter(node_emb)\n",
    "        self.anchor_emb = cluster_emb\n",
    "        \n",
    "    def forward(self, idx_U, idx_V):\n",
    "        lhs = self.nodes_embedding[idx_U]\n",
    "        rhs = self.nodes_embedding[idx_V]\n",
    "        \n",
    "        # Calculate the attention score (in nodes) for each src node to each cluster\n",
    "        src_norm_squared = torch.sum((lhs.unsqueeze(1) - self.anchor_emb)**2, 2)   # Distance from the node to the center of mass\n",
    "        src_numerator = 1.0 / (1.0 + (src_norm_squared / DEC))\n",
    "        src_numerator = src_numerator**self.power   \n",
    "        src_soft_assignments = (src_numerator.t() / torch.sum(src_numerator, 1)).t()       #soft assignment using t-distribution\n",
    "        \n",
    "        # Calculate the attention score (in nodes) for each dst node to each cluster\n",
    "        dst_norm_squared = torch.sum((rhs.unsqueeze(1) - self.anchor_emb)**2, 2)   # Distance from the node to the center of mass\n",
    "        dst_numerator = 1.0 / (1.0 + (dst_norm_squared / DEC))\n",
    "        dst_numerator = dst_numerator**self.power   \n",
    "        dst_soft_assignments = (dst_numerator.t() / torch.sum(dst_numerator, 1)).t()       #soft assignment using t-distribution\n",
    "        \n",
    "        # Calculate the cluster similarity between nodes (cosine similarity): the larger the value, the more conducive to edge formation\n",
    "#         prod = torch.mm(src_soft_assignments, dst_soft_assignments.t())       #  numerator\n",
    "#         norm = torch.norm(soft_assignments,p=2,dim=1).unsqueeze(0)      #    denominator \n",
    "#         clusters_similar = prod.div(torch.mm(norm.t(),norm))\n",
    "        clusters_similar = torch.cosine_similarity(src_soft_assignments, dst_soft_assignments)\n",
    "        \n",
    "        # Calculate the distance between nodes: smaller values favor edge formation\n",
    "#         nodes_distance = torch.norm(lhs[:, None]-rhs, dim=2, p=2)\n",
    "#         nodes_distance = torch.div(nodes_distance, torch.max(nodes_distance))\n",
    "        \n",
    "        # distance_U_V = torch.sum((lhs.unsqueeze(1) - rhs)**2, 2)\n",
    "        distance_U_V = F.pairwise_distance(lhs, rhs)\n",
    "        nodes_distance = torch.div(distance_U_V, torch.max(distance_U_V))\n",
    "#         distance_U_V = 1.0 / (1.0 + (distance_U_V / DEC))\n",
    "#         nodes_distance = distance_U_V**self.power\n",
    "        \n",
    "        # The formation probability of the edges is calculated\n",
    "        distance_similar = torch.div(beta*nodes_distance, clusters_similar)      \n",
    "        LINK_PROB = torch.exp(-distance_similar)\n",
    "\n",
    "#         LINK_PROB = 0.5*nodes_distance + 0.5*clusters_similar\n",
    "        \n",
    "        return LINK_PROB\n",
    "\n",
    "\n",
    "class update_nodes_embedding(nn.Module):\n",
    "    def __init__(self, NODE_EMB, CLUSTERS_EMB):\n",
    "        super(update_nodes_embedding, self).__init__()\n",
    "        self.prediction_module = get_node_emb(NODE_EMB, CLUSTERS_EMB)     \n",
    "        self.optimizer = torch.optim.SGD(params=self.prediction_module.parameters(), lr=LR, momentum=0.9)\n",
    "        self.loss_function = torch.nn.MSELoss(reduction='sum')\n",
    "        \n",
    "    def forward(self, DATA_TRAIN, DATA_TEST):\n",
    "        self.prediction_module.train()\n",
    "        for epoch in range(5):\n",
    "#             self.optimizer.zero_grad()\n",
    "#             graph_reconstruction = self.reconstruction_module(CC)\n",
    "#             graph_train = torch.take(graph_reconstruction, edge_train)\n",
    "#             loss = self.loss_function(g, graph_train)\n",
    "#             loss.backward()\n",
    "#             self.optimizer.step()\n",
    "            #print(f'Epoch: {epoch:02d}, Loss: {loss.item():.4f}')\n",
    "            total_loss = 0\n",
    "\n",
    "            ##################### 批训练 #################\n",
    "            for step, (edge_U, edge_V, edge_label) in enumerate(DATA_TRAIN):\n",
    "            \n",
    "                nodes_similar = self.prediction_module(edge_U, edge_V)\n",
    "\n",
    "                loss = self.loss_function(nodes_similar, edge_label)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "            \n",
    "                loss.backward()\n",
    "\n",
    "                self.optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            print(f'Epoch: {epoch:02d}, Loss: {total_loss:.4f}')\n",
    "    \n",
    "        FINAL_NODE_EMDS = self.prediction_module.nodes_embedding.detach()\n",
    "        FINAL_cluster_EMDS = self.prediction_module.anchor_emb.detach()\n",
    "    \n",
    "        all_predict_edges = []\n",
    "        all_true_edges = []\n",
    "        for _, (test_idx_U, test_idx_V, test_edge_label) in enumerate(DATA_TEST):\n",
    "        \n",
    "            test_lhs = FINAL_NODE_EMDS[test_idx_U]\n",
    "            test_rhs = FINAL_NODE_EMDS[test_idx_V]\n",
    "\n",
    "            predict_test_edges = predict_edges(test_lhs, test_rhs, FINAL_cluster_EMDS)\n",
    "\n",
    "            predict_label = np.array(predict_test_edges)\n",
    "            true_label = np.array(test_edge_label)\n",
    "            all_predict_edges = np.concatenate((all_predict_edges, predict_label), axis=0)\n",
    "            all_true_edges = np.concatenate((all_true_edges, true_label),axis=0)\n",
    "    \n",
    "        predict_ap, predict_auc = computer_prediction(all_true_edges, all_predict_edges)\n",
    "        print('epoch: ', epoch, 'predict_auc_pr = ', predict_ap, 'predict_auc_roc = ', predict_auc)\n",
    "\n",
    "        return FINAL_NODE_EMDS, predict_ap, predict_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "551d00d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T03:06:50.069034Z",
     "start_time": "2024-05-15T03:06:50.050163Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"  Type-centroid update aimed at maximizing the prediction effect   \"\"\"\n",
    "class get_clusters_emb(nn.Module):\n",
    "    \"\"\"  Given the node representation, update the cluster centroids  \"\"\"\n",
    "    def __init__(self, node_emb, cluster_emb, alpha=1.0):\n",
    "        super(get_clusters_emb, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.power = float(self.alpha + 1) / 2\n",
    "        self.anchor_emb = Parameter(cluster_emb)\n",
    "        self.nodes_embedding = node_emb\n",
    "        \n",
    "    \n",
    "    def forward(self, idx_U, idx_V):\n",
    "        lhs = self.nodes_embedding[idx_U]\n",
    "        rhs = self.nodes_embedding[idx_V]\n",
    "        \n",
    "        # Calculate the attention score (in nodes) for each src node to each cluster\n",
    "        src_norm_squared = torch.sum((lhs.unsqueeze(1) - self.anchor_emb)**2, 2)   # Distance from the node to the center of mass\n",
    "        src_numerator = 1.0 / (1.0 + (src_norm_squared / DEC))\n",
    "        src_numerator = src_numerator**self.power   \n",
    "        src_soft_assignments = (src_numerator.t() / torch.sum(src_numerator, 1)).t()       #soft assignment using t-distribution\n",
    "        \n",
    "        # Calculate the attention score (in nodes) for each dst node to each cluster\n",
    "        dst_norm_squared = torch.sum((rhs.unsqueeze(1) - self.anchor_emb)**2, 2)   # Distance from the node to the center of mass\n",
    "        dst_numerator = 1.0 / (1.0 + (dst_norm_squared / DEC))\n",
    "        dst_numerator = dst_numerator**self.power   \n",
    "        dst_soft_assignments = (dst_numerator.t() / torch.sum(dst_numerator, 1)).t()       #soft assignment using t-distribution\n",
    "        \n",
    "        # Calculate the cluster similarity between nodes (cosine similarity): the larger the value, the more conducive to edge formation\n",
    "#         prod = torch.mm(src_soft_assignments, dst_soft_assignments.t())       #  numerator\n",
    "#         norm = torch.norm(soft_assignments,p=2,dim=1).unsqueeze(0)      #    denominator \n",
    "#         clusters_similar = prod.div(torch.mm(norm.t(),norm))\n",
    "        clusters_similar = torch.cosine_similarity(src_soft_assignments, dst_soft_assignments)\n",
    "        \n",
    "        # Calculate the distance between nodes: smaller values favor edge formation\n",
    "#         nodes_distance = torch.norm(lhs[:, None]-rhs, dim=2, p=2)\n",
    "#         nodes_distance = torch.div(nodes_distance, torch.max(nodes_distance))\n",
    "        \n",
    "        # distance_U_V = torch.sum((lhs.unsqueeze(1) - rhs)**2, 2)\n",
    "        distance_U_V = F.pairwise_distance(lhs, rhs)\n",
    "        nodes_distance = torch.div(distance_U_V, torch.max(distance_U_V))\n",
    "#         distance_U_V = 1.0 / (1.0 + (distance_U_V / DEC))\n",
    "#         nodes_distance = distance_U_V**self.power\n",
    "        \n",
    "        # The formation probability of the edges is calculated\n",
    "        distance_similar = torch.div(beta*nodes_distance, clusters_similar)      \n",
    "        LINK_PROB = torch.exp(-distance_similar)\n",
    "#         LINK_PROB = 0.5*nodes_distance + 0.5*clusters_similar\n",
    "        \n",
    "        \n",
    "        return LINK_PROB\n",
    "    \n",
    "    \n",
    "class update_cluster_embedding(nn.Module):\n",
    "    def __init__(self, NODE_EMB, CLUSTERS_EMB):\n",
    "        super(update_cluster_embedding, self).__init__()\n",
    "        self.prediction_module = get_clusters_emb(NODE_EMB, CLUSTERS_EMB)     \n",
    "        self.optimizer = torch.optim.SGD(params=self.prediction_module.parameters(), lr=LR, momentum=0.9)\n",
    "        self.loss_function = torch.nn.MSELoss(reduction='sum')\n",
    "        \n",
    "    def forward(self, DATA_TRAIN, DATA_TEST):\n",
    "        self.prediction_module.train()\n",
    "        for epoch in range(5):\n",
    "#             self.optimizer.zero_grad()\n",
    "#             graph_reconstruction = self.reconstruction_module(CC)\n",
    "#             graph_train = torch.take(graph_reconstruction, edge_train)\n",
    "#             loss = self.loss_function(g, graph_train)\n",
    "#             loss.backward()\n",
    "#             self.optimizer.step()\n",
    "            #print(f'Epoch: {epoch:02d}, Loss: {loss.item():.4f}')\n",
    "            total_loss = 0\n",
    "\n",
    "            ##################### 批训练 #################\n",
    "            for step, (edge_U, edge_V, edge_label) in enumerate(DATA_TRAIN):\n",
    "            \n",
    "                nodes_similar = self.prediction_module(edge_U, edge_V)\n",
    "\n",
    "                loss = self.loss_function(nodes_similar, edge_label)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "            \n",
    "                loss.backward()\n",
    "\n",
    "                self.optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            print(f'Epoch: {epoch:02d}, Loss: {total_loss:.4f}')\n",
    "    \n",
    "        FINAL_NODE_EMDS = self.prediction_module.nodes_embedding.detach()\n",
    "        FINAL_cluster_EMDS = self.prediction_module.anchor_emb.detach()\n",
    "    \n",
    "        all_predict_edges = []\n",
    "        all_true_edges = []\n",
    "        for _, (test_idx_U, test_idx_V, test_edge_label) in enumerate(DATA_TEST):\n",
    "        \n",
    "            test_lhs = FINAL_NODE_EMDS[test_idx_U]\n",
    "            test_rhs = FINAL_NODE_EMDS[test_idx_V]\n",
    "\n",
    "            predict_test_edges = predict_edges(test_lhs, test_rhs, FINAL_cluster_EMDS)\n",
    "\n",
    "            predict_label = np.array(predict_test_edges)\n",
    "            true_label = np.array(test_edge_label)\n",
    "            all_predict_edges = np.concatenate((all_predict_edges, predict_label), axis=0)\n",
    "            all_true_edges = np.concatenate((all_true_edges, true_label),axis=0)\n",
    "    \n",
    "        predict_ap, predict_auc = computer_prediction(all_true_edges, all_predict_edges)\n",
    "        print('epoch: ', epoch, 'predict_auc_pr = ', predict_ap, 'predict_auc_roc = ', predict_auc)\n",
    "\n",
    "        return FINAL_cluster_EMDS, predict_ap, predict_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ab424d7c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T03:06:50.611280Z",
     "start_time": "2024-05-15T03:06:50.592508Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_train_test_data(filename_adj, nodes_number):\n",
    "\n",
    "    edge_index = pd.read_csv(filename_adj, header=None, sep=' ')\n",
    "    edges = torch.from_numpy(edge_index.values).t().contiguous()\n",
    "    \n",
    "    #print(\"edges number ： \", edges.shape[1])\n",
    "    \n",
    "    graph_np = np.zeros((nodes_number, nodes_number))\n",
    "    for i in range(edges.shape[1]):\n",
    "        graph_np[edges[0,i], edges[1,i]]=1\n",
    "        graph_np[edges[1,i], edges[0,i]]=1\n",
    "\n",
    "    posi_edge = np.argwhere(graph_np == 1)\n",
    "    posi_edge = posi_edge[posi_edge[:,0]<posi_edge[:,1]]         # Only the upper-left corner matrix is taken\n",
    "    posi_edge_number = posi_edge.shape[0]\n",
    "    \n",
    "    nega_edge = np.argwhere(graph_np == 0) \n",
    "    nega_edge = nega_edge[nega_edge[:,0]<nega_edge[:,1]]     # Only the upper-left corner matrix is taken\n",
    "    \n",
    "    positive_index = np.random.choice(range(posi_edge_number),int(posi_edge_number*0.9),replace=False)\n",
    "    choose_positive = posi_edge[positive_index]\n",
    "    posi_not_choose = np.setdiff1d(range(posi_edge_number), positive_index)\n",
    "    test_positive = posi_edge[posi_not_choose]\n",
    "    \n",
    "    negative_index = np.random.choice(range(nega_edge.shape[0]),int(posi_edge_number*0.9)*4,replace=False)\n",
    "    choose_negative = nega_edge[negative_index]\n",
    "    nega_not_choose = np.setdiff1d(range(nega_edge.shape[0]), negative_index)\n",
    "    test_nega_index = np.random.choice(nega_not_choose,len(posi_not_choose),replace=False)\n",
    "    test_negative = nega_edge[test_nega_index]\n",
    "    \n",
    "    train_posi_edges_number = len(choose_positive)\n",
    "    train_neg_edges_number = len(choose_negative)\n",
    "\n",
    "    test_posi_edges_number = len(test_positive)\n",
    "    test_neg_edges_number = len(test_negative)\n",
    "    \n",
    "    train_posi = [list(choose_positive[i]) for i in range(train_posi_edges_number)]\n",
    "    train_nega = [list(choose_negative[i]) for i in range(train_neg_edges_number)]\n",
    "    train_index = train_posi + train_nega\n",
    "    train_edge_index = torch.tensor(train_index)\n",
    "    train_U_TRAIN_INDEX = train_edge_index[:, 0]\n",
    "    train_V_TRAIN_INDEX = train_edge_index[:, 1]\n",
    "\n",
    "    train_edge_label = list(np.ones(train_posi_edges_number)) + list(np.zeros(train_neg_edges_number))\n",
    "    train_edge_label = torch.tensor(train_edge_label)\n",
    "\n",
    "    # 先转换成torch能识别的dataset\n",
    "    train_dataset = data.TensorDataset(train_U_TRAIN_INDEX, train_V_TRAIN_INDEX, train_edge_label.float())\n",
    "\n",
    "    # 把dataset放入DataLoader\n",
    "    train_data_loader = data.DataLoader(\n",
    "        dataset = train_dataset,\n",
    "        batch_size = 128,             # 每批提取的数量\n",
    "        shuffle = True,             # 要不要打乱数据（打乱比较好）\n",
    "        num_workers = 2             # 多少线程来读取数据\n",
    "    )\n",
    "\n",
    "\n",
    "    test_posi = [list(test_positive[i]) for i in range(test_posi_edges_number)]\n",
    "    test_nega = [list(test_negative[i]) for i in range(test_neg_edges_number)]\n",
    "    test_index = test_posi + test_nega\n",
    "    test_edge_index = torch.tensor(test_index)\n",
    "    test_U_TRAIN_INDEX = test_edge_index[:, 0]\n",
    "    test_V_TRAIN_INDEX = test_edge_index[:, 1]\n",
    "    test_edge_label = list(np.ones(test_posi_edges_number)) + list(np.zeros(test_neg_edges_number))\n",
    "    test_edge_label = torch.tensor(test_edge_label)\n",
    "\n",
    "    # 先转换成torch能识别的dataset\n",
    "    test_dataset = data.TensorDataset(test_U_TRAIN_INDEX, test_V_TRAIN_INDEX, test_edge_label.float())\n",
    "\n",
    "    # 把dataset放入DataLoader\n",
    "    test_data_loader = data.DataLoader(\n",
    "        dataset = test_dataset,\n",
    "        batch_size = 128,             # 每批提取的数量\n",
    "        shuffle = True,             # 要不要打乱数据（打乱比较好）\n",
    "        num_workers = 1             # 多少线程来读取数据\n",
    "    )\n",
    "    \n",
    "    return train_data_loader, test_data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d2dd64e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T03:06:51.068661Z",
     "start_time": "2024-05-15T03:06:51.055545Z"
    }
   },
   "outputs": [],
   "source": [
    "def set_random_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3a450091",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T03:06:51.535650Z",
     "start_time": "2024-05-15T03:06:51.521027Z"
    }
   },
   "outputs": [],
   "source": [
    "class get_ini_emds(nn.Module):\n",
    "    def __init__(self, dim, nodes_number, num_CLUSTER):\n",
    "        super(get_ini_emds, self).__init__()\n",
    "        self.number_clusters = num_CLUSTER\n",
    "        self.NODES_EMB = nn.Embedding(nodes_number, dim)\n",
    "        \n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_normal_(self.NODES_EMB.weight)\n",
    "\n",
    "    def forward(self):\n",
    "        INI_NODES_EMB = self.NODES_EMB.weight.detach()\n",
    "\n",
    "        NODES_kmeans = KMeans(n_clusters = self.number_clusters, random_state=42).fit(INI_NODES_EMB)\n",
    "        cluster_centers = NODES_kmeans.cluster_centers_\n",
    "        anchor_emb = torch.tensor(cluster_centers, dtype=torch.float)     # Initial cluster centroids\n",
    "\n",
    "        return INI_NODES_EMB, anchor_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "21bf54b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T03:25:55.485791Z",
     "start_time": "2024-05-15T03:19:16.259214Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################### Module cycle ： 0 ##########################\n",
      "Epoch: 00, Loss: 1780.7840\n",
      "Epoch: 01, Loss: 1428.1260\n",
      "Epoch: 02, Loss: 1343.4122\n",
      "Epoch: 03, Loss: 1298.0459\n",
      "Epoch: 04, Loss: 1255.4125\n",
      "AP：  0.7422924208790077\n",
      "AUC SCORE:  0.7084910762574365\n",
      "epoch:  4 predict_auc_pr =  0.7422924208790077 predict_auc_roc =  0.7084910762574365\n",
      "Epoch: 00, Loss: 1202.2656\n",
      "Epoch: 01, Loss: 1202.5471\n",
      "Epoch: 02, Loss: 1204.5871\n",
      "Epoch: 03, Loss: 1196.3201\n",
      "Epoch: 04, Loss: 1192.6495\n",
      "AP：  0.7582220088870051\n",
      "AUC SCORE:  0.7196322336398053\n",
      "epoch:  4 predict_auc_pr =  0.7582220088870051 predict_auc_roc =  0.7196322336398053\n",
      "######################### Module cycle ： 1 ##########################\n",
      "Epoch: 00, Loss: 1218.5131\n",
      "Epoch: 01, Loss: 1175.8962\n",
      "Epoch: 02, Loss: 1145.5180\n",
      "Epoch: 03, Loss: 1125.2375\n",
      "Epoch: 04, Loss: 1101.9931\n",
      "AP：  0.7988658415565674\n",
      "AUC SCORE:  0.7648025959978366\n",
      "epoch:  4 predict_auc_pr =  0.7988658415565674 predict_auc_roc =  0.7648025959978366\n",
      "Epoch: 00, Loss: 1037.0204\n",
      "Epoch: 01, Loss: 1031.3842\n",
      "Epoch: 02, Loss: 1028.1150\n",
      "Epoch: 03, Loss: 1026.3973\n",
      "Epoch: 04, Loss: 1024.8050\n",
      "AP：  0.7935924264857825\n",
      "AUC SCORE:  0.773802055164954\n",
      "epoch:  4 predict_auc_pr =  0.7935924264857825 predict_auc_roc =  0.773802055164954\n",
      "######################### Module cycle ： 2 ##########################\n",
      "Epoch: 00, Loss: 1058.2303\n",
      "Epoch: 01, Loss: 1033.3275\n",
      "Epoch: 02, Loss: 1006.7877\n",
      "Epoch: 03, Loss: 996.5306\n",
      "Epoch: 04, Loss: 976.7698\n",
      "AP：  0.8269085008493665\n",
      "AUC SCORE:  0.8208761492698756\n",
      "epoch:  4 predict_auc_pr =  0.8269085008493665 predict_auc_roc =  0.8208761492698756\n",
      "Epoch: 00, Loss: 923.9440\n",
      "Epoch: 01, Loss: 918.0033\n",
      "Epoch: 02, Loss: 916.2831\n",
      "Epoch: 03, Loss: 912.9846\n",
      "Epoch: 04, Loss: 910.2757\n",
      "AP：  0.8349078405806947\n",
      "AUC SCORE:  0.8280367766360195\n",
      "epoch:  4 predict_auc_pr =  0.8349078405806947 predict_auc_roc =  0.8280367766360195\n",
      "######################### Module cycle ： 3 ##########################\n",
      "Epoch: 00, Loss: 956.6997\n",
      "Epoch: 01, Loss: 938.6493\n",
      "Epoch: 02, Loss: 936.4496\n",
      "Epoch: 03, Loss: 922.4053\n",
      "Epoch: 04, Loss: 907.3821\n",
      "AP：  0.8436026221196051\n",
      "AUC SCORE:  0.8522228231476475\n",
      "epoch:  4 predict_auc_pr =  0.8436026221196051 predict_auc_roc =  0.8522228231476475\n",
      "Epoch: 00, Loss: 854.7459\n",
      "Epoch: 01, Loss: 854.0479\n",
      "Epoch: 02, Loss: 852.7172\n",
      "Epoch: 03, Loss: 854.2775\n",
      "Epoch: 04, Loss: 855.8167\n",
      "AP：  0.837558548701331\n",
      "AUC SCORE:  0.8433964305029746\n",
      "epoch:  4 predict_auc_pr =  0.837558548701331 predict_auc_roc =  0.8433964305029746\n",
      "######################### Module cycle ： 4 ##########################\n",
      "Epoch: 00, Loss: 889.5280\n",
      "Epoch: 01, Loss: 891.3426\n",
      "Epoch: 02, Loss: 870.1128\n",
      "Epoch: 03, Loss: 868.8712\n",
      "Epoch: 04, Loss: 856.8573\n",
      "AP：  0.8506951137511444\n",
      "AUC SCORE:  0.8594483504597079\n",
      "epoch:  4 predict_auc_pr =  0.8506951137511444 predict_auc_roc =  0.8594483504597079\n",
      "Epoch: 00, Loss: 804.3845\n",
      "Epoch: 01, Loss: 812.2078\n",
      "Epoch: 02, Loss: 803.6366\n",
      "Epoch: 03, Loss: 807.0805\n",
      "Epoch: 04, Loss: 805.4657\n",
      "AP：  0.8350206309798389\n",
      "AUC SCORE:  0.8490427257977284\n",
      "epoch:  4 predict_auc_pr =  0.8350206309798389 predict_auc_roc =  0.8490427257977284\n",
      "######################### Module cycle ： 5 ##########################\n",
      "Epoch: 00, Loss: 855.5497\n",
      "Epoch: 01, Loss: 845.4389\n",
      "Epoch: 02, Loss: 840.0958\n",
      "Epoch: 03, Loss: 830.0309\n",
      "Epoch: 04, Loss: 830.4201\n",
      "AP：  0.8574296557365761\n",
      "AUC SCORE:  0.8681882098431586\n",
      "epoch:  4 predict_auc_pr =  0.8574296557365761 predict_auc_roc =  0.8681882098431586\n",
      "Epoch: 00, Loss: 782.9897\n",
      "Epoch: 01, Loss: 782.0625\n",
      "Epoch: 02, Loss: 787.1624\n",
      "Epoch: 03, Loss: 787.0420\n",
      "Epoch: 04, Loss: 785.8257\n",
      "AP：  0.8514880053168523\n",
      "AUC SCORE:  0.8656138453217955\n",
      "epoch:  4 predict_auc_pr =  0.8514880053168523 predict_auc_roc =  0.8656138453217955\n",
      "######################### Module cycle ： 6 ##########################\n",
      "Epoch: 00, Loss: 826.4463\n",
      "Epoch: 01, Loss: 816.8931\n",
      "Epoch: 02, Loss: 816.8145\n",
      "Epoch: 03, Loss: 803.6437\n",
      "Epoch: 04, Loss: 803.2483\n",
      "AP：  0.8536524353892975\n",
      "AUC SCORE:  0.8715630070308275\n",
      "epoch:  4 predict_auc_pr =  0.8536524353892975 predict_auc_roc =  0.8715630070308275\n",
      "Epoch: 00, Loss: 760.1309\n",
      "Epoch: 01, Loss: 758.7382\n",
      "Epoch: 02, Loss: 767.7238\n",
      "Epoch: 03, Loss: 763.4045\n",
      "Epoch: 04, Loss: 768.5735\n",
      "AP：  0.8527306046371572\n",
      "AUC SCORE:  0.8719740400216334\n",
      "epoch:  4 predict_auc_pr =  0.8527306046371572 predict_auc_roc =  0.8719740400216334\n",
      "######################### Module cycle ： 7 ##########################\n",
      "Epoch: 00, Loss: 802.3051\n",
      "Epoch: 01, Loss: 800.4572\n",
      "Epoch: 02, Loss: 798.8126\n",
      "Epoch: 03, Loss: 793.9003\n",
      "Epoch: 04, Loss: 788.3854\n",
      "AP：  0.8665317650659203\n",
      "AUC SCORE:  0.873704705246079\n",
      "epoch:  4 predict_auc_pr =  0.8665317650659203 predict_auc_roc =  0.873704705246079\n",
      "Epoch: 00, Loss: 755.8784\n",
      "Epoch: 01, Loss: 756.6309\n",
      "Epoch: 02, Loss: 755.0319\n",
      "Epoch: 03, Loss: 747.9572\n",
      "Epoch: 04, Loss: 756.0025\n",
      "AP：  0.8629328824982478\n",
      "AUC SCORE:  0.8793510005408328\n",
      "epoch:  4 predict_auc_pr =  0.8629328824982478 predict_auc_roc =  0.8793510005408328\n",
      "######################### Module cycle ： 8 ##########################\n",
      "Epoch: 00, Loss: 787.0867\n",
      "Epoch: 01, Loss: 783.8759\n",
      "Epoch: 02, Loss: 789.1239\n",
      "Epoch: 03, Loss: 779.2832\n",
      "Epoch: 04, Loss: 774.9893\n",
      "AP：  0.8493605637719226\n",
      "AUC SCORE:  0.8706976744186046\n",
      "epoch:  4 predict_auc_pr =  0.8493605637719226 predict_auc_roc =  0.8706976744186046\n",
      "Epoch: 00, Loss: 741.0520\n",
      "Epoch: 01, Loss: 739.4368\n",
      "Epoch: 02, Loss: 750.3746\n",
      "Epoch: 03, Loss: 747.3879\n",
      "Epoch: 04, Loss: 738.3015\n",
      "AP：  0.8523697769388925\n",
      "AUC SCORE:  0.8741373715521905\n",
      "epoch:  4 predict_auc_pr =  0.8523697769388925 predict_auc_roc =  0.8741373715521905\n",
      "######################### Module cycle ： 9 ##########################\n",
      "Epoch: 00, Loss: 773.8696\n",
      "Epoch: 01, Loss: 777.5717\n",
      "Epoch: 02, Loss: 774.8419\n",
      "Epoch: 03, Loss: 774.5686\n",
      "Epoch: 04, Loss: 776.9192\n",
      "AP：  0.8592842859563368\n",
      "AUC SCORE:  0.879134667387777\n",
      "epoch:  4 predict_auc_pr =  0.8592842859563368 predict_auc_roc =  0.879134667387777\n",
      "Epoch: 00, Loss: 734.8748\n",
      "Epoch: 01, Loss: 733.4838\n",
      "Epoch: 02, Loss: 735.5535\n",
      "Epoch: 03, Loss: 737.8907\n",
      "Epoch: 04, Loss: 732.5096\n",
      "AP：  0.8631297977723453\n",
      "AUC SCORE:  0.8819686316928069\n",
      "epoch:  4 predict_auc_pr =  0.8631297977723453 predict_auc_roc =  0.8819686316928069\n",
      "######################### Module cycle ： 10 ##########################\n",
      "Epoch: 00, Loss: 773.3361\n",
      "Epoch: 01, Loss: 769.4183\n",
      "Epoch: 02, Loss: 766.6500\n",
      "Epoch: 03, Loss: 763.2480\n",
      "Epoch: 04, Loss: 759.3337\n",
      "AP：  0.8634355734226623\n",
      "AUC SCORE:  0.8795673336938885\n",
      "epoch:  4 predict_auc_pr =  0.8634355734226623 predict_auc_roc =  0.8795673336938885\n",
      "Epoch: 00, Loss: 726.0334\n",
      "Epoch: 01, Loss: 722.8004\n",
      "Epoch: 02, Loss: 725.9857\n",
      "Epoch: 03, Loss: 728.8435\n",
      "Epoch: 04, Loss: 728.1867\n",
      "AP：  0.8644993005952897\n",
      "AUC SCORE:  0.8829421308815576\n",
      "epoch:  4 predict_auc_pr =  0.8644993005952897 predict_auc_roc =  0.8829421308815576\n",
      "######################### Module cycle ： 11 ##########################\n",
      "Epoch: 00, Loss: 756.8385\n",
      "Epoch: 01, Loss: 758.2875\n",
      "Epoch: 02, Loss: 754.0081\n",
      "Epoch: 03, Loss: 768.7369\n",
      "Epoch: 04, Loss: 756.2781\n",
      "AP：  0.8688603600608644\n",
      "AUC SCORE:  0.8843266630611141\n",
      "epoch:  4 predict_auc_pr =  0.8688603600608644 predict_auc_roc =  0.8843266630611141\n",
      "Epoch: 00, Loss: 720.8192\n",
      "Epoch: 01, Loss: 725.0372\n",
      "Epoch: 02, Loss: 726.5729\n",
      "Epoch: 03, Loss: 722.7947\n",
      "Epoch: 04, Loss: 720.9920\n",
      "AP：  0.8674120905545587\n",
      "AUC SCORE:  0.8870091941590049\n",
      "epoch:  4 predict_auc_pr =  0.8674120905545587 predict_auc_roc =  0.8870091941590049\n",
      "######################### Module cycle ： 12 ##########################\n",
      "Epoch: 00, Loss: 754.4114\n",
      "Epoch: 01, Loss: 751.0562\n",
      "Epoch: 02, Loss: 751.0581\n",
      "Epoch: 03, Loss: 749.9929\n",
      "Epoch: 04, Loss: 750.1624\n",
      "AP：  0.851129053151843\n",
      "AUC SCORE:  0.866349378042185\n",
      "epoch:  4 predict_auc_pr =  0.851129053151843 predict_auc_roc =  0.866349378042185\n",
      "Epoch: 00, Loss: 712.8595\n",
      "Epoch: 01, Loss: 712.7688\n",
      "Epoch: 02, Loss: 717.6413\n",
      "Epoch: 03, Loss: 718.7757\n",
      "Epoch: 04, Loss: 716.6597\n",
      "AP：  0.8543253079042923\n",
      "AUC SCORE:  0.8750027041644132\n",
      "epoch:  4 predict_auc_pr =  0.8543253079042923 predict_auc_roc =  0.8750027041644132\n",
      "######################### Module cycle ： 13 ##########################\n",
      "Epoch: 00, Loss: 751.9294\n",
      "Epoch: 01, Loss: 757.1154\n",
      "Epoch: 02, Loss: 750.1935\n",
      "Epoch: 03, Loss: 753.9799\n",
      "Epoch: 04, Loss: 746.8353\n",
      "AP：  0.8601448921381782\n",
      "AUC SCORE:  0.884175229853975\n",
      "epoch:  4 predict_auc_pr =  0.8601448921381782 predict_auc_roc =  0.884175229853975\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00, Loss: 718.0312\n",
      "Epoch: 01, Loss: 715.1085\n",
      "Epoch: 02, Loss: 717.2638\n",
      "Epoch: 03, Loss: 710.2417\n",
      "Epoch: 04, Loss: 718.5746\n",
      "AP：  0.8550399040504184\n",
      "AUC SCORE:  0.8785289345592211\n",
      "epoch:  4 predict_auc_pr =  0.8550399040504184 predict_auc_roc =  0.8785289345592211\n",
      "######################### Module cycle ： 14 ##########################\n",
      "Epoch: 00, Loss: 743.4529\n",
      "Epoch: 01, Loss: 749.3724\n",
      "Epoch: 02, Loss: 739.9807\n",
      "Epoch: 03, Loss: 745.4055\n",
      "Epoch: 04, Loss: 746.3704\n",
      "AP：  0.8497770278255323\n",
      "AUC SCORE:  0.8655273120605733\n",
      "epoch:  4 predict_auc_pr =  0.8497770278255323 predict_auc_roc =  0.8655273120605733\n",
      "Epoch: 00, Loss: 707.5552\n",
      "Epoch: 01, Loss: 721.0302\n",
      "Epoch: 02, Loss: 710.3438\n",
      "Epoch: 03, Loss: 713.3712\n",
      "Epoch: 04, Loss: 708.4431\n",
      "AP：  0.8573503952312405\n",
      "AUC SCORE:  0.8844997295835587\n",
      "epoch:  4 predict_auc_pr =  0.8573503952312405 predict_auc_roc =  0.8844997295835587\n",
      "######################### Module cycle ： 15 ##########################\n",
      "Epoch: 00, Loss: 748.1851\n",
      "Epoch: 01, Loss: 747.4840\n",
      "Epoch: 02, Loss: 741.8568\n",
      "Epoch: 03, Loss: 740.8607\n",
      "Epoch: 04, Loss: 733.9071\n",
      "AP：  0.8652485730745929\n",
      "AUC SCORE:  0.8867495943753381\n",
      "epoch:  4 predict_auc_pr =  0.8652485730745929 predict_auc_roc =  0.8867495943753381\n",
      "Epoch: 00, Loss: 715.1639\n",
      "Epoch: 01, Loss: 709.0378\n",
      "Epoch: 02, Loss: 705.7960\n",
      "Epoch: 03, Loss: 706.8572\n",
      "Epoch: 04, Loss: 705.8418\n",
      "AP：  0.8629148596168654\n",
      "AUC SCORE:  0.8852568956192536\n",
      "epoch:  4 predict_auc_pr =  0.8629148596168654 predict_auc_roc =  0.8852568956192536\n",
      "######################### Module cycle ： 16 ##########################\n",
      "Epoch: 00, Loss: 731.8549\n",
      "Epoch: 01, Loss: 740.6449\n",
      "Epoch: 02, Loss: 736.6513\n",
      "Epoch: 03, Loss: 735.9106\n",
      "Epoch: 04, Loss: 731.3813\n",
      "AP：  0.86027318786044\n",
      "AUC SCORE:  0.883569497025419\n",
      "epoch:  4 predict_auc_pr =  0.86027318786044 predict_auc_roc =  0.883569497025419\n",
      "Epoch: 00, Loss: 709.2885\n",
      "Epoch: 01, Loss: 710.7664\n",
      "Epoch: 02, Loss: 708.2581\n",
      "Epoch: 03, Loss: 701.5469\n",
      "Epoch: 04, Loss: 704.9252\n",
      "AP：  0.861069938171345\n",
      "AUC SCORE:  0.886749594375338\n",
      "epoch:  4 predict_auc_pr =  0.861069938171345 predict_auc_roc =  0.886749594375338\n",
      "######################### Module cycle ： 17 ##########################\n",
      "Epoch: 00, Loss: 736.8790\n",
      "Epoch: 01, Loss: 733.8652\n",
      "Epoch: 02, Loss: 731.5657\n",
      "Epoch: 03, Loss: 732.3333\n",
      "Epoch: 04, Loss: 730.0068\n",
      "AP：  0.8614349126841657\n",
      "AUC SCORE:  0.8851054624121146\n",
      "epoch:  4 predict_auc_pr =  0.8614349126841657 predict_auc_roc =  0.8851054624121146\n",
      "Epoch: 00, Loss: 699.5443\n",
      "Epoch: 01, Loss: 700.0275\n",
      "Epoch: 02, Loss: 698.6625\n",
      "Epoch: 03, Loss: 706.4586\n",
      "Epoch: 04, Loss: 701.0917\n",
      "AP：  0.8508696690595565\n",
      "AUC SCORE:  0.8667171444023796\n",
      "epoch:  4 predict_auc_pr =  0.8508696690595565 predict_auc_roc =  0.8667171444023796\n",
      "######################### Module cycle ： 18 ##########################\n",
      "Epoch: 00, Loss: 734.3748\n",
      "Epoch: 01, Loss: 725.2559\n",
      "Epoch: 02, Loss: 729.4294\n",
      "Epoch: 03, Loss: 731.3301\n",
      "Epoch: 04, Loss: 734.6698\n",
      "AP：  0.8594640828478728\n",
      "AUC SCORE:  0.8785505678745268\n",
      "epoch:  4 predict_auc_pr =  0.8594640828478728 predict_auc_roc =  0.8785505678745268\n",
      "Epoch: 00, Loss: 700.3254\n",
      "Epoch: 01, Loss: 707.0312\n",
      "Epoch: 02, Loss: 697.9778\n",
      "Epoch: 03, Loss: 698.0581\n",
      "Epoch: 04, Loss: 696.7080\n",
      "AP：  0.8606834752622894\n",
      "AUC SCORE:  0.8857328285559761\n",
      "epoch:  4 predict_auc_pr =  0.8606834752622894 predict_auc_roc =  0.8857328285559761\n",
      "######################### Module cycle ： 19 ##########################\n",
      "Epoch: 00, Loss: 726.8293\n",
      "Epoch: 01, Loss: 724.0176\n",
      "Epoch: 02, Loss: 724.3851\n",
      "Epoch: 03, Loss: 725.5072\n",
      "Epoch: 04, Loss: 726.3781\n",
      "AP：  0.8504113503373403\n",
      "AUC SCORE:  0.8762574364521363\n",
      "epoch:  4 predict_auc_pr =  0.8504113503373403 predict_auc_roc =  0.8762574364521363\n",
      "Epoch: 00, Loss: 703.8938\n",
      "Epoch: 01, Loss: 696.3992\n",
      "Epoch: 02, Loss: 703.2353\n",
      "Epoch: 03, Loss: 703.7484\n",
      "Epoch: 04, Loss: 704.3895\n",
      "AP：  0.8664178603823707\n",
      "AUC SCORE:  0.8836127636560303\n",
      "epoch:  4 predict_auc_pr =  0.8664178603823707 predict_auc_roc =  0.8836127636560303\n"
     ]
    }
   ],
   "source": [
    "filename = 'datasets/Celegans.txt'    # The node index starts at 0, sep=None\n",
    "nodes_number = 297\n",
    "\n",
    "embedding_dim = 8\n",
    "n_clusters = 24\n",
    "alpha = 1\n",
    "beta = 4.4\n",
    "DEC = 5.0\n",
    "LR = 0.4\n",
    "\n",
    "all_ap_first_ap = []\n",
    "all_ap_first_auc = []\n",
    "\n",
    "all_auc_first_ap = []\n",
    "all_auc_first_auc = []\n",
    "\n",
    "\n",
    "train_data_loader, test_data_loader = get_train_test_data(filename, nodes_number)\n",
    "\n",
    "nodes_emds = get_ini_emds(embedding_dim, nodes_number, n_clusters)\n",
    "raw_nodes_embedding, raw_cluster_centers = nodes_emds()\n",
    "    \n",
    "ap_first__ap = 0\n",
    "ap_first_auc = 0\n",
    "auc_first__ap = 0\n",
    "auc_first_auc = 0\n",
    "for module_epoch in range(20):\n",
    "    print(\"######################### Module cycle ： %d ##########################\"%module_epoch)\n",
    "    update_nodes_module = update_nodes_embedding(raw_nodes_embedding, raw_cluster_centers)\n",
    "    raw_nodes_embedding, nodes_ap, nodes_auc = update_nodes_module(train_data_loader, test_data_loader)\n",
    "    if nodes_ap > ap_first__ap:\n",
    "        ap_first__ap = nodes_ap\n",
    "        ap_first_auc = nodes_auc\n",
    "    if nodes_auc > auc_first_auc:\n",
    "        auc_first_auc = nodes_auc\n",
    "        auc_first__ap = nodes_ap\n",
    "        \n",
    "    update_CLUSTERS_module = update_cluster_embedding(raw_nodes_embedding, raw_cluster_centers)\n",
    "    raw_cluster_centers, cluster_ap, cluster_auc = update_CLUSTERS_module(train_data_loader, test_data_loader)\n",
    "    if cluster_ap > ap_first__ap:\n",
    "        ap_first__ap = cluster_ap\n",
    "        ap_first_auc = cluster_auc\n",
    "    if cluster_auc > auc_first_auc:\n",
    "        auc_first_auc = cluster_auc\n",
    "        auc_first__ap = cluster_ap\n",
    "all_ap_first_ap.append(ap_first__ap)\n",
    "all_ap_first_auc.append(ap_first_auc)\n",
    "all_auc_first_ap.append(auc_first__ap)\n",
    "all_auc_first_auc.append(auc_first_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4c38cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85932e6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-env",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
