{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c5a046d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-28T10:00:56.305404Z",
     "start_time": "2022-10-28T10:00:54.186403Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import os, sys\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=RuntimeWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "#sys.path.append(os.path.join(os.path.dirname(os.path.realpath(__file__)), os.pardir))\n",
    "# For replicating the experiments\n",
    "SEED = 42\n",
    "import argparse\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from model import LinTrans, LogReg\n",
    "from optimizer import loss_function\n",
    "from utils import *\n",
    "from sklearn.cluster import SpectralClustering, KMeans\n",
    "from clustering_metric import clustering_metrics\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import normalize, MinMaxScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, f1_score, average_precision_score, roc_auc_score\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07edf20b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-28T10:00:56.352591Z",
     "start_time": "2022-10-28T10:00:56.338594Z"
    }
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--gnnlayers', type=int, default=1, help=\"Number of gnn layers\")\n",
    "parser.add_argument('--linlayers', type=int, default=1, help=\"Number of hidden layers\")\n",
    "parser.add_argument('--epochs', type=int, default=400, help='Number of epochs to train.')\n",
    "parser.add_argument('--dims', type=int, default=[500], help='Number of units in hidden layer 1.')\n",
    "parser.add_argument('--lr', type=float, default=0.001, help='Initial learning rate.')\n",
    "parser.add_argument('--upth_st', type=float, default=0.0011, help='Upper Threshold start.')\n",
    "parser.add_argument('--lowth_st', type=float, default=0.1, help='Lower Threshold start.')\n",
    "parser.add_argument('--upth_ed', type=float, default=0.001, help='Upper Threshold end.')\n",
    "parser.add_argument('--lowth_ed', type=float, default=0.5, help='Lower Threshold end.')\n",
    "parser.add_argument('--upd', type=int, default=10, help='Update epoch.')\n",
    "parser.add_argument('--bs', type=int, default=100, help='Batchsize.')\n",
    "parser.add_argument('--dataset', type=str, default='polbooks', help='type of dataset.')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='Disables CUDA training.')\n",
    "args,_ = parser.parse_known_args()\n",
    "args.cuda = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70b9a62c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-28T10:00:56.399688Z",
     "start_time": "2022-10-28T10:00:56.385798Z"
    }
   },
   "outputs": [],
   "source": [
    "def update_similarity(z, upper_threshold, lower_treshold, pos_num, neg_num):\n",
    "    f_adj = np.matmul(z, np.transpose(z))\n",
    "    cosine = f_adj\n",
    "    cosine = cosine.reshape([-1,])\n",
    "    pos_num = round(upper_threshold * len(cosine))\n",
    "    neg_num = round((1-lower_treshold) * len(cosine))\n",
    "    \n",
    "    pos_inds = np.argpartition(-cosine, pos_num)[:pos_num]\n",
    "    neg_inds = np.argpartition(cosine, neg_num)[:neg_num]\n",
    "    \n",
    "    return np.array(pos_inds), np.array(neg_inds)\n",
    "\n",
    "def update_threshold(upper_threshold, lower_treshold, up_eta, low_eta):\n",
    "    upth = upper_threshold + up_eta\n",
    "    lowth = lower_treshold + low_eta\n",
    "    return upth, lowth\n",
    "\n",
    "def load_network_data(adj_name, nodes_numbers):\n",
    "    raw_edges = pd.read_csv(\"data/\"+adj_name+\".txt\",header=None,sep=' ')\n",
    "    drop_self_loop = raw_edges[raw_edges[0]!=raw_edges[1]]\n",
    "    graph_np = np.zeros((nodes_numbers, nodes_numbers))\n",
    "    for i in range(drop_self_loop.shape[0]):\n",
    "        graph_np[drop_self_loop.iloc[i,0], drop_self_loop.iloc[i,1]]=1\n",
    "        graph_np[drop_self_loop.iloc[i,1], drop_self_loop.iloc[i,0]]=1\n",
    "    adj = nx.adjacency_matrix(nx.from_numpy_matrix(graph_np))\n",
    "    features = np.eye(nodes_numbers)\n",
    "    return adj, features\n",
    "\n",
    "def get_scores(emb, adj_orig, edges_pos, edges_neg):\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    # Predict on test set of edges\n",
    "    adj_rec = np.dot(emb, emb.T)\n",
    "    preds = []\n",
    "    pos = []\n",
    "    for e in edges_pos:\n",
    "        preds.append(sigmoid(adj_rec[e[0], e[1]]))\n",
    "        pos.append(adj_orig[e[0], e[1]])\n",
    "\n",
    "    preds_neg = []\n",
    "    neg = []\n",
    "    for e in edges_neg:\n",
    "        preds_neg.append(sigmoid(adj_rec[e[0], e[1]]))\n",
    "        neg.append(adj_orig[e[0], e[1]])\n",
    "\n",
    "    preds_all = np.hstack([preds, preds_neg])\n",
    "    labels_all = np.hstack([np.ones(len(preds)), np.zeros(len(preds))])\n",
    "    \n",
    "    roc_score = roc_auc_score(labels_all, preds_all)\n",
    "    ap_score = average_precision_score(labels_all, preds_all)\n",
    "\n",
    "    return roc_score, ap_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "335d3230",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-28T10:00:56.461254Z",
     "start_time": "2022-10-28T10:00:56.431933Z"
    }
   },
   "outputs": [],
   "source": [
    "def gae_for(args):\n",
    "    print(\"Using {} dataset\".format(args.dataset))\n",
    "    \n",
    "    if args.dataset == 'cora':\n",
    "        nodes_number = 2708    # 这里需要输入网络的节点数量\n",
    "        n_clusters = 7     # 指定类簇数量\n",
    "    elif args.dataset == 'citeseer':\n",
    "        nodes_number = 3327    # 这里需要输入网络的节点数量\n",
    "        n_clusters = 6     # 指定类簇数量\n",
    "    elif args.dataset == 'wiki':\n",
    "        nodes_number = 2405    # 这里需要输入网络的节点数量\n",
    "        n_clusters = 17     # 指定类簇数量\n",
    "    elif args.dataset == 'celegans':\n",
    "        nodes_number = 297    # 这里需要输入网络的节点数量\n",
    "        n_clusters = 10     # 指定类簇数量\n",
    "    elif args.dataset == 'email':\n",
    "        nodes_number = 986    # 这里需要输入网络的节点数量\n",
    "        n_clusters = 10     # 指定类簇数量\n",
    "    elif args.dataset == 'polbooks':\n",
    "        nodes_number = 105    # 这里需要输入网络的节点数量\n",
    "        n_clusters = 10     # 指定类簇数量\n",
    "    elif args.dataset == 'texas':\n",
    "        nodes_number = 183    # 这里需要输入网络的节点数量\n",
    "        n_clusters = 10     # 指定类簇数量\n",
    "    elif args.dataset == 'wisconsin':\n",
    "        nodes_number = 215    # 这里需要输入网络的节点数量\n",
    "        n_clusters = 10     # 指定类簇数量\n",
    "        \n",
    "    Cluster = SpectralClustering(n_clusters=n_clusters, affinity = 'precomputed', random_state=0)\n",
    "    adj, features = load_network_data(args.dataset, nodes_number)\n",
    "\n",
    "    n_nodes, feat_dim = features.shape\n",
    "    dims = [feat_dim] + args.dims\n",
    "    \n",
    "    layers = args.linlayers\n",
    "    # Store original adjacency matrix (without diagonal entries) for later\n",
    "    \n",
    "    adj = adj - sp.dia_matrix((adj.diagonal()[np.newaxis, :], [0]), shape=adj.shape)\n",
    "    adj.eliminate_zeros()\n",
    "    adj_orig = adj\n",
    "\n",
    "    #adj_train, train_edges, val_edges, val_edges_false, test_edges, test_edges_false = mask_test_edges(adj)    # val:0.05  test:0.1\n",
    "    adj_train, train_edges, test_edges, test_edges_false = mask_test_edges(adj)   \n",
    "    adj = adj_train\n",
    "    n = adj.shape[0]\n",
    "\n",
    "    adj_norm_s = preprocess_graph(adj, args.gnnlayers, norm='sym', renorm=True)\n",
    "    sm_fea_s = sp.csr_matrix(features).toarray()\n",
    "    \n",
    "    print('Laplacian Smoothing...')\n",
    "    for a in adj_norm_s:\n",
    "        sm_fea_s = a.dot(sm_fea_s)\n",
    "    adj_1st = (adj + sp.eye(n)).toarray()\n",
    "\n",
    "    adj_label = torch.FloatTensor(adj_1st)\n",
    "    \n",
    "    model = LinTrans(layers, dims)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "    \n",
    "    sm_fea_s = torch.FloatTensor(sm_fea_s)\n",
    "    adj_label = adj_label.reshape([-1,])\n",
    "\n",
    "    inx = sm_fea_s\n",
    "    \n",
    "    pos_num = len(adj.indices)\n",
    "    neg_num = n_nodes*n_nodes-pos_num\n",
    "\n",
    "    up_eta = (args.upth_ed - args.upth_st) / (args.epochs/args.upd)\n",
    "    low_eta = (args.lowth_ed - args.lowth_st) / (args.epochs/args.upd)\n",
    "\n",
    "    pos_inds, neg_inds = update_similarity(normalize(sm_fea_s.numpy()), args.upth_st, args.lowth_st, pos_num, neg_num)\n",
    "    upth, lowth = update_threshold(args.upth_st, args.lowth_st, up_eta, low_eta)\n",
    "\n",
    "    bs = min(args.bs, len(pos_inds))\n",
    "    length = len(pos_inds)\n",
    "    \n",
    "    pos_inds_cuda = torch.LongTensor(pos_inds)\n",
    "    best_lp = 0.\n",
    "    print('Start Training...')\n",
    "    for epoch in range(args.epochs):\n",
    "        st, ed = 0, bs\n",
    "        batch_num = 0\n",
    "        model.train()\n",
    "        length = len(pos_inds)\n",
    "        \n",
    "        while ( ed <= length ):\n",
    "            sampled_neg = torch.LongTensor(np.random.choice(neg_inds, size=ed-st))\n",
    "            sampled_inds = torch.cat((pos_inds_cuda[st:ed], sampled_neg), 0)\n",
    "            t = time.time()\n",
    "            optimizer.zero_grad()\n",
    "            xind = sampled_inds // n_nodes\n",
    "            yind = sampled_inds % n_nodes\n",
    "            x = torch.index_select(inx, 0, xind)\n",
    "            y = torch.index_select(inx, 0, yind)\n",
    "            zx = model(x)\n",
    "            zy = model(y)\n",
    "            batch_label = torch.cat((torch.ones(ed-st), torch.zeros(ed-st)))\n",
    "            batch_pred = model.dcs(zx, zy)\n",
    "            loss = loss_function(adj_preds=batch_pred, adj_labels=batch_label, n_nodes=ed-st)\n",
    "            \n",
    "            loss.backward()\n",
    "            cur_loss = loss.item()\n",
    "            optimizer.step()\n",
    "            \n",
    "            st = ed\n",
    "            batch_num += 1\n",
    "            if ed < length and ed + bs >= length:\n",
    "                ed += length - ed\n",
    "            else:\n",
    "                ed += bs\n",
    "\n",
    "        if (epoch + 1) % args.upd == 0:\n",
    "            model.eval()\n",
    "            mu = model(inx)\n",
    "            hidden_emb = mu.cpu().data.numpy()\n",
    "            upth, lowth = update_threshold(upth, lowth, up_eta, low_eta)\n",
    "            pos_inds, neg_inds = update_similarity(hidden_emb, upth, lowth, pos_num, neg_num)\n",
    "            bs = min(args.bs, len(pos_inds))\n",
    "            pos_inds_cuda = torch.LongTensor(pos_inds)\n",
    "            \n",
    "            #val_auc, val_ap, val_acc, val_f1 = get_scores(hidden_emb, adj_orig, val_edges, val_edges_false)\n",
    "            #if val_auc + val_ap >= best_lp:\n",
    "                #best_lp = val_auc + val_ap\n",
    "                #best_emb = hidden_emb\n",
    "            #print(\"Epoch: {}, train_loss_gae={:.5f}, val_ap={:.5f}, val_acc={:.5f},time={:.5f}\".format(epoch + 1, cur_loss, val_ap, val_acc, time.time() - t))\n",
    "    \n",
    "    print(\"Optimization Finished!\")\n",
    "    #auc_score, ap_score, acc_score, f1_score = get_scores(best_emb, adj_orig, test_edges, test_edges_false)\n",
    "    auc_score, ap_score = get_scores(hidden_emb, adj_orig, test_edges, test_edges_false)\n",
    "    print('Test AP score: ',ap_score)\n",
    "    print('Test AUC score: ',auc_score)\n",
    "    return auc_score, ap_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f4ac9f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-28T10:01:07.022358Z",
     "start_time": "2022-10-28T10:00:56.691202Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using polbooks dataset\n",
      "Laplacian Smoothing...\n",
      "Start Training...\n",
      "Optimization Finished!\n",
      "Test AP score:  0.87041669233205\n",
      "Test AUC score:  0.8873966942148761\n",
      "Using polbooks dataset\n",
      "Laplacian Smoothing...\n",
      "Start Training...\n",
      "Optimization Finished!\n",
      "Test AP score:  0.8245656277839583\n",
      "Test AUC score:  0.8465909090909092\n",
      "Using polbooks dataset\n",
      "Laplacian Smoothing...\n",
      "Start Training...\n",
      "Optimization Finished!\n",
      "Test AP score:  0.9221858957396772\n",
      "Test AUC score:  0.9034090909090909\n",
      "Using polbooks dataset\n",
      "Laplacian Smoothing...\n",
      "Start Training...\n",
      "Optimization Finished!\n",
      "Test AP score:  0.8679877995776042\n",
      "Test AUC score:  0.8657024793388429\n",
      "Using polbooks dataset\n",
      "Laplacian Smoothing...\n",
      "Start Training...\n",
      "Optimization Finished!\n",
      "Test AP score:  0.9058008525407801\n",
      "Test AUC score:  0.8884297520661157\n",
      "Using polbooks dataset\n",
      "Laplacian Smoothing...\n",
      "Start Training...\n",
      "Optimization Finished!\n",
      "Test AP score:  0.8534911177346045\n",
      "Test AUC score:  0.8698347107438016\n",
      "Using polbooks dataset\n",
      "Laplacian Smoothing...\n",
      "Start Training...\n",
      "Optimization Finished!\n",
      "Test AP score:  0.824817266418036\n",
      "Test AUC score:  0.8254132231404958\n",
      "Using polbooks dataset\n",
      "Laplacian Smoothing...\n",
      "Start Training...\n",
      "Optimization Finished!\n",
      "Test AP score:  0.8244061303592151\n",
      "Test AUC score:  0.8414256198347108\n",
      "Using polbooks dataset\n",
      "Laplacian Smoothing...\n",
      "Start Training...\n",
      "Optimization Finished!\n",
      "Test AP score:  0.83907525270286\n",
      "Test AUC score:  0.8662190082644629\n",
      "Using polbooks dataset\n",
      "Laplacian Smoothing...\n",
      "Start Training...\n",
      "Optimization Finished!\n",
      "Test AP score:  0.8602938847263557\n",
      "Test AUC score:  0.8806818181818181\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    all_auc = []\n",
    "    all_ap = []\n",
    "    for i in range(10):\n",
    "        auc, ap = gae_for(args)\n",
    "        all_auc.append(auc)\n",
    "        all_ap.append(ap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b084b2b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-28T10:01:07.068394Z",
     "start_time": "2022-10-28T10:01:07.055353Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AP MEAN :  0.859304051991514\n",
      "AP STD :  0.032207006545178454\n",
      "AUC MEAN :  0.8675103305785123\n",
      "AUC STD :  0.022794381106312316\n"
     ]
    }
   ],
   "source": [
    "print(\"AP MEAN : \", np.array(all_ap).mean())\n",
    "print(\"AP STD : \", np.array(all_ap).std())\n",
    "\n",
    "print(\"AUC MEAN : \", np.array(all_auc).mean())\n",
    "print(\"AUC STD : \", np.array(all_auc).std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d642e543",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-env",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
