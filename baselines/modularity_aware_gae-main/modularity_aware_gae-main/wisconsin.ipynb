{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67dbcca0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-28T09:18:27.315291Z",
     "start_time": "2022-10-28T09:18:25.161083Z"
    }
   },
   "outputs": [],
   "source": [
    "from modularity_aware_gae.evaluation import community_detection, link_prediction\n",
    "from modularity_aware_gae.input_data import load_data, load_labels\n",
    "from modularity_aware_gae.louvain import louvain_clustering\n",
    "from modularity_aware_gae.model import  GCNModelAE, GCNModelVAE, LinearModelAE, LinearModelVAE\n",
    "from modularity_aware_gae.optimizer import OptimizerAE, OptimizerVAE\n",
    "from modularity_aware_gae.preprocessing import *\n",
    "from modularity_aware_gae.sampling import get_distribution, node_sampling\n",
    "import numpy as np\n",
    "import os\n",
    "import scipy.sparse as sp\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel')#添加的，不报错\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14ffd620",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-28T09:18:27.361309Z",
     "start_time": "2022-10-28T09:18:27.348267Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " \n",
      " \n",
      "[MODULARITY-AWARE GRAPH AUTOENCODERS]\n",
      " \n",
      " \n",
      " \n",
      "\n",
      "EXPERIMENTAL SETTING \n",
      "\n",
      "- Graph dataset: wisconsin\n",
      "- Mode name: linear_vae\n",
      "- Number of models to train: 10\n",
      "- Number of training iterations for each model: 200\n",
      "- Learning rate: 0.01\n",
      "- Dropout rate: 0.0\n",
      "- Use of node features in the input layer: False\n",
      "- Dimension of the output layer: 16\n",
      "- lambda: 0.0\n",
      "- beta: 0.0\n",
      "- gamma: 1.0\n",
      "- s: 2\n",
      "- FastGAE: no \n",
      "\n",
      "Final embedding vectors will be evaluated on:\n",
      "- Task 2, i.e., joint community detection and link prediction\n",
      "\n",
      " \n",
      " \n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "# Parameters related to the input data\n",
    "flags.DEFINE_string('dataset', 'wisconsin', 'Graph dataset, among: cora, citeseer, wiki, celegans, email, polbooks, texas, wisconsin')\n",
    "\n",
    "flags.DEFINE_boolean('features', False, 'Whether to include node features')\n",
    "\n",
    "\n",
    "# Parameters related to the Modularity-Aware GAE/VGAE model to train\n",
    "\n",
    "# 1/3 - General parameters associated with GAE/VGAE\n",
    "flags.DEFINE_string('model', 'linear_vae', 'Model to train, among: gcn_ae, gcn_vae, \\\n",
    "                                            linear_ae, linear_vae')\n",
    "flags.DEFINE_float('dropout', 0., 'Dropout rate')\n",
    "flags.DEFINE_integer('iterations', 200, 'Number of iterations in training')\n",
    "flags.DEFINE_float('learning_rate', 0.01, 'Initial learning rate (Adam)')\n",
    "flags.DEFINE_integer('hidden', 32, 'Dimension of the GCN hidden layer')\n",
    "flags.DEFINE_integer('dimension', 16, 'Dimension of the output layer, i.e., \\\n",
    "                                       dimension of the embedding space')\n",
    "\n",
    "# 2/3 - Additional parameters, specific to Modularity-Aware models\n",
    "flags.DEFINE_float('beta', 0.0, 'Beta hyperparameter of Mod.-Aware models')\n",
    "flags.DEFINE_float('lamb', 0.0, 'Lambda hyperparameter of Mod.-Aware models')\n",
    "flags.DEFINE_float('gamma', 1.0, 'Gamma hyperparameter of Mod.-Aware models')\n",
    "flags.DEFINE_integer('s_reg', 2, 's hyperparameter of Mod.-Aware models')\n",
    "\n",
    "# 3/3 - Additional parameters, aiming to improve scalability\n",
    "flags.DEFINE_boolean('fastgae', False, 'Whether to use the FastGAE framework')\n",
    "flags.DEFINE_integer('nb_node_samples', 1000, 'In FastGAE, number of nodes to \\\n",
    "                                               sample at each iteration, i.e., \\\n",
    "                                               size of decoded subgraph')\n",
    "flags.DEFINE_string('measure', 'degree', 'In FastGAE, node importance measure used \\\n",
    "                                          for sampling: degree, core, or uniform')\n",
    "flags.DEFINE_float('alpha', 1.0, 'alpha hyperparameter associated with the FastGAE sampling')\n",
    "flags.DEFINE_boolean('replace', False, 'Sample nodes with or without replacement')\n",
    "flags.DEFINE_boolean('simple', True, 'Use simpler (and faster) modularity in optimizers')\n",
    "\n",
    "\n",
    "# Parameters related to the experimental setup\n",
    "flags.DEFINE_string('task', 'task_2', 'task_1: pure community detection \\\n",
    "                                       task_2: joint link prediction and \\\n",
    "                                               community detection')\n",
    "flags.DEFINE_integer('nb_run', 10, 'Number of model run + test')\n",
    "flags.DEFINE_float('prop_val', 1., 'Proportion of edges in validation set \\\n",
    "                                    for the link prediction task')\n",
    "flags.DEFINE_float('prop_test', 10., 'Proportion of edges in test set \\\n",
    "                                      for the link prediction task')\n",
    "flags.DEFINE_boolean('validation', False, 'Whether to compute validation \\\n",
    "                                           results at each iteration, for \\\n",
    "                                           the link prediction task')\n",
    "flags.DEFINE_boolean('verbose', True, 'Whether to print all comments details')\n",
    "\n",
    "\n",
    "# Introductory message\n",
    "if FLAGS.verbose:\n",
    "    introductory_message()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ce16ae4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-28T09:18:27.407779Z",
     "start_time": "2022-10-28T09:18:27.393275Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADING DATA\n",
      "\n",
      "Loading the wisconsin graph\n",
      "- Number of nodes: 251\n",
      "- Use of node features: False\n",
      "Done! \n",
      " \n",
      " \n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize lists to collect final results\n",
    "mean_ami = []\n",
    "mean_ari = []\n",
    "\n",
    "mean_roc = []\n",
    "mean_ap = []\n",
    "\n",
    "# Load data\n",
    "if FLAGS.verbose:\n",
    "    print(\"LOADING DATA\\n\")\n",
    "    print(\"Loading the\", FLAGS.dataset, \"graph\")\n",
    "adj_init, features_init = load_data(FLAGS.dataset)\n",
    "#labels = load_labels(adj_init.shape[0])\n",
    "if FLAGS.verbose:\n",
    "    print(\"- Number of nodes:\", adj_init.shape[0])\n",
    "    #print(\"- Number of communities:\", len(np.unique(labels)))\n",
    "    print(\"- Use of node features:\", FLAGS.features)\n",
    "    print(\"Done! \\n \\n \\n \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd19b933",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-28T09:18:42.395366Z",
     "start_time": "2022-10-28T09:18:27.440814Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENTS ON MODEL 1 / 10 \n",
      "\n",
      "STEP 1/3 - PREPROCESSING STEPS \n",
      "\n",
      "Masking some edges from the training graph, for link prediction\n",
      "(validation set: 1.0 % of edges - test set: 10.0 % of edges)\n",
      "Done! \n",
      "\n",
      "Preprocessing node features\n",
      "Done! \n",
      "\n",
      "Running the Louvain algorithm for community detection\n",
      "as a preprocessing step for the encoder\n",
      "Done! Louvain has found 21 communities \n",
      "\n",
      "Setting up the model and the optimizer\n",
      "Done! \n",
      "\n",
      "Preprocessing on message passing matrices\n",
      "Done! \n",
      "\n",
      "Initializing TF session\n",
      "Done! \n",
      "\n",
      "STEP 2/3 - MODEL TRAINING \n",
      "\n",
      "Starting training\n",
      "Iteration: 0001 Loss: 1.74619 Time: 0.11804\n",
      "Iteration: 0002 Loss: 1.63763 Time: 0.00296\n",
      "Iteration: 0003 Loss: 1.53932 Time: 0.00304\n",
      "Iteration: 0004 Loss: 1.57755 Time: 0.00401\n",
      "Iteration: 0005 Loss: 1.49109 Time: 0.00399\n",
      "Iteration: 0006 Loss: 1.61124 Time: 0.00396\n",
      "Iteration: 0007 Loss: 1.55641 Time: 0.00306\n",
      "Iteration: 0008 Loss: 1.54304 Time: 0.00399\n",
      "Iteration: 0009 Loss: 1.52376 Time: 0.00400\n",
      "Iteration: 0010 Loss: 1.47705 Time: 0.00407\n",
      "Iteration: 0011 Loss: 1.54970 Time: 0.00200\n",
      "Iteration: 0012 Loss: 1.51715 Time: 0.00404\n",
      "Iteration: 0013 Loss: 1.41572 Time: 0.00396\n",
      "Iteration: 0014 Loss: 1.50250 Time: 0.00400\n",
      "Iteration: 0015 Loss: 1.42814 Time: 0.00301\n",
      "Iteration: 0016 Loss: 1.53370 Time: 0.00294\n",
      "Iteration: 0017 Loss: 1.44866 Time: 0.00400\n",
      "Iteration: 0018 Loss: 1.51301 Time: 0.00396\n",
      "Iteration: 0019 Loss: 1.38764 Time: 0.00404\n",
      "Iteration: 0020 Loss: 1.48282 Time: 0.00401\n",
      "Iteration: 0021 Loss: 1.41136 Time: 0.00298\n",
      "Iteration: 0022 Loss: 1.38485 Time: 0.00312\n",
      "Iteration: 0023 Loss: 1.32196 Time: 0.00401\n",
      "Iteration: 0024 Loss: 1.32193 Time: 0.00299\n",
      "Iteration: 0025 Loss: 1.29782 Time: 0.00297\n",
      "Iteration: 0026 Loss: 1.39774 Time: 0.00351\n",
      "Iteration: 0027 Loss: 1.25025 Time: 0.00252\n",
      "Iteration: 0028 Loss: 1.27030 Time: 0.00304\n",
      "Iteration: 0029 Loss: 1.34554 Time: 0.00303\n",
      "Iteration: 0030 Loss: 1.30224 Time: 0.00393\n",
      "Iteration: 0031 Loss: 1.29718 Time: 0.00309\n",
      "Iteration: 0032 Loss: 1.24248 Time: 0.00397\n",
      "Iteration: 0033 Loss: 1.24287 Time: 0.00399\n",
      "Iteration: 0034 Loss: 1.21131 Time: 0.00404\n",
      "Iteration: 0035 Loss: 1.26132 Time: 0.00496\n",
      "Iteration: 0036 Loss: 1.16570 Time: 0.00306\n",
      "Iteration: 0037 Loss: 1.26689 Time: 0.00421\n",
      "Iteration: 0038 Loss: 1.12679 Time: 0.00374\n",
      "Iteration: 0039 Loss: 1.23914 Time: 0.00310\n",
      "Iteration: 0040 Loss: 1.20766 Time: 0.00394\n",
      "Iteration: 0041 Loss: 1.20469 Time: 0.00303\n",
      "Iteration: 0042 Loss: 1.17506 Time: 0.00297\n",
      "Iteration: 0043 Loss: 1.09320 Time: 0.00300\n",
      "Iteration: 0044 Loss: 1.18863 Time: 0.00297\n",
      "Iteration: 0045 Loss: 1.15146 Time: 0.00403\n",
      "Iteration: 0046 Loss: 1.13541 Time: 0.00298\n",
      "Iteration: 0047 Loss: 1.19125 Time: 0.00398\n",
      "Iteration: 0048 Loss: 1.14752 Time: 0.00302\n",
      "Iteration: 0049 Loss: 1.12223 Time: 0.00301\n",
      "Iteration: 0050 Loss: 1.12283 Time: 0.00401\n",
      "Iteration: 0051 Loss: 1.13422 Time: 0.00402\n",
      "Iteration: 0052 Loss: 1.10023 Time: 0.00493\n",
      "Iteration: 0053 Loss: 1.12338 Time: 0.00310\n",
      "Iteration: 0054 Loss: 1.07602 Time: 0.00306\n",
      "Iteration: 0055 Loss: 1.12241 Time: 0.00311\n",
      "Iteration: 0056 Loss: 1.04570 Time: 0.00301\n",
      "Iteration: 0057 Loss: 1.03520 Time: 0.00304\n",
      "Iteration: 0058 Loss: 1.09175 Time: 0.00393\n",
      "Iteration: 0059 Loss: 1.05543 Time: 0.00303\n",
      "Iteration: 0060 Loss: 1.04488 Time: 0.00300\n",
      "Iteration: 0061 Loss: 1.03024 Time: 0.00496\n",
      "Iteration: 0062 Loss: 1.06785 Time: 0.00204\n",
      "Iteration: 0063 Loss: 1.07251 Time: 0.00306\n",
      "Iteration: 0064 Loss: 1.03958 Time: 0.00400\n",
      "Iteration: 0065 Loss: 1.04373 Time: 0.00200\n",
      "Iteration: 0066 Loss: 1.01551 Time: 0.00304\n",
      "Iteration: 0067 Loss: 1.08620 Time: 0.00403\n",
      "Iteration: 0068 Loss: 1.02609 Time: 0.00204\n",
      "Iteration: 0069 Loss: 0.99815 Time: 0.00300\n",
      "Iteration: 0070 Loss: 1.02648 Time: 0.00301\n",
      "Iteration: 0071 Loss: 0.98863 Time: 0.00201\n",
      "Iteration: 0072 Loss: 0.99643 Time: 0.00403\n",
      "Iteration: 0073 Loss: 1.03679 Time: 0.00306\n",
      "Iteration: 0074 Loss: 1.01959 Time: 0.00393\n",
      "Iteration: 0075 Loss: 0.99738 Time: 0.00398\n",
      "Iteration: 0076 Loss: 1.02466 Time: 0.00307\n",
      "Iteration: 0077 Loss: 1.00654 Time: 0.00313\n",
      "Iteration: 0078 Loss: 0.95028 Time: 0.00313\n",
      "Iteration: 0079 Loss: 0.95063 Time: 0.00434\n",
      "Iteration: 0080 Loss: 0.98814 Time: 0.00293\n",
      "Iteration: 0081 Loss: 0.98706 Time: 0.00301\n",
      "Iteration: 0082 Loss: 0.94637 Time: 0.00295\n",
      "Iteration: 0083 Loss: 0.92978 Time: 0.00404\n",
      "Iteration: 0084 Loss: 0.96667 Time: 0.00400\n",
      "Iteration: 0085 Loss: 0.98348 Time: 0.00300\n",
      "Iteration: 0086 Loss: 0.95701 Time: 0.00400\n",
      "Iteration: 0087 Loss: 0.93782 Time: 0.00302\n",
      "Iteration: 0088 Loss: 0.95179 Time: 0.00298\n",
      "Iteration: 0089 Loss: 0.94087 Time: 0.00293\n",
      "Iteration: 0090 Loss: 0.97466 Time: 0.00408\n",
      "Iteration: 0091 Loss: 0.92005 Time: 0.00292\n",
      "Iteration: 0092 Loss: 0.90264 Time: 0.00300\n",
      "Iteration: 0093 Loss: 0.91223 Time: 0.00403\n",
      "Iteration: 0094 Loss: 0.90989 Time: 0.00394\n",
      "Iteration: 0095 Loss: 0.91690 Time: 0.00400\n",
      "Iteration: 0096 Loss: 0.90662 Time: 0.00400\n",
      "Iteration: 0097 Loss: 0.91862 Time: 0.00402\n",
      "Iteration: 0098 Loss: 0.91005 Time: 0.00300\n",
      "Iteration: 0099 Loss: 0.85471 Time: 0.00506\n",
      "Iteration: 0100 Loss: 0.90365 Time: 0.00394\n",
      "Iteration: 0101 Loss: 0.89075 Time: 0.00301\n",
      "Iteration: 0102 Loss: 0.90193 Time: 0.00301\n",
      "Iteration: 0103 Loss: 0.89122 Time: 0.00199\n",
      "Iteration: 0104 Loss: 0.90763 Time: 0.00300\n",
      "Iteration: 0105 Loss: 0.88339 Time: 0.00304\n",
      "Iteration: 0106 Loss: 0.87516 Time: 0.00296\n",
      "Iteration: 0107 Loss: 0.89339 Time: 0.00300\n",
      "Iteration: 0108 Loss: 0.87094 Time: 0.00294\n",
      "Iteration: 0109 Loss: 0.83918 Time: 0.00300\n",
      "Iteration: 0110 Loss: 0.89331 Time: 0.00301\n",
      "Iteration: 0111 Loss: 0.85685 Time: 0.00399\n",
      "Iteration: 0112 Loss: 0.84854 Time: 0.00300\n",
      "Iteration: 0113 Loss: 0.83994 Time: 0.00300\n",
      "Iteration: 0114 Loss: 0.81884 Time: 0.00303\n",
      "Iteration: 0115 Loss: 0.84493 Time: 0.00392\n",
      "Iteration: 0116 Loss: 0.83147 Time: 0.00303\n",
      "Iteration: 0117 Loss: 0.82988 Time: 0.00301\n",
      "Iteration: 0118 Loss: 0.83653 Time: 0.00465\n",
      "Iteration: 0119 Loss: 0.83773 Time: 0.00300\n",
      "Iteration: 0120 Loss: 0.81041 Time: 0.00297\n",
      "Iteration: 0121 Loss: 0.84126 Time: 0.00332\n",
      "Iteration: 0122 Loss: 0.83040 Time: 0.00299\n",
      "Iteration: 0123 Loss: 0.82297 Time: 0.00300\n",
      "Iteration: 0124 Loss: 0.80349 Time: 0.00400\n",
      "Iteration: 0125 Loss: 0.81986 Time: 0.00299\n",
      "Iteration: 0126 Loss: 0.80542 Time: 0.00500\n",
      "Iteration: 0127 Loss: 0.81479 Time: 0.00400\n",
      "Iteration: 0128 Loss: 0.81953 Time: 0.00300\n",
      "Iteration: 0129 Loss: 0.77640 Time: 0.00300\n",
      "Iteration: 0130 Loss: 0.82539 Time: 0.00200\n",
      "Iteration: 0131 Loss: 0.79803 Time: 0.00300\n",
      "Iteration: 0132 Loss: 0.78102 Time: 0.00305\n",
      "Iteration: 0133 Loss: 0.81651 Time: 0.00301\n",
      "Iteration: 0134 Loss: 0.77739 Time: 0.00209\n",
      "Iteration: 0135 Loss: 0.77844 Time: 0.00291\n",
      "Iteration: 0136 Loss: 0.78874 Time: 0.00295\n",
      "Iteration: 0137 Loss: 0.77091 Time: 0.00400\n",
      "Iteration: 0138 Loss: 0.77364 Time: 0.00301\n",
      "Iteration: 0139 Loss: 0.75264 Time: 0.00399\n",
      "Iteration: 0140 Loss: 0.75256 Time: 0.00305\n",
      "Iteration: 0141 Loss: 0.78188 Time: 0.00297\n",
      "Iteration: 0142 Loss: 0.76238 Time: 0.00296\n",
      "Iteration: 0143 Loss: 0.74998 Time: 0.00400\n",
      "Iteration: 0144 Loss: 0.75699 Time: 0.00374\n",
      "Iteration: 0145 Loss: 0.77476 Time: 0.00424\n",
      "Iteration: 0146 Loss: 0.73844 Time: 0.00677\n",
      "Iteration: 0147 Loss: 0.74323 Time: 0.00299\n",
      "Iteration: 0148 Loss: 0.74285 Time: 0.00207\n",
      "Iteration: 0149 Loss: 0.74547 Time: 0.00300\n",
      "Iteration: 0150 Loss: 0.73065 Time: 0.00619\n",
      "Iteration: 0151 Loss: 0.70262 Time: 0.00380\n",
      "Iteration: 0152 Loss: 0.73471 Time: 0.00304\n",
      "Iteration: 0153 Loss: 0.73155 Time: 0.00401\n",
      "Iteration: 0154 Loss: 0.74096 Time: 0.00399\n",
      "Iteration: 0155 Loss: 0.73376 Time: 0.00400\n",
      "Iteration: 0156 Loss: 0.74950 Time: 0.00301\n",
      "Iteration: 0157 Loss: 0.73553 Time: 0.00399\n",
      "Iteration: 0158 Loss: 0.71198 Time: 0.00408\n",
      "Iteration: 0159 Loss: 0.74494 Time: 0.00296\n",
      "Iteration: 0160 Loss: 0.73076 Time: 0.00196\n",
      "Iteration: 0161 Loss: 0.73617 Time: 0.00302\n",
      "Iteration: 0162 Loss: 0.71665 Time: 0.00300\n",
      "Iteration: 0163 Loss: 0.73264 Time: 0.00402\n",
      "Iteration: 0164 Loss: 0.69617 Time: 0.00452\n",
      "Iteration: 0165 Loss: 0.70776 Time: 0.00295\n",
      "Iteration: 0166 Loss: 0.70073 Time: 0.00300\n",
      "Iteration: 0167 Loss: 0.69640 Time: 0.00300\n",
      "Iteration: 0168 Loss: 0.70774 Time: 0.00298\n",
      "Iteration: 0169 Loss: 0.68781 Time: 0.00301\n",
      "Iteration: 0170 Loss: 0.68687 Time: 0.00297\n",
      "Iteration: 0171 Loss: 0.70885 Time: 0.00400\n",
      "Iteration: 0172 Loss: 0.68049 Time: 0.00301\n",
      "Iteration: 0173 Loss: 0.69911 Time: 0.00300\n",
      "Iteration: 0174 Loss: 0.70273 Time: 0.00290\n",
      "Iteration: 0175 Loss: 0.68012 Time: 0.00300\n",
      "Iteration: 0176 Loss: 0.67885 Time: 0.00299\n",
      "Iteration: 0177 Loss: 0.70872 Time: 0.00300\n",
      "Iteration: 0178 Loss: 0.69707 Time: 0.00299\n",
      "Iteration: 0179 Loss: 0.66597 Time: 0.00436\n",
      "Iteration: 0180 Loss: 0.69292 Time: 0.00264\n",
      "Iteration: 0181 Loss: 0.69741 Time: 0.00299\n",
      "Iteration: 0182 Loss: 0.68893 Time: 0.00300\n",
      "Iteration: 0183 Loss: 0.67935 Time: 0.00300\n",
      "Iteration: 0184 Loss: 0.71237 Time: 0.00300\n",
      "Iteration: 0185 Loss: 0.67452 Time: 0.00301\n",
      "Iteration: 0186 Loss: 0.68053 Time: 0.00303\n",
      "Iteration: 0187 Loss: 0.65933 Time: 0.00296\n",
      "Iteration: 0188 Loss: 0.66087 Time: 0.00297\n",
      "Iteration: 0189 Loss: 0.67669 Time: 0.00298\n",
      "Iteration: 0190 Loss: 0.66493 Time: 0.00396\n",
      "Iteration: 0191 Loss: 0.67796 Time: 0.00300\n",
      "Iteration: 0192 Loss: 0.67400 Time: 0.00300\n",
      "Iteration: 0193 Loss: 0.66903 Time: 0.00304\n",
      "Iteration: 0194 Loss: 0.65502 Time: 0.00397\n",
      "Iteration: 0195 Loss: 0.67126 Time: 0.00399\n",
      "Iteration: 0196 Loss: 0.65025 Time: 0.00304\n",
      "Iteration: 0197 Loss: 0.64555 Time: 0.00296\n",
      "Iteration: 0198 Loss: 0.67949 Time: 0.00400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0199 Loss: 0.64592 Time: 0.00406\n",
      "Iteration: 0200 Loss: 0.66051 Time: 0.00395\n",
      "Done! \n",
      "\n",
      "STEP 3/3 - MODEL EVALUATION \n",
      "\n",
      "Computing the final embedding vectors, for evaluation\n",
      "Done! \n",
      "\n",
      "Testing: link prediction\n",
      "Done! \n",
      "\n",
      "EXPERIMENTS ON MODEL 2 / 10 \n",
      "\n",
      "STEP 1/3 - PREPROCESSING STEPS \n",
      "\n",
      "Masking some edges from the training graph, for link prediction\n",
      "(validation set: 1.0 % of edges - test set: 10.0 % of edges)\n",
      "Done! \n",
      "\n",
      "Preprocessing node features\n",
      "Done! \n",
      "\n",
      "Running the Louvain algorithm for community detection\n",
      "as a preprocessing step for the encoder\n",
      "Done! Louvain has found 22 communities \n",
      "\n",
      "Setting up the model and the optimizer\n",
      "Done! \n",
      "\n",
      "Preprocessing on message passing matrices\n",
      "Done! \n",
      "\n",
      "Initializing TF session\n",
      "Done! \n",
      "\n",
      "STEP 2/3 - MODEL TRAINING \n",
      "\n",
      "Starting training\n",
      "Iteration: 0001 Loss: 1.81158 Time: 0.08162\n",
      "Iteration: 0002 Loss: 1.65810 Time: 0.00400\n",
      "Iteration: 0003 Loss: 1.68281 Time: 0.00301\n",
      "Iteration: 0004 Loss: 1.59383 Time: 0.00301\n",
      "Iteration: 0005 Loss: 1.53572 Time: 0.00400\n",
      "Iteration: 0006 Loss: 1.57809 Time: 0.00299\n",
      "Iteration: 0007 Loss: 1.48266 Time: 0.00301\n",
      "Iteration: 0008 Loss: 1.51554 Time: 0.00411\n",
      "Iteration: 0009 Loss: 1.52857 Time: 0.00304\n",
      "Iteration: 0010 Loss: 1.49659 Time: 0.00300\n",
      "Iteration: 0011 Loss: 1.57414 Time: 0.00400\n",
      "Iteration: 0012 Loss: 1.43015 Time: 0.00400\n",
      "Iteration: 0013 Loss: 1.43596 Time: 0.00404\n",
      "Iteration: 0014 Loss: 1.42605 Time: 0.00295\n",
      "Iteration: 0015 Loss: 1.41168 Time: 0.00297\n",
      "Iteration: 0016 Loss: 1.50014 Time: 0.00293\n",
      "Iteration: 0017 Loss: 1.39005 Time: 0.00399\n",
      "Iteration: 0018 Loss: 1.45036 Time: 0.00313\n",
      "Iteration: 0019 Loss: 1.40876 Time: 0.00313\n",
      "Iteration: 0020 Loss: 1.35502 Time: 0.00306\n",
      "Iteration: 0021 Loss: 1.44137 Time: 0.00299\n",
      "Iteration: 0022 Loss: 1.30959 Time: 0.00500\n",
      "Iteration: 0023 Loss: 1.35845 Time: 0.00436\n",
      "Iteration: 0024 Loss: 1.32490 Time: 0.00375\n",
      "Iteration: 0025 Loss: 1.35986 Time: 0.00307\n",
      "Iteration: 0026 Loss: 1.30465 Time: 0.00316\n",
      "Iteration: 0027 Loss: 1.27685 Time: 0.00200\n",
      "Iteration: 0028 Loss: 1.29616 Time: 0.00402\n",
      "Iteration: 0029 Loss: 1.36884 Time: 0.00449\n",
      "Iteration: 0030 Loss: 1.23045 Time: 0.00248\n",
      "Iteration: 0031 Loss: 1.25538 Time: 0.00200\n",
      "Iteration: 0032 Loss: 1.31556 Time: 0.00300\n",
      "Iteration: 0033 Loss: 1.27001 Time: 0.00411\n",
      "Iteration: 0034 Loss: 1.16428 Time: 0.00295\n",
      "Iteration: 0035 Loss: 1.25876 Time: 0.00398\n",
      "Iteration: 0036 Loss: 1.20729 Time: 0.00297\n",
      "Iteration: 0037 Loss: 1.17863 Time: 0.00300\n",
      "Iteration: 0038 Loss: 1.20455 Time: 0.00405\n",
      "Iteration: 0039 Loss: 1.18929 Time: 0.00299\n",
      "Iteration: 0040 Loss: 1.17360 Time: 0.00314\n",
      "Iteration: 0041 Loss: 1.16503 Time: 0.00334\n",
      "Iteration: 0042 Loss: 1.18400 Time: 0.00304\n",
      "Iteration: 0043 Loss: 1.20189 Time: 0.00400\n",
      "Iteration: 0044 Loss: 1.19411 Time: 0.00439\n",
      "Iteration: 0045 Loss: 1.10866 Time: 0.00308\n",
      "Iteration: 0046 Loss: 1.13541 Time: 0.00392\n",
      "Iteration: 0047 Loss: 1.16080 Time: 0.00304\n",
      "Iteration: 0048 Loss: 1.14436 Time: 0.00296\n",
      "Iteration: 0049 Loss: 1.12296 Time: 0.00309\n",
      "Iteration: 0050 Loss: 1.10000 Time: 0.00290\n",
      "Iteration: 0051 Loss: 1.15822 Time: 0.00302\n",
      "Iteration: 0052 Loss: 1.10430 Time: 0.00396\n",
      "Iteration: 0053 Loss: 1.08243 Time: 0.00302\n",
      "Iteration: 0054 Loss: 1.08714 Time: 0.00403\n",
      "Iteration: 0055 Loss: 1.08021 Time: 0.00294\n",
      "Iteration: 0056 Loss: 1.04692 Time: 0.00403\n",
      "Iteration: 0057 Loss: 1.08202 Time: 0.00295\n",
      "Iteration: 0058 Loss: 1.07803 Time: 0.00307\n",
      "Iteration: 0059 Loss: 1.11379 Time: 0.00400\n",
      "Iteration: 0060 Loss: 1.04341 Time: 0.00304\n",
      "Iteration: 0061 Loss: 1.07923 Time: 0.00296\n",
      "Iteration: 0062 Loss: 1.03502 Time: 0.00303\n",
      "Iteration: 0063 Loss: 1.06242 Time: 0.00304\n",
      "Iteration: 0064 Loss: 1.06600 Time: 0.00296\n",
      "Iteration: 0065 Loss: 1.03992 Time: 0.00305\n",
      "Iteration: 0066 Loss: 1.04487 Time: 0.00300\n",
      "Iteration: 0067 Loss: 1.02553 Time: 0.00309\n",
      "Iteration: 0068 Loss: 1.00821 Time: 0.00394\n",
      "Iteration: 0069 Loss: 1.05351 Time: 0.00305\n",
      "Iteration: 0070 Loss: 1.00466 Time: 0.00206\n",
      "Iteration: 0071 Loss: 0.97715 Time: 0.00397\n",
      "Iteration: 0072 Loss: 1.02977 Time: 0.00297\n",
      "Iteration: 0073 Loss: 0.99109 Time: 0.00308\n",
      "Iteration: 0074 Loss: 0.96104 Time: 0.00398\n",
      "Iteration: 0075 Loss: 1.00443 Time: 0.00301\n",
      "Iteration: 0076 Loss: 1.00787 Time: 0.00419\n",
      "Iteration: 0077 Loss: 1.02286 Time: 0.00404\n",
      "Iteration: 0078 Loss: 0.94668 Time: 0.00308\n",
      "Iteration: 0079 Loss: 0.92648 Time: 0.00354\n",
      "Iteration: 0080 Loss: 0.98664 Time: 0.00301\n",
      "Iteration: 0081 Loss: 0.99622 Time: 0.00400\n",
      "Iteration: 0082 Loss: 0.92634 Time: 0.00297\n",
      "Iteration: 0083 Loss: 0.97721 Time: 0.00297\n",
      "Iteration: 0084 Loss: 0.94453 Time: 0.00407\n",
      "Iteration: 0085 Loss: 0.93895 Time: 0.00403\n",
      "Iteration: 0086 Loss: 0.92555 Time: 0.00293\n",
      "Iteration: 0087 Loss: 0.94079 Time: 0.00395\n",
      "Iteration: 0088 Loss: 0.92724 Time: 0.00310\n",
      "Iteration: 0089 Loss: 0.97800 Time: 0.00302\n",
      "Iteration: 0090 Loss: 0.91630 Time: 0.00400\n",
      "Iteration: 0091 Loss: 0.97012 Time: 0.00302\n",
      "Iteration: 0092 Loss: 0.93713 Time: 0.00393\n",
      "Iteration: 0093 Loss: 0.92738 Time: 0.00310\n",
      "Iteration: 0094 Loss: 0.92286 Time: 0.00299\n",
      "Iteration: 0095 Loss: 0.90816 Time: 0.00292\n",
      "Iteration: 0096 Loss: 0.90670 Time: 0.00304\n",
      "Iteration: 0097 Loss: 0.90739 Time: 0.00298\n",
      "Iteration: 0098 Loss: 0.92421 Time: 0.00411\n",
      "Iteration: 0099 Loss: 0.90019 Time: 0.00300\n",
      "Iteration: 0100 Loss: 0.86375 Time: 0.00396\n",
      "Iteration: 0101 Loss: 0.88592 Time: 0.00400\n",
      "Iteration: 0102 Loss: 0.89012 Time: 0.00304\n",
      "Iteration: 0103 Loss: 0.87610 Time: 0.00296\n",
      "Iteration: 0104 Loss: 0.87607 Time: 0.00400\n",
      "Iteration: 0105 Loss: 0.90223 Time: 0.00404\n",
      "Iteration: 0106 Loss: 0.84436 Time: 0.00404\n",
      "Iteration: 0107 Loss: 0.87657 Time: 0.00440\n",
      "Iteration: 0108 Loss: 0.88100 Time: 0.00354\n",
      "Iteration: 0109 Loss: 0.86011 Time: 0.00304\n",
      "Iteration: 0110 Loss: 0.84630 Time: 0.00294\n",
      "Iteration: 0111 Loss: 0.85281 Time: 0.00435\n",
      "Iteration: 0112 Loss: 0.87081 Time: 0.00375\n",
      "Iteration: 0113 Loss: 0.82994 Time: 0.00305\n",
      "Iteration: 0114 Loss: 0.88067 Time: 0.00298\n",
      "Iteration: 0115 Loss: 0.85446 Time: 0.00359\n",
      "Iteration: 0116 Loss: 0.84044 Time: 0.00346\n",
      "Iteration: 0117 Loss: 0.85296 Time: 0.00301\n",
      "Iteration: 0118 Loss: 0.82485 Time: 0.00394\n",
      "Iteration: 0119 Loss: 0.82903 Time: 0.00298\n",
      "Iteration: 0120 Loss: 0.78913 Time: 0.00298\n",
      "Iteration: 0121 Loss: 0.82584 Time: 0.00404\n",
      "Iteration: 0122 Loss: 0.81236 Time: 0.00400\n",
      "Iteration: 0123 Loss: 0.82832 Time: 0.00204\n",
      "Iteration: 0124 Loss: 0.81260 Time: 0.00304\n",
      "Iteration: 0125 Loss: 0.80126 Time: 0.00396\n",
      "Iteration: 0126 Loss: 0.81165 Time: 0.00300\n",
      "Iteration: 0127 Loss: 0.76483 Time: 0.00296\n",
      "Iteration: 0128 Loss: 0.79804 Time: 0.00307\n",
      "Iteration: 0129 Loss: 0.79533 Time: 0.00394\n",
      "Iteration: 0130 Loss: 0.76587 Time: 0.00400\n",
      "Iteration: 0131 Loss: 0.77507 Time: 0.00398\n",
      "Iteration: 0132 Loss: 0.75146 Time: 0.00401\n",
      "Iteration: 0133 Loss: 0.77625 Time: 0.00399\n",
      "Iteration: 0134 Loss: 0.76568 Time: 0.00304\n",
      "Iteration: 0135 Loss: 0.77977 Time: 0.00300\n",
      "Iteration: 0136 Loss: 0.74908 Time: 0.00300\n",
      "Iteration: 0137 Loss: 0.77741 Time: 0.00315\n",
      "Iteration: 0138 Loss: 0.75663 Time: 0.00309\n",
      "Iteration: 0139 Loss: 0.76685 Time: 0.00296\n",
      "Iteration: 0140 Loss: 0.73432 Time: 0.00304\n",
      "Iteration: 0141 Loss: 0.75416 Time: 0.00399\n",
      "Iteration: 0142 Loss: 0.74253 Time: 0.00405\n",
      "Iteration: 0143 Loss: 0.72605 Time: 0.00291\n",
      "Iteration: 0144 Loss: 0.74161 Time: 0.00307\n",
      "Iteration: 0145 Loss: 0.72417 Time: 0.00299\n",
      "Iteration: 0146 Loss: 0.78891 Time: 0.00302\n",
      "Iteration: 0147 Loss: 0.73874 Time: 0.00396\n",
      "Iteration: 0148 Loss: 0.72653 Time: 0.00305\n",
      "Iteration: 0149 Loss: 0.75420 Time: 0.00398\n",
      "Iteration: 0150 Loss: 0.74755 Time: 0.00299\n",
      "Iteration: 0151 Loss: 0.72647 Time: 0.00300\n",
      "Iteration: 0152 Loss: 0.71288 Time: 0.00408\n",
      "Iteration: 0153 Loss: 0.73028 Time: 0.00300\n",
      "Iteration: 0154 Loss: 0.74338 Time: 0.00299\n",
      "Iteration: 0155 Loss: 0.73622 Time: 0.00291\n",
      "Iteration: 0156 Loss: 0.74429 Time: 0.00305\n",
      "Iteration: 0157 Loss: 0.72722 Time: 0.00299\n",
      "Iteration: 0158 Loss: 0.72525 Time: 0.00306\n",
      "Iteration: 0159 Loss: 0.71177 Time: 0.00394\n",
      "Iteration: 0160 Loss: 0.71259 Time: 0.00398\n",
      "Iteration: 0161 Loss: 0.71737 Time: 0.00304\n",
      "Iteration: 0162 Loss: 0.71747 Time: 0.00297\n",
      "Iteration: 0163 Loss: 0.69057 Time: 0.00295\n",
      "Iteration: 0164 Loss: 0.71993 Time: 0.00307\n",
      "Iteration: 0165 Loss: 0.73052 Time: 0.00398\n",
      "Iteration: 0166 Loss: 0.71755 Time: 0.00403\n",
      "Iteration: 0167 Loss: 0.70364 Time: 0.00304\n",
      "Iteration: 0168 Loss: 0.69737 Time: 0.00497\n",
      "Iteration: 0169 Loss: 0.69886 Time: 0.00434\n",
      "Iteration: 0170 Loss: 0.69851 Time: 0.00372\n",
      "Iteration: 0171 Loss: 0.69870 Time: 0.00495\n",
      "Iteration: 0172 Loss: 0.68793 Time: 0.00302\n",
      "Iteration: 0173 Loss: 0.71129 Time: 0.00303\n",
      "Iteration: 0174 Loss: 0.69351 Time: 0.00401\n",
      "Iteration: 0175 Loss: 0.69785 Time: 0.00412\n",
      "Iteration: 0176 Loss: 0.66828 Time: 0.00202\n",
      "Iteration: 0177 Loss: 0.69545 Time: 0.00297\n",
      "Iteration: 0178 Loss: 0.68697 Time: 0.00400\n",
      "Iteration: 0179 Loss: 0.70093 Time: 0.00304\n",
      "Iteration: 0180 Loss: 0.67470 Time: 0.00299\n",
      "Iteration: 0181 Loss: 0.67464 Time: 0.00302\n",
      "Iteration: 0182 Loss: 0.68224 Time: 0.00296\n",
      "Iteration: 0183 Loss: 0.68164 Time: 0.00402\n",
      "Iteration: 0184 Loss: 0.65985 Time: 0.00425\n",
      "Iteration: 0185 Loss: 0.67610 Time: 0.00275\n",
      "Iteration: 0186 Loss: 0.68832 Time: 0.00402\n",
      "Iteration: 0187 Loss: 0.66163 Time: 0.00297\n",
      "Iteration: 0188 Loss: 0.65935 Time: 0.00423\n",
      "Iteration: 0189 Loss: 0.66859 Time: 0.00277\n",
      "Iteration: 0190 Loss: 0.66389 Time: 0.00309\n",
      "Iteration: 0191 Loss: 0.65738 Time: 0.00297\n",
      "Iteration: 0192 Loss: 0.66801 Time: 0.00301\n",
      "Iteration: 0193 Loss: 0.66285 Time: 0.00407\n",
      "Iteration: 0194 Loss: 0.64558 Time: 0.00397\n",
      "Iteration: 0195 Loss: 0.65928 Time: 0.00396\n",
      "Iteration: 0196 Loss: 0.65674 Time: 0.00302\n",
      "Iteration: 0197 Loss: 0.66057 Time: 0.00402\n",
      "Iteration: 0198 Loss: 0.65369 Time: 0.00310\n",
      "Iteration: 0199 Loss: 0.64277 Time: 0.00303\n",
      "Iteration: 0200 Loss: 0.65106 Time: 0.00394\n",
      "Done! \n",
      "\n",
      "STEP 3/3 - MODEL EVALUATION \n",
      "\n",
      "Computing the final embedding vectors, for evaluation\n",
      "Done! \n",
      "\n",
      "Testing: link prediction\n",
      "Done! \n",
      "\n",
      "EXPERIMENTS ON MODEL 3 / 10 \n",
      "\n",
      "STEP 1/3 - PREPROCESSING STEPS \n",
      "\n",
      "Masking some edges from the training graph, for link prediction\n",
      "(validation set: 1.0 % of edges - test set: 10.0 % of edges)\n",
      "Done! \n",
      "\n",
      "Preprocessing node features\n",
      "Done! \n",
      "\n",
      "Running the Louvain algorithm for community detection\n",
      "as a preprocessing step for the encoder\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Louvain has found 20 communities \n",
      "\n",
      "Setting up the model and the optimizer\n",
      "Done! \n",
      "\n",
      "Preprocessing on message passing matrices\n",
      "Done! \n",
      "\n",
      "Initializing TF session\n",
      "Done! \n",
      "\n",
      "STEP 2/3 - MODEL TRAINING \n",
      "\n",
      "Starting training\n",
      "Iteration: 0001 Loss: 1.62493 Time: 0.09499\n",
      "Iteration: 0002 Loss: 1.66961 Time: 0.00601\n",
      "Iteration: 0003 Loss: 1.65361 Time: 0.00499\n",
      "Iteration: 0004 Loss: 1.67895 Time: 0.00301\n",
      "Iteration: 0005 Loss: 1.50678 Time: 0.00399\n",
      "Iteration: 0006 Loss: 1.67259 Time: 0.00300\n",
      "Iteration: 0007 Loss: 1.58754 Time: 0.00300\n",
      "Iteration: 0008 Loss: 1.62127 Time: 0.00400\n",
      "Iteration: 0009 Loss: 1.61312 Time: 0.00400\n",
      "Iteration: 0010 Loss: 1.53098 Time: 0.00400\n",
      "Iteration: 0011 Loss: 1.55412 Time: 0.00300\n",
      "Iteration: 0012 Loss: 1.47267 Time: 0.00401\n",
      "Iteration: 0013 Loss: 1.53109 Time: 0.00299\n",
      "Iteration: 0014 Loss: 1.41535 Time: 0.00300\n",
      "Iteration: 0015 Loss: 1.43485 Time: 0.00301\n",
      "Iteration: 0016 Loss: 1.41306 Time: 0.00305\n",
      "Iteration: 0017 Loss: 1.52886 Time: 0.00400\n",
      "Iteration: 0018 Loss: 1.44919 Time: 0.00395\n",
      "Iteration: 0019 Loss: 1.39658 Time: 0.00306\n",
      "Iteration: 0020 Loss: 1.29209 Time: 0.00305\n",
      "Iteration: 0021 Loss: 1.37911 Time: 0.00297\n",
      "Iteration: 0022 Loss: 1.33124 Time: 0.00507\n",
      "Iteration: 0023 Loss: 1.29344 Time: 0.00501\n",
      "Iteration: 0024 Loss: 1.32415 Time: 0.00391\n",
      "Iteration: 0025 Loss: 1.26460 Time: 0.00400\n",
      "Iteration: 0026 Loss: 1.33449 Time: 0.00299\n",
      "Iteration: 0027 Loss: 1.32387 Time: 0.00400\n",
      "Iteration: 0028 Loss: 1.37876 Time: 0.00401\n",
      "Iteration: 0029 Loss: 1.28582 Time: 0.00401\n",
      "Iteration: 0030 Loss: 1.27488 Time: 0.00397\n",
      "Iteration: 0031 Loss: 1.20833 Time: 0.00205\n",
      "Iteration: 0032 Loss: 1.29041 Time: 0.00294\n",
      "Iteration: 0033 Loss: 1.16054 Time: 0.00302\n",
      "Iteration: 0034 Loss: 1.23478 Time: 0.00402\n",
      "Iteration: 0035 Loss: 1.20071 Time: 0.00307\n",
      "Iteration: 0036 Loss: 1.25604 Time: 0.00421\n",
      "Iteration: 0037 Loss: 1.14616 Time: 0.00371\n",
      "Iteration: 0038 Loss: 1.17326 Time: 0.00401\n",
      "Iteration: 0039 Loss: 1.19988 Time: 0.00294\n",
      "Iteration: 0040 Loss: 1.20474 Time: 0.00296\n",
      "Iteration: 0041 Loss: 1.17362 Time: 0.00196\n",
      "Iteration: 0042 Loss: 1.12366 Time: 0.00304\n",
      "Iteration: 0043 Loss: 1.12491 Time: 0.00398\n",
      "Iteration: 0044 Loss: 1.16890 Time: 0.00394\n",
      "Iteration: 0045 Loss: 1.14778 Time: 0.00400\n",
      "Iteration: 0046 Loss: 1.19614 Time: 0.00301\n",
      "Iteration: 0047 Loss: 1.14220 Time: 0.00306\n",
      "Iteration: 0048 Loss: 1.16529 Time: 0.00401\n",
      "Iteration: 0049 Loss: 1.11919 Time: 0.00296\n",
      "Iteration: 0050 Loss: 1.08378 Time: 0.00404\n",
      "Iteration: 0051 Loss: 1.09450 Time: 0.00397\n",
      "Iteration: 0052 Loss: 1.09917 Time: 0.00404\n",
      "Iteration: 0053 Loss: 1.10304 Time: 0.00301\n",
      "Iteration: 0054 Loss: 1.09823 Time: 0.00496\n",
      "Iteration: 0055 Loss: 1.10712 Time: 0.00303\n",
      "Iteration: 0056 Loss: 1.12597 Time: 0.00300\n",
      "Iteration: 0057 Loss: 1.07619 Time: 0.00401\n",
      "Iteration: 0058 Loss: 1.07696 Time: 0.00408\n",
      "Iteration: 0059 Loss: 1.07183 Time: 0.00390\n",
      "Iteration: 0060 Loss: 1.04290 Time: 0.00303\n",
      "Iteration: 0061 Loss: 1.07056 Time: 0.00400\n",
      "Iteration: 0062 Loss: 1.02323 Time: 0.00397\n",
      "Iteration: 0063 Loss: 1.04656 Time: 0.00304\n",
      "Iteration: 0064 Loss: 1.06271 Time: 0.00309\n",
      "Iteration: 0065 Loss: 1.06385 Time: 0.00390\n",
      "Iteration: 0066 Loss: 1.10032 Time: 0.00400\n",
      "Iteration: 0067 Loss: 1.03494 Time: 0.00304\n",
      "Iteration: 0068 Loss: 1.02597 Time: 0.00401\n",
      "Iteration: 0069 Loss: 0.98924 Time: 0.00402\n",
      "Iteration: 0070 Loss: 0.97380 Time: 0.00294\n",
      "Iteration: 0071 Loss: 1.00529 Time: 0.00305\n",
      "Iteration: 0072 Loss: 1.02084 Time: 0.00295\n",
      "Iteration: 0073 Loss: 1.00009 Time: 0.00410\n",
      "Iteration: 0074 Loss: 0.96056 Time: 0.00301\n",
      "Iteration: 0075 Loss: 1.02146 Time: 0.00390\n",
      "Iteration: 0076 Loss: 1.00629 Time: 0.00304\n",
      "Iteration: 0077 Loss: 0.97000 Time: 0.00290\n",
      "Iteration: 0078 Loss: 0.99052 Time: 0.00396\n",
      "Iteration: 0079 Loss: 0.94887 Time: 0.00399\n",
      "Iteration: 0080 Loss: 0.99118 Time: 0.00299\n",
      "Iteration: 0081 Loss: 1.00058 Time: 0.00304\n",
      "Iteration: 0082 Loss: 0.96358 Time: 0.00300\n",
      "Iteration: 0083 Loss: 0.94917 Time: 0.00307\n",
      "Iteration: 0084 Loss: 0.95305 Time: 0.00296\n",
      "Iteration: 0085 Loss: 0.94089 Time: 0.00301\n",
      "Iteration: 0086 Loss: 0.96527 Time: 0.00307\n",
      "Iteration: 0087 Loss: 0.94420 Time: 0.00403\n",
      "Iteration: 0088 Loss: 0.93925 Time: 0.00299\n",
      "Iteration: 0089 Loss: 0.93178 Time: 0.00307\n",
      "Iteration: 0090 Loss: 0.90277 Time: 0.00395\n",
      "Iteration: 0091 Loss: 0.96245 Time: 0.00402\n",
      "Iteration: 0092 Loss: 0.93230 Time: 0.00396\n",
      "Iteration: 0093 Loss: 0.95997 Time: 0.00300\n",
      "Iteration: 0094 Loss: 0.91179 Time: 0.00362\n",
      "Iteration: 0095 Loss: 0.90020 Time: 0.00448\n",
      "Iteration: 0096 Loss: 0.93973 Time: 0.00390\n",
      "Iteration: 0097 Loss: 0.89165 Time: 0.00401\n",
      "Iteration: 0098 Loss: 0.90432 Time: 0.00304\n",
      "Iteration: 0099 Loss: 0.92515 Time: 0.00400\n",
      "Iteration: 0100 Loss: 0.87411 Time: 0.00300\n",
      "Iteration: 0101 Loss: 0.90258 Time: 0.00404\n",
      "Iteration: 0102 Loss: 0.92076 Time: 0.00299\n",
      "Iteration: 0103 Loss: 0.91502 Time: 0.00397\n",
      "Iteration: 0104 Loss: 0.90994 Time: 0.00299\n",
      "Iteration: 0105 Loss: 0.91265 Time: 0.00300\n",
      "Iteration: 0106 Loss: 0.89804 Time: 0.00300\n",
      "Iteration: 0107 Loss: 0.87688 Time: 0.00300\n",
      "Iteration: 0108 Loss: 0.89840 Time: 0.00498\n",
      "Iteration: 0109 Loss: 0.88301 Time: 0.00295\n",
      "Iteration: 0110 Loss: 0.87488 Time: 0.00306\n",
      "Iteration: 0111 Loss: 0.85120 Time: 0.00299\n",
      "Iteration: 0112 Loss: 0.86999 Time: 0.00301\n",
      "Iteration: 0113 Loss: 0.83913 Time: 0.00297\n",
      "Iteration: 0114 Loss: 0.83644 Time: 0.00301\n",
      "Iteration: 0115 Loss: 0.82851 Time: 0.00303\n",
      "Iteration: 0116 Loss: 0.85863 Time: 0.00303\n",
      "Iteration: 0117 Loss: 0.86485 Time: 0.00591\n",
      "Iteration: 0118 Loss: 0.85697 Time: 0.00307\n",
      "Iteration: 0119 Loss: 0.84229 Time: 0.00302\n",
      "Iteration: 0120 Loss: 0.83150 Time: 0.00305\n",
      "Iteration: 0121 Loss: 0.84693 Time: 0.00401\n",
      "Iteration: 0122 Loss: 0.83452 Time: 0.00395\n",
      "Iteration: 0123 Loss: 0.84478 Time: 0.00399\n",
      "Iteration: 0124 Loss: 0.82017 Time: 0.00507\n",
      "Iteration: 0125 Loss: 0.82089 Time: 0.00393\n",
      "Iteration: 0126 Loss: 0.84428 Time: 0.00302\n",
      "Iteration: 0127 Loss: 0.83025 Time: 0.00397\n",
      "Iteration: 0128 Loss: 0.81884 Time: 0.00357\n",
      "Iteration: 0129 Loss: 0.81567 Time: 0.00296\n",
      "Iteration: 0130 Loss: 0.84118 Time: 0.00304\n",
      "Iteration: 0131 Loss: 0.80132 Time: 0.00306\n",
      "Iteration: 0132 Loss: 0.78892 Time: 0.00201\n",
      "Iteration: 0133 Loss: 0.81354 Time: 0.00304\n",
      "Iteration: 0134 Loss: 0.81468 Time: 0.00308\n",
      "Iteration: 0135 Loss: 0.79175 Time: 0.00401\n",
      "Iteration: 0136 Loss: 0.79777 Time: 0.00300\n",
      "Iteration: 0137 Loss: 0.81976 Time: 0.00305\n",
      "Iteration: 0138 Loss: 0.80240 Time: 0.00492\n",
      "Iteration: 0139 Loss: 0.79102 Time: 0.00300\n",
      "Iteration: 0140 Loss: 0.77994 Time: 0.00401\n",
      "Iteration: 0141 Loss: 0.76431 Time: 0.00398\n",
      "Iteration: 0142 Loss: 0.76929 Time: 0.00402\n",
      "Iteration: 0143 Loss: 0.76506 Time: 0.00398\n",
      "Iteration: 0144 Loss: 0.76814 Time: 0.00200\n",
      "Iteration: 0145 Loss: 0.78091 Time: 0.00295\n",
      "Iteration: 0146 Loss: 0.75957 Time: 0.00337\n",
      "Iteration: 0147 Loss: 0.76438 Time: 0.00262\n",
      "Iteration: 0148 Loss: 0.75457 Time: 0.00400\n",
      "Iteration: 0149 Loss: 0.76314 Time: 0.00396\n",
      "Iteration: 0150 Loss: 0.75378 Time: 0.00310\n",
      "Iteration: 0151 Loss: 0.74870 Time: 0.00401\n",
      "Iteration: 0152 Loss: 0.75770 Time: 0.00393\n",
      "Iteration: 0153 Loss: 0.74439 Time: 0.00402\n",
      "Iteration: 0154 Loss: 0.74987 Time: 0.00398\n",
      "Iteration: 0155 Loss: 0.74865 Time: 0.00309\n",
      "Iteration: 0156 Loss: 0.74039 Time: 0.00398\n",
      "Iteration: 0157 Loss: 0.74026 Time: 0.00495\n",
      "Iteration: 0158 Loss: 0.71960 Time: 0.00399\n",
      "Iteration: 0159 Loss: 0.70953 Time: 0.00313\n",
      "Iteration: 0160 Loss: 0.72371 Time: 0.00389\n",
      "Iteration: 0161 Loss: 0.76012 Time: 0.00404\n",
      "Iteration: 0162 Loss: 0.71027 Time: 0.00392\n",
      "Iteration: 0163 Loss: 0.73500 Time: 0.00317\n",
      "Iteration: 0164 Loss: 0.74115 Time: 0.00402\n",
      "Iteration: 0165 Loss: 0.69953 Time: 0.00299\n",
      "Iteration: 0166 Loss: 0.71957 Time: 0.00295\n",
      "Iteration: 0167 Loss: 0.73559 Time: 0.00306\n",
      "Iteration: 0168 Loss: 0.70291 Time: 0.00395\n",
      "Iteration: 0169 Loss: 0.70713 Time: 0.00404\n",
      "Iteration: 0170 Loss: 0.70943 Time: 0.00305\n",
      "Iteration: 0171 Loss: 0.70188 Time: 0.00296\n",
      "Iteration: 0172 Loss: 0.69336 Time: 0.00307\n",
      "Iteration: 0173 Loss: 0.71829 Time: 0.00397\n",
      "Iteration: 0174 Loss: 0.69243 Time: 0.00303\n",
      "Iteration: 0175 Loss: 0.70132 Time: 0.00403\n",
      "Iteration: 0176 Loss: 0.69548 Time: 0.00298\n",
      "Iteration: 0177 Loss: 0.68391 Time: 0.00393\n",
      "Iteration: 0178 Loss: 0.70505 Time: 0.00305\n",
      "Iteration: 0179 Loss: 0.70409 Time: 0.00303\n",
      "Iteration: 0180 Loss: 0.71689 Time: 0.00295\n",
      "Iteration: 0181 Loss: 0.67898 Time: 0.00405\n",
      "Iteration: 0182 Loss: 0.66810 Time: 0.00295\n",
      "Iteration: 0183 Loss: 0.66864 Time: 0.00299\n",
      "Iteration: 0184 Loss: 0.69004 Time: 0.00302\n",
      "Iteration: 0185 Loss: 0.66631 Time: 0.00403\n",
      "Iteration: 0186 Loss: 0.66996 Time: 0.00395\n",
      "Iteration: 0187 Loss: 0.68454 Time: 0.00299\n",
      "Iteration: 0188 Loss: 0.68694 Time: 0.00467\n",
      "Iteration: 0189 Loss: 0.65926 Time: 0.00291\n",
      "Iteration: 0190 Loss: 0.69252 Time: 0.00297\n",
      "Iteration: 0191 Loss: 0.67126 Time: 0.00299\n",
      "Iteration: 0192 Loss: 0.67896 Time: 0.00403\n",
      "Iteration: 0193 Loss: 0.67422 Time: 0.00405\n",
      "Iteration: 0194 Loss: 0.67697 Time: 0.00298\n",
      "Iteration: 0195 Loss: 0.66358 Time: 0.00313\n",
      "Iteration: 0196 Loss: 0.65669 Time: 0.00407\n",
      "Iteration: 0197 Loss: 0.65949 Time: 0.00296\n",
      "Iteration: 0198 Loss: 0.66506 Time: 0.00402\n",
      "Iteration: 0199 Loss: 0.66840 Time: 0.00403\n",
      "Iteration: 0200 Loss: 0.65606 Time: 0.00398\n",
      "Done! \n",
      "\n",
      "STEP 3/3 - MODEL EVALUATION \n",
      "\n",
      "Computing the final embedding vectors, for evaluation\n",
      "Done! \n",
      "\n",
      "Testing: link prediction\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! \n",
      "\n",
      "EXPERIMENTS ON MODEL 4 / 10 \n",
      "\n",
      "STEP 1/3 - PREPROCESSING STEPS \n",
      "\n",
      "Masking some edges from the training graph, for link prediction\n",
      "(validation set: 1.0 % of edges - test set: 10.0 % of edges)\n",
      "Done! \n",
      "\n",
      "Preprocessing node features\n",
      "Done! \n",
      "\n",
      "Running the Louvain algorithm for community detection\n",
      "as a preprocessing step for the encoder\n",
      "Done! Louvain has found 21 communities \n",
      "\n",
      "Setting up the model and the optimizer\n",
      "Done! \n",
      "\n",
      "Preprocessing on message passing matrices\n",
      "Done! \n",
      "\n",
      "Initializing TF session\n",
      "Done! \n",
      "\n",
      "STEP 2/3 - MODEL TRAINING \n",
      "\n",
      "Starting training\n",
      "Iteration: 0001 Loss: 1.69639 Time: 0.10100\n",
      "Iteration: 0002 Loss: 1.53420 Time: 0.00400\n",
      "Iteration: 0003 Loss: 1.78776 Time: 0.00521\n",
      "Iteration: 0004 Loss: 1.64417 Time: 0.00479\n",
      "Iteration: 0005 Loss: 1.68520 Time: 0.00499\n",
      "Iteration: 0006 Loss: 1.63230 Time: 0.00301\n",
      "Iteration: 0007 Loss: 1.58250 Time: 0.00400\n",
      "Iteration: 0008 Loss: 1.59965 Time: 0.00535\n",
      "Iteration: 0009 Loss: 1.58913 Time: 0.00200\n",
      "Iteration: 0010 Loss: 1.61724 Time: 0.00300\n",
      "Iteration: 0011 Loss: 1.61485 Time: 0.00401\n",
      "Iteration: 0012 Loss: 1.33716 Time: 0.00301\n",
      "Iteration: 0013 Loss: 1.49795 Time: 0.00399\n",
      "Iteration: 0014 Loss: 1.50086 Time: 0.00306\n",
      "Iteration: 0015 Loss: 1.43902 Time: 0.00297\n",
      "Iteration: 0016 Loss: 1.43048 Time: 0.00402\n",
      "Iteration: 0017 Loss: 1.47608 Time: 0.00401\n",
      "Iteration: 0018 Loss: 1.44838 Time: 0.00397\n",
      "Iteration: 0019 Loss: 1.42700 Time: 0.00303\n",
      "Iteration: 0020 Loss: 1.44719 Time: 0.00296\n",
      "Iteration: 0021 Loss: 1.42160 Time: 0.00395\n",
      "Iteration: 0022 Loss: 1.40663 Time: 0.00300\n",
      "Iteration: 0023 Loss: 1.37185 Time: 0.00306\n",
      "Iteration: 0024 Loss: 1.27979 Time: 0.00396\n",
      "Iteration: 0025 Loss: 1.34381 Time: 0.00300\n",
      "Iteration: 0026 Loss: 1.23292 Time: 0.00400\n",
      "Iteration: 0027 Loss: 1.31258 Time: 0.00317\n",
      "Iteration: 0028 Loss: 1.32672 Time: 0.00296\n",
      "Iteration: 0029 Loss: 1.32418 Time: 0.00407\n",
      "Iteration: 0030 Loss: 1.32313 Time: 0.00397\n",
      "Iteration: 0031 Loss: 1.23940 Time: 0.00405\n",
      "Iteration: 0032 Loss: 1.24791 Time: 0.00395\n",
      "Iteration: 0033 Loss: 1.21574 Time: 0.00396\n",
      "Iteration: 0034 Loss: 1.29816 Time: 0.00400\n",
      "Iteration: 0035 Loss: 1.16689 Time: 0.00404\n",
      "Iteration: 0036 Loss: 1.22853 Time: 0.00401\n",
      "Iteration: 0037 Loss: 1.24509 Time: 0.00395\n",
      "Iteration: 0038 Loss: 1.18955 Time: 0.00405\n",
      "Iteration: 0039 Loss: 1.18229 Time: 0.00400\n",
      "Iteration: 0040 Loss: 1.21533 Time: 0.00295\n",
      "Iteration: 0041 Loss: 1.11520 Time: 0.00308\n",
      "Iteration: 0042 Loss: 1.18631 Time: 0.00393\n",
      "Iteration: 0043 Loss: 1.16297 Time: 0.00305\n",
      "Iteration: 0044 Loss: 1.19685 Time: 0.00295\n",
      "Iteration: 0045 Loss: 1.17550 Time: 0.00302\n",
      "Iteration: 0046 Loss: 1.14850 Time: 0.00398\n",
      "Iteration: 0047 Loss: 1.15294 Time: 0.00401\n",
      "Iteration: 0048 Loss: 1.10565 Time: 0.00303\n",
      "Iteration: 0049 Loss: 1.07541 Time: 0.00207\n",
      "Iteration: 0050 Loss: 1.06357 Time: 0.00390\n",
      "Iteration: 0051 Loss: 1.09874 Time: 0.00403\n",
      "Iteration: 0052 Loss: 1.07496 Time: 0.00303\n",
      "Iteration: 0053 Loss: 1.17007 Time: 0.00300\n",
      "Iteration: 0054 Loss: 1.09401 Time: 0.00355\n",
      "Iteration: 0055 Loss: 1.10218 Time: 0.00306\n",
      "Iteration: 0056 Loss: 1.03353 Time: 0.00300\n",
      "Iteration: 0057 Loss: 1.07307 Time: 0.00300\n",
      "Iteration: 0058 Loss: 1.09785 Time: 0.00297\n",
      "Iteration: 0059 Loss: 1.11115 Time: 0.00399\n",
      "Iteration: 0060 Loss: 1.05804 Time: 0.00400\n",
      "Iteration: 0061 Loss: 1.09031 Time: 0.00201\n",
      "Iteration: 0062 Loss: 1.05116 Time: 0.00404\n",
      "Iteration: 0063 Loss: 1.12103 Time: 0.00207\n",
      "Iteration: 0064 Loss: 1.02262 Time: 0.00400\n",
      "Iteration: 0065 Loss: 1.06666 Time: 0.00403\n",
      "Iteration: 0066 Loss: 1.05734 Time: 0.00297\n",
      "Iteration: 0067 Loss: 0.97620 Time: 0.00306\n",
      "Iteration: 0068 Loss: 1.04891 Time: 0.00399\n",
      "Iteration: 0069 Loss: 1.04923 Time: 0.00396\n",
      "Iteration: 0070 Loss: 1.04355 Time: 0.00399\n",
      "Iteration: 0071 Loss: 1.02333 Time: 0.00304\n",
      "Iteration: 0072 Loss: 0.96902 Time: 0.00298\n",
      "Iteration: 0073 Loss: 1.02451 Time: 0.00405\n",
      "Iteration: 0074 Loss: 0.97533 Time: 0.00444\n",
      "Iteration: 0075 Loss: 0.95787 Time: 0.00295\n",
      "Iteration: 0076 Loss: 0.93326 Time: 0.00304\n",
      "Iteration: 0077 Loss: 0.97961 Time: 0.00427\n",
      "Iteration: 0078 Loss: 1.02940 Time: 0.00294\n",
      "Iteration: 0079 Loss: 1.01065 Time: 0.00300\n",
      "Iteration: 0080 Loss: 0.96608 Time: 0.00302\n",
      "Iteration: 0081 Loss: 0.98029 Time: 0.00303\n",
      "Iteration: 0082 Loss: 0.96369 Time: 0.00300\n",
      "Iteration: 0083 Loss: 0.96807 Time: 0.00399\n",
      "Iteration: 0084 Loss: 0.92575 Time: 0.00297\n",
      "Iteration: 0085 Loss: 0.98204 Time: 0.00298\n",
      "Iteration: 0086 Loss: 0.97676 Time: 0.00412\n",
      "Iteration: 0087 Loss: 0.98053 Time: 0.00391\n",
      "Iteration: 0088 Loss: 0.97528 Time: 0.00400\n",
      "Iteration: 0089 Loss: 0.91711 Time: 0.00307\n",
      "Iteration: 0090 Loss: 0.91039 Time: 0.00348\n",
      "Iteration: 0091 Loss: 0.94456 Time: 0.00398\n",
      "Iteration: 0092 Loss: 0.90883 Time: 0.00302\n",
      "Iteration: 0093 Loss: 0.92848 Time: 0.00413\n",
      "Iteration: 0094 Loss: 0.93009 Time: 0.00296\n",
      "Iteration: 0095 Loss: 0.93828 Time: 0.00439\n",
      "Iteration: 0096 Loss: 0.90311 Time: 0.00268\n",
      "Iteration: 0097 Loss: 0.87468 Time: 0.00395\n",
      "Iteration: 0098 Loss: 0.88613 Time: 0.00311\n",
      "Iteration: 0099 Loss: 0.86966 Time: 0.00400\n",
      "Iteration: 0100 Loss: 0.92133 Time: 0.00303\n",
      "Iteration: 0101 Loss: 0.89122 Time: 0.00197\n",
      "Iteration: 0102 Loss: 0.87773 Time: 0.00302\n",
      "Iteration: 0103 Loss: 0.89220 Time: 0.00497\n",
      "Iteration: 0104 Loss: 0.86855 Time: 0.00400\n",
      "Iteration: 0105 Loss: 0.85199 Time: 0.00308\n",
      "Iteration: 0106 Loss: 0.85865 Time: 0.00198\n",
      "Iteration: 0107 Loss: 0.86100 Time: 0.00400\n",
      "Iteration: 0108 Loss: 0.84211 Time: 0.00398\n",
      "Iteration: 0109 Loss: 0.84331 Time: 0.00304\n",
      "Iteration: 0110 Loss: 0.84544 Time: 0.00302\n",
      "Iteration: 0111 Loss: 0.86174 Time: 0.00296\n",
      "Iteration: 0112 Loss: 0.81801 Time: 0.00348\n",
      "Iteration: 0113 Loss: 0.86407 Time: 0.00299\n",
      "Iteration: 0114 Loss: 0.81407 Time: 0.00398\n",
      "Iteration: 0115 Loss: 0.84640 Time: 0.00400\n",
      "Iteration: 0116 Loss: 0.81677 Time: 0.00397\n",
      "Iteration: 0117 Loss: 0.84793 Time: 0.00408\n",
      "Iteration: 0118 Loss: 0.82660 Time: 0.00401\n",
      "Iteration: 0119 Loss: 0.81962 Time: 0.00297\n",
      "Iteration: 0120 Loss: 0.82853 Time: 0.00301\n",
      "Iteration: 0121 Loss: 0.82438 Time: 0.00401\n",
      "Iteration: 0122 Loss: 0.80478 Time: 0.00298\n",
      "Iteration: 0123 Loss: 0.82239 Time: 0.00301\n",
      "Iteration: 0124 Loss: 0.82989 Time: 0.00401\n",
      "Iteration: 0125 Loss: 0.81954 Time: 0.00404\n",
      "Iteration: 0126 Loss: 0.84117 Time: 0.00302\n",
      "Iteration: 0127 Loss: 0.82524 Time: 0.00316\n",
      "Iteration: 0128 Loss: 0.79542 Time: 0.00301\n",
      "Iteration: 0129 Loss: 0.79924 Time: 0.00299\n",
      "Iteration: 0130 Loss: 0.78769 Time: 0.00413\n",
      "Iteration: 0131 Loss: 0.76351 Time: 0.00302\n",
      "Iteration: 0132 Loss: 0.76663 Time: 0.00404\n",
      "Iteration: 0133 Loss: 0.79149 Time: 0.00429\n",
      "Iteration: 0134 Loss: 0.77203 Time: 0.00302\n",
      "Iteration: 0135 Loss: 0.76807 Time: 0.00300\n",
      "Iteration: 0136 Loss: 0.75943 Time: 0.00300\n",
      "Iteration: 0137 Loss: 0.79027 Time: 0.00296\n",
      "Iteration: 0138 Loss: 0.76922 Time: 0.00408\n",
      "Iteration: 0139 Loss: 0.77856 Time: 0.00301\n",
      "Iteration: 0140 Loss: 0.75287 Time: 0.00304\n",
      "Iteration: 0141 Loss: 0.77211 Time: 0.00295\n",
      "Iteration: 0142 Loss: 0.75531 Time: 0.00312\n",
      "Iteration: 0143 Loss: 0.77776 Time: 0.00312\n",
      "Iteration: 0144 Loss: 0.75453 Time: 0.00304\n",
      "Iteration: 0145 Loss: 0.75682 Time: 0.00400\n",
      "Iteration: 0146 Loss: 0.74696 Time: 0.00312\n",
      "Iteration: 0147 Loss: 0.75913 Time: 0.00487\n",
      "Iteration: 0148 Loss: 0.75735 Time: 0.00399\n",
      "Iteration: 0149 Loss: 0.74910 Time: 0.00304\n",
      "Iteration: 0150 Loss: 0.77602 Time: 0.00288\n",
      "Iteration: 0151 Loss: 0.75487 Time: 0.00499\n",
      "Iteration: 0152 Loss: 0.74116 Time: 0.00418\n",
      "Iteration: 0153 Loss: 0.74069 Time: 0.00297\n",
      "Iteration: 0154 Loss: 0.71303 Time: 0.00305\n",
      "Iteration: 0155 Loss: 0.72855 Time: 0.00546\n",
      "Iteration: 0156 Loss: 0.72727 Time: 0.00354\n",
      "Iteration: 0157 Loss: 0.69983 Time: 0.00442\n",
      "Iteration: 0158 Loss: 0.72463 Time: 0.00288\n",
      "Iteration: 0159 Loss: 0.72115 Time: 0.00300\n",
      "Iteration: 0160 Loss: 0.72412 Time: 0.00400\n",
      "Iteration: 0161 Loss: 0.72935 Time: 0.00300\n",
      "Iteration: 0162 Loss: 0.71865 Time: 0.00305\n",
      "Iteration: 0163 Loss: 0.72367 Time: 0.00294\n",
      "Iteration: 0164 Loss: 0.69485 Time: 0.00300\n",
      "Iteration: 0165 Loss: 0.72915 Time: 0.00300\n",
      "Iteration: 0166 Loss: 0.68717 Time: 0.00404\n",
      "Iteration: 0167 Loss: 0.71353 Time: 0.00397\n",
      "Iteration: 0168 Loss: 0.69412 Time: 0.00399\n",
      "Iteration: 0169 Loss: 0.70995 Time: 0.00500\n",
      "Iteration: 0170 Loss: 0.69459 Time: 0.00400\n",
      "Iteration: 0171 Loss: 0.70564 Time: 0.00300\n",
      "Iteration: 0172 Loss: 0.69676 Time: 0.00300\n",
      "Iteration: 0173 Loss: 0.69277 Time: 0.00301\n",
      "Iteration: 0174 Loss: 0.70744 Time: 0.00300\n",
      "Iteration: 0175 Loss: 0.67821 Time: 0.00300\n",
      "Iteration: 0176 Loss: 0.66964 Time: 0.00300\n",
      "Iteration: 0177 Loss: 0.69121 Time: 0.00300\n",
      "Iteration: 0178 Loss: 0.68085 Time: 0.00300\n",
      "Iteration: 0179 Loss: 0.69353 Time: 0.00300\n",
      "Iteration: 0180 Loss: 0.67527 Time: 0.00195\n",
      "Iteration: 0181 Loss: 0.68508 Time: 0.00400\n",
      "Iteration: 0182 Loss: 0.69072 Time: 0.00401\n",
      "Iteration: 0183 Loss: 0.66575 Time: 0.00199\n",
      "Iteration: 0184 Loss: 0.68270 Time: 0.00199\n",
      "Iteration: 0185 Loss: 0.67975 Time: 0.00300\n",
      "Iteration: 0186 Loss: 0.66613 Time: 0.00300\n",
      "Iteration: 0187 Loss: 0.68968 Time: 0.00306\n",
      "Iteration: 0188 Loss: 0.68014 Time: 0.00400\n",
      "Iteration: 0189 Loss: 0.67277 Time: 0.00299\n",
      "Iteration: 0190 Loss: 0.68131 Time: 0.00396\n",
      "Iteration: 0191 Loss: 0.68598 Time: 0.00424\n",
      "Iteration: 0192 Loss: 0.66597 Time: 0.00199\n",
      "Iteration: 0193 Loss: 0.65778 Time: 0.00402\n",
      "Iteration: 0194 Loss: 0.66507 Time: 0.00299\n",
      "Iteration: 0195 Loss: 0.65167 Time: 0.00313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0196 Loss: 0.65902 Time: 0.00304\n",
      "Iteration: 0197 Loss: 0.66846 Time: 0.00461\n",
      "Iteration: 0198 Loss: 0.68034 Time: 0.00293\n",
      "Iteration: 0199 Loss: 0.66646 Time: 0.00404\n",
      "Iteration: 0200 Loss: 0.65382 Time: 0.00496\n",
      "Done! \n",
      "\n",
      "STEP 3/3 - MODEL EVALUATION \n",
      "\n",
      "Computing the final embedding vectors, for evaluation\n",
      "Done! \n",
      "\n",
      "Testing: link prediction\n",
      "Done! \n",
      "\n",
      "EXPERIMENTS ON MODEL 5 / 10 \n",
      "\n",
      "STEP 1/3 - PREPROCESSING STEPS \n",
      "\n",
      "Masking some edges from the training graph, for link prediction\n",
      "(validation set: 1.0 % of edges - test set: 10.0 % of edges)\n",
      "Done! \n",
      "\n",
      "Preprocessing node features\n",
      "Done! \n",
      "\n",
      "Running the Louvain algorithm for community detection\n",
      "as a preprocessing step for the encoder\n",
      "Done! Louvain has found 19 communities \n",
      "\n",
      "Setting up the model and the optimizer\n",
      "Done! \n",
      "\n",
      "Preprocessing on message passing matrices\n",
      "Done! \n",
      "\n",
      "Initializing TF session\n",
      "Done! \n",
      "\n",
      "STEP 2/3 - MODEL TRAINING \n",
      "\n",
      "Starting training\n",
      "Iteration: 0001 Loss: 1.84294 Time: 0.10463\n",
      "Iteration: 0002 Loss: 1.79830 Time: 0.00495\n",
      "Iteration: 0003 Loss: 1.65162 Time: 0.00404\n",
      "Iteration: 0004 Loss: 1.60767 Time: 0.00296\n",
      "Iteration: 0005 Loss: 1.56507 Time: 0.00300\n",
      "Iteration: 0006 Loss: 1.53486 Time: 0.00300\n",
      "Iteration: 0007 Loss: 1.64742 Time: 0.00300\n",
      "Iteration: 0008 Loss: 1.55101 Time: 0.00395\n",
      "Iteration: 0009 Loss: 1.57901 Time: 0.00452\n",
      "Iteration: 0010 Loss: 1.53686 Time: 0.00351\n",
      "Iteration: 0011 Loss: 1.48934 Time: 0.00397\n",
      "Iteration: 0012 Loss: 1.44478 Time: 0.00299\n",
      "Iteration: 0013 Loss: 1.35682 Time: 0.00400\n",
      "Iteration: 0014 Loss: 1.55781 Time: 0.00399\n",
      "Iteration: 0015 Loss: 1.54162 Time: 0.00397\n",
      "Iteration: 0016 Loss: 1.47308 Time: 0.00299\n",
      "Iteration: 0017 Loss: 1.42830 Time: 0.00301\n",
      "Iteration: 0018 Loss: 1.44054 Time: 0.00452\n",
      "Iteration: 0019 Loss: 1.42419 Time: 0.00301\n",
      "Iteration: 0020 Loss: 1.35694 Time: 0.00299\n",
      "Iteration: 0021 Loss: 1.48247 Time: 0.00281\n",
      "Iteration: 0022 Loss: 1.26346 Time: 0.00400\n",
      "Iteration: 0023 Loss: 1.34583 Time: 0.00296\n",
      "Iteration: 0024 Loss: 1.27037 Time: 0.00403\n",
      "Iteration: 0025 Loss: 1.26623 Time: 0.00398\n",
      "Iteration: 0026 Loss: 1.29330 Time: 0.00300\n",
      "Iteration: 0027 Loss: 1.28394 Time: 0.00349\n",
      "Iteration: 0028 Loss: 1.27683 Time: 0.00404\n",
      "Iteration: 0029 Loss: 1.20393 Time: 0.00300\n",
      "Iteration: 0030 Loss: 1.26708 Time: 0.00399\n",
      "Iteration: 0031 Loss: 1.23833 Time: 0.00400\n",
      "Iteration: 0032 Loss: 1.22889 Time: 0.00304\n",
      "Iteration: 0033 Loss: 1.20204 Time: 0.00402\n",
      "Iteration: 0034 Loss: 1.25385 Time: 0.00300\n",
      "Iteration: 0035 Loss: 1.25681 Time: 0.00302\n",
      "Iteration: 0036 Loss: 1.23635 Time: 0.00298\n",
      "Iteration: 0037 Loss: 1.13592 Time: 0.00400\n",
      "Iteration: 0038 Loss: 1.22514 Time: 0.00300\n",
      "Iteration: 0039 Loss: 1.18471 Time: 0.00300\n",
      "Iteration: 0040 Loss: 1.20054 Time: 0.00423\n",
      "Iteration: 0041 Loss: 1.16382 Time: 0.00229\n",
      "Iteration: 0042 Loss: 1.16697 Time: 0.00304\n",
      "Iteration: 0043 Loss: 1.19023 Time: 0.00396\n",
      "Iteration: 0044 Loss: 1.16256 Time: 0.00417\n",
      "Iteration: 0045 Loss: 1.12785 Time: 0.00299\n",
      "Iteration: 0046 Loss: 1.20931 Time: 0.00495\n",
      "Iteration: 0047 Loss: 1.21498 Time: 0.00406\n",
      "Iteration: 0048 Loss: 1.11653 Time: 0.00300\n",
      "Iteration: 0049 Loss: 1.22360 Time: 0.00401\n",
      "Iteration: 0050 Loss: 1.17811 Time: 0.00393\n",
      "Iteration: 0051 Loss: 1.07143 Time: 0.00405\n",
      "Iteration: 0052 Loss: 1.13127 Time: 0.00295\n",
      "Iteration: 0053 Loss: 1.13343 Time: 0.00411\n",
      "Iteration: 0054 Loss: 1.17846 Time: 0.00392\n",
      "Iteration: 0055 Loss: 1.06778 Time: 0.00397\n",
      "Iteration: 0056 Loss: 1.09891 Time: 0.00498\n",
      "Iteration: 0057 Loss: 1.07364 Time: 0.00300\n",
      "Iteration: 0058 Loss: 1.11878 Time: 0.00400\n",
      "Iteration: 0059 Loss: 1.07554 Time: 0.00406\n",
      "Iteration: 0060 Loss: 1.08945 Time: 0.00294\n",
      "Iteration: 0061 Loss: 1.05980 Time: 0.00410\n",
      "Iteration: 0062 Loss: 1.08039 Time: 0.00296\n",
      "Iteration: 0063 Loss: 1.06017 Time: 0.00494\n",
      "Iteration: 0064 Loss: 1.07154 Time: 0.00400\n",
      "Iteration: 0065 Loss: 1.03399 Time: 0.00408\n",
      "Iteration: 0066 Loss: 1.01948 Time: 0.00393\n",
      "Iteration: 0067 Loss: 0.98494 Time: 0.00408\n",
      "Iteration: 0068 Loss: 1.02326 Time: 0.00391\n",
      "Iteration: 0069 Loss: 1.05368 Time: 0.00500\n",
      "Iteration: 0070 Loss: 1.03732 Time: 0.00304\n",
      "Iteration: 0071 Loss: 1.04899 Time: 0.00396\n",
      "Iteration: 0072 Loss: 1.02493 Time: 0.00400\n",
      "Iteration: 0073 Loss: 1.01092 Time: 0.00303\n",
      "Iteration: 0074 Loss: 0.99136 Time: 0.00299\n",
      "Iteration: 0075 Loss: 1.02259 Time: 0.00398\n",
      "Iteration: 0076 Loss: 1.00824 Time: 0.00302\n",
      "Iteration: 0077 Loss: 0.96868 Time: 0.00297\n",
      "Iteration: 0078 Loss: 0.95324 Time: 0.00401\n",
      "Iteration: 0079 Loss: 1.00420 Time: 0.00299\n",
      "Iteration: 0080 Loss: 0.96148 Time: 0.00300\n",
      "Iteration: 0081 Loss: 0.90444 Time: 0.00299\n",
      "Iteration: 0082 Loss: 0.97146 Time: 0.00301\n",
      "Iteration: 0083 Loss: 0.96545 Time: 0.00300\n",
      "Iteration: 0084 Loss: 0.97367 Time: 0.00400\n",
      "Iteration: 0085 Loss: 0.98584 Time: 0.00430\n",
      "Iteration: 0086 Loss: 0.93256 Time: 0.00270\n",
      "Iteration: 0087 Loss: 0.94792 Time: 0.00300\n",
      "Iteration: 0088 Loss: 0.93884 Time: 0.00296\n",
      "Iteration: 0089 Loss: 0.94836 Time: 0.00396\n",
      "Iteration: 0090 Loss: 0.93590 Time: 0.00307\n",
      "Iteration: 0091 Loss: 0.94043 Time: 0.00403\n",
      "Iteration: 0092 Loss: 0.97847 Time: 0.00296\n",
      "Iteration: 0093 Loss: 0.91142 Time: 0.00401\n",
      "Iteration: 0094 Loss: 0.94130 Time: 0.00298\n",
      "Iteration: 0095 Loss: 0.89251 Time: 0.00296\n",
      "Iteration: 0096 Loss: 0.90554 Time: 0.00400\n",
      "Iteration: 0097 Loss: 0.92827 Time: 0.00401\n",
      "Iteration: 0098 Loss: 0.88463 Time: 0.00404\n",
      "Iteration: 0099 Loss: 0.89622 Time: 0.00402\n",
      "Iteration: 0100 Loss: 0.90694 Time: 0.00392\n",
      "Iteration: 0101 Loss: 0.90340 Time: 0.00400\n",
      "Iteration: 0102 Loss: 0.89231 Time: 0.00447\n",
      "Iteration: 0103 Loss: 0.87455 Time: 0.00353\n",
      "Iteration: 0104 Loss: 0.87233 Time: 0.00302\n",
      "Iteration: 0105 Loss: 0.85756 Time: 0.00393\n",
      "Iteration: 0106 Loss: 0.87480 Time: 0.00402\n",
      "Iteration: 0107 Loss: 0.85318 Time: 0.00303\n",
      "Iteration: 0108 Loss: 0.88463 Time: 0.00302\n",
      "Iteration: 0109 Loss: 0.84958 Time: 0.00298\n",
      "Iteration: 0110 Loss: 0.84553 Time: 0.00295\n",
      "Iteration: 0111 Loss: 0.89367 Time: 0.00404\n",
      "Iteration: 0112 Loss: 0.88037 Time: 0.00304\n",
      "Iteration: 0113 Loss: 0.87224 Time: 0.00399\n",
      "Iteration: 0114 Loss: 0.88410 Time: 0.00300\n",
      "Iteration: 0115 Loss: 0.84412 Time: 0.00434\n",
      "Iteration: 0116 Loss: 0.84015 Time: 0.00294\n",
      "Iteration: 0117 Loss: 0.82057 Time: 0.00400\n",
      "Iteration: 0118 Loss: 0.86509 Time: 0.00395\n",
      "Iteration: 0119 Loss: 0.85272 Time: 0.00402\n",
      "Iteration: 0120 Loss: 0.83540 Time: 0.00299\n",
      "Iteration: 0121 Loss: 0.81583 Time: 0.00400\n",
      "Iteration: 0122 Loss: 0.81451 Time: 0.00309\n",
      "Iteration: 0123 Loss: 0.81525 Time: 0.00486\n",
      "Iteration: 0124 Loss: 0.83572 Time: 0.00301\n",
      "Iteration: 0125 Loss: 0.79267 Time: 0.00400\n",
      "Iteration: 0126 Loss: 0.82360 Time: 0.00297\n",
      "Iteration: 0127 Loss: 0.81834 Time: 0.00299\n",
      "Iteration: 0128 Loss: 0.79651 Time: 0.00401\n",
      "Iteration: 0129 Loss: 0.79630 Time: 0.00299\n",
      "Iteration: 0130 Loss: 0.78797 Time: 0.00300\n",
      "Iteration: 0131 Loss: 0.77425 Time: 0.00305\n",
      "Iteration: 0132 Loss: 0.75957 Time: 0.00347\n",
      "Iteration: 0133 Loss: 0.79728 Time: 0.00349\n",
      "Iteration: 0134 Loss: 0.81061 Time: 0.00372\n",
      "Iteration: 0135 Loss: 0.81648 Time: 0.00380\n",
      "Iteration: 0136 Loss: 0.79857 Time: 0.00305\n",
      "Iteration: 0137 Loss: 0.80141 Time: 0.00300\n",
      "Iteration: 0138 Loss: 0.75848 Time: 0.00398\n",
      "Iteration: 0139 Loss: 0.79365 Time: 0.00301\n",
      "Iteration: 0140 Loss: 0.78289 Time: 0.00299\n",
      "Iteration: 0141 Loss: 0.73697 Time: 0.00300\n",
      "Iteration: 0142 Loss: 0.74610 Time: 0.00399\n",
      "Iteration: 0143 Loss: 0.76939 Time: 0.00342\n",
      "Iteration: 0144 Loss: 0.75292 Time: 0.00358\n",
      "Iteration: 0145 Loss: 0.74399 Time: 0.00301\n",
      "Iteration: 0146 Loss: 0.74369 Time: 0.00399\n",
      "Iteration: 0147 Loss: 0.73197 Time: 0.00306\n",
      "Iteration: 0148 Loss: 0.74802 Time: 0.00295\n",
      "Iteration: 0149 Loss: 0.74778 Time: 0.00295\n",
      "Iteration: 0150 Loss: 0.71907 Time: 0.00438\n",
      "Iteration: 0151 Loss: 0.74628 Time: 0.00526\n",
      "Iteration: 0152 Loss: 0.73449 Time: 0.00298\n",
      "Iteration: 0153 Loss: 0.75334 Time: 0.00303\n",
      "Iteration: 0154 Loss: 0.76433 Time: 0.00395\n",
      "Iteration: 0155 Loss: 0.74830 Time: 0.00400\n",
      "Iteration: 0156 Loss: 0.73225 Time: 0.00399\n",
      "Iteration: 0157 Loss: 0.73485 Time: 0.00399\n",
      "Iteration: 0158 Loss: 0.73127 Time: 0.00400\n",
      "Iteration: 0159 Loss: 0.72304 Time: 0.00500\n",
      "Iteration: 0160 Loss: 0.74949 Time: 0.00300\n",
      "Iteration: 0161 Loss: 0.70419 Time: 0.00307\n",
      "Iteration: 0162 Loss: 0.71795 Time: 0.00300\n",
      "Iteration: 0163 Loss: 0.70898 Time: 0.00295\n",
      "Iteration: 0164 Loss: 0.70192 Time: 0.00410\n",
      "Iteration: 0165 Loss: 0.72475 Time: 0.00301\n",
      "Iteration: 0166 Loss: 0.72221 Time: 0.00399\n",
      "Iteration: 0167 Loss: 0.72533 Time: 0.00300\n",
      "Iteration: 0168 Loss: 0.71849 Time: 0.00400\n",
      "Iteration: 0169 Loss: 0.71240 Time: 0.00307\n",
      "Iteration: 0170 Loss: 0.71259 Time: 0.00298\n",
      "Iteration: 0171 Loss: 0.70480 Time: 0.00300\n",
      "Iteration: 0172 Loss: 0.71816 Time: 0.00496\n",
      "Iteration: 0173 Loss: 0.70091 Time: 0.00300\n",
      "Iteration: 0174 Loss: 0.69127 Time: 0.00400\n",
      "Iteration: 0175 Loss: 0.68667 Time: 0.00306\n",
      "Iteration: 0176 Loss: 0.70244 Time: 0.00396\n",
      "Iteration: 0177 Loss: 0.69378 Time: 0.00298\n",
      "Iteration: 0178 Loss: 0.68763 Time: 0.00300\n",
      "Iteration: 0179 Loss: 0.72375 Time: 0.00300\n",
      "Iteration: 0180 Loss: 0.68593 Time: 0.00301\n",
      "Iteration: 0181 Loss: 0.67666 Time: 0.00301\n",
      "Iteration: 0182 Loss: 0.69750 Time: 0.00304\n",
      "Iteration: 0183 Loss: 0.67263 Time: 0.00400\n",
      "Iteration: 0184 Loss: 0.70126 Time: 0.00195\n",
      "Iteration: 0185 Loss: 0.69488 Time: 0.00297\n",
      "Iteration: 0186 Loss: 0.66612 Time: 0.00399\n",
      "Iteration: 0187 Loss: 0.69643 Time: 0.00402\n",
      "Iteration: 0188 Loss: 0.66852 Time: 0.00504\n",
      "Iteration: 0189 Loss: 0.68148 Time: 0.00298\n",
      "Iteration: 0190 Loss: 0.67724 Time: 0.00497\n",
      "Iteration: 0191 Loss: 0.67023 Time: 0.00350\n",
      "Iteration: 0192 Loss: 0.67333 Time: 0.00445\n",
      "Iteration: 0193 Loss: 0.65780 Time: 0.00300\n",
      "Iteration: 0194 Loss: 0.66819 Time: 0.00298\n",
      "Iteration: 0195 Loss: 0.66962 Time: 0.00565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0196 Loss: 0.66061 Time: 0.00336\n",
      "Iteration: 0197 Loss: 0.66313 Time: 0.00300\n",
      "Iteration: 0198 Loss: 0.67603 Time: 0.00295\n",
      "Iteration: 0199 Loss: 0.67139 Time: 0.00399\n",
      "Iteration: 0200 Loss: 0.67474 Time: 0.00300\n",
      "Done! \n",
      "\n",
      "STEP 3/3 - MODEL EVALUATION \n",
      "\n",
      "Computing the final embedding vectors, for evaluation\n",
      "Done! \n",
      "\n",
      "Testing: link prediction\n",
      "Done! \n",
      "\n",
      "EXPERIMENTS ON MODEL 6 / 10 \n",
      "\n",
      "STEP 1/3 - PREPROCESSING STEPS \n",
      "\n",
      "Masking some edges from the training graph, for link prediction\n",
      "(validation set: 1.0 % of edges - test set: 10.0 % of edges)\n",
      "Done! \n",
      "\n",
      "Preprocessing node features\n",
      "Done! \n",
      "\n",
      "Running the Louvain algorithm for community detection\n",
      "as a preprocessing step for the encoder\n",
      "Done! Louvain has found 17 communities \n",
      "\n",
      "Setting up the model and the optimizer\n",
      "Done! \n",
      "\n",
      "Preprocessing on message passing matrices\n",
      "Done! \n",
      "\n",
      "Initializing TF session\n",
      "Done! \n",
      "\n",
      "STEP 2/3 - MODEL TRAINING \n",
      "\n",
      "Starting training\n",
      "Iteration: 0001 Loss: 1.71497 Time: 0.12204\n",
      "Iteration: 0002 Loss: 1.72950 Time: 0.00398\n",
      "Iteration: 0003 Loss: 1.65705 Time: 0.00303\n",
      "Iteration: 0004 Loss: 1.63166 Time: 0.00401\n",
      "Iteration: 0005 Loss: 1.65299 Time: 0.00402\n",
      "Iteration: 0006 Loss: 1.65426 Time: 0.00304\n",
      "Iteration: 0007 Loss: 1.61169 Time: 0.00308\n",
      "Iteration: 0008 Loss: 1.56209 Time: 0.00387\n",
      "Iteration: 0009 Loss: 1.48307 Time: 0.00612\n",
      "Iteration: 0010 Loss: 1.59208 Time: 0.00391\n",
      "Iteration: 0011 Loss: 1.42879 Time: 0.00300\n",
      "Iteration: 0012 Loss: 1.42616 Time: 0.00302\n",
      "Iteration: 0013 Loss: 1.52974 Time: 0.00397\n",
      "Iteration: 0014 Loss: 1.48876 Time: 0.00402\n",
      "Iteration: 0015 Loss: 1.44974 Time: 0.00297\n",
      "Iteration: 0016 Loss: 1.36613 Time: 0.00593\n",
      "Iteration: 0017 Loss: 1.41132 Time: 0.00300\n",
      "Iteration: 0018 Loss: 1.38892 Time: 0.00393\n",
      "Iteration: 0019 Loss: 1.36143 Time: 0.00308\n",
      "Iteration: 0020 Loss: 1.36151 Time: 0.00299\n",
      "Iteration: 0021 Loss: 1.38791 Time: 0.00403\n",
      "Iteration: 0022 Loss: 1.41670 Time: 0.00299\n",
      "Iteration: 0023 Loss: 1.26400 Time: 0.00296\n",
      "Iteration: 0024 Loss: 1.30916 Time: 0.00405\n",
      "Iteration: 0025 Loss: 1.32560 Time: 0.00305\n",
      "Iteration: 0026 Loss: 1.29779 Time: 0.00392\n",
      "Iteration: 0027 Loss: 1.27458 Time: 0.00301\n",
      "Iteration: 0028 Loss: 1.25149 Time: 0.00305\n",
      "Iteration: 0029 Loss: 1.28375 Time: 0.00397\n",
      "Iteration: 0030 Loss: 1.32798 Time: 0.00400\n",
      "Iteration: 0031 Loss: 1.26126 Time: 0.00300\n",
      "Iteration: 0032 Loss: 1.19272 Time: 0.00400\n",
      "Iteration: 0033 Loss: 1.25336 Time: 0.00407\n",
      "Iteration: 0034 Loss: 1.18224 Time: 0.00395\n",
      "Iteration: 0035 Loss: 1.20766 Time: 0.00300\n",
      "Iteration: 0036 Loss: 1.21089 Time: 0.00395\n",
      "Iteration: 0037 Loss: 1.21197 Time: 0.00305\n",
      "Iteration: 0038 Loss: 1.23064 Time: 0.00294\n",
      "Iteration: 0039 Loss: 1.23480 Time: 0.00400\n",
      "Iteration: 0040 Loss: 1.17957 Time: 0.00304\n",
      "Iteration: 0041 Loss: 1.15281 Time: 0.00304\n",
      "Iteration: 0042 Loss: 1.21199 Time: 0.00503\n",
      "Iteration: 0043 Loss: 1.17098 Time: 0.00396\n",
      "Iteration: 0044 Loss: 1.18031 Time: 0.00299\n",
      "Iteration: 0045 Loss: 1.20532 Time: 0.00302\n",
      "Iteration: 0046 Loss: 1.12153 Time: 0.00398\n",
      "Iteration: 0047 Loss: 1.17540 Time: 0.00395\n",
      "Iteration: 0048 Loss: 1.12653 Time: 0.00305\n",
      "Iteration: 0049 Loss: 1.17925 Time: 0.00397\n",
      "Iteration: 0050 Loss: 1.12470 Time: 0.00398\n",
      "Iteration: 0051 Loss: 1.11793 Time: 0.00500\n",
      "Iteration: 0052 Loss: 1.11208 Time: 0.00305\n",
      "Iteration: 0053 Loss: 1.03663 Time: 0.00395\n",
      "Iteration: 0054 Loss: 1.10604 Time: 0.00309\n",
      "Iteration: 0055 Loss: 1.11453 Time: 0.00300\n",
      "Iteration: 0056 Loss: 1.07940 Time: 0.00203\n",
      "Iteration: 0057 Loss: 1.09198 Time: 0.00297\n",
      "Iteration: 0058 Loss: 1.07714 Time: 0.00303\n",
      "Iteration: 0059 Loss: 1.07472 Time: 0.00300\n",
      "Iteration: 0060 Loss: 1.04434 Time: 0.00299\n",
      "Iteration: 0061 Loss: 1.07612 Time: 0.00304\n",
      "Iteration: 0062 Loss: 1.05359 Time: 0.00300\n",
      "Iteration: 0063 Loss: 1.08335 Time: 0.00301\n",
      "Iteration: 0064 Loss: 1.05835 Time: 0.00395\n",
      "Iteration: 0065 Loss: 1.02534 Time: 0.00298\n",
      "Iteration: 0066 Loss: 1.01489 Time: 0.00303\n",
      "Iteration: 0067 Loss: 1.05520 Time: 0.00410\n",
      "Iteration: 0068 Loss: 1.04374 Time: 0.00388\n",
      "Iteration: 0069 Loss: 1.02136 Time: 0.00403\n",
      "Iteration: 0070 Loss: 1.00247 Time: 0.00394\n",
      "Iteration: 0071 Loss: 1.01887 Time: 0.00304\n",
      "Iteration: 0072 Loss: 1.05685 Time: 0.00396\n",
      "Iteration: 0073 Loss: 1.01153 Time: 0.00407\n",
      "Iteration: 0074 Loss: 0.95663 Time: 0.00443\n",
      "Iteration: 0075 Loss: 1.00521 Time: 0.00360\n",
      "Iteration: 0076 Loss: 0.97149 Time: 0.00294\n",
      "Iteration: 0077 Loss: 1.03764 Time: 0.00399\n",
      "Iteration: 0078 Loss: 0.99274 Time: 0.00407\n",
      "Iteration: 0079 Loss: 0.99061 Time: 0.00297\n",
      "Iteration: 0080 Loss: 0.95768 Time: 0.00306\n",
      "Iteration: 0081 Loss: 0.96612 Time: 0.00398\n",
      "Iteration: 0082 Loss: 0.97100 Time: 0.00298\n",
      "Iteration: 0083 Loss: 0.96100 Time: 0.00304\n",
      "Iteration: 0084 Loss: 0.96004 Time: 0.00298\n",
      "Iteration: 0085 Loss: 1.00330 Time: 0.00403\n",
      "Iteration: 0086 Loss: 0.96092 Time: 0.00401\n",
      "Iteration: 0087 Loss: 0.91502 Time: 0.00401\n",
      "Iteration: 0088 Loss: 0.93700 Time: 0.00308\n",
      "Iteration: 0089 Loss: 0.89952 Time: 0.00492\n",
      "Iteration: 0090 Loss: 0.93694 Time: 0.00299\n",
      "Iteration: 0091 Loss: 0.93119 Time: 0.00397\n",
      "Iteration: 0092 Loss: 0.90950 Time: 0.00405\n",
      "Iteration: 0093 Loss: 0.91396 Time: 0.00398\n",
      "Iteration: 0094 Loss: 0.91800 Time: 0.00401\n",
      "Iteration: 0095 Loss: 0.94442 Time: 0.00499\n",
      "Iteration: 0096 Loss: 0.91795 Time: 0.00400\n",
      "Iteration: 0097 Loss: 0.86968 Time: 0.00399\n",
      "Iteration: 0098 Loss: 0.89808 Time: 0.00507\n",
      "Iteration: 0099 Loss: 0.89493 Time: 0.00300\n",
      "Iteration: 0100 Loss: 0.87042 Time: 0.00301\n",
      "Iteration: 0101 Loss: 0.94633 Time: 0.00304\n",
      "Iteration: 0102 Loss: 0.87411 Time: 0.00394\n",
      "Iteration: 0103 Loss: 0.90822 Time: 0.00397\n",
      "Iteration: 0104 Loss: 0.87714 Time: 0.00435\n",
      "Iteration: 0105 Loss: 0.91471 Time: 0.00371\n",
      "Iteration: 0106 Loss: 0.86663 Time: 0.00394\n",
      "Iteration: 0107 Loss: 0.88633 Time: 0.00304\n",
      "Iteration: 0108 Loss: 0.84588 Time: 0.00398\n",
      "Iteration: 0109 Loss: 0.87349 Time: 0.00402\n",
      "Iteration: 0110 Loss: 0.89329 Time: 0.00301\n",
      "Iteration: 0111 Loss: 0.88575 Time: 0.00298\n",
      "Iteration: 0112 Loss: 0.88834 Time: 0.00401\n",
      "Iteration: 0113 Loss: 0.84719 Time: 0.00402\n",
      "Iteration: 0114 Loss: 0.84820 Time: 0.00398\n",
      "Iteration: 0115 Loss: 0.84957 Time: 0.00413\n",
      "Iteration: 0116 Loss: 0.82758 Time: 0.00300\n",
      "Iteration: 0117 Loss: 0.82264 Time: 0.00401\n",
      "Iteration: 0118 Loss: 0.81568 Time: 0.00307\n",
      "Iteration: 0119 Loss: 0.83687 Time: 0.00401\n",
      "Iteration: 0120 Loss: 0.82009 Time: 0.00312\n",
      "Iteration: 0121 Loss: 0.86013 Time: 0.00403\n",
      "Iteration: 0122 Loss: 0.82483 Time: 0.00301\n",
      "Iteration: 0123 Loss: 0.82633 Time: 0.00453\n",
      "Iteration: 0124 Loss: 0.84733 Time: 0.00405\n",
      "Iteration: 0125 Loss: 0.80835 Time: 0.00310\n",
      "Iteration: 0126 Loss: 0.80777 Time: 0.00297\n",
      "Iteration: 0127 Loss: 0.81642 Time: 0.00300\n",
      "Iteration: 0128 Loss: 0.83086 Time: 0.00295\n",
      "Iteration: 0129 Loss: 0.80908 Time: 0.00305\n",
      "Iteration: 0130 Loss: 0.81348 Time: 0.00306\n",
      "Iteration: 0131 Loss: 0.77925 Time: 0.00310\n",
      "Iteration: 0132 Loss: 0.79732 Time: 0.00303\n",
      "Iteration: 0133 Loss: 0.78164 Time: 0.00304\n",
      "Iteration: 0134 Loss: 0.76574 Time: 0.00410\n",
      "Iteration: 0135 Loss: 0.75446 Time: 0.00301\n",
      "Iteration: 0136 Loss: 0.80910 Time: 0.00395\n",
      "Iteration: 0137 Loss: 0.77250 Time: 0.00304\n",
      "Iteration: 0138 Loss: 0.79833 Time: 0.00399\n",
      "Iteration: 0139 Loss: 0.78171 Time: 0.00296\n",
      "Iteration: 0140 Loss: 0.78772 Time: 0.00405\n",
      "Iteration: 0141 Loss: 0.76703 Time: 0.00306\n",
      "Iteration: 0142 Loss: 0.76761 Time: 0.00391\n",
      "Iteration: 0143 Loss: 0.76452 Time: 0.00444\n",
      "Iteration: 0144 Loss: 0.76375 Time: 0.00557\n",
      "Iteration: 0145 Loss: 0.75022 Time: 0.00301\n",
      "Iteration: 0146 Loss: 0.77234 Time: 0.00287\n",
      "Iteration: 0147 Loss: 0.74808 Time: 0.00303\n",
      "Iteration: 0148 Loss: 0.72964 Time: 0.00304\n",
      "Iteration: 0149 Loss: 0.74068 Time: 0.00314\n",
      "Iteration: 0150 Loss: 0.77125 Time: 0.00301\n",
      "Iteration: 0151 Loss: 0.72767 Time: 0.00307\n",
      "Iteration: 0152 Loss: 0.75801 Time: 0.00299\n",
      "Iteration: 0153 Loss: 0.72382 Time: 0.00399\n",
      "Iteration: 0154 Loss: 0.73598 Time: 0.00304\n",
      "Iteration: 0155 Loss: 0.74391 Time: 0.00307\n",
      "Iteration: 0156 Loss: 0.73498 Time: 0.00296\n",
      "Iteration: 0157 Loss: 0.75292 Time: 0.00316\n",
      "Iteration: 0158 Loss: 0.72459 Time: 0.00306\n",
      "Iteration: 0159 Loss: 0.73367 Time: 0.00395\n",
      "Iteration: 0160 Loss: 0.71054 Time: 0.00302\n",
      "Iteration: 0161 Loss: 0.71714 Time: 0.00402\n",
      "Iteration: 0162 Loss: 0.72123 Time: 0.00361\n",
      "Iteration: 0163 Loss: 0.73175 Time: 0.00399\n",
      "Iteration: 0164 Loss: 0.72609 Time: 0.00400\n",
      "Iteration: 0165 Loss: 0.73759 Time: 0.00407\n",
      "Iteration: 0166 Loss: 0.71490 Time: 0.00399\n",
      "Iteration: 0167 Loss: 0.69663 Time: 0.00298\n",
      "Iteration: 0168 Loss: 0.72077 Time: 0.00400\n",
      "Iteration: 0169 Loss: 0.72834 Time: 0.00454\n",
      "Iteration: 0170 Loss: 0.71967 Time: 0.00308\n",
      "Iteration: 0171 Loss: 0.67436 Time: 0.00337\n",
      "Iteration: 0172 Loss: 0.72506 Time: 0.00400\n",
      "Iteration: 0173 Loss: 0.70856 Time: 0.00402\n",
      "Iteration: 0174 Loss: 0.67449 Time: 0.00397\n",
      "Iteration: 0175 Loss: 0.68929 Time: 0.00310\n",
      "Iteration: 0176 Loss: 0.71099 Time: 0.00297\n",
      "Iteration: 0177 Loss: 0.69347 Time: 0.00434\n",
      "Iteration: 0178 Loss: 0.66865 Time: 0.00404\n",
      "Iteration: 0179 Loss: 0.68037 Time: 0.00405\n",
      "Iteration: 0180 Loss: 0.70571 Time: 0.00294\n",
      "Iteration: 0181 Loss: 0.69973 Time: 0.00396\n",
      "Iteration: 0182 Loss: 0.69364 Time: 0.00317\n",
      "Iteration: 0183 Loss: 0.71494 Time: 0.00303\n",
      "Iteration: 0184 Loss: 0.69331 Time: 0.00304\n",
      "Iteration: 0185 Loss: 0.70154 Time: 0.00352\n",
      "Iteration: 0186 Loss: 0.66323 Time: 0.00400\n",
      "Iteration: 0187 Loss: 0.69981 Time: 0.00408\n",
      "Iteration: 0188 Loss: 0.68743 Time: 0.00298\n",
      "Iteration: 0189 Loss: 0.67118 Time: 0.00305\n",
      "Iteration: 0190 Loss: 0.68277 Time: 0.00300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0191 Loss: 0.67240 Time: 0.00400\n",
      "Iteration: 0192 Loss: 0.67730 Time: 0.00295\n",
      "Iteration: 0193 Loss: 0.65997 Time: 0.00299\n",
      "Iteration: 0194 Loss: 0.65887 Time: 0.00403\n",
      "Iteration: 0195 Loss: 0.66973 Time: 0.00300\n",
      "Iteration: 0196 Loss: 0.65982 Time: 0.00435\n",
      "Iteration: 0197 Loss: 0.67909 Time: 0.00561\n",
      "Iteration: 0198 Loss: 0.66450 Time: 0.00404\n",
      "Iteration: 0199 Loss: 0.66464 Time: 0.00297\n",
      "Iteration: 0200 Loss: 0.66139 Time: 0.00410\n",
      "Done! \n",
      "\n",
      "STEP 3/3 - MODEL EVALUATION \n",
      "\n",
      "Computing the final embedding vectors, for evaluation\n",
      "Done! \n",
      "\n",
      "Testing: link prediction\n",
      "Done! \n",
      "\n",
      "EXPERIMENTS ON MODEL 7 / 10 \n",
      "\n",
      "STEP 1/3 - PREPROCESSING STEPS \n",
      "\n",
      "Masking some edges from the training graph, for link prediction\n",
      "(validation set: 1.0 % of edges - test set: 10.0 % of edges)\n",
      "Done! \n",
      "\n",
      "Preprocessing node features\n",
      "Done! \n",
      "\n",
      "Running the Louvain algorithm for community detection\n",
      "as a preprocessing step for the encoder\n",
      "Done! Louvain has found 21 communities \n",
      "\n",
      "Setting up the model and the optimizer\n",
      "Done! \n",
      "\n",
      "Preprocessing on message passing matrices\n",
      "Done! \n",
      "\n",
      "Initializing TF session\n",
      "Done! \n",
      "\n",
      "STEP 2/3 - MODEL TRAINING \n",
      "\n",
      "Starting training\n",
      "Iteration: 0001 Loss: 1.68333 Time: 0.14700\n",
      "Iteration: 0002 Loss: 1.80066 Time: 0.00301\n",
      "Iteration: 0003 Loss: 1.68479 Time: 0.00499\n",
      "Iteration: 0004 Loss: 1.69837 Time: 0.00401\n",
      "Iteration: 0005 Loss: 1.62299 Time: 0.00301\n",
      "Iteration: 0006 Loss: 1.67838 Time: 0.00300\n",
      "Iteration: 0007 Loss: 1.67045 Time: 0.00300\n",
      "Iteration: 0008 Loss: 1.60515 Time: 0.00399\n",
      "Iteration: 0009 Loss: 1.59173 Time: 0.00401\n",
      "Iteration: 0010 Loss: 1.61885 Time: 0.00400\n",
      "Iteration: 0011 Loss: 1.51197 Time: 0.00299\n",
      "Iteration: 0012 Loss: 1.56010 Time: 0.00401\n",
      "Iteration: 0013 Loss: 1.47955 Time: 0.00411\n",
      "Iteration: 0014 Loss: 1.49043 Time: 0.00297\n",
      "Iteration: 0015 Loss: 1.50333 Time: 0.00305\n",
      "Iteration: 0016 Loss: 1.48407 Time: 0.00413\n",
      "Iteration: 0017 Loss: 1.33643 Time: 0.00388\n",
      "Iteration: 0018 Loss: 1.37620 Time: 0.00295\n",
      "Iteration: 0019 Loss: 1.36194 Time: 0.00401\n",
      "Iteration: 0020 Loss: 1.44377 Time: 0.00299\n",
      "Iteration: 0021 Loss: 1.34705 Time: 0.00300\n",
      "Iteration: 0022 Loss: 1.35364 Time: 0.00305\n",
      "Iteration: 0023 Loss: 1.37977 Time: 0.00299\n",
      "Iteration: 0024 Loss: 1.33336 Time: 0.00304\n",
      "Iteration: 0025 Loss: 1.38864 Time: 0.00305\n",
      "Iteration: 0026 Loss: 1.35815 Time: 0.00304\n",
      "Iteration: 0027 Loss: 1.31992 Time: 0.00402\n",
      "Iteration: 0028 Loss: 1.32486 Time: 0.00304\n",
      "Iteration: 0029 Loss: 1.23030 Time: 0.00497\n",
      "Iteration: 0030 Loss: 1.25215 Time: 0.00303\n",
      "Iteration: 0031 Loss: 1.22690 Time: 0.00395\n",
      "Iteration: 0032 Loss: 1.27723 Time: 0.00407\n",
      "Iteration: 0033 Loss: 1.24240 Time: 0.00393\n",
      "Iteration: 0034 Loss: 1.18625 Time: 0.00405\n",
      "Iteration: 0035 Loss: 1.16317 Time: 0.00397\n",
      "Iteration: 0036 Loss: 1.21171 Time: 0.00308\n",
      "Iteration: 0037 Loss: 1.22392 Time: 0.00394\n",
      "Iteration: 0038 Loss: 1.16866 Time: 0.00405\n",
      "Iteration: 0039 Loss: 1.15487 Time: 0.00395\n",
      "Iteration: 0040 Loss: 1.24055 Time: 0.00305\n",
      "Iteration: 0041 Loss: 1.22404 Time: 0.00503\n",
      "Iteration: 0042 Loss: 1.17917 Time: 0.00496\n",
      "Iteration: 0043 Loss: 1.19789 Time: 0.00396\n",
      "Iteration: 0044 Loss: 1.21122 Time: 0.00400\n",
      "Iteration: 0045 Loss: 1.20335 Time: 0.00400\n",
      "Iteration: 0046 Loss: 1.14417 Time: 0.00400\n",
      "Iteration: 0047 Loss: 1.12082 Time: 0.00307\n",
      "Iteration: 0048 Loss: 1.20703 Time: 0.00397\n",
      "Iteration: 0049 Loss: 1.08722 Time: 0.00397\n",
      "Iteration: 0050 Loss: 1.06969 Time: 0.00299\n",
      "Iteration: 0051 Loss: 1.09831 Time: 0.00398\n",
      "Iteration: 0052 Loss: 1.09079 Time: 0.00395\n",
      "Iteration: 0053 Loss: 1.08812 Time: 0.00305\n",
      "Iteration: 0054 Loss: 1.11123 Time: 0.00398\n",
      "Iteration: 0055 Loss: 1.07908 Time: 0.00302\n",
      "Iteration: 0056 Loss: 1.07848 Time: 0.00407\n",
      "Iteration: 0057 Loss: 1.09606 Time: 0.00298\n",
      "Iteration: 0058 Loss: 1.09091 Time: 0.00498\n",
      "Iteration: 0059 Loss: 1.13852 Time: 0.00400\n",
      "Iteration: 0060 Loss: 1.06273 Time: 0.00402\n",
      "Iteration: 0061 Loss: 1.05597 Time: 0.00400\n",
      "Iteration: 0062 Loss: 0.98289 Time: 0.00394\n",
      "Iteration: 0063 Loss: 1.06371 Time: 0.00402\n",
      "Iteration: 0064 Loss: 1.03622 Time: 0.00402\n",
      "Iteration: 0065 Loss: 1.05822 Time: 0.00304\n",
      "Iteration: 0066 Loss: 1.03036 Time: 0.00302\n",
      "Iteration: 0067 Loss: 1.01035 Time: 0.00400\n",
      "Iteration: 0068 Loss: 1.00860 Time: 0.00300\n",
      "Iteration: 0069 Loss: 1.05773 Time: 0.00327\n",
      "Iteration: 0070 Loss: 1.02413 Time: 0.00381\n",
      "Iteration: 0071 Loss: 0.97051 Time: 0.00391\n",
      "Iteration: 0072 Loss: 1.01581 Time: 0.00404\n",
      "Iteration: 0073 Loss: 1.02446 Time: 0.00398\n",
      "Iteration: 0074 Loss: 0.98354 Time: 0.00303\n",
      "Iteration: 0075 Loss: 0.98428 Time: 0.00498\n",
      "Iteration: 0076 Loss: 1.04534 Time: 0.00305\n",
      "Iteration: 0077 Loss: 0.98299 Time: 0.00305\n",
      "Iteration: 0078 Loss: 1.01395 Time: 0.00402\n",
      "Iteration: 0079 Loss: 1.03245 Time: 0.00298\n",
      "Iteration: 0080 Loss: 0.92434 Time: 0.00304\n",
      "Iteration: 0081 Loss: 0.99786 Time: 0.00296\n",
      "Iteration: 0082 Loss: 0.97590 Time: 0.00304\n",
      "Iteration: 0083 Loss: 0.96392 Time: 0.00300\n",
      "Iteration: 0084 Loss: 0.97553 Time: 0.00403\n",
      "Iteration: 0085 Loss: 1.00450 Time: 0.00195\n",
      "Iteration: 0086 Loss: 0.96055 Time: 0.00404\n",
      "Iteration: 0087 Loss: 0.96126 Time: 0.00500\n",
      "Iteration: 0088 Loss: 0.90863 Time: 0.00400\n",
      "Iteration: 0089 Loss: 0.95991 Time: 0.00302\n",
      "Iteration: 0090 Loss: 0.95026 Time: 0.00401\n",
      "Iteration: 0091 Loss: 0.93666 Time: 0.00498\n",
      "Iteration: 0092 Loss: 0.92711 Time: 0.00396\n",
      "Iteration: 0093 Loss: 0.93499 Time: 0.00403\n",
      "Iteration: 0094 Loss: 0.92797 Time: 0.00300\n",
      "Iteration: 0095 Loss: 0.92336 Time: 0.00403\n",
      "Iteration: 0096 Loss: 0.90120 Time: 0.00301\n",
      "Iteration: 0097 Loss: 0.89062 Time: 0.00409\n",
      "Iteration: 0098 Loss: 0.92017 Time: 0.00301\n",
      "Iteration: 0099 Loss: 0.91713 Time: 0.00398\n",
      "Iteration: 0100 Loss: 0.88887 Time: 0.00399\n",
      "Iteration: 0101 Loss: 0.90340 Time: 0.00415\n",
      "Iteration: 0102 Loss: 0.87195 Time: 0.00302\n",
      "Iteration: 0103 Loss: 0.88712 Time: 0.00299\n",
      "Iteration: 0104 Loss: 0.88631 Time: 0.00401\n",
      "Iteration: 0105 Loss: 0.84732 Time: 0.00396\n",
      "Iteration: 0106 Loss: 0.89299 Time: 0.00407\n",
      "Iteration: 0107 Loss: 0.83784 Time: 0.00400\n",
      "Iteration: 0108 Loss: 0.86703 Time: 0.00403\n",
      "Iteration: 0109 Loss: 0.89170 Time: 0.00296\n",
      "Iteration: 0110 Loss: 0.83993 Time: 0.00299\n",
      "Iteration: 0111 Loss: 0.86892 Time: 0.00405\n",
      "Iteration: 0112 Loss: 0.86854 Time: 0.00303\n",
      "Iteration: 0113 Loss: 0.82178 Time: 0.00398\n",
      "Iteration: 0114 Loss: 0.85390 Time: 0.00398\n",
      "Iteration: 0115 Loss: 0.85294 Time: 0.00310\n",
      "Iteration: 0116 Loss: 0.86323 Time: 0.00396\n",
      "Iteration: 0117 Loss: 0.84517 Time: 0.00396\n",
      "Iteration: 0118 Loss: 0.84324 Time: 0.00300\n",
      "Iteration: 0119 Loss: 0.87151 Time: 0.00400\n",
      "Iteration: 0120 Loss: 0.84271 Time: 0.00299\n",
      "Iteration: 0121 Loss: 0.83075 Time: 0.00296\n",
      "Iteration: 0122 Loss: 0.84031 Time: 0.00408\n",
      "Iteration: 0123 Loss: 0.83606 Time: 0.00422\n",
      "Iteration: 0124 Loss: 0.80716 Time: 0.00358\n",
      "Iteration: 0125 Loss: 0.84510 Time: 0.00300\n",
      "Iteration: 0126 Loss: 0.79278 Time: 0.00306\n",
      "Iteration: 0127 Loss: 0.81280 Time: 0.00299\n",
      "Iteration: 0128 Loss: 0.81278 Time: 0.00400\n",
      "Iteration: 0129 Loss: 0.79427 Time: 0.00304\n",
      "Iteration: 0130 Loss: 0.80811 Time: 0.00399\n",
      "Iteration: 0131 Loss: 0.80002 Time: 0.00298\n",
      "Iteration: 0132 Loss: 0.78037 Time: 0.00295\n",
      "Iteration: 0133 Loss: 0.80014 Time: 0.00303\n",
      "Iteration: 0134 Loss: 0.81521 Time: 0.00450\n",
      "Iteration: 0135 Loss: 0.76140 Time: 0.00306\n",
      "Iteration: 0136 Loss: 0.76219 Time: 0.00390\n",
      "Iteration: 0137 Loss: 0.75213 Time: 0.00297\n",
      "Iteration: 0138 Loss: 0.77052 Time: 0.00411\n",
      "Iteration: 0139 Loss: 0.75810 Time: 0.00403\n",
      "Iteration: 0140 Loss: 0.78939 Time: 0.00392\n",
      "Iteration: 0141 Loss: 0.77570 Time: 0.00514\n",
      "Iteration: 0142 Loss: 0.76701 Time: 0.00310\n",
      "Iteration: 0143 Loss: 0.75655 Time: 0.00303\n",
      "Iteration: 0144 Loss: 0.73677 Time: 0.00404\n",
      "Iteration: 0145 Loss: 0.78516 Time: 0.00398\n",
      "Iteration: 0146 Loss: 0.78915 Time: 0.00302\n",
      "Iteration: 0147 Loss: 0.74449 Time: 0.00299\n",
      "Iteration: 0148 Loss: 0.76710 Time: 0.00395\n",
      "Iteration: 0149 Loss: 0.77242 Time: 0.00407\n",
      "Iteration: 0150 Loss: 0.75939 Time: 0.00400\n",
      "Iteration: 0151 Loss: 0.73096 Time: 0.00404\n",
      "Iteration: 0152 Loss: 0.72723 Time: 0.00396\n",
      "Iteration: 0153 Loss: 0.73174 Time: 0.00392\n",
      "Iteration: 0154 Loss: 0.71734 Time: 0.00300\n",
      "Iteration: 0155 Loss: 0.73882 Time: 0.00397\n",
      "Iteration: 0156 Loss: 0.75874 Time: 0.00415\n",
      "Iteration: 0157 Loss: 0.72921 Time: 0.00298\n",
      "Iteration: 0158 Loss: 0.72550 Time: 0.00302\n",
      "Iteration: 0159 Loss: 0.70866 Time: 0.00406\n",
      "Iteration: 0160 Loss: 0.71314 Time: 0.00296\n",
      "Iteration: 0161 Loss: 0.69880 Time: 0.00301\n",
      "Iteration: 0162 Loss: 0.69755 Time: 0.00401\n",
      "Iteration: 0163 Loss: 0.72414 Time: 0.00397\n",
      "Iteration: 0164 Loss: 0.70620 Time: 0.00304\n",
      "Iteration: 0165 Loss: 0.71076 Time: 0.00301\n",
      "Iteration: 0166 Loss: 0.71236 Time: 0.00295\n",
      "Iteration: 0167 Loss: 0.71487 Time: 0.00309\n",
      "Iteration: 0168 Loss: 0.70164 Time: 0.00395\n",
      "Iteration: 0169 Loss: 0.70530 Time: 0.00506\n",
      "Iteration: 0170 Loss: 0.68707 Time: 0.00435\n",
      "Iteration: 0171 Loss: 0.70480 Time: 0.00365\n",
      "Iteration: 0172 Loss: 0.70554 Time: 0.00306\n",
      "Iteration: 0173 Loss: 0.71352 Time: 0.00304\n",
      "Iteration: 0174 Loss: 0.69117 Time: 0.00396\n",
      "Iteration: 0175 Loss: 0.67150 Time: 0.00442\n",
      "Iteration: 0176 Loss: 0.69581 Time: 0.00362\n",
      "Iteration: 0177 Loss: 0.68751 Time: 0.00405\n",
      "Iteration: 0178 Loss: 0.68163 Time: 0.00402\n",
      "Iteration: 0179 Loss: 0.67451 Time: 0.00403\n",
      "Iteration: 0180 Loss: 0.67475 Time: 0.00302\n",
      "Iteration: 0181 Loss: 0.67355 Time: 0.00400\n",
      "Iteration: 0182 Loss: 0.70395 Time: 0.00400\n",
      "Iteration: 0183 Loss: 0.68395 Time: 0.00401\n",
      "Iteration: 0184 Loss: 0.67729 Time: 0.00301\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0185 Loss: 0.69200 Time: 0.00302\n",
      "Iteration: 0186 Loss: 0.67477 Time: 0.00392\n",
      "Iteration: 0187 Loss: 0.68063 Time: 0.00301\n",
      "Iteration: 0188 Loss: 0.67507 Time: 0.00407\n",
      "Iteration: 0189 Loss: 0.65989 Time: 0.00306\n",
      "Iteration: 0190 Loss: 0.67543 Time: 0.00300\n",
      "Iteration: 0191 Loss: 0.68951 Time: 0.00503\n",
      "Iteration: 0192 Loss: 0.67184 Time: 0.00297\n",
      "Iteration: 0193 Loss: 0.66833 Time: 0.00299\n",
      "Iteration: 0194 Loss: 0.67915 Time: 0.00423\n",
      "Iteration: 0195 Loss: 0.66121 Time: 0.00300\n",
      "Iteration: 0196 Loss: 0.67109 Time: 0.00308\n",
      "Iteration: 0197 Loss: 0.64711 Time: 0.00391\n",
      "Iteration: 0198 Loss: 0.67299 Time: 0.00405\n",
      "Iteration: 0199 Loss: 0.66494 Time: 0.00397\n",
      "Iteration: 0200 Loss: 0.64985 Time: 0.00201\n",
      "Done! \n",
      "\n",
      "STEP 3/3 - MODEL EVALUATION \n",
      "\n",
      "Computing the final embedding vectors, for evaluation\n",
      "Done! \n",
      "\n",
      "Testing: link prediction\n",
      "Done! \n",
      "\n",
      "EXPERIMENTS ON MODEL 8 / 10 \n",
      "\n",
      "STEP 1/3 - PREPROCESSING STEPS \n",
      "\n",
      "Masking some edges from the training graph, for link prediction\n",
      "(validation set: 1.0 % of edges - test set: 10.0 % of edges)\n",
      "Done! \n",
      "\n",
      "Preprocessing node features\n",
      "Done! \n",
      "\n",
      "Running the Louvain algorithm for community detection\n",
      "as a preprocessing step for the encoder\n",
      "Done! Louvain has found 21 communities \n",
      "\n",
      "Setting up the model and the optimizer\n",
      "Done! \n",
      "\n",
      "Preprocessing on message passing matrices\n",
      "Done! \n",
      "\n",
      "Initializing TF session\n",
      "Done! \n",
      "\n",
      "STEP 2/3 - MODEL TRAINING \n",
      "\n",
      "Starting training\n",
      "Iteration: 0001 Loss: 1.86450 Time: 0.14500\n",
      "Iteration: 0002 Loss: 1.71344 Time: 0.00401\n",
      "Iteration: 0003 Loss: 1.61453 Time: 0.00399\n",
      "Iteration: 0004 Loss: 1.63434 Time: 0.00401\n",
      "Iteration: 0005 Loss: 1.64321 Time: 0.00301\n",
      "Iteration: 0006 Loss: 1.55627 Time: 0.00300\n",
      "Iteration: 0007 Loss: 1.62816 Time: 0.00400\n",
      "Iteration: 0008 Loss: 1.54120 Time: 0.00401\n",
      "Iteration: 0009 Loss: 1.55754 Time: 0.00301\n",
      "Iteration: 0010 Loss: 1.51315 Time: 0.00199\n",
      "Iteration: 0011 Loss: 1.49716 Time: 0.00399\n",
      "Iteration: 0012 Loss: 1.42585 Time: 0.00401\n",
      "Iteration: 0013 Loss: 1.45513 Time: 0.00308\n",
      "Iteration: 0014 Loss: 1.49081 Time: 0.00300\n",
      "Iteration: 0015 Loss: 1.41706 Time: 0.00401\n",
      "Iteration: 0016 Loss: 1.44790 Time: 0.00402\n",
      "Iteration: 0017 Loss: 1.42090 Time: 0.00396\n",
      "Iteration: 0018 Loss: 1.42931 Time: 0.00396\n",
      "Iteration: 0019 Loss: 1.39023 Time: 0.00305\n",
      "Iteration: 0020 Loss: 1.38589 Time: 0.00397\n",
      "Iteration: 0021 Loss: 1.31943 Time: 0.00498\n",
      "Iteration: 0022 Loss: 1.37536 Time: 0.00401\n",
      "Iteration: 0023 Loss: 1.34950 Time: 0.00399\n",
      "Iteration: 0024 Loss: 1.37956 Time: 0.00304\n",
      "Iteration: 0025 Loss: 1.35614 Time: 0.00401\n",
      "Iteration: 0026 Loss: 1.33376 Time: 0.00399\n",
      "Iteration: 0027 Loss: 1.34157 Time: 0.00301\n",
      "Iteration: 0028 Loss: 1.37045 Time: 0.00309\n",
      "Iteration: 0029 Loss: 1.27497 Time: 0.00308\n",
      "Iteration: 0030 Loss: 1.25409 Time: 0.00497\n",
      "Iteration: 0031 Loss: 1.30380 Time: 0.00304\n",
      "Iteration: 0032 Loss: 1.27540 Time: 0.00403\n",
      "Iteration: 0033 Loss: 1.25295 Time: 0.00397\n",
      "Iteration: 0034 Loss: 1.20705 Time: 0.00400\n",
      "Iteration: 0035 Loss: 1.23735 Time: 0.00308\n",
      "Iteration: 0036 Loss: 1.20620 Time: 0.00488\n",
      "Iteration: 0037 Loss: 1.28619 Time: 0.00503\n",
      "Iteration: 0038 Loss: 1.19562 Time: 0.00400\n",
      "Iteration: 0039 Loss: 1.17518 Time: 0.00402\n",
      "Iteration: 0040 Loss: 1.15743 Time: 0.00300\n",
      "Iteration: 0041 Loss: 1.21725 Time: 0.00401\n",
      "Iteration: 0042 Loss: 1.17944 Time: 0.00300\n",
      "Iteration: 0043 Loss: 1.16986 Time: 0.00405\n",
      "Iteration: 0044 Loss: 1.08180 Time: 0.00399\n",
      "Iteration: 0045 Loss: 1.14461 Time: 0.00299\n",
      "Iteration: 0046 Loss: 1.16336 Time: 0.00305\n",
      "Iteration: 0047 Loss: 1.17068 Time: 0.00318\n",
      "Iteration: 0048 Loss: 1.14832 Time: 0.00310\n",
      "Iteration: 0049 Loss: 1.10610 Time: 0.00391\n",
      "Iteration: 0050 Loss: 1.11377 Time: 0.00499\n",
      "Iteration: 0051 Loss: 1.08765 Time: 0.00300\n",
      "Iteration: 0052 Loss: 1.11947 Time: 0.00409\n",
      "Iteration: 0053 Loss: 1.08399 Time: 0.00325\n",
      "Iteration: 0054 Loss: 1.09669 Time: 0.00480\n",
      "Iteration: 0055 Loss: 1.08397 Time: 0.00301\n",
      "Iteration: 0056 Loss: 1.12697 Time: 0.00396\n",
      "Iteration: 0057 Loss: 1.08322 Time: 0.00302\n",
      "Iteration: 0058 Loss: 1.05742 Time: 0.00305\n",
      "Iteration: 0059 Loss: 1.08333 Time: 0.00399\n",
      "Iteration: 0060 Loss: 1.07538 Time: 0.00433\n",
      "Iteration: 0061 Loss: 1.04902 Time: 0.00300\n",
      "Iteration: 0062 Loss: 1.06951 Time: 0.00405\n",
      "Iteration: 0063 Loss: 1.08927 Time: 0.00397\n",
      "Iteration: 0064 Loss: 1.06229 Time: 0.00307\n",
      "Iteration: 0065 Loss: 1.01054 Time: 0.00399\n",
      "Iteration: 0066 Loss: 1.06665 Time: 0.00400\n",
      "Iteration: 0067 Loss: 1.05532 Time: 0.00406\n",
      "Iteration: 0068 Loss: 0.97340 Time: 0.00396\n",
      "Iteration: 0069 Loss: 1.03826 Time: 0.00400\n",
      "Iteration: 0070 Loss: 0.99080 Time: 0.00396\n",
      "Iteration: 0071 Loss: 1.02420 Time: 0.00285\n",
      "Iteration: 0072 Loss: 1.04078 Time: 0.00387\n",
      "Iteration: 0073 Loss: 1.03253 Time: 0.00401\n",
      "Iteration: 0074 Loss: 1.02809 Time: 0.00504\n",
      "Iteration: 0075 Loss: 1.02046 Time: 0.00395\n",
      "Iteration: 0076 Loss: 0.99617 Time: 0.00299\n",
      "Iteration: 0077 Loss: 1.02521 Time: 0.00497\n",
      "Iteration: 0078 Loss: 0.95136 Time: 0.00307\n",
      "Iteration: 0079 Loss: 0.99304 Time: 0.00303\n",
      "Iteration: 0080 Loss: 1.02545 Time: 0.00294\n",
      "Iteration: 0081 Loss: 1.04817 Time: 0.00305\n",
      "Iteration: 0082 Loss: 0.94515 Time: 0.00413\n",
      "Iteration: 0083 Loss: 1.00598 Time: 0.00485\n",
      "Iteration: 0084 Loss: 0.97709 Time: 0.00308\n",
      "Iteration: 0085 Loss: 0.97319 Time: 0.00299\n",
      "Iteration: 0086 Loss: 0.94304 Time: 0.00399\n",
      "Iteration: 0087 Loss: 0.96511 Time: 0.00399\n",
      "Iteration: 0088 Loss: 0.90908 Time: 0.00305\n",
      "Iteration: 0089 Loss: 0.97391 Time: 0.00303\n",
      "Iteration: 0090 Loss: 0.91028 Time: 0.00402\n",
      "Iteration: 0091 Loss: 0.90529 Time: 0.00306\n",
      "Iteration: 0092 Loss: 0.90708 Time: 0.00399\n",
      "Iteration: 0093 Loss: 0.95954 Time: 0.00297\n",
      "Iteration: 0094 Loss: 0.94621 Time: 0.00400\n",
      "Iteration: 0095 Loss: 0.91324 Time: 0.00509\n",
      "Iteration: 0096 Loss: 0.90047 Time: 0.00391\n",
      "Iteration: 0097 Loss: 0.91049 Time: 0.00401\n",
      "Iteration: 0098 Loss: 0.90791 Time: 0.00408\n",
      "Iteration: 0099 Loss: 0.93179 Time: 0.00398\n",
      "Iteration: 0100 Loss: 0.88112 Time: 0.00314\n",
      "Iteration: 0101 Loss: 0.89839 Time: 0.00306\n",
      "Iteration: 0102 Loss: 0.92018 Time: 0.00399\n",
      "Iteration: 0103 Loss: 0.87268 Time: 0.00399\n",
      "Iteration: 0104 Loss: 0.86825 Time: 0.00296\n",
      "Iteration: 0105 Loss: 0.87694 Time: 0.00402\n",
      "Iteration: 0106 Loss: 0.86498 Time: 0.00512\n",
      "Iteration: 0107 Loss: 0.86344 Time: 0.00388\n",
      "Iteration: 0108 Loss: 0.87446 Time: 0.00298\n",
      "Iteration: 0109 Loss: 0.84863 Time: 0.00309\n",
      "Iteration: 0110 Loss: 0.84308 Time: 0.00302\n",
      "Iteration: 0111 Loss: 0.86692 Time: 0.00399\n",
      "Iteration: 0112 Loss: 0.86127 Time: 0.00335\n",
      "Iteration: 0113 Loss: 0.87212 Time: 0.00296\n",
      "Iteration: 0114 Loss: 0.87646 Time: 0.00505\n",
      "Iteration: 0115 Loss: 0.85499 Time: 0.00395\n",
      "Iteration: 0116 Loss: 0.85019 Time: 0.00401\n",
      "Iteration: 0117 Loss: 0.82233 Time: 0.00402\n",
      "Iteration: 0118 Loss: 0.87209 Time: 0.00403\n",
      "Iteration: 0119 Loss: 0.83721 Time: 0.00598\n",
      "Iteration: 0120 Loss: 0.81852 Time: 0.00300\n",
      "Iteration: 0121 Loss: 0.83141 Time: 0.00504\n",
      "Iteration: 0122 Loss: 0.83501 Time: 0.00397\n",
      "Iteration: 0123 Loss: 0.81665 Time: 0.00301\n",
      "Iteration: 0124 Loss: 0.82736 Time: 0.00299\n",
      "Iteration: 0125 Loss: 0.81579 Time: 0.00404\n",
      "Iteration: 0126 Loss: 0.82262 Time: 0.00496\n",
      "Iteration: 0127 Loss: 0.79533 Time: 0.00402\n",
      "Iteration: 0128 Loss: 0.79246 Time: 0.00400\n",
      "Iteration: 0129 Loss: 0.78569 Time: 0.00397\n",
      "Iteration: 0130 Loss: 0.79384 Time: 0.00399\n",
      "Iteration: 0131 Loss: 0.78590 Time: 0.00405\n",
      "Iteration: 0132 Loss: 0.80350 Time: 0.00201\n",
      "Iteration: 0133 Loss: 0.79519 Time: 0.00303\n",
      "Iteration: 0134 Loss: 0.77894 Time: 0.00302\n",
      "Iteration: 0135 Loss: 0.77953 Time: 0.00392\n",
      "Iteration: 0136 Loss: 0.77650 Time: 0.00412\n",
      "Iteration: 0137 Loss: 0.77685 Time: 0.00302\n",
      "Iteration: 0138 Loss: 0.78384 Time: 0.00305\n",
      "Iteration: 0139 Loss: 0.80787 Time: 0.00400\n",
      "Iteration: 0140 Loss: 0.77727 Time: 0.00306\n",
      "Iteration: 0141 Loss: 0.74364 Time: 0.00298\n",
      "Iteration: 0142 Loss: 0.74038 Time: 0.00302\n",
      "Iteration: 0143 Loss: 0.76318 Time: 0.00400\n",
      "Iteration: 0144 Loss: 0.76654 Time: 0.00400\n",
      "Iteration: 0145 Loss: 0.73901 Time: 0.00503\n",
      "Iteration: 0146 Loss: 0.74427 Time: 0.00396\n",
      "Iteration: 0147 Loss: 0.75657 Time: 0.00400\n",
      "Iteration: 0148 Loss: 0.75575 Time: 0.00409\n",
      "Iteration: 0149 Loss: 0.75065 Time: 0.00395\n",
      "Iteration: 0150 Loss: 0.77241 Time: 0.00498\n",
      "Iteration: 0151 Loss: 0.75624 Time: 0.00398\n",
      "Iteration: 0152 Loss: 0.74359 Time: 0.00404\n",
      "Iteration: 0153 Loss: 0.74691 Time: 0.00399\n",
      "Iteration: 0154 Loss: 0.71623 Time: 0.00301\n",
      "Iteration: 0155 Loss: 0.71356 Time: 0.00502\n",
      "Iteration: 0156 Loss: 0.73289 Time: 0.00402\n",
      "Iteration: 0157 Loss: 0.72744 Time: 0.00391\n",
      "Iteration: 0158 Loss: 0.71553 Time: 0.00302\n",
      "Iteration: 0159 Loss: 0.71645 Time: 0.00298\n",
      "Iteration: 0160 Loss: 0.71763 Time: 0.00404\n",
      "Iteration: 0161 Loss: 0.73032 Time: 0.00400\n",
      "Iteration: 0162 Loss: 0.72175 Time: 0.00294\n",
      "Iteration: 0163 Loss: 0.72668 Time: 0.00337\n",
      "Iteration: 0164 Loss: 0.70938 Time: 0.00464\n",
      "Iteration: 0165 Loss: 0.71646 Time: 0.00307\n",
      "Iteration: 0166 Loss: 0.71624 Time: 0.00298\n",
      "Iteration: 0167 Loss: 0.69487 Time: 0.00311\n",
      "Iteration: 0168 Loss: 0.70582 Time: 0.00493\n",
      "Iteration: 0169 Loss: 0.70588 Time: 0.00399\n",
      "Iteration: 0170 Loss: 0.70346 Time: 0.00405\n",
      "Iteration: 0171 Loss: 0.70921 Time: 0.00301\n",
      "Iteration: 0172 Loss: 0.70327 Time: 0.00401\n",
      "Iteration: 0173 Loss: 0.69692 Time: 0.00395\n",
      "Iteration: 0174 Loss: 0.70783 Time: 0.00304\n",
      "Iteration: 0175 Loss: 0.70713 Time: 0.00402\n",
      "Iteration: 0176 Loss: 0.70281 Time: 0.00301\n",
      "Iteration: 0177 Loss: 0.72362 Time: 0.00413\n",
      "Iteration: 0178 Loss: 0.69956 Time: 0.00400\n",
      "Iteration: 0179 Loss: 0.68820 Time: 0.00400\n",
      "Iteration: 0180 Loss: 0.71228 Time: 0.00330\n",
      "Iteration: 0181 Loss: 0.67515 Time: 0.00369\n",
      "Iteration: 0182 Loss: 0.68589 Time: 0.00400\n",
      "Iteration: 0183 Loss: 0.68610 Time: 0.00302\n",
      "Iteration: 0184 Loss: 0.67689 Time: 0.00297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0185 Loss: 0.68114 Time: 0.00405\n",
      "Iteration: 0186 Loss: 0.67403 Time: 0.00402\n",
      "Iteration: 0187 Loss: 0.68597 Time: 0.00300\n",
      "Iteration: 0188 Loss: 0.65675 Time: 0.00381\n",
      "Iteration: 0189 Loss: 0.68856 Time: 0.00395\n",
      "Iteration: 0190 Loss: 0.66519 Time: 0.00399\n",
      "Iteration: 0191 Loss: 0.68140 Time: 0.00405\n",
      "Iteration: 0192 Loss: 0.66461 Time: 0.00405\n",
      "Iteration: 0193 Loss: 0.66928 Time: 0.00395\n",
      "Iteration: 0194 Loss: 0.69348 Time: 0.00295\n",
      "Iteration: 0195 Loss: 0.66157 Time: 0.00306\n",
      "Iteration: 0196 Loss: 0.68450 Time: 0.00405\n",
      "Iteration: 0197 Loss: 0.66049 Time: 0.00406\n",
      "Iteration: 0198 Loss: 0.65770 Time: 0.00300\n",
      "Iteration: 0199 Loss: 0.65599 Time: 0.00409\n",
      "Iteration: 0200 Loss: 0.64630 Time: 0.00389\n",
      "Done! \n",
      "\n",
      "STEP 3/3 - MODEL EVALUATION \n",
      "\n",
      "Computing the final embedding vectors, for evaluation\n",
      "Done! \n",
      "\n",
      "Testing: link prediction\n",
      "Done! \n",
      "\n",
      "EXPERIMENTS ON MODEL 9 / 10 \n",
      "\n",
      "STEP 1/3 - PREPROCESSING STEPS \n",
      "\n",
      "Masking some edges from the training graph, for link prediction\n",
      "(validation set: 1.0 % of edges - test set: 10.0 % of edges)\n",
      "Done! \n",
      "\n",
      "Preprocessing node features\n",
      "Done! \n",
      "\n",
      "Running the Louvain algorithm for community detection\n",
      "as a preprocessing step for the encoder\n",
      "Done! Louvain has found 21 communities \n",
      "\n",
      "Setting up the model and the optimizer\n",
      "Done! \n",
      "\n",
      "Preprocessing on message passing matrices\n",
      "Done! \n",
      "\n",
      "Initializing TF session\n",
      "Done! \n",
      "\n",
      "STEP 2/3 - MODEL TRAINING \n",
      "\n",
      "Starting training\n",
      "Iteration: 0001 Loss: 1.72506 Time: 0.14635\n",
      "Iteration: 0002 Loss: 1.65091 Time: 0.00404\n",
      "Iteration: 0003 Loss: 1.64670 Time: 0.00398\n",
      "Iteration: 0004 Loss: 1.67012 Time: 0.00496\n",
      "Iteration: 0005 Loss: 1.74445 Time: 0.00308\n",
      "Iteration: 0006 Loss: 1.59258 Time: 0.00400\n",
      "Iteration: 0007 Loss: 1.45586 Time: 0.00451\n",
      "Iteration: 0008 Loss: 1.52242 Time: 0.00348\n",
      "Iteration: 0009 Loss: 1.53381 Time: 0.00411\n",
      "Iteration: 0010 Loss: 1.63368 Time: 0.00304\n",
      "Iteration: 0011 Loss: 1.53967 Time: 0.00408\n",
      "Iteration: 0012 Loss: 1.42606 Time: 0.00399\n",
      "Iteration: 0013 Loss: 1.43400 Time: 0.00305\n",
      "Iteration: 0014 Loss: 1.40968 Time: 0.00399\n",
      "Iteration: 0015 Loss: 1.39406 Time: 0.00397\n",
      "Iteration: 0016 Loss: 1.43480 Time: 0.00399\n",
      "Iteration: 0017 Loss: 1.43406 Time: 0.00406\n",
      "Iteration: 0018 Loss: 1.44900 Time: 0.00505\n",
      "Iteration: 0019 Loss: 1.42370 Time: 0.00395\n",
      "Iteration: 0020 Loss: 1.38159 Time: 0.00400\n",
      "Iteration: 0021 Loss: 1.32844 Time: 0.00505\n",
      "Iteration: 0022 Loss: 1.28229 Time: 0.00397\n",
      "Iteration: 0023 Loss: 1.33934 Time: 0.00405\n",
      "Iteration: 0024 Loss: 1.32451 Time: 0.00493\n",
      "Iteration: 0025 Loss: 1.28496 Time: 0.00401\n",
      "Iteration: 0026 Loss: 1.32827 Time: 0.00502\n",
      "Iteration: 0027 Loss: 1.30859 Time: 0.00405\n",
      "Iteration: 0028 Loss: 1.26781 Time: 0.00299\n",
      "Iteration: 0029 Loss: 1.25931 Time: 0.00406\n",
      "Iteration: 0030 Loss: 1.25728 Time: 0.00398\n",
      "Iteration: 0031 Loss: 1.27126 Time: 0.00400\n",
      "Iteration: 0032 Loss: 1.22922 Time: 0.00513\n",
      "Iteration: 0033 Loss: 1.22204 Time: 0.00291\n",
      "Iteration: 0034 Loss: 1.27526 Time: 0.00400\n",
      "Iteration: 0035 Loss: 1.23282 Time: 0.00297\n",
      "Iteration: 0036 Loss: 1.22164 Time: 0.00305\n",
      "Iteration: 0037 Loss: 1.20854 Time: 0.00403\n",
      "Iteration: 0038 Loss: 1.22509 Time: 0.00396\n",
      "Iteration: 0039 Loss: 1.17085 Time: 0.00442\n",
      "Iteration: 0040 Loss: 1.14196 Time: 0.00300\n",
      "Iteration: 0041 Loss: 1.19674 Time: 0.00301\n",
      "Iteration: 0042 Loss: 1.19290 Time: 0.00400\n",
      "Iteration: 0043 Loss: 1.12799 Time: 0.00398\n",
      "Iteration: 0044 Loss: 1.18865 Time: 0.00398\n",
      "Iteration: 0045 Loss: 1.09456 Time: 0.00409\n",
      "Iteration: 0046 Loss: 1.09144 Time: 0.00495\n",
      "Iteration: 0047 Loss: 1.21840 Time: 0.00295\n",
      "Iteration: 0048 Loss: 1.14604 Time: 0.00402\n",
      "Iteration: 0049 Loss: 1.16891 Time: 0.00406\n",
      "Iteration: 0050 Loss: 1.11174 Time: 0.00397\n",
      "Iteration: 0051 Loss: 1.12291 Time: 0.00296\n",
      "Iteration: 0052 Loss: 1.09569 Time: 0.00299\n",
      "Iteration: 0053 Loss: 1.06290 Time: 0.00303\n",
      "Iteration: 0054 Loss: 1.09108 Time: 0.00404\n",
      "Iteration: 0055 Loss: 1.07023 Time: 0.00395\n",
      "Iteration: 0056 Loss: 1.05982 Time: 0.00303\n",
      "Iteration: 0057 Loss: 1.06615 Time: 0.00304\n",
      "Iteration: 0058 Loss: 1.05631 Time: 0.00397\n",
      "Iteration: 0059 Loss: 1.09283 Time: 0.00399\n",
      "Iteration: 0060 Loss: 1.07678 Time: 0.00305\n",
      "Iteration: 0061 Loss: 1.02730 Time: 0.00407\n",
      "Iteration: 0062 Loss: 1.08926 Time: 0.00296\n",
      "Iteration: 0063 Loss: 1.04362 Time: 0.00393\n",
      "Iteration: 0064 Loss: 1.03641 Time: 0.00309\n",
      "Iteration: 0065 Loss: 1.04150 Time: 0.00303\n",
      "Iteration: 0066 Loss: 1.04881 Time: 0.00403\n",
      "Iteration: 0067 Loss: 1.06451 Time: 0.00301\n",
      "Iteration: 0068 Loss: 1.02915 Time: 0.00401\n",
      "Iteration: 0069 Loss: 1.03438 Time: 0.00399\n",
      "Iteration: 0070 Loss: 1.02898 Time: 0.00303\n",
      "Iteration: 0071 Loss: 1.02572 Time: 0.00306\n",
      "Iteration: 0072 Loss: 0.98799 Time: 0.00396\n",
      "Iteration: 0073 Loss: 1.02910 Time: 0.00495\n",
      "Iteration: 0074 Loss: 0.95316 Time: 0.00405\n",
      "Iteration: 0075 Loss: 1.01223 Time: 0.00396\n",
      "Iteration: 0076 Loss: 0.99219 Time: 0.00308\n",
      "Iteration: 0077 Loss: 1.00216 Time: 0.00406\n",
      "Iteration: 0078 Loss: 0.94329 Time: 0.00401\n",
      "Iteration: 0079 Loss: 0.93824 Time: 0.00499\n",
      "Iteration: 0080 Loss: 1.01588 Time: 0.00304\n",
      "Iteration: 0081 Loss: 0.97495 Time: 0.00300\n",
      "Iteration: 0082 Loss: 0.95459 Time: 0.00505\n",
      "Iteration: 0083 Loss: 0.99107 Time: 0.00429\n",
      "Iteration: 0084 Loss: 0.92590 Time: 0.00366\n",
      "Iteration: 0085 Loss: 0.92969 Time: 0.00307\n",
      "Iteration: 0086 Loss: 0.93383 Time: 0.00396\n",
      "Iteration: 0087 Loss: 0.94559 Time: 0.00296\n",
      "Iteration: 0088 Loss: 0.93323 Time: 0.00300\n",
      "Iteration: 0089 Loss: 0.94832 Time: 0.00395\n",
      "Iteration: 0090 Loss: 0.93354 Time: 0.00304\n",
      "Iteration: 0091 Loss: 0.94836 Time: 0.00296\n",
      "Iteration: 0092 Loss: 0.93583 Time: 0.00500\n",
      "Iteration: 0093 Loss: 0.90505 Time: 0.00310\n",
      "Iteration: 0094 Loss: 0.98910 Time: 0.00394\n",
      "Iteration: 0095 Loss: 0.95286 Time: 0.00199\n",
      "Iteration: 0096 Loss: 0.90663 Time: 0.00401\n",
      "Iteration: 0097 Loss: 0.88033 Time: 0.00409\n",
      "Iteration: 0098 Loss: 0.92657 Time: 0.00302\n",
      "Iteration: 0099 Loss: 0.91390 Time: 0.00397\n",
      "Iteration: 0100 Loss: 0.92564 Time: 0.00504\n",
      "Iteration: 0101 Loss: 0.91095 Time: 0.00401\n",
      "Iteration: 0102 Loss: 0.91684 Time: 0.00298\n",
      "Iteration: 0103 Loss: 0.89177 Time: 0.00396\n",
      "Iteration: 0104 Loss: 0.88416 Time: 0.00309\n",
      "Iteration: 0105 Loss: 0.87655 Time: 0.00500\n",
      "Iteration: 0106 Loss: 0.90409 Time: 0.00399\n",
      "Iteration: 0107 Loss: 0.88515 Time: 0.00400\n",
      "Iteration: 0108 Loss: 0.89029 Time: 0.00401\n",
      "Iteration: 0109 Loss: 0.87452 Time: 0.00300\n",
      "Iteration: 0110 Loss: 0.84689 Time: 0.00400\n",
      "Iteration: 0111 Loss: 0.85561 Time: 0.00301\n",
      "Iteration: 0112 Loss: 0.85703 Time: 0.00299\n",
      "Iteration: 0113 Loss: 0.85498 Time: 0.00405\n",
      "Iteration: 0114 Loss: 0.86139 Time: 0.00402\n",
      "Iteration: 0115 Loss: 0.85478 Time: 0.00301\n",
      "Iteration: 0116 Loss: 0.86166 Time: 0.00393\n",
      "Iteration: 0117 Loss: 0.85334 Time: 0.00406\n",
      "Iteration: 0118 Loss: 0.82080 Time: 0.00300\n",
      "Iteration: 0119 Loss: 0.83735 Time: 0.00409\n",
      "Iteration: 0120 Loss: 0.83567 Time: 0.00386\n",
      "Iteration: 0121 Loss: 0.84872 Time: 0.00400\n",
      "Iteration: 0122 Loss: 0.83108 Time: 0.00299\n",
      "Iteration: 0123 Loss: 0.83787 Time: 0.00301\n",
      "Iteration: 0124 Loss: 0.81739 Time: 0.00311\n",
      "Iteration: 0125 Loss: 0.83044 Time: 0.00300\n",
      "Iteration: 0126 Loss: 0.78853 Time: 0.00415\n",
      "Iteration: 0127 Loss: 0.80615 Time: 0.00285\n",
      "Iteration: 0128 Loss: 0.79942 Time: 0.00300\n",
      "Iteration: 0129 Loss: 0.81598 Time: 0.00199\n",
      "Iteration: 0130 Loss: 0.78291 Time: 0.00301\n",
      "Iteration: 0131 Loss: 0.79058 Time: 0.00399\n",
      "Iteration: 0132 Loss: 0.77282 Time: 0.00401\n",
      "Iteration: 0133 Loss: 0.78432 Time: 0.00299\n",
      "Iteration: 0134 Loss: 0.75806 Time: 0.00310\n",
      "Iteration: 0135 Loss: 0.78910 Time: 0.00400\n",
      "Iteration: 0136 Loss: 0.77778 Time: 0.00395\n",
      "Iteration: 0137 Loss: 0.77040 Time: 0.00400\n",
      "Iteration: 0138 Loss: 0.78561 Time: 0.00400\n",
      "Iteration: 0139 Loss: 0.76045 Time: 0.00497\n",
      "Iteration: 0140 Loss: 0.76051 Time: 0.00304\n",
      "Iteration: 0141 Loss: 0.76284 Time: 0.00301\n",
      "Iteration: 0142 Loss: 0.75651 Time: 0.00396\n",
      "Iteration: 0143 Loss: 0.75400 Time: 0.00401\n",
      "Iteration: 0144 Loss: 0.75039 Time: 0.00504\n",
      "Iteration: 0145 Loss: 0.74655 Time: 0.00395\n",
      "Iteration: 0146 Loss: 0.74703 Time: 0.00306\n",
      "Iteration: 0147 Loss: 0.72936 Time: 0.00397\n",
      "Iteration: 0148 Loss: 0.76265 Time: 0.00401\n",
      "Iteration: 0149 Loss: 0.73932 Time: 0.00300\n",
      "Iteration: 0150 Loss: 0.71663 Time: 0.00396\n",
      "Iteration: 0151 Loss: 0.73213 Time: 0.00307\n",
      "Iteration: 0152 Loss: 0.73658 Time: 0.00293\n",
      "Iteration: 0153 Loss: 0.73759 Time: 0.00399\n",
      "Iteration: 0154 Loss: 0.72330 Time: 0.00400\n",
      "Iteration: 0155 Loss: 0.74028 Time: 0.00395\n",
      "Iteration: 0156 Loss: 0.74261 Time: 0.00300\n",
      "Iteration: 0157 Loss: 0.73459 Time: 0.00408\n",
      "Iteration: 0158 Loss: 0.75380 Time: 0.00402\n",
      "Iteration: 0159 Loss: 0.73679 Time: 0.00394\n",
      "Iteration: 0160 Loss: 0.73293 Time: 0.00401\n",
      "Iteration: 0161 Loss: 0.70570 Time: 0.00299\n",
      "Iteration: 0162 Loss: 0.71902 Time: 0.00302\n",
      "Iteration: 0163 Loss: 0.72162 Time: 0.00403\n",
      "Iteration: 0164 Loss: 0.74004 Time: 0.00399\n",
      "Iteration: 0165 Loss: 0.71615 Time: 0.00402\n",
      "Iteration: 0166 Loss: 0.71324 Time: 0.00294\n",
      "Iteration: 0167 Loss: 0.71748 Time: 0.00305\n",
      "Iteration: 0168 Loss: 0.70700 Time: 0.00299\n",
      "Iteration: 0169 Loss: 0.70438 Time: 0.00296\n",
      "Iteration: 0170 Loss: 0.70629 Time: 0.00301\n",
      "Iteration: 0171 Loss: 0.71943 Time: 0.00399\n",
      "Iteration: 0172 Loss: 0.70086 Time: 0.00297\n",
      "Iteration: 0173 Loss: 0.69206 Time: 0.00508\n",
      "Iteration: 0174 Loss: 0.67628 Time: 0.00402\n",
      "Iteration: 0175 Loss: 0.69190 Time: 0.00394\n",
      "Iteration: 0176 Loss: 0.68759 Time: 0.00301\n",
      "Iteration: 0177 Loss: 0.70937 Time: 0.00297\n",
      "Iteration: 0178 Loss: 0.69097 Time: 0.00300\n",
      "Iteration: 0179 Loss: 0.69187 Time: 0.00405\n",
      "Iteration: 0180 Loss: 0.67636 Time: 0.00300\n",
      "Iteration: 0181 Loss: 0.69543 Time: 0.00398\n",
      "Iteration: 0182 Loss: 0.67065 Time: 0.00339\n",
      "Iteration: 0183 Loss: 0.67318 Time: 0.00364\n",
      "Iteration: 0184 Loss: 0.68656 Time: 0.00399\n",
      "Iteration: 0185 Loss: 0.68411 Time: 0.00301\n",
      "Iteration: 0186 Loss: 0.67136 Time: 0.00401\n",
      "Iteration: 0187 Loss: 0.65841 Time: 0.00305\n",
      "Iteration: 0188 Loss: 0.66574 Time: 0.00395\n",
      "Iteration: 0189 Loss: 0.69620 Time: 0.00308\n",
      "Iteration: 0190 Loss: 0.66475 Time: 0.00307\n",
      "Iteration: 0191 Loss: 0.67593 Time: 0.00417\n",
      "Iteration: 0192 Loss: 0.67086 Time: 0.00297\n",
      "Iteration: 0193 Loss: 0.67989 Time: 0.00496\n",
      "Iteration: 0194 Loss: 0.64914 Time: 0.00406\n",
      "Iteration: 0195 Loss: 0.65327 Time: 0.00396\n",
      "Iteration: 0196 Loss: 0.65456 Time: 0.00405\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0197 Loss: 0.65711 Time: 0.00397\n",
      "Iteration: 0198 Loss: 0.66241 Time: 0.00408\n",
      "Iteration: 0199 Loss: 0.66301 Time: 0.00399\n",
      "Iteration: 0200 Loss: 0.64391 Time: 0.00398\n",
      "Done! \n",
      "\n",
      "STEP 3/3 - MODEL EVALUATION \n",
      "\n",
      "Computing the final embedding vectors, for evaluation\n",
      "Done! \n",
      "\n",
      "Testing: link prediction\n",
      "Done! \n",
      "\n",
      "EXPERIMENTS ON MODEL 10 / 10 \n",
      "\n",
      "STEP 1/3 - PREPROCESSING STEPS \n",
      "\n",
      "Masking some edges from the training graph, for link prediction\n",
      "(validation set: 1.0 % of edges - test set: 10.0 % of edges)\n",
      "Done! \n",
      "\n",
      "Preprocessing node features\n",
      "Done! \n",
      "\n",
      "Running the Louvain algorithm for community detection\n",
      "as a preprocessing step for the encoder\n",
      "Done! Louvain has found 23 communities \n",
      "\n",
      "Setting up the model and the optimizer\n",
      "Done! \n",
      "\n",
      "Preprocessing on message passing matrices\n",
      "Done! \n",
      "\n",
      "Initializing TF session\n",
      "Done! \n",
      "\n",
      "STEP 2/3 - MODEL TRAINING \n",
      "\n",
      "Starting training\n",
      "Iteration: 0001 Loss: 1.77960 Time: 0.15466\n",
      "Iteration: 0002 Loss: 1.82481 Time: 0.00562\n",
      "Iteration: 0003 Loss: 1.68307 Time: 0.00727\n",
      "Iteration: 0004 Loss: 1.71829 Time: 0.00288\n",
      "Iteration: 0005 Loss: 1.74104 Time: 0.00298\n",
      "Iteration: 0006 Loss: 1.69917 Time: 0.00406\n",
      "Iteration: 0007 Loss: 1.60735 Time: 0.00593\n",
      "Iteration: 0008 Loss: 1.58427 Time: 0.00299\n",
      "Iteration: 0009 Loss: 1.56126 Time: 0.00495\n",
      "Iteration: 0010 Loss: 1.50002 Time: 0.00302\n",
      "Iteration: 0011 Loss: 1.47399 Time: 0.00396\n",
      "Iteration: 0012 Loss: 1.48560 Time: 0.00406\n",
      "Iteration: 0013 Loss: 1.50700 Time: 0.00501\n",
      "Iteration: 0014 Loss: 1.46749 Time: 0.00297\n",
      "Iteration: 0015 Loss: 1.38909 Time: 0.00308\n",
      "Iteration: 0016 Loss: 1.50714 Time: 0.00399\n",
      "Iteration: 0017 Loss: 1.40310 Time: 0.00299\n",
      "Iteration: 0018 Loss: 1.41094 Time: 0.00301\n",
      "Iteration: 0019 Loss: 1.44477 Time: 0.00306\n",
      "Iteration: 0020 Loss: 1.48318 Time: 0.00300\n",
      "Iteration: 0021 Loss: 1.33550 Time: 0.00295\n",
      "Iteration: 0022 Loss: 1.34085 Time: 0.00400\n",
      "Iteration: 0023 Loss: 1.40403 Time: 0.00307\n",
      "Iteration: 0024 Loss: 1.36196 Time: 0.00299\n",
      "Iteration: 0025 Loss: 1.31991 Time: 0.00401\n",
      "Iteration: 0026 Loss: 1.31122 Time: 0.00303\n",
      "Iteration: 0027 Loss: 1.30910 Time: 0.00300\n",
      "Iteration: 0028 Loss: 1.31201 Time: 0.00395\n",
      "Iteration: 0029 Loss: 1.27251 Time: 0.00306\n",
      "Iteration: 0030 Loss: 1.30425 Time: 0.00398\n",
      "Iteration: 0031 Loss: 1.32129 Time: 0.00495\n",
      "Iteration: 0032 Loss: 1.27805 Time: 0.00558\n",
      "Iteration: 0033 Loss: 1.25628 Time: 0.00404\n",
      "Iteration: 0034 Loss: 1.25673 Time: 0.00401\n",
      "Iteration: 0035 Loss: 1.21489 Time: 0.00300\n",
      "Iteration: 0036 Loss: 1.26882 Time: 0.00301\n",
      "Iteration: 0037 Loss: 1.23042 Time: 0.00406\n",
      "Iteration: 0038 Loss: 1.22273 Time: 0.00343\n",
      "Iteration: 0039 Loss: 1.21749 Time: 0.00439\n",
      "Iteration: 0040 Loss: 1.23571 Time: 0.00334\n",
      "Iteration: 0041 Loss: 1.18610 Time: 0.00399\n",
      "Iteration: 0042 Loss: 1.19679 Time: 0.00398\n",
      "Iteration: 0043 Loss: 1.20598 Time: 0.00396\n",
      "Iteration: 0044 Loss: 1.16251 Time: 0.00403\n",
      "Iteration: 0045 Loss: 1.12544 Time: 0.00408\n",
      "Iteration: 0046 Loss: 1.11830 Time: 0.00393\n",
      "Iteration: 0047 Loss: 1.14575 Time: 0.00302\n",
      "Iteration: 0048 Loss: 1.16264 Time: 0.00403\n",
      "Iteration: 0049 Loss: 1.16923 Time: 0.00400\n",
      "Iteration: 0050 Loss: 1.09339 Time: 0.00305\n",
      "Iteration: 0051 Loss: 1.12088 Time: 0.00302\n",
      "Iteration: 0052 Loss: 1.11278 Time: 0.00304\n",
      "Iteration: 0053 Loss: 1.07886 Time: 0.00394\n",
      "Iteration: 0054 Loss: 1.10364 Time: 0.00396\n",
      "Iteration: 0055 Loss: 1.06091 Time: 0.00405\n",
      "Iteration: 0056 Loss: 1.07867 Time: 0.00397\n",
      "Iteration: 0057 Loss: 1.12576 Time: 0.00304\n",
      "Iteration: 0058 Loss: 1.12944 Time: 0.00400\n",
      "Iteration: 0059 Loss: 1.09527 Time: 0.00308\n",
      "Iteration: 0060 Loss: 1.05961 Time: 0.00297\n",
      "Iteration: 0061 Loss: 1.11391 Time: 0.00398\n",
      "Iteration: 0062 Loss: 1.09740 Time: 0.00300\n",
      "Iteration: 0063 Loss: 1.00085 Time: 0.00300\n",
      "Iteration: 0064 Loss: 1.02583 Time: 0.00297\n",
      "Iteration: 0065 Loss: 1.05359 Time: 0.00401\n",
      "Iteration: 0066 Loss: 1.09730 Time: 0.00301\n",
      "Iteration: 0067 Loss: 1.02126 Time: 0.00397\n",
      "Iteration: 0068 Loss: 1.02873 Time: 0.00396\n",
      "Iteration: 0069 Loss: 1.00779 Time: 0.00304\n",
      "Iteration: 0070 Loss: 1.02233 Time: 0.00403\n",
      "Iteration: 0071 Loss: 1.06379 Time: 0.00500\n",
      "Iteration: 0072 Loss: 1.00180 Time: 0.00400\n",
      "Iteration: 0073 Loss: 1.03800 Time: 0.00309\n",
      "Iteration: 0074 Loss: 1.01716 Time: 0.00303\n",
      "Iteration: 0075 Loss: 1.01551 Time: 0.00397\n",
      "Iteration: 0076 Loss: 1.03833 Time: 0.00446\n",
      "Iteration: 0077 Loss: 0.97463 Time: 0.00302\n",
      "Iteration: 0078 Loss: 0.97570 Time: 0.00305\n",
      "Iteration: 0079 Loss: 0.96765 Time: 0.00301\n",
      "Iteration: 0080 Loss: 0.96329 Time: 0.00405\n",
      "Iteration: 0081 Loss: 0.97307 Time: 0.00295\n",
      "Iteration: 0082 Loss: 0.97371 Time: 0.00404\n",
      "Iteration: 0083 Loss: 0.99852 Time: 0.00299\n",
      "Iteration: 0084 Loss: 0.94169 Time: 0.00297\n",
      "Iteration: 0085 Loss: 0.94271 Time: 0.00297\n",
      "Iteration: 0086 Loss: 0.96608 Time: 0.00452\n",
      "Iteration: 0087 Loss: 0.93914 Time: 0.00291\n",
      "Iteration: 0088 Loss: 0.93995 Time: 0.00499\n",
      "Iteration: 0089 Loss: 0.90867 Time: 0.00304\n",
      "Iteration: 0090 Loss: 0.91775 Time: 0.00297\n",
      "Iteration: 0091 Loss: 0.92717 Time: 0.00301\n",
      "Iteration: 0092 Loss: 0.91108 Time: 0.00395\n",
      "Iteration: 0093 Loss: 0.96069 Time: 0.00404\n",
      "Iteration: 0094 Loss: 0.95098 Time: 0.00302\n",
      "Iteration: 0095 Loss: 0.88924 Time: 0.00303\n",
      "Iteration: 0096 Loss: 0.90595 Time: 0.00301\n",
      "Iteration: 0097 Loss: 0.91154 Time: 0.00400\n",
      "Iteration: 0098 Loss: 0.91082 Time: 0.00308\n",
      "Iteration: 0099 Loss: 0.88894 Time: 0.00399\n",
      "Iteration: 0100 Loss: 0.87715 Time: 0.00399\n",
      "Iteration: 0101 Loss: 0.89285 Time: 0.00397\n",
      "Iteration: 0102 Loss: 0.88907 Time: 0.00304\n",
      "Iteration: 0103 Loss: 0.91932 Time: 0.00298\n",
      "Iteration: 0104 Loss: 0.85632 Time: 0.00397\n",
      "Iteration: 0105 Loss: 0.84334 Time: 0.00501\n",
      "Iteration: 0106 Loss: 0.85538 Time: 0.00305\n",
      "Iteration: 0107 Loss: 0.85355 Time: 0.00304\n",
      "Iteration: 0108 Loss: 0.83332 Time: 0.00304\n",
      "Iteration: 0109 Loss: 0.88871 Time: 0.00392\n",
      "Iteration: 0110 Loss: 0.88380 Time: 0.00304\n",
      "Iteration: 0111 Loss: 0.87629 Time: 0.00415\n",
      "Iteration: 0112 Loss: 0.85389 Time: 0.00390\n",
      "Iteration: 0113 Loss: 0.86693 Time: 0.00402\n",
      "Iteration: 0114 Loss: 0.88861 Time: 0.00399\n",
      "Iteration: 0115 Loss: 0.86093 Time: 0.00403\n",
      "Iteration: 0116 Loss: 0.85149 Time: 0.00392\n",
      "Iteration: 0117 Loss: 0.84002 Time: 0.00408\n",
      "Iteration: 0118 Loss: 0.80045 Time: 0.00298\n",
      "Iteration: 0119 Loss: 0.80325 Time: 0.00305\n",
      "Iteration: 0120 Loss: 0.84549 Time: 0.00302\n",
      "Iteration: 0121 Loss: 0.82723 Time: 0.00299\n",
      "Iteration: 0122 Loss: 0.82633 Time: 0.00413\n",
      "Iteration: 0123 Loss: 0.82024 Time: 0.00306\n",
      "Iteration: 0124 Loss: 0.85966 Time: 0.00302\n",
      "Iteration: 0125 Loss: 0.81851 Time: 0.00396\n",
      "Iteration: 0126 Loss: 0.80262 Time: 0.00493\n",
      "Iteration: 0127 Loss: 0.81575 Time: 0.00304\n",
      "Iteration: 0128 Loss: 0.83097 Time: 0.00297\n",
      "Iteration: 0129 Loss: 0.81794 Time: 0.00304\n",
      "Iteration: 0130 Loss: 0.79053 Time: 0.00298\n",
      "Iteration: 0131 Loss: 0.79369 Time: 0.00404\n",
      "Iteration: 0132 Loss: 0.79887 Time: 0.00397\n",
      "Iteration: 0133 Loss: 0.77235 Time: 0.00406\n",
      "Iteration: 0134 Loss: 0.78547 Time: 0.00403\n",
      "Iteration: 0135 Loss: 0.78656 Time: 0.00408\n",
      "Iteration: 0136 Loss: 0.79897 Time: 0.00293\n",
      "Iteration: 0137 Loss: 0.81809 Time: 0.00407\n",
      "Iteration: 0138 Loss: 0.77427 Time: 0.00297\n",
      "Iteration: 0139 Loss: 0.75591 Time: 0.00399\n",
      "Iteration: 0140 Loss: 0.77528 Time: 0.00309\n",
      "Iteration: 0141 Loss: 0.75688 Time: 0.00491\n",
      "Iteration: 0142 Loss: 0.74425 Time: 0.00308\n",
      "Iteration: 0143 Loss: 0.76315 Time: 0.00302\n",
      "Iteration: 0144 Loss: 0.77108 Time: 0.00510\n",
      "Iteration: 0145 Loss: 0.75480 Time: 0.00491\n",
      "Iteration: 0146 Loss: 0.73504 Time: 0.00399\n",
      "Iteration: 0147 Loss: 0.73705 Time: 0.00398\n",
      "Iteration: 0148 Loss: 0.75819 Time: 0.00302\n",
      "Iteration: 0149 Loss: 0.73724 Time: 0.00304\n",
      "Iteration: 0150 Loss: 0.75244 Time: 0.00396\n",
      "Iteration: 0151 Loss: 0.73201 Time: 0.00417\n",
      "Iteration: 0152 Loss: 0.74005 Time: 0.00289\n",
      "Iteration: 0153 Loss: 0.72986 Time: 0.00299\n",
      "Iteration: 0154 Loss: 0.75603 Time: 0.00309\n",
      "Iteration: 0155 Loss: 0.72083 Time: 0.00390\n",
      "Iteration: 0156 Loss: 0.73348 Time: 0.00307\n",
      "Iteration: 0157 Loss: 0.72774 Time: 0.00297\n",
      "Iteration: 0158 Loss: 0.73409 Time: 0.00301\n",
      "Iteration: 0159 Loss: 0.74211 Time: 0.00404\n",
      "Iteration: 0160 Loss: 0.70243 Time: 0.00399\n",
      "Iteration: 0161 Loss: 0.72319 Time: 0.00407\n",
      "Iteration: 0162 Loss: 0.72339 Time: 0.00388\n",
      "Iteration: 0163 Loss: 0.73257 Time: 0.00401\n",
      "Iteration: 0164 Loss: 0.70197 Time: 0.00412\n",
      "Iteration: 0165 Loss: 0.69627 Time: 0.00390\n",
      "Iteration: 0166 Loss: 0.70647 Time: 0.00397\n",
      "Iteration: 0167 Loss: 0.70662 Time: 0.00398\n",
      "Iteration: 0168 Loss: 0.69716 Time: 0.00304\n",
      "Iteration: 0169 Loss: 0.71825 Time: 0.00299\n",
      "Iteration: 0170 Loss: 0.71239 Time: 0.00402\n",
      "Iteration: 0171 Loss: 0.69004 Time: 0.00461\n",
      "Iteration: 0172 Loss: 0.70961 Time: 0.00335\n",
      "Iteration: 0173 Loss: 0.70467 Time: 0.00401\n",
      "Iteration: 0174 Loss: 0.68942 Time: 0.00399\n",
      "Iteration: 0175 Loss: 0.69396 Time: 0.00302\n",
      "Iteration: 0176 Loss: 0.68834 Time: 0.00501\n",
      "Iteration: 0177 Loss: 0.68473 Time: 0.00297\n",
      "Iteration: 0178 Loss: 0.70634 Time: 0.00398\n",
      "Iteration: 0179 Loss: 0.68467 Time: 0.00397\n",
      "Iteration: 0180 Loss: 0.67984 Time: 0.00406\n",
      "Iteration: 0181 Loss: 0.67862 Time: 0.00304\n",
      "Iteration: 0182 Loss: 0.66828 Time: 0.00292\n",
      "Iteration: 0183 Loss: 0.68443 Time: 0.00294\n",
      "Iteration: 0184 Loss: 0.68508 Time: 0.00403\n",
      "Iteration: 0185 Loss: 0.66438 Time: 0.00299\n",
      "Iteration: 0186 Loss: 0.66294 Time: 0.00314\n",
      "Iteration: 0187 Loss: 0.66133 Time: 0.00296\n",
      "Iteration: 0188 Loss: 0.65193 Time: 0.00401\n",
      "Iteration: 0189 Loss: 0.67778 Time: 0.00300\n",
      "Iteration: 0190 Loss: 0.66837 Time: 0.00312\n",
      "Iteration: 0191 Loss: 0.65873 Time: 0.00301\n",
      "Iteration: 0192 Loss: 0.66036 Time: 0.00399\n",
      "Iteration: 0193 Loss: 0.67915 Time: 0.00404\n",
      "Iteration: 0194 Loss: 0.68186 Time: 0.00300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0195 Loss: 0.65109 Time: 0.00399\n",
      "Iteration: 0196 Loss: 0.67401 Time: 0.00405\n",
      "Iteration: 0197 Loss: 0.67855 Time: 0.00498\n",
      "Iteration: 0198 Loss: 0.65919 Time: 0.00394\n",
      "Iteration: 0199 Loss: 0.66709 Time: 0.00305\n",
      "Iteration: 0200 Loss: 0.63248 Time: 0.00400\n",
      "Done! \n",
      "\n",
      "STEP 3/3 - MODEL EVALUATION \n",
      "\n",
      "Computing the final embedding vectors, for evaluation\n",
      "Done! \n",
      "\n",
      "Testing: link prediction\n",
      "Done! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We repeat the entire training+test process FLAGS.nb_run times\n",
    "for i in range(FLAGS.nb_run):\n",
    "    if FLAGS.verbose:\n",
    "        print(\"EXPERIMENTS ON MODEL\", i + 1, \"/\", FLAGS.nb_run, \"\\n\")\n",
    "        print(\"STEP 1/3 - PREPROCESSING STEPS \\n\")\n",
    "\n",
    "\n",
    "    # Edge masking for Link Prediction:\n",
    "    if FLAGS.task == 'task_2':\n",
    "        # Compute Train/Validation/Test sets\n",
    "        if FLAGS.verbose:\n",
    "            print(\"Masking some edges from the training graph, for link prediction\")\n",
    "            print(\"(validation set:\", FLAGS.prop_val, \"% of edges - test set:\",\n",
    "                  FLAGS.prop_test, \"% of edges)\")\n",
    "        #adj, val_edges, val_edges_false, test_edges, test_edges_false = mask_test_edges(adj_init, FLAGS.prop_test, FLAGS.prop_val)\n",
    "        adj, test_edges, test_edges_false = mask_test_edges(adj_init, FLAGS.prop_test)\n",
    "        if FLAGS.verbose:\n",
    "            print(\"Done! \\n\")\n",
    "    else:\n",
    "        adj = adj_init\n",
    "\n",
    "    # Compute the number of nodes\n",
    "    num_nodes = adj.shape[0]\n",
    "\n",
    "    # Preprocessing on node features\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Preprocessing node features\")\n",
    "    if FLAGS.features:\n",
    "        features = features_init\n",
    "    else:\n",
    "        # If features are not used, replace feature matrix by identity matrix\n",
    "        features = sp.identity(num_nodes)\n",
    "    features = sparse_to_tuple(features)\n",
    "    num_features = features[2][1]\n",
    "    features_nonzero = features[1].shape[0]\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Done! \\n\")\n",
    "\n",
    "\n",
    "    # Community detection using Louvain, as a preprocessing step\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Running the Louvain algorithm for community detection\")\n",
    "        print(\"as a preprocessing step for the encoder\")\n",
    "    # Get binary community matrix (adj_louvain_init[i,j] = 1 if nodes i and j are\n",
    "    # in the same community) as well as number of communities found by Louvain\n",
    "    adj_louvain_init, nb_communities_louvain, partition = louvain_clustering(adj, FLAGS.s_reg)\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Done! Louvain has found\", nb_communities_louvain, \"communities \\n\")\n",
    "\n",
    "    # FastGAE: node sampling for stochastic subgraph decoding - used when facing scalability issues\n",
    "    if FLAGS.fastgae:\n",
    "        if FLAGS.verbose:\n",
    "            print(\"Preprocessing operations related to the FastGAE method\")\n",
    "        # Node-level p_i degree-based, core-based or uniform distribution\n",
    "        node_distribution = get_distribution(FLAGS.measure, FLAGS.alpha, adj)\n",
    "        # Node sampling for initializations\n",
    "        sampled_nodes, adj_label, adj_sampled_sparse = node_sampling(adj, node_distribution, FLAGS.nb_node_samples, FLAGS.replace)\n",
    "        if FLAGS.verbose:\n",
    "            print(\"Done! \\n\")\n",
    "    else:\n",
    "        sampled_nodes = np.array(range(FLAGS.nb_node_samples))\n",
    "\n",
    "\n",
    "    # Placeholders\n",
    "    if FLAGS.verbose:\n",
    "        print('Setting up the model and the optimizer')\n",
    "    placeholders = {\n",
    "        'features': tf.sparse_placeholder(tf.float32),\n",
    "        'adj': tf.sparse_placeholder(tf.float32),\n",
    "        'adj_layer2': tf.sparse_placeholder(tf.float32), # Only used for 2-layer GCN encoders\n",
    "        'degree_matrix': tf.sparse_placeholder(tf.float32),\n",
    "        'adj_orig': tf.sparse_placeholder(tf.float32),\n",
    "        'dropout': tf.placeholder_with_default(0., shape = ()),\n",
    "        'sampled_nodes': tf.placeholder_with_default(sampled_nodes, shape = [FLAGS.nb_node_samples])\n",
    "    }\n",
    "\n",
    "\n",
    "    # Create model\n",
    "    if FLAGS.model == 'linear_ae':\n",
    "        # Linear Graph Autoencoder\n",
    "        model = LinearModelAE(placeholders, num_features, features_nonzero)\n",
    "    elif FLAGS.model == 'linear_vae':\n",
    "        # Linear Graph Variational Autoencoder\n",
    "        model = LinearModelVAE(placeholders, num_features, num_nodes, features_nonzero)\n",
    "    elif FLAGS.model == 'gcn_ae':\n",
    "        # 2-layer GCN Graph Autoencoder\n",
    "        model = GCNModelAE(placeholders, num_features, features_nonzero)\n",
    "    elif FLAGS.model == 'gcn_vae':\n",
    "        # 2-layer GCN Graph Variational Autoencoder\n",
    "        model = GCNModelVAE(placeholders, num_features, num_nodes, features_nonzero)\n",
    "    else:\n",
    "        raise ValueError('Undefined model!')\n",
    "\n",
    "\n",
    "    # Optimizer\n",
    "    if FLAGS.fastgae:\n",
    "        num_sampled = adj_sampled_sparse.shape[0]\n",
    "        sum_sampled = adj_sampled_sparse.sum()\n",
    "        pos_weight = float(num_sampled * num_sampled - sum_sampled) / sum_sampled\n",
    "        norm = num_sampled * num_sampled / float((num_sampled * num_sampled - sum_sampled) * 2)\n",
    "    else:\n",
    "        pos_weight = float(num_nodes * num_nodes - adj.sum()) / adj.sum()\n",
    "        norm = num_nodes * num_nodes / float((num_nodes * num_nodes - adj.sum()) * 2)\n",
    "\n",
    "    if FLAGS.model in ('gcn_ae', 'linear_ae'):\n",
    "        opt = OptimizerAE(preds = model.reconstructions,\n",
    "                          labels = tf.reshape(tf.sparse_tensor_to_dense(placeholders['adj_orig'], validate_indices = False), [-1]),\n",
    "                          degree_matrix = tf.reshape(tf.sparse_tensor_to_dense(placeholders['degree_matrix'], validate_indices = False), [-1]),\n",
    "                          num_nodes = num_nodes,\n",
    "                          pos_weight = pos_weight,\n",
    "                          norm = norm,\n",
    "                          clusters_distance = model.clusters)\n",
    "    else:\n",
    "        opt = OptimizerVAE(preds = model.reconstructions,\n",
    "                           labels = tf.reshape(tf.sparse_tensor_to_dense(placeholders['adj_orig'], validate_indices = False), [-1]),\n",
    "                           degree_matrix = tf.reshape(tf.sparse_tensor_to_dense(placeholders['degree_matrix'], validate_indices = False), [-1]),\n",
    "                           model = model,\n",
    "                           num_nodes = num_nodes,\n",
    "                           pos_weight = pos_weight,\n",
    "                           norm = norm,\n",
    "                           clusters_distance = model.clusters)\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Done! \\n\")\n",
    "\n",
    "\n",
    "    # Symmetrically normalized \"message passing\" matrices\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Preprocessing on message passing matrices\")\n",
    "    adj_norm = preprocess_graph(adj + FLAGS.lamb*adj_louvain_init)\n",
    "    adj_norm_layer2 = preprocess_graph(adj)\n",
    "    if not FLAGS.fastgae:\n",
    "        adj_label = sparse_to_tuple(adj + sp.eye(num_nodes))\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Done! \\n\")\n",
    "\n",
    "\n",
    "    # Degree matrices\n",
    "    deg_matrix, deg_matrix_init = preprocess_degree(adj, FLAGS.simple)\n",
    "\n",
    "\n",
    "    # Initialize TF session\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Initializing TF session\")\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Done! \\n\")\n",
    "\n",
    "\n",
    "    # Model training\n",
    "    if FLAGS.verbose:\n",
    "        print(\"STEP 2/3 - MODEL TRAINING \\n\")\n",
    "        print(\"Starting training\")\n",
    "\n",
    "    for iter in range(FLAGS.iterations):\n",
    "\n",
    "        # Flag to compute running time for each iteration\n",
    "        t = time.time()\n",
    "\n",
    "        # Construct feed dictionary\n",
    "        feed_dict = construct_feed_dict(adj_norm, adj_norm_layer2, adj_label, features, deg_matrix, placeholders)\n",
    "        if FLAGS.fastgae:\n",
    "            # Update sampled subgraph and sampled Louvain matrix\n",
    "            feed_dict.update({placeholders['sampled_nodes']: sampled_nodes})\n",
    "            # New node sampling\n",
    "            sampled_nodes, adj_label, adj_label_sparse, = node_sampling(adj, node_distribution, FLAGS.nb_node_samples, FLAGS.replace)\n",
    "\n",
    "        # Weights update\n",
    "        outs = sess.run([opt.opt_op, opt.cost, opt.cost_adj, opt.cost_mod],\n",
    "                        feed_dict = feed_dict)\n",
    "\n",
    "        # Compute average loss\n",
    "        avg_cost = outs[1]\n",
    "        if FLAGS.verbose:\n",
    "            # Display information on the iteration\n",
    "            print(\"Iteration:\", '%04d' % (iter + 1), \"Loss:\", \"{:.5f}\".format(avg_cost),\n",
    "                  \"Time:\", \"{:.5f}\".format(time.time() - t))\n",
    "\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Done! \\n\")\n",
    "\n",
    "    # Compute embedding vectors, for evaluation\n",
    "    if FLAGS.verbose:\n",
    "        print(\"STEP 3/3 - MODEL EVALUATION \\n\")\n",
    "        print(\"Computing the final embedding vectors, for evaluation\")\n",
    "    emb = sess.run(model.z_mean, feed_dict = feed_dict)\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Done! \\n\")\n",
    "\n",
    "    # Test model: link prediction (classification edges/non-edges)\n",
    "\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Testing: link prediction\")\n",
    "    # Get ROC and AP scores\n",
    "    roc_score, ap_score = link_prediction(test_edges, test_edges_false, emb)\n",
    "    mean_roc.append(roc_score)\n",
    "    mean_ap.append(ap_score)\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Done! \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bef29d76",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-28T09:18:42.475080Z",
     "start_time": "2022-10-28T09:18:42.461853Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL RESULTS \n",
      "\n",
      "Recall: the selected task was \"Task 2\", i.e., joint community detection and link prediction, on wisconsin\n",
      "All scores reported below are computed over the 10 run(s) \n",
      "\n",
      "Link prediction:\n",
      "\n",
      "Mean AUC score:  0.6578765432098765\n",
      "Std of AUC scores:  0.038040593115574145 \n",
      "\n",
      "Mean AP score:  0.7437334045204061\n",
      "Std of AP scores:  0.03917624111360606 \n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Report final results\n",
    "print(\"FINAL RESULTS \\n\")\n",
    "\n",
    "print('Recall: the selected task was \"Task 2\", i.e., joint community detection and link prediction, on', FLAGS.dataset)\n",
    "\n",
    "print(\"All scores reported below are computed over the\", FLAGS.nb_run, \"run(s) \\n\")\n",
    "\n",
    "print(\"Link prediction:\\n\")\n",
    "print(\"Mean AUC score: \", np.mean(mean_roc))\n",
    "print(\"Std of AUC scores: \", np.std(mean_roc), \"\\n\")\n",
    "\n",
    "print(\"Mean AP score: \", np.mean(mean_ap))\n",
    "print(\"Std of AP scores: \", np.std(mean_ap), \"\\n \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553e6eb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf1.15",
   "language": "python",
   "name": "tf1.15"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
