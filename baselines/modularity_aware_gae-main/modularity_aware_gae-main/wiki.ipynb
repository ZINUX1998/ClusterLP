{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67dbcca0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-28T08:59:28.375393Z",
     "start_time": "2022-10-28T08:59:26.211678Z"
    }
   },
   "outputs": [],
   "source": [
    "from modularity_aware_gae.evaluation import community_detection, link_prediction\n",
    "from modularity_aware_gae.input_data import load_data, load_labels\n",
    "from modularity_aware_gae.louvain import louvain_clustering\n",
    "from modularity_aware_gae.model import  GCNModelAE, GCNModelVAE, LinearModelAE, LinearModelVAE\n",
    "from modularity_aware_gae.optimizer import OptimizerAE, OptimizerVAE\n",
    "from modularity_aware_gae.preprocessing import *\n",
    "from modularity_aware_gae.sampling import get_distribution, node_sampling\n",
    "import numpy as np\n",
    "import os\n",
    "import scipy.sparse as sp\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel')#添加的，不报错\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14ffd620",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-28T08:59:28.452829Z",
     "start_time": "2022-10-28T08:59:28.438584Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " \n",
      " \n",
      "[MODULARITY-AWARE GRAPH AUTOENCODERS]\n",
      " \n",
      " \n",
      " \n",
      "\n",
      "EXPERIMENTAL SETTING \n",
      "\n",
      "- Graph dataset: wiki\n",
      "- Mode name: linear_vae\n",
      "- Number of models to train: 10\n",
      "- Number of training iterations for each model: 200\n",
      "- Learning rate: 0.01\n",
      "- Dropout rate: 0.0\n",
      "- Use of node features in the input layer: False\n",
      "- Dimension of the output layer: 16\n",
      "- lambda: 0.0\n",
      "- beta: 0.0\n",
      "- gamma: 1.0\n",
      "- s: 2\n",
      "- FastGAE: no \n",
      "\n",
      "Final embedding vectors will be evaluated on:\n",
      "- Task 2, i.e., joint community detection and link prediction\n",
      "\n",
      " \n",
      " \n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "# Parameters related to the input data\n",
    "flags.DEFINE_string('dataset', 'wiki', 'Graph dataset, among: cora, citeseer, wiki, celegans, email, polbooks, texas, wisconsin')\n",
    "\n",
    "flags.DEFINE_boolean('features', False, 'Whether to include node features')\n",
    "\n",
    "\n",
    "# Parameters related to the Modularity-Aware GAE/VGAE model to train\n",
    "\n",
    "# 1/3 - General parameters associated with GAE/VGAE\n",
    "flags.DEFINE_string('model', 'linear_vae', 'Model to train, among: gcn_ae, gcn_vae, \\\n",
    "                                            linear_ae, linear_vae')\n",
    "flags.DEFINE_float('dropout', 0., 'Dropout rate')\n",
    "flags.DEFINE_integer('iterations', 200, 'Number of iterations in training')\n",
    "flags.DEFINE_float('learning_rate', 0.01, 'Initial learning rate (Adam)')\n",
    "flags.DEFINE_integer('hidden', 32, 'Dimension of the GCN hidden layer')\n",
    "flags.DEFINE_integer('dimension', 16, 'Dimension of the output layer, i.e., \\\n",
    "                                       dimension of the embedding space')\n",
    "\n",
    "# 2/3 - Additional parameters, specific to Modularity-Aware models\n",
    "flags.DEFINE_float('beta', 0.0, 'Beta hyperparameter of Mod.-Aware models')\n",
    "flags.DEFINE_float('lamb', 0.0, 'Lambda hyperparameter of Mod.-Aware models')\n",
    "flags.DEFINE_float('gamma', 1.0, 'Gamma hyperparameter of Mod.-Aware models')\n",
    "flags.DEFINE_integer('s_reg', 2, 's hyperparameter of Mod.-Aware models')\n",
    "\n",
    "# 3/3 - Additional parameters, aiming to improve scalability\n",
    "flags.DEFINE_boolean('fastgae', False, 'Whether to use the FastGAE framework')\n",
    "flags.DEFINE_integer('nb_node_samples', 1000, 'In FastGAE, number of nodes to \\\n",
    "                                               sample at each iteration, i.e., \\\n",
    "                                               size of decoded subgraph')\n",
    "flags.DEFINE_string('measure', 'degree', 'In FastGAE, node importance measure used \\\n",
    "                                          for sampling: degree, core, or uniform')\n",
    "flags.DEFINE_float('alpha', 1.0, 'alpha hyperparameter associated with the FastGAE sampling')\n",
    "flags.DEFINE_boolean('replace', False, 'Sample nodes with or without replacement')\n",
    "flags.DEFINE_boolean('simple', True, 'Use simpler (and faster) modularity in optimizers')\n",
    "\n",
    "\n",
    "# Parameters related to the experimental setup\n",
    "flags.DEFINE_string('task', 'task_2', 'task_1: pure community detection \\\n",
    "                                       task_2: joint link prediction and \\\n",
    "                                               community detection')\n",
    "flags.DEFINE_integer('nb_run', 10, 'Number of model run + test')\n",
    "flags.DEFINE_float('prop_val', 1., 'Proportion of edges in validation set \\\n",
    "                                    for the link prediction task')\n",
    "flags.DEFINE_float('prop_test', 10., 'Proportion of edges in test set \\\n",
    "                                      for the link prediction task')\n",
    "flags.DEFINE_boolean('validation', False, 'Whether to compute validation \\\n",
    "                                           results at each iteration, for \\\n",
    "                                           the link prediction task')\n",
    "flags.DEFINE_boolean('verbose', True, 'Whether to print all comments details')\n",
    "\n",
    "\n",
    "# Introductory message\n",
    "if FLAGS.verbose:\n",
    "    introductory_message()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ce16ae4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-28T08:59:28.578993Z",
     "start_time": "2022-10-28T08:59:28.516718Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADING DATA\n",
      "\n",
      "Loading the wiki graph\n",
      "- Number of nodes: 2405\n",
      "- Use of node features: False\n",
      "Done! \n",
      " \n",
      " \n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize lists to collect final results\n",
    "mean_ami = []\n",
    "mean_ari = []\n",
    "\n",
    "mean_roc = []\n",
    "mean_ap = []\n",
    "\n",
    "# Load data\n",
    "if FLAGS.verbose:\n",
    "    print(\"LOADING DATA\\n\")\n",
    "    print(\"Loading the\", FLAGS.dataset, \"graph\")\n",
    "adj_init, features_init = load_data(FLAGS.dataset)\n",
    "#labels = load_labels(adj_init.shape[0])\n",
    "if FLAGS.verbose:\n",
    "    print(\"- Number of nodes:\", adj_init.shape[0])\n",
    "    #print(\"- Number of communities:\", len(np.unique(labels)))\n",
    "    print(\"- Use of node features:\", FLAGS.features)\n",
    "    print(\"Done! \\n \\n \\n \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd19b933",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-28T09:06:07.836272Z",
     "start_time": "2022-10-28T08:59:28.642941Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENTS ON MODEL 1 / 10 \n",
      "\n",
      "STEP 1/3 - PREPROCESSING STEPS \n",
      "\n",
      "Masking some edges from the training graph, for link prediction\n",
      "(validation set: 1.0 % of edges - test set: 10.0 % of edges)\n",
      "Done! \n",
      "\n",
      "Preprocessing node features\n",
      "Done! \n",
      "\n",
      "Running the Louvain algorithm for community detection\n",
      "as a preprocessing step for the encoder\n",
      "Done! Louvain has found 88 communities \n",
      "\n",
      "Setting up the model and the optimizer\n",
      "Done! \n",
      "\n",
      "Preprocessing on message passing matrices\n",
      "Done! \n",
      "\n",
      "Initializing TF session\n",
      "Done! \n",
      "\n",
      "STEP 2/3 - MODEL TRAINING \n",
      "\n",
      "Starting training\n",
      "Iteration: 0001 Loss: 1.71464 Time: 0.41946\n",
      "Iteration: 0002 Loss: 1.70685 Time: 0.19829\n",
      "Iteration: 0003 Loss: 1.68361 Time: 0.20042\n",
      "Iteration: 0004 Loss: 1.63546 Time: 0.19514\n",
      "Iteration: 0005 Loss: 1.62812 Time: 0.20058\n",
      "Iteration: 0006 Loss: 1.61413 Time: 0.20000\n",
      "Iteration: 0007 Loss: 1.58619 Time: 0.19592\n",
      "Iteration: 0008 Loss: 1.56906 Time: 0.19473\n",
      "Iteration: 0009 Loss: 1.56394 Time: 0.18913\n",
      "Iteration: 0010 Loss: 1.47852 Time: 0.20143\n",
      "Iteration: 0011 Loss: 1.49387 Time: 0.19011\n",
      "Iteration: 0012 Loss: 1.46724 Time: 0.18928\n",
      "Iteration: 0013 Loss: 1.44367 Time: 0.19308\n",
      "Iteration: 0014 Loss: 1.44797 Time: 0.19091\n",
      "Iteration: 0015 Loss: 1.42177 Time: 0.19656\n",
      "Iteration: 0016 Loss: 1.40759 Time: 0.20050\n",
      "Iteration: 0017 Loss: 1.38333 Time: 0.19715\n",
      "Iteration: 0018 Loss: 1.37886 Time: 0.19371\n",
      "Iteration: 0019 Loss: 1.34006 Time: 0.18817\n",
      "Iteration: 0020 Loss: 1.35064 Time: 0.18857\n",
      "Iteration: 0021 Loss: 1.32602 Time: 0.19836\n",
      "Iteration: 0022 Loss: 1.30687 Time: 0.19504\n",
      "Iteration: 0023 Loss: 1.27857 Time: 0.19190\n",
      "Iteration: 0024 Loss: 1.28949 Time: 0.19136\n",
      "Iteration: 0025 Loss: 1.26400 Time: 0.19263\n",
      "Iteration: 0026 Loss: 1.26304 Time: 0.19150\n",
      "Iteration: 0027 Loss: 1.24812 Time: 0.20015\n",
      "Iteration: 0028 Loss: 1.22899 Time: 0.19874\n",
      "Iteration: 0029 Loss: 1.23570 Time: 0.19001\n",
      "Iteration: 0030 Loss: 1.20964 Time: 0.19197\n",
      "Iteration: 0031 Loss: 1.19679 Time: 0.18798\n",
      "Iteration: 0032 Loss: 1.20088 Time: 0.19013\n",
      "Iteration: 0033 Loss: 1.18784 Time: 0.19565\n",
      "Iteration: 0034 Loss: 1.17072 Time: 0.19521\n",
      "Iteration: 0035 Loss: 1.15919 Time: 0.19436\n",
      "Iteration: 0036 Loss: 1.13854 Time: 0.19256\n",
      "Iteration: 0037 Loss: 1.14798 Time: 0.19697\n",
      "Iteration: 0038 Loss: 1.13922 Time: 0.19287\n",
      "Iteration: 0039 Loss: 1.13210 Time: 0.19257\n",
      "Iteration: 0040 Loss: 1.11004 Time: 0.19692\n",
      "Iteration: 0041 Loss: 1.10679 Time: 0.19224\n",
      "Iteration: 0042 Loss: 1.07792 Time: 0.18490\n",
      "Iteration: 0043 Loss: 1.06681 Time: 0.19626\n",
      "Iteration: 0044 Loss: 1.08775 Time: 0.18930\n",
      "Iteration: 0045 Loss: 1.06390 Time: 0.19253\n",
      "Iteration: 0046 Loss: 1.05162 Time: 0.19502\n",
      "Iteration: 0047 Loss: 1.04212 Time: 0.19975\n",
      "Iteration: 0048 Loss: 1.03798 Time: 0.20115\n",
      "Iteration: 0049 Loss: 1.03493 Time: 0.19627\n",
      "Iteration: 0050 Loss: 1.02911 Time: 0.20183\n",
      "Iteration: 0051 Loss: 1.02898 Time: 0.19952\n",
      "Iteration: 0052 Loss: 1.00559 Time: 0.19245\n",
      "Iteration: 0053 Loss: 1.00331 Time: 0.18942\n",
      "Iteration: 0054 Loss: 0.99579 Time: 0.19914\n",
      "Iteration: 0055 Loss: 0.97368 Time: 0.20217\n",
      "Iteration: 0056 Loss: 0.96824 Time: 0.19272\n",
      "Iteration: 0057 Loss: 0.96530 Time: 0.19643\n",
      "Iteration: 0058 Loss: 0.95797 Time: 0.20124\n",
      "Iteration: 0059 Loss: 0.96311 Time: 0.19559\n",
      "Iteration: 0060 Loss: 0.94951 Time: 0.19270\n",
      "Iteration: 0061 Loss: 0.93674 Time: 0.18989\n",
      "Iteration: 0062 Loss: 0.94264 Time: 0.19204\n",
      "Iteration: 0063 Loss: 0.92129 Time: 0.19827\n",
      "Iteration: 0064 Loss: 0.92350 Time: 0.20351\n",
      "Iteration: 0065 Loss: 0.90917 Time: 0.20227\n",
      "Iteration: 0066 Loss: 0.90980 Time: 0.19600\n",
      "Iteration: 0067 Loss: 0.90599 Time: 0.19309\n",
      "Iteration: 0068 Loss: 0.89866 Time: 0.20230\n",
      "Iteration: 0069 Loss: 0.88837 Time: 0.19471\n",
      "Iteration: 0070 Loss: 0.88561 Time: 0.20352\n",
      "Iteration: 0071 Loss: 0.88138 Time: 0.19936\n",
      "Iteration: 0072 Loss: 0.86514 Time: 0.19870\n",
      "Iteration: 0073 Loss: 0.85417 Time: 0.19722\n",
      "Iteration: 0074 Loss: 0.85162 Time: 0.19497\n",
      "Iteration: 0075 Loss: 0.85112 Time: 0.19802\n",
      "Iteration: 0076 Loss: 0.83809 Time: 0.20400\n",
      "Iteration: 0077 Loss: 0.84518 Time: 0.19355\n",
      "Iteration: 0078 Loss: 0.83523 Time: 0.19330\n",
      "Iteration: 0079 Loss: 0.82910 Time: 0.20340\n",
      "Iteration: 0080 Loss: 0.82558 Time: 0.20070\n",
      "Iteration: 0081 Loss: 0.81808 Time: 0.19911\n",
      "Iteration: 0082 Loss: 0.81266 Time: 0.19586\n",
      "Iteration: 0083 Loss: 0.80440 Time: 0.19664\n",
      "Iteration: 0084 Loss: 0.80140 Time: 0.19819\n",
      "Iteration: 0085 Loss: 0.79668 Time: 0.19531\n",
      "Iteration: 0086 Loss: 0.78730 Time: 0.19622\n",
      "Iteration: 0087 Loss: 0.79484 Time: 0.19606\n",
      "Iteration: 0088 Loss: 0.77743 Time: 0.19983\n",
      "Iteration: 0089 Loss: 0.77565 Time: 0.19812\n",
      "Iteration: 0090 Loss: 0.76811 Time: 0.18790\n",
      "Iteration: 0091 Loss: 0.76795 Time: 0.19514\n",
      "Iteration: 0092 Loss: 0.76320 Time: 0.19326\n",
      "Iteration: 0093 Loss: 0.76179 Time: 0.18789\n",
      "Iteration: 0094 Loss: 0.75080 Time: 0.19325\n",
      "Iteration: 0095 Loss: 0.75274 Time: 0.19057\n",
      "Iteration: 0096 Loss: 0.74982 Time: 0.19307\n",
      "Iteration: 0097 Loss: 0.74108 Time: 0.19271\n",
      "Iteration: 0098 Loss: 0.74487 Time: 0.19101\n",
      "Iteration: 0099 Loss: 0.73645 Time: 0.19700\n",
      "Iteration: 0100 Loss: 0.72963 Time: 0.19768\n",
      "Iteration: 0101 Loss: 0.72867 Time: 0.19060\n",
      "Iteration: 0102 Loss: 0.72439 Time: 0.19412\n",
      "Iteration: 0103 Loss: 0.72512 Time: 0.18742\n",
      "Iteration: 0104 Loss: 0.71482 Time: 0.17973\n",
      "Iteration: 0105 Loss: 0.71502 Time: 0.20126\n",
      "Iteration: 0106 Loss: 0.70458 Time: 0.19571\n",
      "Iteration: 0107 Loss: 0.70924 Time: 0.19215\n",
      "Iteration: 0108 Loss: 0.70454 Time: 0.19403\n",
      "Iteration: 0109 Loss: 0.70689 Time: 0.19599\n",
      "Iteration: 0110 Loss: 0.70186 Time: 0.19935\n",
      "Iteration: 0111 Loss: 0.69344 Time: 0.19783\n",
      "Iteration: 0112 Loss: 0.69015 Time: 0.19667\n",
      "Iteration: 0113 Loss: 0.69309 Time: 0.19195\n",
      "Iteration: 0114 Loss: 0.68664 Time: 0.19213\n",
      "Iteration: 0115 Loss: 0.68497 Time: 0.19162\n",
      "Iteration: 0116 Loss: 0.67356 Time: 0.19255\n",
      "Iteration: 0117 Loss: 0.68134 Time: 0.18952\n",
      "Iteration: 0118 Loss: 0.67335 Time: 0.19297\n",
      "Iteration: 0119 Loss: 0.66980 Time: 0.19447\n",
      "Iteration: 0120 Loss: 0.67234 Time: 0.18999\n",
      "Iteration: 0121 Loss: 0.66960 Time: 0.19200\n",
      "Iteration: 0122 Loss: 0.66501 Time: 0.18822\n",
      "Iteration: 0123 Loss: 0.66590 Time: 0.18981\n",
      "Iteration: 0124 Loss: 0.66804 Time: 0.18897\n",
      "Iteration: 0125 Loss: 0.65888 Time: 0.18561\n",
      "Iteration: 0126 Loss: 0.66093 Time: 0.19154\n",
      "Iteration: 0127 Loss: 0.65733 Time: 0.18763\n",
      "Iteration: 0128 Loss: 0.65676 Time: 0.18809\n",
      "Iteration: 0129 Loss: 0.65397 Time: 0.19083\n",
      "Iteration: 0130 Loss: 0.65373 Time: 0.18629\n",
      "Iteration: 0131 Loss: 0.65078 Time: 0.19429\n",
      "Iteration: 0132 Loss: 0.65058 Time: 0.19561\n",
      "Iteration: 0133 Loss: 0.65040 Time: 0.19005\n",
      "Iteration: 0134 Loss: 0.65113 Time: 0.18289\n",
      "Iteration: 0135 Loss: 0.64354 Time: 0.18245\n",
      "Iteration: 0136 Loss: 0.64021 Time: 0.18921\n",
      "Iteration: 0137 Loss: 0.64510 Time: 0.18931\n",
      "Iteration: 0138 Loss: 0.63796 Time: 0.18727\n",
      "Iteration: 0139 Loss: 0.64264 Time: 0.18643\n",
      "Iteration: 0140 Loss: 0.63648 Time: 0.18690\n",
      "Iteration: 0141 Loss: 0.63589 Time: 0.18263\n",
      "Iteration: 0142 Loss: 0.63111 Time: 0.20031\n",
      "Iteration: 0143 Loss: 0.62677 Time: 0.19839\n",
      "Iteration: 0144 Loss: 0.62487 Time: 0.19220\n",
      "Iteration: 0145 Loss: 0.62562 Time: 0.18281\n",
      "Iteration: 0146 Loss: 0.62818 Time: 0.18968\n",
      "Iteration: 0147 Loss: 0.63123 Time: 0.18688\n",
      "Iteration: 0148 Loss: 0.61949 Time: 0.18800\n",
      "Iteration: 0149 Loss: 0.62515 Time: 0.19538\n",
      "Iteration: 0150 Loss: 0.61967 Time: 0.19148\n",
      "Iteration: 0151 Loss: 0.61561 Time: 0.18836\n",
      "Iteration: 0152 Loss: 0.62183 Time: 0.19054\n",
      "Iteration: 0153 Loss: 0.61473 Time: 0.19069\n",
      "Iteration: 0154 Loss: 0.61622 Time: 0.19539\n",
      "Iteration: 0155 Loss: 0.61473 Time: 0.19019\n",
      "Iteration: 0156 Loss: 0.61172 Time: 0.19009\n",
      "Iteration: 0157 Loss: 0.61393 Time: 0.19997\n",
      "Iteration: 0158 Loss: 0.60757 Time: 0.20096\n",
      "Iteration: 0159 Loss: 0.60652 Time: 0.19164\n",
      "Iteration: 0160 Loss: 0.60583 Time: 0.20488\n",
      "Iteration: 0161 Loss: 0.60869 Time: 0.19936\n",
      "Iteration: 0162 Loss: 0.60814 Time: 0.19509\n",
      "Iteration: 0163 Loss: 0.60258 Time: 0.18994\n",
      "Iteration: 0164 Loss: 0.60314 Time: 0.19158\n",
      "Iteration: 0165 Loss: 0.59807 Time: 0.18846\n",
      "Iteration: 0166 Loss: 0.60011 Time: 0.17833\n",
      "Iteration: 0167 Loss: 0.60010 Time: 0.20460\n",
      "Iteration: 0168 Loss: 0.59694 Time: 0.18962\n",
      "Iteration: 0169 Loss: 0.59615 Time: 0.18672\n",
      "Iteration: 0170 Loss: 0.59819 Time: 0.19092\n",
      "Iteration: 0171 Loss: 0.59298 Time: 0.18873\n",
      "Iteration: 0172 Loss: 0.59617 Time: 0.18808\n",
      "Iteration: 0173 Loss: 0.58881 Time: 0.19567\n",
      "Iteration: 0174 Loss: 0.59056 Time: 0.18933\n",
      "Iteration: 0175 Loss: 0.59463 Time: 0.18595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0176 Loss: 0.58976 Time: 0.18412\n",
      "Iteration: 0177 Loss: 0.58526 Time: 0.18519\n",
      "Iteration: 0178 Loss: 0.59000 Time: 0.18872\n",
      "Iteration: 0179 Loss: 0.58635 Time: 0.18807\n",
      "Iteration: 0180 Loss: 0.58751 Time: 0.18729\n",
      "Iteration: 0181 Loss: 0.58561 Time: 0.19054\n",
      "Iteration: 0182 Loss: 0.57999 Time: 0.18390\n",
      "Iteration: 0183 Loss: 0.58093 Time: 0.19272\n",
      "Iteration: 0184 Loss: 0.58419 Time: 0.18649\n",
      "Iteration: 0185 Loss: 0.57750 Time: 0.19115\n",
      "Iteration: 0186 Loss: 0.57773 Time: 0.18542\n",
      "Iteration: 0187 Loss: 0.57751 Time: 0.17567\n",
      "Iteration: 0188 Loss: 0.57732 Time: 0.18506\n",
      "Iteration: 0189 Loss: 0.57610 Time: 0.19555\n",
      "Iteration: 0190 Loss: 0.57421 Time: 0.18642\n",
      "Iteration: 0191 Loss: 0.57170 Time: 0.18699\n",
      "Iteration: 0192 Loss: 0.57248 Time: 0.18858\n",
      "Iteration: 0193 Loss: 0.57034 Time: 0.19014\n",
      "Iteration: 0194 Loss: 0.57016 Time: 0.18930\n",
      "Iteration: 0195 Loss: 0.56836 Time: 0.18797\n",
      "Iteration: 0196 Loss: 0.56671 Time: 0.19115\n",
      "Iteration: 0197 Loss: 0.56802 Time: 0.18797\n",
      "Iteration: 0198 Loss: 0.56146 Time: 0.18605\n",
      "Iteration: 0199 Loss: 0.56831 Time: 0.18725\n",
      "Iteration: 0200 Loss: 0.56592 Time: 0.18950\n",
      "Done! \n",
      "\n",
      "STEP 3/3 - MODEL EVALUATION \n",
      "\n",
      "Computing the final embedding vectors, for evaluation\n",
      "Done! \n",
      "\n",
      "Testing: link prediction\n",
      "Done! \n",
      "\n",
      "EXPERIMENTS ON MODEL 2 / 10 \n",
      "\n",
      "STEP 1/3 - PREPROCESSING STEPS \n",
      "\n",
      "Masking some edges from the training graph, for link prediction\n",
      "(validation set: 1.0 % of edges - test set: 10.0 % of edges)\n",
      "Done! \n",
      "\n",
      "Preprocessing node features\n",
      "Done! \n",
      "\n",
      "Running the Louvain algorithm for community detection\n",
      "as a preprocessing step for the encoder\n",
      "Done! Louvain has found 86 communities \n",
      "\n",
      "Setting up the model and the optimizer\n",
      "Done! \n",
      "\n",
      "Preprocessing on message passing matrices\n",
      "Done! \n",
      "\n",
      "Initializing TF session\n",
      "Done! \n",
      "\n",
      "STEP 2/3 - MODEL TRAINING \n",
      "\n",
      "Starting training\n",
      "Iteration: 0001 Loss: 1.70757 Time: 0.38894\n",
      "Iteration: 0002 Loss: 1.70291 Time: 0.19234\n",
      "Iteration: 0003 Loss: 1.66434 Time: 0.18836\n",
      "Iteration: 0004 Loss: 1.67606 Time: 0.18528\n",
      "Iteration: 0005 Loss: 1.61155 Time: 0.18326\n",
      "Iteration: 0006 Loss: 1.62585 Time: 0.19004\n",
      "Iteration: 0007 Loss: 1.56939 Time: 0.19187\n",
      "Iteration: 0008 Loss: 1.54686 Time: 0.19127\n",
      "Iteration: 0009 Loss: 1.52668 Time: 0.18298\n",
      "Iteration: 0010 Loss: 1.49807 Time: 0.18322\n",
      "Iteration: 0011 Loss: 1.49955 Time: 0.19066\n",
      "Iteration: 0012 Loss: 1.45561 Time: 0.18941\n",
      "Iteration: 0013 Loss: 1.46532 Time: 0.19049\n",
      "Iteration: 0014 Loss: 1.45604 Time: 0.18396\n",
      "Iteration: 0015 Loss: 1.42525 Time: 0.18887\n",
      "Iteration: 0016 Loss: 1.41738 Time: 0.18885\n",
      "Iteration: 0017 Loss: 1.36713 Time: 0.18680\n",
      "Iteration: 0018 Loss: 1.38429 Time: 0.19447\n",
      "Iteration: 0019 Loss: 1.34656 Time: 0.18831\n",
      "Iteration: 0020 Loss: 1.34394 Time: 0.17979\n",
      "Iteration: 0021 Loss: 1.33951 Time: 0.19020\n",
      "Iteration: 0022 Loss: 1.30755 Time: 0.19190\n",
      "Iteration: 0023 Loss: 1.28745 Time: 0.18706\n",
      "Iteration: 0024 Loss: 1.28124 Time: 0.18717\n",
      "Iteration: 0025 Loss: 1.27426 Time: 0.18782\n",
      "Iteration: 0026 Loss: 1.25842 Time: 0.18816\n",
      "Iteration: 0027 Loss: 1.24886 Time: 0.18535\n",
      "Iteration: 0028 Loss: 1.23690 Time: 0.18992\n",
      "Iteration: 0029 Loss: 1.22372 Time: 0.18982\n",
      "Iteration: 0030 Loss: 1.19918 Time: 0.18372\n",
      "Iteration: 0031 Loss: 1.19476 Time: 0.18958\n",
      "Iteration: 0032 Loss: 1.17371 Time: 0.18835\n",
      "Iteration: 0033 Loss: 1.16499 Time: 0.18934\n",
      "Iteration: 0034 Loss: 1.15453 Time: 0.19320\n",
      "Iteration: 0035 Loss: 1.13866 Time: 0.18764\n",
      "Iteration: 0036 Loss: 1.14068 Time: 0.18598\n",
      "Iteration: 0037 Loss: 1.14195 Time: 0.18552\n",
      "Iteration: 0038 Loss: 1.14321 Time: 0.18961\n",
      "Iteration: 0039 Loss: 1.09656 Time: 0.19281\n",
      "Iteration: 0040 Loss: 1.10605 Time: 0.19267\n",
      "Iteration: 0041 Loss: 1.09624 Time: 0.18398\n",
      "Iteration: 0042 Loss: 1.09592 Time: 0.18238\n",
      "Iteration: 0043 Loss: 1.07799 Time: 0.18747\n",
      "Iteration: 0044 Loss: 1.09184 Time: 0.18917\n",
      "Iteration: 0045 Loss: 1.06215 Time: 0.18772\n",
      "Iteration: 0046 Loss: 1.05320 Time: 0.19010\n",
      "Iteration: 0047 Loss: 1.03963 Time: 0.18508\n",
      "Iteration: 0048 Loss: 1.03727 Time: 0.18617\n",
      "Iteration: 0049 Loss: 1.02889 Time: 0.18899\n",
      "Iteration: 0050 Loss: 1.00980 Time: 0.18538\n",
      "Iteration: 0051 Loss: 1.01655 Time: 0.18733\n",
      "Iteration: 0052 Loss: 1.00853 Time: 0.18503\n",
      "Iteration: 0053 Loss: 1.00232 Time: 0.18851\n",
      "Iteration: 0054 Loss: 0.99555 Time: 0.18842\n",
      "Iteration: 0055 Loss: 0.98059 Time: 0.18829\n",
      "Iteration: 0056 Loss: 0.96071 Time: 0.19082\n",
      "Iteration: 0057 Loss: 0.95811 Time: 0.18990\n",
      "Iteration: 0058 Loss: 0.96982 Time: 0.19342\n",
      "Iteration: 0059 Loss: 0.94791 Time: 0.18208\n",
      "Iteration: 0060 Loss: 0.95400 Time: 0.18545\n",
      "Iteration: 0061 Loss: 0.95430 Time: 0.18645\n",
      "Iteration: 0062 Loss: 0.92877 Time: 0.18448\n",
      "Iteration: 0063 Loss: 0.91576 Time: 0.18369\n",
      "Iteration: 0064 Loss: 0.90906 Time: 0.19531\n",
      "Iteration: 0065 Loss: 0.91225 Time: 0.18713\n",
      "Iteration: 0066 Loss: 0.90171 Time: 0.18618\n",
      "Iteration: 0067 Loss: 0.90389 Time: 0.18576\n",
      "Iteration: 0068 Loss: 0.89520 Time: 0.18356\n",
      "Iteration: 0069 Loss: 0.88152 Time: 0.19068\n",
      "Iteration: 0070 Loss: 0.88371 Time: 0.18845\n",
      "Iteration: 0071 Loss: 0.87816 Time: 0.18635\n",
      "Iteration: 0072 Loss: 0.85689 Time: 0.18941\n",
      "Iteration: 0073 Loss: 0.86778 Time: 0.18006\n",
      "Iteration: 0074 Loss: 0.85186 Time: 0.17942\n",
      "Iteration: 0075 Loss: 0.84244 Time: 0.18712\n",
      "Iteration: 0076 Loss: 0.83891 Time: 0.20016\n",
      "Iteration: 0077 Loss: 0.83399 Time: 0.18971\n",
      "Iteration: 0078 Loss: 0.82442 Time: 0.18785\n",
      "Iteration: 0079 Loss: 0.82291 Time: 0.18409\n",
      "Iteration: 0080 Loss: 0.81586 Time: 0.19051\n",
      "Iteration: 0081 Loss: 0.81749 Time: 0.18049\n",
      "Iteration: 0082 Loss: 0.81441 Time: 0.19461\n",
      "Iteration: 0083 Loss: 0.80321 Time: 0.18531\n",
      "Iteration: 0084 Loss: 0.80073 Time: 0.17940\n",
      "Iteration: 0085 Loss: 0.78406 Time: 0.18452\n",
      "Iteration: 0086 Loss: 0.78293 Time: 0.19403\n",
      "Iteration: 0087 Loss: 0.78646 Time: 0.18638\n",
      "Iteration: 0088 Loss: 0.78030 Time: 0.19346\n",
      "Iteration: 0089 Loss: 0.76319 Time: 0.18847\n",
      "Iteration: 0090 Loss: 0.77436 Time: 0.18712\n",
      "Iteration: 0091 Loss: 0.75968 Time: 0.19171\n",
      "Iteration: 0092 Loss: 0.76241 Time: 0.19093\n",
      "Iteration: 0093 Loss: 0.75991 Time: 0.18931\n",
      "Iteration: 0094 Loss: 0.74906 Time: 0.18894\n",
      "Iteration: 0095 Loss: 0.74884 Time: 0.17964\n",
      "Iteration: 0096 Loss: 0.74436 Time: 0.19092\n",
      "Iteration: 0097 Loss: 0.73466 Time: 0.18572\n",
      "Iteration: 0098 Loss: 0.72913 Time: 0.18834\n",
      "Iteration: 0099 Loss: 0.73339 Time: 0.18911\n",
      "Iteration: 0100 Loss: 0.72934 Time: 0.18767\n",
      "Iteration: 0101 Loss: 0.72334 Time: 0.18897\n",
      "Iteration: 0102 Loss: 0.71956 Time: 0.19317\n",
      "Iteration: 0103 Loss: 0.71738 Time: 0.19078\n",
      "Iteration: 0104 Loss: 0.70617 Time: 0.18883\n",
      "Iteration: 0105 Loss: 0.71342 Time: 0.18695\n",
      "Iteration: 0106 Loss: 0.70891 Time: 0.18790\n",
      "Iteration: 0107 Loss: 0.70021 Time: 0.18437\n",
      "Iteration: 0108 Loss: 0.69981 Time: 0.18972\n",
      "Iteration: 0109 Loss: 0.69986 Time: 0.18545\n",
      "Iteration: 0110 Loss: 0.68737 Time: 0.19078\n",
      "Iteration: 0111 Loss: 0.69462 Time: 0.19049\n",
      "Iteration: 0112 Loss: 0.68479 Time: 0.18392\n",
      "Iteration: 0113 Loss: 0.68695 Time: 0.19033\n",
      "Iteration: 0114 Loss: 0.68306 Time: 0.19151\n",
      "Iteration: 0115 Loss: 0.68553 Time: 0.18439\n",
      "Iteration: 0116 Loss: 0.68217 Time: 0.18268\n",
      "Iteration: 0117 Loss: 0.67766 Time: 0.18242\n",
      "Iteration: 0118 Loss: 0.67874 Time: 0.18990\n",
      "Iteration: 0119 Loss: 0.66982 Time: 0.18814\n",
      "Iteration: 0120 Loss: 0.67530 Time: 0.18736\n",
      "Iteration: 0121 Loss: 0.67110 Time: 0.18918\n",
      "Iteration: 0122 Loss: 0.67106 Time: 0.18559\n",
      "Iteration: 0123 Loss: 0.66567 Time: 0.18458\n",
      "Iteration: 0124 Loss: 0.66385 Time: 0.19117\n",
      "Iteration: 0125 Loss: 0.66303 Time: 0.19103\n",
      "Iteration: 0126 Loss: 0.65856 Time: 0.18584\n",
      "Iteration: 0127 Loss: 0.65604 Time: 0.18520\n",
      "Iteration: 0128 Loss: 0.66081 Time: 0.18371\n",
      "Iteration: 0129 Loss: 0.65167 Time: 0.19100\n",
      "Iteration: 0130 Loss: 0.65179 Time: 0.19410\n",
      "Iteration: 0131 Loss: 0.65215 Time: 0.18443\n",
      "Iteration: 0132 Loss: 0.64470 Time: 0.18513\n",
      "Iteration: 0133 Loss: 0.64503 Time: 0.18700\n",
      "Iteration: 0134 Loss: 0.64306 Time: 0.18903\n",
      "Iteration: 0135 Loss: 0.63689 Time: 0.18946\n",
      "Iteration: 0136 Loss: 0.64246 Time: 0.18885\n",
      "Iteration: 0137 Loss: 0.63913 Time: 0.18210\n",
      "Iteration: 0138 Loss: 0.63987 Time: 0.18169\n",
      "Iteration: 0139 Loss: 0.63325 Time: 0.18643\n",
      "Iteration: 0140 Loss: 0.63586 Time: 0.18599\n",
      "Iteration: 0141 Loss: 0.63042 Time: 0.18762\n",
      "Iteration: 0142 Loss: 0.62838 Time: 0.19874\n",
      "Iteration: 0143 Loss: 0.62879 Time: 0.19123\n",
      "Iteration: 0144 Loss: 0.63046 Time: 0.18504\n",
      "Iteration: 0145 Loss: 0.63019 Time: 0.19068\n",
      "Iteration: 0146 Loss: 0.62113 Time: 0.18886\n",
      "Iteration: 0147 Loss: 0.62044 Time: 0.18603\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0148 Loss: 0.62261 Time: 0.18994\n",
      "Iteration: 0149 Loss: 0.62103 Time: 0.19054\n",
      "Iteration: 0150 Loss: 0.61758 Time: 0.19136\n",
      "Iteration: 0151 Loss: 0.61547 Time: 0.18745\n",
      "Iteration: 0152 Loss: 0.61512 Time: 0.18649\n",
      "Iteration: 0153 Loss: 0.61098 Time: 0.18964\n",
      "Iteration: 0154 Loss: 0.61685 Time: 0.19153\n",
      "Iteration: 0155 Loss: 0.61258 Time: 0.18690\n",
      "Iteration: 0156 Loss: 0.61043 Time: 0.19133\n",
      "Iteration: 0157 Loss: 0.61111 Time: 0.18897\n",
      "Iteration: 0158 Loss: 0.60721 Time: 0.18089\n",
      "Iteration: 0159 Loss: 0.60914 Time: 0.18307\n",
      "Iteration: 0160 Loss: 0.60562 Time: 0.18577\n",
      "Iteration: 0161 Loss: 0.61006 Time: 0.18800\n",
      "Iteration: 0162 Loss: 0.60202 Time: 0.18516\n",
      "Iteration: 0163 Loss: 0.60536 Time: 0.20192\n",
      "Iteration: 0164 Loss: 0.60206 Time: 0.18902\n",
      "Iteration: 0165 Loss: 0.60160 Time: 0.19748\n",
      "Iteration: 0166 Loss: 0.59920 Time: 0.19650\n",
      "Iteration: 0167 Loss: 0.59540 Time: 0.19873\n",
      "Iteration: 0168 Loss: 0.59285 Time: 0.19207\n",
      "Iteration: 0169 Loss: 0.59598 Time: 0.18235\n",
      "Iteration: 0170 Loss: 0.59463 Time: 0.19122\n",
      "Iteration: 0171 Loss: 0.59540 Time: 0.18829\n",
      "Iteration: 0172 Loss: 0.59169 Time: 0.19002\n",
      "Iteration: 0173 Loss: 0.59315 Time: 0.20034\n",
      "Iteration: 0174 Loss: 0.58964 Time: 0.19606\n",
      "Iteration: 0175 Loss: 0.58358 Time: 0.19147\n",
      "Iteration: 0176 Loss: 0.58780 Time: 0.20186\n",
      "Iteration: 0177 Loss: 0.58567 Time: 0.19193\n",
      "Iteration: 0178 Loss: 0.59039 Time: 0.19346\n",
      "Iteration: 0179 Loss: 0.58462 Time: 0.18718\n",
      "Iteration: 0180 Loss: 0.58212 Time: 0.17995\n",
      "Iteration: 0181 Loss: 0.58135 Time: 0.19535\n",
      "Iteration: 0182 Loss: 0.58410 Time: 0.19300\n",
      "Iteration: 0183 Loss: 0.58232 Time: 0.19313\n",
      "Iteration: 0184 Loss: 0.57855 Time: 0.19901\n",
      "Iteration: 0185 Loss: 0.57854 Time: 0.19340\n",
      "Iteration: 0186 Loss: 0.57580 Time: 0.19265\n",
      "Iteration: 0187 Loss: 0.57354 Time: 0.19301\n",
      "Iteration: 0188 Loss: 0.57537 Time: 0.19358\n",
      "Iteration: 0189 Loss: 0.57488 Time: 0.18304\n",
      "Iteration: 0190 Loss: 0.57496 Time: 0.19168\n",
      "Iteration: 0191 Loss: 0.57063 Time: 0.18200\n",
      "Iteration: 0192 Loss: 0.57455 Time: 0.18863\n",
      "Iteration: 0193 Loss: 0.57065 Time: 0.18699\n",
      "Iteration: 0194 Loss: 0.57079 Time: 0.18498\n",
      "Iteration: 0195 Loss: 0.57092 Time: 0.18613\n",
      "Iteration: 0196 Loss: 0.56497 Time: 0.19405\n",
      "Iteration: 0197 Loss: 0.56738 Time: 0.18881\n",
      "Iteration: 0198 Loss: 0.57094 Time: 0.18752\n",
      "Iteration: 0199 Loss: 0.56751 Time: 0.19052\n",
      "Iteration: 0200 Loss: 0.56811 Time: 0.18464\n",
      "Done! \n",
      "\n",
      "STEP 3/3 - MODEL EVALUATION \n",
      "\n",
      "Computing the final embedding vectors, for evaluation\n",
      "Done! \n",
      "\n",
      "Testing: link prediction\n",
      "Done! \n",
      "\n",
      "EXPERIMENTS ON MODEL 3 / 10 \n",
      "\n",
      "STEP 1/3 - PREPROCESSING STEPS \n",
      "\n",
      "Masking some edges from the training graph, for link prediction\n",
      "(validation set: 1.0 % of edges - test set: 10.0 % of edges)\n",
      "Done! \n",
      "\n",
      "Preprocessing node features\n",
      "Done! \n",
      "\n",
      "Running the Louvain algorithm for community detection\n",
      "as a preprocessing step for the encoder\n",
      "Done! Louvain has found 91 communities \n",
      "\n",
      "Setting up the model and the optimizer\n",
      "Done! \n",
      "\n",
      "Preprocessing on message passing matrices\n",
      "Done! \n",
      "\n",
      "Initializing TF session\n",
      "Done! \n",
      "\n",
      "STEP 2/3 - MODEL TRAINING \n",
      "\n",
      "Starting training\n",
      "Iteration: 0001 Loss: 1.70267 Time: 0.39157\n",
      "Iteration: 0002 Loss: 1.69246 Time: 0.19011\n",
      "Iteration: 0003 Loss: 1.66953 Time: 0.19051\n",
      "Iteration: 0004 Loss: 1.63553 Time: 0.18957\n",
      "Iteration: 0005 Loss: 1.61775 Time: 0.18594\n",
      "Iteration: 0006 Loss: 1.58648 Time: 0.18832\n",
      "Iteration: 0007 Loss: 1.60070 Time: 0.19108\n",
      "Iteration: 0008 Loss: 1.56016 Time: 0.18746\n",
      "Iteration: 0009 Loss: 1.55301 Time: 0.19155\n",
      "Iteration: 0010 Loss: 1.50234 Time: 0.19013\n",
      "Iteration: 0011 Loss: 1.50495 Time: 0.18308\n",
      "Iteration: 0012 Loss: 1.46157 Time: 0.18443\n",
      "Iteration: 0013 Loss: 1.43153 Time: 0.18704\n",
      "Iteration: 0014 Loss: 1.45082 Time: 0.19000\n",
      "Iteration: 0015 Loss: 1.43099 Time: 0.19481\n",
      "Iteration: 0016 Loss: 1.39771 Time: 0.18732\n",
      "Iteration: 0017 Loss: 1.38792 Time: 0.19072\n",
      "Iteration: 0018 Loss: 1.35597 Time: 0.18474\n",
      "Iteration: 0019 Loss: 1.34097 Time: 0.19115\n",
      "Iteration: 0020 Loss: 1.33019 Time: 0.18932\n",
      "Iteration: 0021 Loss: 1.33357 Time: 0.19384\n",
      "Iteration: 0022 Loss: 1.31681 Time: 0.17792\n",
      "Iteration: 0023 Loss: 1.29828 Time: 0.18281\n",
      "Iteration: 0024 Loss: 1.27009 Time: 0.19100\n",
      "Iteration: 0025 Loss: 1.26347 Time: 0.18432\n",
      "Iteration: 0026 Loss: 1.27682 Time: 0.18826\n",
      "Iteration: 0027 Loss: 1.22303 Time: 0.18923\n",
      "Iteration: 0028 Loss: 1.22906 Time: 0.18752\n",
      "Iteration: 0029 Loss: 1.21751 Time: 0.18265\n",
      "Iteration: 0030 Loss: 1.20537 Time: 0.18414\n",
      "Iteration: 0031 Loss: 1.19591 Time: 0.18941\n",
      "Iteration: 0032 Loss: 1.19368 Time: 0.19118\n",
      "Iteration: 0033 Loss: 1.18191 Time: 0.18052\n",
      "Iteration: 0034 Loss: 1.16086 Time: 0.18618\n",
      "Iteration: 0035 Loss: 1.14879 Time: 0.18985\n",
      "Iteration: 0036 Loss: 1.15753 Time: 0.19161\n",
      "Iteration: 0037 Loss: 1.13724 Time: 0.18756\n",
      "Iteration: 0038 Loss: 1.12664 Time: 0.19051\n",
      "Iteration: 0039 Loss: 1.11879 Time: 0.18780\n",
      "Iteration: 0040 Loss: 1.09412 Time: 0.18990\n",
      "Iteration: 0041 Loss: 1.09469 Time: 0.18910\n",
      "Iteration: 0042 Loss: 1.09703 Time: 0.18931\n",
      "Iteration: 0043 Loss: 1.07508 Time: 0.18740\n",
      "Iteration: 0044 Loss: 1.05794 Time: 0.18681\n",
      "Iteration: 0045 Loss: 1.04730 Time: 0.18830\n",
      "Iteration: 0046 Loss: 1.03999 Time: 0.18912\n",
      "Iteration: 0047 Loss: 1.03716 Time: 0.18953\n",
      "Iteration: 0048 Loss: 1.04093 Time: 0.18766\n",
      "Iteration: 0049 Loss: 1.03573 Time: 0.18927\n",
      "Iteration: 0050 Loss: 1.00765 Time: 0.18250\n",
      "Iteration: 0051 Loss: 0.99979 Time: 0.19390\n",
      "Iteration: 0052 Loss: 1.00226 Time: 0.18970\n",
      "Iteration: 0053 Loss: 1.00148 Time: 0.18252\n",
      "Iteration: 0054 Loss: 0.99191 Time: 0.18759\n",
      "Iteration: 0055 Loss: 0.96582 Time: 0.18786\n",
      "Iteration: 0056 Loss: 0.97980 Time: 0.18499\n",
      "Iteration: 0057 Loss: 0.96168 Time: 0.18621\n",
      "Iteration: 0058 Loss: 0.94555 Time: 0.18937\n",
      "Iteration: 0059 Loss: 0.95060 Time: 0.19269\n",
      "Iteration: 0060 Loss: 0.93931 Time: 0.18454\n",
      "Iteration: 0061 Loss: 0.93422 Time: 0.18776\n",
      "Iteration: 0062 Loss: 0.91152 Time: 0.18629\n",
      "Iteration: 0063 Loss: 0.90930 Time: 0.19017\n",
      "Iteration: 0064 Loss: 0.91330 Time: 0.18086\n",
      "Iteration: 0065 Loss: 0.90361 Time: 0.18409\n",
      "Iteration: 0066 Loss: 0.89142 Time: 0.18720\n",
      "Iteration: 0067 Loss: 0.89279 Time: 0.18038\n",
      "Iteration: 0068 Loss: 0.88165 Time: 0.19540\n",
      "Iteration: 0069 Loss: 0.87420 Time: 0.19259\n",
      "Iteration: 0070 Loss: 0.87769 Time: 0.19008\n",
      "Iteration: 0071 Loss: 0.86934 Time: 0.18585\n",
      "Iteration: 0072 Loss: 0.86229 Time: 0.18783\n",
      "Iteration: 0073 Loss: 0.86113 Time: 0.19104\n",
      "Iteration: 0074 Loss: 0.85167 Time: 0.19116\n",
      "Iteration: 0075 Loss: 0.84484 Time: 0.18551\n",
      "Iteration: 0076 Loss: 0.84560 Time: 0.18811\n",
      "Iteration: 0077 Loss: 0.83867 Time: 0.19063\n",
      "Iteration: 0078 Loss: 0.82378 Time: 0.18782\n",
      "Iteration: 0079 Loss: 0.81686 Time: 0.19300\n",
      "Iteration: 0080 Loss: 0.82314 Time: 0.18800\n",
      "Iteration: 0081 Loss: 0.80984 Time: 0.19097\n",
      "Iteration: 0082 Loss: 0.81203 Time: 0.18919\n",
      "Iteration: 0083 Loss: 0.79675 Time: 0.18515\n",
      "Iteration: 0084 Loss: 0.80224 Time: 0.18623\n",
      "Iteration: 0085 Loss: 0.78851 Time: 0.18753\n",
      "Iteration: 0086 Loss: 0.79683 Time: 0.18223\n",
      "Iteration: 0087 Loss: 0.77874 Time: 0.18830\n",
      "Iteration: 0088 Loss: 0.78107 Time: 0.18683\n",
      "Iteration: 0089 Loss: 0.77314 Time: 0.18687\n",
      "Iteration: 0090 Loss: 0.77384 Time: 0.18527\n",
      "Iteration: 0091 Loss: 0.76841 Time: 0.19141\n",
      "Iteration: 0092 Loss: 0.75819 Time: 0.17854\n",
      "Iteration: 0093 Loss: 0.75725 Time: 0.19516\n",
      "Iteration: 0094 Loss: 0.75027 Time: 0.18377\n",
      "Iteration: 0095 Loss: 0.75223 Time: 0.18480\n",
      "Iteration: 0096 Loss: 0.74318 Time: 0.18279\n",
      "Iteration: 0097 Loss: 0.74549 Time: 0.18219\n",
      "Iteration: 0098 Loss: 0.73693 Time: 0.19187\n",
      "Iteration: 0099 Loss: 0.73852 Time: 0.18939\n",
      "Iteration: 0100 Loss: 0.72863 Time: 0.18879\n",
      "Iteration: 0101 Loss: 0.72632 Time: 0.18678\n",
      "Iteration: 0102 Loss: 0.72985 Time: 0.18909\n",
      "Iteration: 0103 Loss: 0.72069 Time: 0.18356\n",
      "Iteration: 0104 Loss: 0.72262 Time: 0.18366\n",
      "Iteration: 0105 Loss: 0.70838 Time: 0.19681\n",
      "Iteration: 0106 Loss: 0.70685 Time: 0.19591\n",
      "Iteration: 0107 Loss: 0.71011 Time: 0.18446\n",
      "Iteration: 0108 Loss: 0.70775 Time: 0.18957\n",
      "Iteration: 0109 Loss: 0.70825 Time: 0.19259\n",
      "Iteration: 0110 Loss: 0.70013 Time: 0.18398\n",
      "Iteration: 0111 Loss: 0.70042 Time: 0.18698\n",
      "Iteration: 0112 Loss: 0.69403 Time: 0.19018\n",
      "Iteration: 0113 Loss: 0.69689 Time: 0.18909\n",
      "Iteration: 0114 Loss: 0.68780 Time: 0.18457\n",
      "Iteration: 0115 Loss: 0.68270 Time: 0.18795\n",
      "Iteration: 0116 Loss: 0.68355 Time: 0.18979\n",
      "Iteration: 0117 Loss: 0.68611 Time: 0.18450\n",
      "Iteration: 0118 Loss: 0.68069 Time: 0.18275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0119 Loss: 0.67292 Time: 0.18869\n",
      "Iteration: 0120 Loss: 0.67525 Time: 0.18876\n",
      "Iteration: 0121 Loss: 0.67766 Time: 0.17993\n",
      "Iteration: 0122 Loss: 0.67090 Time: 0.18983\n",
      "Iteration: 0123 Loss: 0.66999 Time: 0.18982\n",
      "Iteration: 0124 Loss: 0.66428 Time: 0.18976\n",
      "Iteration: 0125 Loss: 0.66902 Time: 0.18625\n",
      "Iteration: 0126 Loss: 0.65871 Time: 0.18930\n",
      "Iteration: 0127 Loss: 0.65961 Time: 0.18525\n",
      "Iteration: 0128 Loss: 0.66095 Time: 0.18446\n",
      "Iteration: 0129 Loss: 0.66139 Time: 0.19107\n",
      "Iteration: 0130 Loss: 0.65441 Time: 0.18404\n",
      "Iteration: 0131 Loss: 0.65000 Time: 0.19517\n",
      "Iteration: 0132 Loss: 0.65709 Time: 0.18225\n",
      "Iteration: 0133 Loss: 0.64624 Time: 0.19408\n",
      "Iteration: 0134 Loss: 0.64395 Time: 0.19314\n",
      "Iteration: 0135 Loss: 0.64578 Time: 0.18993\n",
      "Iteration: 0136 Loss: 0.63977 Time: 0.19248\n",
      "Iteration: 0137 Loss: 0.64278 Time: 0.19117\n",
      "Iteration: 0138 Loss: 0.64398 Time: 0.18803\n",
      "Iteration: 0139 Loss: 0.63727 Time: 0.18339\n",
      "Iteration: 0140 Loss: 0.64150 Time: 0.17909\n",
      "Iteration: 0141 Loss: 0.63485 Time: 0.19267\n",
      "Iteration: 0142 Loss: 0.62980 Time: 0.18743\n",
      "Iteration: 0143 Loss: 0.62987 Time: 0.19141\n",
      "Iteration: 0144 Loss: 0.62801 Time: 0.18500\n",
      "Iteration: 0145 Loss: 0.62903 Time: 0.19260\n",
      "Iteration: 0146 Loss: 0.62540 Time: 0.18670\n",
      "Iteration: 0147 Loss: 0.62543 Time: 0.19450\n",
      "Iteration: 0148 Loss: 0.63029 Time: 0.17812\n",
      "Iteration: 0149 Loss: 0.61967 Time: 0.18594\n",
      "Iteration: 0150 Loss: 0.62149 Time: 0.17958\n",
      "Iteration: 0151 Loss: 0.61486 Time: 0.18847\n",
      "Iteration: 0152 Loss: 0.61843 Time: 0.19040\n",
      "Iteration: 0153 Loss: 0.61581 Time: 0.19367\n",
      "Iteration: 0154 Loss: 0.61761 Time: 0.19192\n",
      "Iteration: 0155 Loss: 0.61342 Time: 0.18726\n",
      "Iteration: 0156 Loss: 0.61070 Time: 0.19104\n",
      "Iteration: 0157 Loss: 0.61126 Time: 0.18695\n",
      "Iteration: 0158 Loss: 0.61121 Time: 0.18579\n",
      "Iteration: 0159 Loss: 0.60994 Time: 0.19171\n",
      "Iteration: 0160 Loss: 0.60525 Time: 0.17801\n",
      "Iteration: 0161 Loss: 0.60576 Time: 0.18133\n",
      "Iteration: 0162 Loss: 0.60584 Time: 0.18520\n",
      "Iteration: 0163 Loss: 0.60236 Time: 0.18878\n",
      "Iteration: 0164 Loss: 0.60183 Time: 0.18763\n",
      "Iteration: 0165 Loss: 0.59906 Time: 0.18891\n",
      "Iteration: 0166 Loss: 0.60415 Time: 0.18956\n",
      "Iteration: 0167 Loss: 0.59725 Time: 0.18655\n",
      "Iteration: 0168 Loss: 0.59869 Time: 0.18900\n",
      "Iteration: 0169 Loss: 0.59891 Time: 0.18854\n",
      "Iteration: 0170 Loss: 0.59194 Time: 0.18300\n",
      "Iteration: 0171 Loss: 0.59869 Time: 0.18730\n",
      "Iteration: 0172 Loss: 0.59250 Time: 0.19010\n",
      "Iteration: 0173 Loss: 0.59143 Time: 0.18622\n",
      "Iteration: 0174 Loss: 0.59454 Time: 0.19045\n",
      "Iteration: 0175 Loss: 0.59061 Time: 0.18901\n",
      "Iteration: 0176 Loss: 0.58811 Time: 0.18692\n",
      "Iteration: 0177 Loss: 0.58703 Time: 0.18752\n",
      "Iteration: 0178 Loss: 0.58889 Time: 0.18627\n",
      "Iteration: 0179 Loss: 0.58606 Time: 0.18812\n",
      "Iteration: 0180 Loss: 0.58120 Time: 0.18427\n",
      "Iteration: 0181 Loss: 0.58571 Time: 0.18636\n",
      "Iteration: 0182 Loss: 0.58185 Time: 0.18444\n",
      "Iteration: 0183 Loss: 0.58443 Time: 0.18791\n",
      "Iteration: 0184 Loss: 0.58050 Time: 0.18994\n",
      "Iteration: 0185 Loss: 0.57871 Time: 0.18598\n",
      "Iteration: 0186 Loss: 0.58019 Time: 0.19472\n",
      "Iteration: 0187 Loss: 0.57606 Time: 0.18800\n",
      "Iteration: 0188 Loss: 0.57657 Time: 0.18520\n",
      "Iteration: 0189 Loss: 0.57796 Time: 0.18409\n",
      "Iteration: 0190 Loss: 0.57532 Time: 0.18672\n",
      "Iteration: 0191 Loss: 0.57636 Time: 0.19004\n",
      "Iteration: 0192 Loss: 0.57428 Time: 0.18028\n",
      "Iteration: 0193 Loss: 0.57555 Time: 0.18289\n",
      "Iteration: 0194 Loss: 0.57325 Time: 0.18392\n",
      "Iteration: 0195 Loss: 0.57150 Time: 0.19029\n",
      "Iteration: 0196 Loss: 0.56832 Time: 0.19160\n",
      "Iteration: 0197 Loss: 0.57058 Time: 0.18596\n",
      "Iteration: 0198 Loss: 0.56550 Time: 0.18577\n",
      "Iteration: 0199 Loss: 0.56945 Time: 0.18769\n",
      "Iteration: 0200 Loss: 0.56590 Time: 0.19122\n",
      "Done! \n",
      "\n",
      "STEP 3/3 - MODEL EVALUATION \n",
      "\n",
      "Computing the final embedding vectors, for evaluation\n",
      "Done! \n",
      "\n",
      "Testing: link prediction\n",
      "Done! \n",
      "\n",
      "EXPERIMENTS ON MODEL 4 / 10 \n",
      "\n",
      "STEP 1/3 - PREPROCESSING STEPS \n",
      "\n",
      "Masking some edges from the training graph, for link prediction\n",
      "(validation set: 1.0 % of edges - test set: 10.0 % of edges)\n",
      "Done! \n",
      "\n",
      "Preprocessing node features\n",
      "Done! \n",
      "\n",
      "Running the Louvain algorithm for community detection\n",
      "as a preprocessing step for the encoder\n",
      "Done! Louvain has found 90 communities \n",
      "\n",
      "Setting up the model and the optimizer\n",
      "Done! \n",
      "\n",
      "Preprocessing on message passing matrices\n",
      "Done! \n",
      "\n",
      "Initializing TF session\n",
      "Done! \n",
      "\n",
      "STEP 2/3 - MODEL TRAINING \n",
      "\n",
      "Starting training\n",
      "Iteration: 0001 Loss: 1.73508 Time: 0.40081\n",
      "Iteration: 0002 Loss: 1.70556 Time: 0.18839\n",
      "Iteration: 0003 Loss: 1.67794 Time: 0.18374\n",
      "Iteration: 0004 Loss: 1.64837 Time: 0.18191\n",
      "Iteration: 0005 Loss: 1.61777 Time: 0.18399\n",
      "Iteration: 0006 Loss: 1.58529 Time: 0.18900\n",
      "Iteration: 0007 Loss: 1.56621 Time: 0.19304\n",
      "Iteration: 0008 Loss: 1.54991 Time: 0.18889\n",
      "Iteration: 0009 Loss: 1.53536 Time: 0.18802\n",
      "Iteration: 0010 Loss: 1.51453 Time: 0.19261\n",
      "Iteration: 0011 Loss: 1.50986 Time: 0.18788\n",
      "Iteration: 0012 Loss: 1.47283 Time: 0.18670\n",
      "Iteration: 0013 Loss: 1.44918 Time: 0.18516\n",
      "Iteration: 0014 Loss: 1.42568 Time: 0.18492\n",
      "Iteration: 0015 Loss: 1.42373 Time: 0.18638\n",
      "Iteration: 0016 Loss: 1.38895 Time: 0.19000\n",
      "Iteration: 0017 Loss: 1.37884 Time: 0.19022\n",
      "Iteration: 0018 Loss: 1.37835 Time: 0.18833\n",
      "Iteration: 0019 Loss: 1.35289 Time: 0.18275\n",
      "Iteration: 0020 Loss: 1.34453 Time: 0.19051\n",
      "Iteration: 0021 Loss: 1.32657 Time: 0.19477\n",
      "Iteration: 0022 Loss: 1.32387 Time: 0.18906\n",
      "Iteration: 0023 Loss: 1.29661 Time: 0.18960\n",
      "Iteration: 0024 Loss: 1.27888 Time: 0.18496\n",
      "Iteration: 0025 Loss: 1.27720 Time: 0.17735\n",
      "Iteration: 0026 Loss: 1.25194 Time: 0.19257\n",
      "Iteration: 0027 Loss: 1.24844 Time: 0.18523\n",
      "Iteration: 0028 Loss: 1.21576 Time: 0.18354\n",
      "Iteration: 0029 Loss: 1.21484 Time: 0.18517\n",
      "Iteration: 0030 Loss: 1.19863 Time: 0.19780\n",
      "Iteration: 0031 Loss: 1.19080 Time: 0.19276\n",
      "Iteration: 0032 Loss: 1.18081 Time: 0.19178\n",
      "Iteration: 0033 Loss: 1.17189 Time: 0.19025\n",
      "Iteration: 0034 Loss: 1.17840 Time: 0.18414\n",
      "Iteration: 0035 Loss: 1.16295 Time: 0.18836\n",
      "Iteration: 0036 Loss: 1.15074 Time: 0.18669\n",
      "Iteration: 0037 Loss: 1.14844 Time: 0.18425\n",
      "Iteration: 0038 Loss: 1.12137 Time: 0.19021\n",
      "Iteration: 0039 Loss: 1.11521 Time: 0.18589\n",
      "Iteration: 0040 Loss: 1.10344 Time: 0.18535\n",
      "Iteration: 0041 Loss: 1.10064 Time: 0.19028\n",
      "Iteration: 0042 Loss: 1.10328 Time: 0.18700\n",
      "Iteration: 0043 Loss: 1.08713 Time: 0.18657\n",
      "Iteration: 0044 Loss: 1.07585 Time: 0.18990\n",
      "Iteration: 0045 Loss: 1.06848 Time: 0.18089\n",
      "Iteration: 0046 Loss: 1.05833 Time: 0.18358\n",
      "Iteration: 0047 Loss: 1.04475 Time: 0.18275\n",
      "Iteration: 0048 Loss: 1.05306 Time: 0.18527\n",
      "Iteration: 0049 Loss: 1.04377 Time: 0.19258\n",
      "Iteration: 0050 Loss: 1.02275 Time: 0.18801\n",
      "Iteration: 0051 Loss: 1.01016 Time: 0.18803\n",
      "Iteration: 0052 Loss: 0.99789 Time: 0.18860\n",
      "Iteration: 0053 Loss: 1.00614 Time: 0.18649\n",
      "Iteration: 0054 Loss: 0.99114 Time: 0.18888\n",
      "Iteration: 0055 Loss: 0.97315 Time: 0.19033\n",
      "Iteration: 0056 Loss: 0.97357 Time: 0.18859\n",
      "Iteration: 0057 Loss: 0.97193 Time: 0.18215\n",
      "Iteration: 0058 Loss: 0.96039 Time: 0.18564\n",
      "Iteration: 0059 Loss: 0.94979 Time: 0.18367\n",
      "Iteration: 0060 Loss: 0.95090 Time: 0.19078\n",
      "Iteration: 0061 Loss: 0.93825 Time: 0.19527\n",
      "Iteration: 0062 Loss: 0.93009 Time: 0.19283\n",
      "Iteration: 0063 Loss: 0.92759 Time: 0.19075\n",
      "Iteration: 0064 Loss: 0.91659 Time: 0.18002\n",
      "Iteration: 0065 Loss: 0.90813 Time: 0.19207\n",
      "Iteration: 0066 Loss: 0.90126 Time: 0.18527\n",
      "Iteration: 0067 Loss: 0.89275 Time: 0.18898\n",
      "Iteration: 0068 Loss: 0.89278 Time: 0.18464\n",
      "Iteration: 0069 Loss: 0.88163 Time: 0.18346\n",
      "Iteration: 0070 Loss: 0.88287 Time: 0.19256\n",
      "Iteration: 0071 Loss: 0.86493 Time: 0.18757\n",
      "Iteration: 0072 Loss: 0.86171 Time: 0.18505\n",
      "Iteration: 0073 Loss: 0.85341 Time: 0.18692\n",
      "Iteration: 0074 Loss: 0.85625 Time: 0.19174\n",
      "Iteration: 0075 Loss: 0.84034 Time: 0.18419\n",
      "Iteration: 0076 Loss: 0.84181 Time: 0.18839\n",
      "Iteration: 0077 Loss: 0.83403 Time: 0.18897\n",
      "Iteration: 0078 Loss: 0.82581 Time: 0.18254\n",
      "Iteration: 0079 Loss: 0.82247 Time: 0.18004\n",
      "Iteration: 0080 Loss: 0.81992 Time: 0.19109\n",
      "Iteration: 0081 Loss: 0.80599 Time: 0.18772\n",
      "Iteration: 0082 Loss: 0.80977 Time: 0.19044\n",
      "Iteration: 0083 Loss: 0.80970 Time: 0.18728\n",
      "Iteration: 0084 Loss: 0.80314 Time: 0.18695\n",
      "Iteration: 0085 Loss: 0.78359 Time: 0.18820\n",
      "Iteration: 0086 Loss: 0.78957 Time: 0.18729\n",
      "Iteration: 0087 Loss: 0.78340 Time: 0.19110\n",
      "Iteration: 0088 Loss: 0.78615 Time: 0.18693\n",
      "Iteration: 0089 Loss: 0.77869 Time: 0.18369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0090 Loss: 0.77555 Time: 0.18633\n",
      "Iteration: 0091 Loss: 0.76714 Time: 0.18321\n",
      "Iteration: 0092 Loss: 0.76616 Time: 0.19119\n",
      "Iteration: 0093 Loss: 0.75664 Time: 0.18824\n",
      "Iteration: 0094 Loss: 0.74590 Time: 0.18829\n",
      "Iteration: 0095 Loss: 0.75463 Time: 0.18635\n",
      "Iteration: 0096 Loss: 0.74858 Time: 0.19096\n",
      "Iteration: 0097 Loss: 0.74309 Time: 0.19220\n",
      "Iteration: 0098 Loss: 0.74087 Time: 0.18888\n",
      "Iteration: 0099 Loss: 0.73476 Time: 0.18343\n",
      "Iteration: 0100 Loss: 0.72862 Time: 0.18075\n",
      "Iteration: 0101 Loss: 0.72870 Time: 0.18703\n",
      "Iteration: 0102 Loss: 0.72864 Time: 0.18953\n",
      "Iteration: 0103 Loss: 0.71877 Time: 0.19036\n",
      "Iteration: 0104 Loss: 0.71383 Time: 0.18782\n",
      "Iteration: 0105 Loss: 0.71582 Time: 0.18899\n",
      "Iteration: 0106 Loss: 0.71533 Time: 0.18714\n",
      "Iteration: 0107 Loss: 0.71220 Time: 0.18618\n",
      "Iteration: 0108 Loss: 0.70863 Time: 0.19077\n",
      "Iteration: 0109 Loss: 0.69767 Time: 0.18371\n",
      "Iteration: 0110 Loss: 0.70053 Time: 0.18676\n",
      "Iteration: 0111 Loss: 0.69973 Time: 0.18615\n",
      "Iteration: 0112 Loss: 0.69098 Time: 0.18779\n",
      "Iteration: 0113 Loss: 0.68884 Time: 0.18755\n",
      "Iteration: 0114 Loss: 0.68817 Time: 0.18810\n",
      "Iteration: 0115 Loss: 0.69313 Time: 0.18801\n",
      "Iteration: 0116 Loss: 0.68015 Time: 0.19368\n",
      "Iteration: 0117 Loss: 0.67512 Time: 0.18370\n",
      "Iteration: 0118 Loss: 0.67565 Time: 0.18565\n",
      "Iteration: 0119 Loss: 0.67768 Time: 0.19236\n",
      "Iteration: 0120 Loss: 0.67174 Time: 0.18285\n",
      "Iteration: 0121 Loss: 0.67018 Time: 0.18277\n",
      "Iteration: 0122 Loss: 0.66765 Time: 0.18383\n",
      "Iteration: 0123 Loss: 0.66665 Time: 0.18998\n",
      "Iteration: 0124 Loss: 0.66449 Time: 0.18802\n",
      "Iteration: 0125 Loss: 0.66020 Time: 0.18900\n",
      "Iteration: 0126 Loss: 0.66287 Time: 0.18250\n",
      "Iteration: 0127 Loss: 0.66081 Time: 0.18900\n",
      "Iteration: 0128 Loss: 0.65800 Time: 0.19271\n",
      "Iteration: 0129 Loss: 0.65602 Time: 0.19843\n",
      "Iteration: 0130 Loss: 0.65114 Time: 0.19535\n",
      "Iteration: 0131 Loss: 0.64746 Time: 0.17478\n",
      "Iteration: 0132 Loss: 0.64923 Time: 0.18415\n",
      "Iteration: 0133 Loss: 0.64219 Time: 0.19232\n",
      "Iteration: 0134 Loss: 0.64367 Time: 0.18600\n",
      "Iteration: 0135 Loss: 0.64848 Time: 0.18168\n",
      "Iteration: 0136 Loss: 0.64771 Time: 0.18602\n",
      "Iteration: 0137 Loss: 0.63765 Time: 0.18478\n",
      "Iteration: 0138 Loss: 0.63733 Time: 0.19201\n",
      "Iteration: 0139 Loss: 0.63353 Time: 0.19210\n",
      "Iteration: 0140 Loss: 0.63369 Time: 0.19711\n",
      "Iteration: 0141 Loss: 0.63636 Time: 0.18305\n",
      "Iteration: 0142 Loss: 0.63634 Time: 0.18126\n",
      "Iteration: 0143 Loss: 0.62893 Time: 0.19152\n",
      "Iteration: 0144 Loss: 0.62682 Time: 0.18754\n",
      "Iteration: 0145 Loss: 0.62692 Time: 0.18863\n",
      "Iteration: 0146 Loss: 0.62675 Time: 0.18880\n",
      "Iteration: 0147 Loss: 0.62183 Time: 0.18801\n",
      "Iteration: 0148 Loss: 0.62634 Time: 0.19200\n",
      "Iteration: 0149 Loss: 0.62171 Time: 0.18856\n",
      "Iteration: 0150 Loss: 0.62172 Time: 0.18966\n",
      "Iteration: 0151 Loss: 0.61810 Time: 0.18714\n",
      "Iteration: 0152 Loss: 0.61604 Time: 0.18409\n",
      "Iteration: 0153 Loss: 0.61302 Time: 0.18511\n",
      "Iteration: 0154 Loss: 0.61548 Time: 0.19282\n",
      "Iteration: 0155 Loss: 0.61508 Time: 0.18532\n",
      "Iteration: 0156 Loss: 0.61342 Time: 0.18594\n",
      "Iteration: 0157 Loss: 0.60856 Time: 0.19167\n",
      "Iteration: 0158 Loss: 0.61025 Time: 0.18345\n",
      "Iteration: 0159 Loss: 0.60851 Time: 0.18810\n",
      "Iteration: 0160 Loss: 0.60574 Time: 0.19212\n",
      "Iteration: 0161 Loss: 0.60620 Time: 0.18343\n",
      "Iteration: 0162 Loss: 0.60248 Time: 0.18879\n",
      "Iteration: 0163 Loss: 0.60299 Time: 0.18107\n",
      "Iteration: 0164 Loss: 0.60349 Time: 0.18432\n",
      "Iteration: 0165 Loss: 0.60519 Time: 0.18776\n",
      "Iteration: 0166 Loss: 0.60033 Time: 0.18876\n",
      "Iteration: 0167 Loss: 0.59714 Time: 0.18630\n",
      "Iteration: 0168 Loss: 0.59995 Time: 0.18979\n",
      "Iteration: 0169 Loss: 0.59633 Time: 0.18483\n",
      "Iteration: 0170 Loss: 0.59508 Time: 0.18879\n",
      "Iteration: 0171 Loss: 0.59309 Time: 0.18618\n",
      "Iteration: 0172 Loss: 0.58880 Time: 0.18355\n",
      "Iteration: 0173 Loss: 0.59447 Time: 0.19096\n",
      "Iteration: 0174 Loss: 0.59005 Time: 0.18253\n",
      "Iteration: 0175 Loss: 0.59143 Time: 0.18678\n",
      "Iteration: 0176 Loss: 0.58986 Time: 0.19645\n",
      "Iteration: 0177 Loss: 0.58162 Time: 0.18619\n",
      "Iteration: 0178 Loss: 0.58660 Time: 0.19150\n",
      "Iteration: 0179 Loss: 0.58357 Time: 0.18974\n",
      "Iteration: 0180 Loss: 0.58300 Time: 0.18750\n",
      "Iteration: 0181 Loss: 0.58065 Time: 0.18362\n",
      "Iteration: 0182 Loss: 0.58211 Time: 0.19573\n",
      "Iteration: 0183 Loss: 0.57972 Time: 0.18476\n",
      "Iteration: 0184 Loss: 0.57955 Time: 0.18103\n",
      "Iteration: 0185 Loss: 0.57991 Time: 0.18579\n",
      "Iteration: 0186 Loss: 0.57896 Time: 0.18769\n",
      "Iteration: 0187 Loss: 0.57738 Time: 0.19229\n",
      "Iteration: 0188 Loss: 0.57663 Time: 0.18825\n",
      "Iteration: 0189 Loss: 0.57460 Time: 0.19274\n",
      "Iteration: 0190 Loss: 0.57263 Time: 0.18763\n",
      "Iteration: 0191 Loss: 0.57289 Time: 0.18732\n",
      "Iteration: 0192 Loss: 0.57421 Time: 0.18274\n",
      "Iteration: 0193 Loss: 0.57263 Time: 0.19115\n",
      "Iteration: 0194 Loss: 0.57316 Time: 0.18637\n",
      "Iteration: 0195 Loss: 0.56857 Time: 0.18445\n",
      "Iteration: 0196 Loss: 0.56953 Time: 0.18306\n",
      "Iteration: 0197 Loss: 0.56574 Time: 0.18329\n",
      "Iteration: 0198 Loss: 0.56281 Time: 0.18587\n",
      "Iteration: 0199 Loss: 0.56418 Time: 0.19416\n",
      "Iteration: 0200 Loss: 0.56524 Time: 0.18854\n",
      "Done! \n",
      "\n",
      "STEP 3/3 - MODEL EVALUATION \n",
      "\n",
      "Computing the final embedding vectors, for evaluation\n",
      "Done! \n",
      "\n",
      "Testing: link prediction\n",
      "Done! \n",
      "\n",
      "EXPERIMENTS ON MODEL 5 / 10 \n",
      "\n",
      "STEP 1/3 - PREPROCESSING STEPS \n",
      "\n",
      "Masking some edges from the training graph, for link prediction\n",
      "(validation set: 1.0 % of edges - test set: 10.0 % of edges)\n",
      "Done! \n",
      "\n",
      "Preprocessing node features\n",
      "Done! \n",
      "\n",
      "Running the Louvain algorithm for community detection\n",
      "as a preprocessing step for the encoder\n",
      "Done! Louvain has found 89 communities \n",
      "\n",
      "Setting up the model and the optimizer\n",
      "Done! \n",
      "\n",
      "Preprocessing on message passing matrices\n",
      "Done! \n",
      "\n",
      "Initializing TF session\n",
      "Done! \n",
      "\n",
      "STEP 2/3 - MODEL TRAINING \n",
      "\n",
      "Starting training\n",
      "Iteration: 0001 Loss: 1.71174 Time: 0.41900\n",
      "Iteration: 0002 Loss: 1.72154 Time: 0.18899\n",
      "Iteration: 0003 Loss: 1.67057 Time: 0.19095\n",
      "Iteration: 0004 Loss: 1.64213 Time: 0.18984\n",
      "Iteration: 0005 Loss: 1.64461 Time: 0.18807\n",
      "Iteration: 0006 Loss: 1.59075 Time: 0.18573\n",
      "Iteration: 0007 Loss: 1.57382 Time: 0.19082\n",
      "Iteration: 0008 Loss: 1.55382 Time: 0.18799\n",
      "Iteration: 0009 Loss: 1.54157 Time: 0.18480\n",
      "Iteration: 0010 Loss: 1.53840 Time: 0.18617\n",
      "Iteration: 0011 Loss: 1.50097 Time: 0.19362\n",
      "Iteration: 0012 Loss: 1.46764 Time: 0.18226\n",
      "Iteration: 0013 Loss: 1.47970 Time: 0.19694\n",
      "Iteration: 0014 Loss: 1.45489 Time: 0.19322\n",
      "Iteration: 0015 Loss: 1.41211 Time: 0.18574\n",
      "Iteration: 0016 Loss: 1.41441 Time: 0.18509\n",
      "Iteration: 0017 Loss: 1.37609 Time: 0.18093\n",
      "Iteration: 0018 Loss: 1.37739 Time: 0.18675\n",
      "Iteration: 0019 Loss: 1.33792 Time: 0.18242\n",
      "Iteration: 0020 Loss: 1.33419 Time: 0.19609\n",
      "Iteration: 0021 Loss: 1.33794 Time: 0.19207\n",
      "Iteration: 0022 Loss: 1.32344 Time: 0.19058\n",
      "Iteration: 0023 Loss: 1.29328 Time: 0.19249\n",
      "Iteration: 0024 Loss: 1.30721 Time: 0.18745\n",
      "Iteration: 0025 Loss: 1.27796 Time: 0.18669\n",
      "Iteration: 0026 Loss: 1.24783 Time: 0.19392\n",
      "Iteration: 0027 Loss: 1.26402 Time: 0.17727\n",
      "Iteration: 0028 Loss: 1.24919 Time: 0.18248\n",
      "Iteration: 0029 Loss: 1.22096 Time: 0.19308\n",
      "Iteration: 0030 Loss: 1.20241 Time: 0.18724\n",
      "Iteration: 0031 Loss: 1.20062 Time: 0.19325\n",
      "Iteration: 0032 Loss: 1.20698 Time: 0.19127\n",
      "Iteration: 0033 Loss: 1.17992 Time: 0.18488\n",
      "Iteration: 0034 Loss: 1.16580 Time: 0.18610\n",
      "Iteration: 0035 Loss: 1.15321 Time: 0.18809\n",
      "Iteration: 0036 Loss: 1.13549 Time: 0.18713\n",
      "Iteration: 0037 Loss: 1.13236 Time: 0.19205\n",
      "Iteration: 0038 Loss: 1.14543 Time: 0.17617\n",
      "Iteration: 0039 Loss: 1.11396 Time: 0.18550\n",
      "Iteration: 0040 Loss: 1.09690 Time: 0.18616\n",
      "Iteration: 0041 Loss: 1.09523 Time: 0.18801\n",
      "Iteration: 0042 Loss: 1.08506 Time: 0.19172\n",
      "Iteration: 0043 Loss: 1.08033 Time: 0.18672\n",
      "Iteration: 0044 Loss: 1.07106 Time: 0.18559\n",
      "Iteration: 0045 Loss: 1.06805 Time: 0.18718\n",
      "Iteration: 0046 Loss: 1.05755 Time: 0.18710\n",
      "Iteration: 0047 Loss: 1.04309 Time: 0.18893\n",
      "Iteration: 0048 Loss: 1.03997 Time: 0.18729\n",
      "Iteration: 0049 Loss: 1.02046 Time: 0.18218\n",
      "Iteration: 0050 Loss: 1.02094 Time: 0.18325\n",
      "Iteration: 0051 Loss: 1.01545 Time: 0.19257\n",
      "Iteration: 0052 Loss: 0.99755 Time: 0.19362\n",
      "Iteration: 0053 Loss: 0.99694 Time: 0.18769\n",
      "Iteration: 0054 Loss: 0.98630 Time: 0.18636\n",
      "Iteration: 0055 Loss: 0.98020 Time: 0.19338\n",
      "Iteration: 0056 Loss: 0.96767 Time: 0.18817\n",
      "Iteration: 0057 Loss: 0.96279 Time: 0.18264\n",
      "Iteration: 0058 Loss: 0.94718 Time: 0.18078\n",
      "Iteration: 0059 Loss: 0.94843 Time: 0.18499\n",
      "Iteration: 0060 Loss: 0.94530 Time: 0.18673\n",
      "Iteration: 0061 Loss: 0.93630 Time: 0.19308\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0062 Loss: 0.93768 Time: 0.18854\n",
      "Iteration: 0063 Loss: 0.92892 Time: 0.18549\n",
      "Iteration: 0064 Loss: 0.92458 Time: 0.19017\n",
      "Iteration: 0065 Loss: 0.91049 Time: 0.18954\n",
      "Iteration: 0066 Loss: 0.89830 Time: 0.18992\n",
      "Iteration: 0067 Loss: 0.90504 Time: 0.18865\n",
      "Iteration: 0068 Loss: 0.88785 Time: 0.18925\n",
      "Iteration: 0069 Loss: 0.88367 Time: 0.18700\n",
      "Iteration: 0070 Loss: 0.87843 Time: 0.18432\n",
      "Iteration: 0071 Loss: 0.88106 Time: 0.18712\n",
      "Iteration: 0072 Loss: 0.86676 Time: 0.18781\n",
      "Iteration: 0073 Loss: 0.86686 Time: 0.19012\n",
      "Iteration: 0074 Loss: 0.85209 Time: 0.18853\n",
      "Iteration: 0075 Loss: 0.84494 Time: 0.18691\n",
      "Iteration: 0076 Loss: 0.84105 Time: 0.18736\n",
      "Iteration: 0077 Loss: 0.83814 Time: 0.18857\n",
      "Iteration: 0078 Loss: 0.82983 Time: 0.18860\n",
      "Iteration: 0079 Loss: 0.82015 Time: 0.19382\n",
      "Iteration: 0080 Loss: 0.81535 Time: 0.18454\n",
      "Iteration: 0081 Loss: 0.81561 Time: 0.18594\n",
      "Iteration: 0082 Loss: 0.81121 Time: 0.19106\n",
      "Iteration: 0083 Loss: 0.80609 Time: 0.18822\n",
      "Iteration: 0084 Loss: 0.80469 Time: 0.18600\n",
      "Iteration: 0085 Loss: 0.79233 Time: 0.19112\n",
      "Iteration: 0086 Loss: 0.78617 Time: 0.19045\n",
      "Iteration: 0087 Loss: 0.78027 Time: 0.18639\n",
      "Iteration: 0088 Loss: 0.78003 Time: 0.18123\n",
      "Iteration: 0089 Loss: 0.77890 Time: 0.18801\n",
      "Iteration: 0090 Loss: 0.76776 Time: 0.18526\n",
      "Iteration: 0091 Loss: 0.76742 Time: 0.18435\n",
      "Iteration: 0092 Loss: 0.75564 Time: 0.18929\n",
      "Iteration: 0093 Loss: 0.75709 Time: 0.19015\n",
      "Iteration: 0094 Loss: 0.75289 Time: 0.18944\n",
      "Iteration: 0095 Loss: 0.74923 Time: 0.18956\n",
      "Iteration: 0096 Loss: 0.74807 Time: 0.19199\n",
      "Iteration: 0097 Loss: 0.73889 Time: 0.18699\n",
      "Iteration: 0098 Loss: 0.74594 Time: 0.19109\n",
      "Iteration: 0099 Loss: 0.73684 Time: 0.19238\n",
      "Iteration: 0100 Loss: 0.73272 Time: 0.19437\n",
      "Iteration: 0101 Loss: 0.73257 Time: 0.18243\n",
      "Iteration: 0102 Loss: 0.72317 Time: 0.18410\n",
      "Iteration: 0103 Loss: 0.72317 Time: 0.19101\n",
      "Iteration: 0104 Loss: 0.71630 Time: 0.19241\n",
      "Iteration: 0105 Loss: 0.71427 Time: 0.18640\n",
      "Iteration: 0106 Loss: 0.70790 Time: 0.18951\n",
      "Iteration: 0107 Loss: 0.71011 Time: 0.19261\n",
      "Iteration: 0108 Loss: 0.70722 Time: 0.18526\n",
      "Iteration: 0109 Loss: 0.69983 Time: 0.19171\n",
      "Iteration: 0110 Loss: 0.69995 Time: 0.19197\n",
      "Iteration: 0111 Loss: 0.70695 Time: 0.18256\n",
      "Iteration: 0112 Loss: 0.68920 Time: 0.18334\n",
      "Iteration: 0113 Loss: 0.69206 Time: 0.18712\n",
      "Iteration: 0114 Loss: 0.68673 Time: 0.19300\n",
      "Iteration: 0115 Loss: 0.68106 Time: 0.18527\n",
      "Iteration: 0116 Loss: 0.68351 Time: 0.18545\n",
      "Iteration: 0117 Loss: 0.68514 Time: 0.19055\n",
      "Iteration: 0118 Loss: 0.67962 Time: 0.18682\n",
      "Iteration: 0119 Loss: 0.68181 Time: 0.19161\n",
      "Iteration: 0120 Loss: 0.67497 Time: 0.18623\n",
      "Iteration: 0121 Loss: 0.67330 Time: 0.18845\n",
      "Iteration: 0122 Loss: 0.66592 Time: 0.18637\n",
      "Iteration: 0123 Loss: 0.67062 Time: 0.18170\n",
      "Iteration: 0124 Loss: 0.66543 Time: 0.18601\n",
      "Iteration: 0125 Loss: 0.66314 Time: 0.18612\n",
      "Iteration: 0126 Loss: 0.66067 Time: 0.19080\n",
      "Iteration: 0127 Loss: 0.65888 Time: 0.18749\n",
      "Iteration: 0128 Loss: 0.65356 Time: 0.18463\n",
      "Iteration: 0129 Loss: 0.65467 Time: 0.19190\n",
      "Iteration: 0130 Loss: 0.65822 Time: 0.18772\n",
      "Iteration: 0131 Loss: 0.65033 Time: 0.18829\n",
      "Iteration: 0132 Loss: 0.65732 Time: 0.18756\n",
      "Iteration: 0133 Loss: 0.64518 Time: 0.18509\n",
      "Iteration: 0134 Loss: 0.64929 Time: 0.18293\n",
      "Iteration: 0135 Loss: 0.64460 Time: 0.18699\n",
      "Iteration: 0136 Loss: 0.64063 Time: 0.18700\n",
      "Iteration: 0137 Loss: 0.64299 Time: 0.18612\n",
      "Iteration: 0138 Loss: 0.63964 Time: 0.18441\n",
      "Iteration: 0139 Loss: 0.63573 Time: 0.19376\n",
      "Iteration: 0140 Loss: 0.63786 Time: 0.18756\n",
      "Iteration: 0141 Loss: 0.63222 Time: 0.19300\n",
      "Iteration: 0142 Loss: 0.62856 Time: 0.19184\n",
      "Iteration: 0143 Loss: 0.62999 Time: 0.18922\n",
      "Iteration: 0144 Loss: 0.63661 Time: 0.18550\n",
      "Iteration: 0145 Loss: 0.63280 Time: 0.19326\n",
      "Iteration: 0146 Loss: 0.62721 Time: 0.18768\n",
      "Iteration: 0147 Loss: 0.63525 Time: 0.18889\n",
      "Iteration: 0148 Loss: 0.62263 Time: 0.19040\n",
      "Iteration: 0149 Loss: 0.62552 Time: 0.19432\n",
      "Iteration: 0150 Loss: 0.62311 Time: 0.19315\n",
      "Iteration: 0151 Loss: 0.62041 Time: 0.19045\n",
      "Iteration: 0152 Loss: 0.61299 Time: 0.19293\n",
      "Iteration: 0153 Loss: 0.61633 Time: 0.18517\n",
      "Iteration: 0154 Loss: 0.62109 Time: 0.18539\n",
      "Iteration: 0155 Loss: 0.61240 Time: 0.18929\n",
      "Iteration: 0156 Loss: 0.61106 Time: 0.19507\n",
      "Iteration: 0157 Loss: 0.61367 Time: 0.18886\n",
      "Iteration: 0158 Loss: 0.61134 Time: 0.18947\n",
      "Iteration: 0159 Loss: 0.60802 Time: 0.18931\n",
      "Iteration: 0160 Loss: 0.60561 Time: 0.18680\n",
      "Iteration: 0161 Loss: 0.60869 Time: 0.19316\n",
      "Iteration: 0162 Loss: 0.61170 Time: 0.19210\n",
      "Iteration: 0163 Loss: 0.60332 Time: 0.18798\n",
      "Iteration: 0164 Loss: 0.60221 Time: 0.19484\n",
      "Iteration: 0165 Loss: 0.60448 Time: 0.18000\n",
      "Iteration: 0166 Loss: 0.60020 Time: 0.18363\n",
      "Iteration: 0167 Loss: 0.59791 Time: 0.18822\n",
      "Iteration: 0168 Loss: 0.59800 Time: 0.18934\n",
      "Iteration: 0169 Loss: 0.59847 Time: 0.19082\n",
      "Iteration: 0170 Loss: 0.59743 Time: 0.18742\n",
      "Iteration: 0171 Loss: 0.59122 Time: 0.18693\n",
      "Iteration: 0172 Loss: 0.59389 Time: 0.18576\n",
      "Iteration: 0173 Loss: 0.59575 Time: 0.18996\n",
      "Iteration: 0174 Loss: 0.59096 Time: 0.19172\n",
      "Iteration: 0175 Loss: 0.59241 Time: 0.18787\n",
      "Iteration: 0176 Loss: 0.58826 Time: 0.18371\n",
      "Iteration: 0177 Loss: 0.58632 Time: 0.18424\n",
      "Iteration: 0178 Loss: 0.58971 Time: 0.18966\n",
      "Iteration: 0179 Loss: 0.58975 Time: 0.19341\n",
      "Iteration: 0180 Loss: 0.58637 Time: 0.19016\n",
      "Iteration: 0181 Loss: 0.57987 Time: 0.19821\n",
      "Iteration: 0182 Loss: 0.58565 Time: 0.18984\n",
      "Iteration: 0183 Loss: 0.58068 Time: 0.18986\n",
      "Iteration: 0184 Loss: 0.57996 Time: 0.19118\n",
      "Iteration: 0185 Loss: 0.58075 Time: 0.18839\n",
      "Iteration: 0186 Loss: 0.57858 Time: 0.18938\n",
      "Iteration: 0187 Loss: 0.57821 Time: 0.18728\n",
      "Iteration: 0188 Loss: 0.57795 Time: 0.19100\n",
      "Iteration: 0189 Loss: 0.57855 Time: 0.18363\n",
      "Iteration: 0190 Loss: 0.57646 Time: 0.18472\n",
      "Iteration: 0191 Loss: 0.56982 Time: 0.18739\n",
      "Iteration: 0192 Loss: 0.57220 Time: 0.18500\n",
      "Iteration: 0193 Loss: 0.57035 Time: 0.18725\n",
      "Iteration: 0194 Loss: 0.57286 Time: 0.18608\n",
      "Iteration: 0195 Loss: 0.57669 Time: 0.18687\n",
      "Iteration: 0196 Loss: 0.57305 Time: 0.18488\n",
      "Iteration: 0197 Loss: 0.56751 Time: 0.18548\n",
      "Iteration: 0198 Loss: 0.56683 Time: 0.18999\n",
      "Iteration: 0199 Loss: 0.56360 Time: 0.19043\n",
      "Iteration: 0200 Loss: 0.56450 Time: 0.18191\n",
      "Done! \n",
      "\n",
      "STEP 3/3 - MODEL EVALUATION \n",
      "\n",
      "Computing the final embedding vectors, for evaluation\n",
      "Done! \n",
      "\n",
      "Testing: link prediction\n",
      "Done! \n",
      "\n",
      "EXPERIMENTS ON MODEL 6 / 10 \n",
      "\n",
      "STEP 1/3 - PREPROCESSING STEPS \n",
      "\n",
      "Masking some edges from the training graph, for link prediction\n",
      "(validation set: 1.0 % of edges - test set: 10.0 % of edges)\n",
      "Done! \n",
      "\n",
      "Preprocessing node features\n",
      "Done! \n",
      "\n",
      "Running the Louvain algorithm for community detection\n",
      "as a preprocessing step for the encoder\n",
      "Done! Louvain has found 84 communities \n",
      "\n",
      "Setting up the model and the optimizer\n",
      "Done! \n",
      "\n",
      "Preprocessing on message passing matrices\n",
      "Done! \n",
      "\n",
      "Initializing TF session\n",
      "Done! \n",
      "\n",
      "STEP 2/3 - MODEL TRAINING \n",
      "\n",
      "Starting training\n",
      "Iteration: 0001 Loss: 1.75109 Time: 0.47968\n",
      "Iteration: 0002 Loss: 1.72522 Time: 0.18959\n",
      "Iteration: 0003 Loss: 1.69013 Time: 0.20058\n",
      "Iteration: 0004 Loss: 1.65398 Time: 0.19682\n",
      "Iteration: 0005 Loss: 1.61911 Time: 0.19534\n",
      "Iteration: 0006 Loss: 1.61090 Time: 0.18539\n",
      "Iteration: 0007 Loss: 1.56945 Time: 0.19508\n",
      "Iteration: 0008 Loss: 1.55973 Time: 0.19312\n",
      "Iteration: 0009 Loss: 1.52130 Time: 0.18718\n",
      "Iteration: 0010 Loss: 1.51487 Time: 0.19911\n",
      "Iteration: 0011 Loss: 1.48337 Time: 0.19708\n",
      "Iteration: 0012 Loss: 1.48715 Time: 0.18934\n",
      "Iteration: 0013 Loss: 1.45504 Time: 0.18999\n",
      "Iteration: 0014 Loss: 1.44873 Time: 0.18394\n",
      "Iteration: 0015 Loss: 1.42127 Time: 0.19756\n",
      "Iteration: 0016 Loss: 1.39701 Time: 0.19527\n",
      "Iteration: 0017 Loss: 1.39673 Time: 0.17729\n",
      "Iteration: 0018 Loss: 1.37032 Time: 0.19047\n",
      "Iteration: 0019 Loss: 1.36816 Time: 0.19007\n",
      "Iteration: 0020 Loss: 1.34241 Time: 0.18765\n",
      "Iteration: 0021 Loss: 1.33433 Time: 0.19085\n",
      "Iteration: 0022 Loss: 1.31609 Time: 0.19178\n",
      "Iteration: 0023 Loss: 1.29858 Time: 0.19002\n",
      "Iteration: 0024 Loss: 1.28207 Time: 0.19211\n",
      "Iteration: 0025 Loss: 1.25007 Time: 0.18699\n",
      "Iteration: 0026 Loss: 1.25960 Time: 0.19412\n",
      "Iteration: 0027 Loss: 1.25345 Time: 0.18388\n",
      "Iteration: 0028 Loss: 1.25120 Time: 0.19096\n",
      "Iteration: 0029 Loss: 1.22394 Time: 0.19175\n",
      "Iteration: 0030 Loss: 1.22308 Time: 0.18562\n",
      "Iteration: 0031 Loss: 1.22180 Time: 0.18801\n",
      "Iteration: 0032 Loss: 1.17906 Time: 0.18629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0033 Loss: 1.17549 Time: 0.18768\n",
      "Iteration: 0034 Loss: 1.16635 Time: 0.19109\n",
      "Iteration: 0035 Loss: 1.15716 Time: 0.18678\n",
      "Iteration: 0036 Loss: 1.14241 Time: 0.19147\n",
      "Iteration: 0037 Loss: 1.12090 Time: 0.18587\n",
      "Iteration: 0038 Loss: 1.11884 Time: 0.18321\n",
      "Iteration: 0039 Loss: 1.11800 Time: 0.18459\n",
      "Iteration: 0040 Loss: 1.10984 Time: 0.19157\n",
      "Iteration: 0041 Loss: 1.09911 Time: 0.19059\n",
      "Iteration: 0042 Loss: 1.07730 Time: 0.19317\n",
      "Iteration: 0043 Loss: 1.08159 Time: 0.18784\n",
      "Iteration: 0044 Loss: 1.07736 Time: 0.18286\n",
      "Iteration: 0045 Loss: 1.06529 Time: 0.18956\n",
      "Iteration: 0046 Loss: 1.07119 Time: 0.19065\n",
      "Iteration: 0047 Loss: 1.04140 Time: 0.18762\n",
      "Iteration: 0048 Loss: 1.03387 Time: 0.18296\n",
      "Iteration: 0049 Loss: 1.03361 Time: 0.18306\n",
      "Iteration: 0050 Loss: 1.02309 Time: 0.18855\n",
      "Iteration: 0051 Loss: 1.02880 Time: 0.19003\n",
      "Iteration: 0052 Loss: 1.00992 Time: 0.18380\n",
      "Iteration: 0053 Loss: 1.00208 Time: 0.19126\n",
      "Iteration: 0054 Loss: 0.99993 Time: 0.18605\n",
      "Iteration: 0055 Loss: 0.98786 Time: 0.19040\n",
      "Iteration: 0056 Loss: 0.98050 Time: 0.19290\n",
      "Iteration: 0057 Loss: 0.96750 Time: 0.18894\n",
      "Iteration: 0058 Loss: 0.96320 Time: 0.19326\n",
      "Iteration: 0059 Loss: 0.95467 Time: 0.18333\n",
      "Iteration: 0060 Loss: 0.95632 Time: 0.18833\n",
      "Iteration: 0061 Loss: 0.94136 Time: 0.18863\n",
      "Iteration: 0062 Loss: 0.94287 Time: 0.18815\n",
      "Iteration: 0063 Loss: 0.93200 Time: 0.18801\n",
      "Iteration: 0064 Loss: 0.93105 Time: 0.19158\n",
      "Iteration: 0065 Loss: 0.90579 Time: 0.18560\n",
      "Iteration: 0066 Loss: 0.90637 Time: 0.18940\n",
      "Iteration: 0067 Loss: 0.88475 Time: 0.19222\n",
      "Iteration: 0068 Loss: 0.89909 Time: 0.19326\n",
      "Iteration: 0069 Loss: 0.88718 Time: 0.18389\n",
      "Iteration: 0070 Loss: 0.87726 Time: 0.18354\n",
      "Iteration: 0071 Loss: 0.87282 Time: 0.19400\n",
      "Iteration: 0072 Loss: 0.87617 Time: 0.18900\n",
      "Iteration: 0073 Loss: 0.86469 Time: 0.18684\n",
      "Iteration: 0074 Loss: 0.85832 Time: 0.19711\n",
      "Iteration: 0075 Loss: 0.85204 Time: 0.19512\n",
      "Iteration: 0076 Loss: 0.83787 Time: 0.19353\n",
      "Iteration: 0077 Loss: 0.83742 Time: 0.19399\n",
      "Iteration: 0078 Loss: 0.83921 Time: 0.18435\n",
      "Iteration: 0079 Loss: 0.83409 Time: 0.18569\n",
      "Iteration: 0080 Loss: 0.81946 Time: 0.18641\n",
      "Iteration: 0081 Loss: 0.79913 Time: 0.18323\n",
      "Iteration: 0082 Loss: 0.80280 Time: 0.18951\n",
      "Iteration: 0083 Loss: 0.80698 Time: 0.19240\n",
      "Iteration: 0084 Loss: 0.80366 Time: 0.19391\n",
      "Iteration: 0085 Loss: 0.79012 Time: 0.19163\n",
      "Iteration: 0086 Loss: 0.79463 Time: 0.19308\n",
      "Iteration: 0087 Loss: 0.78057 Time: 0.18692\n",
      "Iteration: 0088 Loss: 0.77642 Time: 0.18619\n",
      "Iteration: 0089 Loss: 0.77453 Time: 0.18809\n",
      "Iteration: 0090 Loss: 0.76757 Time: 0.19846\n",
      "Iteration: 0091 Loss: 0.76832 Time: 0.18946\n",
      "Iteration: 0092 Loss: 0.76735 Time: 0.19128\n",
      "Iteration: 0093 Loss: 0.74885 Time: 0.19300\n",
      "Iteration: 0094 Loss: 0.75419 Time: 0.19052\n",
      "Iteration: 0095 Loss: 0.74595 Time: 0.18636\n",
      "Iteration: 0096 Loss: 0.74930 Time: 0.18842\n",
      "Iteration: 0097 Loss: 0.74127 Time: 0.18573\n",
      "Iteration: 0098 Loss: 0.73869 Time: 0.19005\n",
      "Iteration: 0099 Loss: 0.73119 Time: 0.19211\n",
      "Iteration: 0100 Loss: 0.73212 Time: 0.18953\n",
      "Iteration: 0101 Loss: 0.72542 Time: 0.18936\n",
      "Iteration: 0102 Loss: 0.71898 Time: 0.18635\n",
      "Iteration: 0103 Loss: 0.71891 Time: 0.18860\n",
      "Iteration: 0104 Loss: 0.71807 Time: 0.18705\n",
      "Iteration: 0105 Loss: 0.71353 Time: 0.19040\n",
      "Iteration: 0106 Loss: 0.71272 Time: 0.18481\n",
      "Iteration: 0107 Loss: 0.70367 Time: 0.19227\n",
      "Iteration: 0108 Loss: 0.69951 Time: 0.19115\n",
      "Iteration: 0109 Loss: 0.69974 Time: 0.19381\n",
      "Iteration: 0110 Loss: 0.70289 Time: 0.19253\n",
      "Iteration: 0111 Loss: 0.69066 Time: 0.19549\n",
      "Iteration: 0112 Loss: 0.69110 Time: 0.18313\n",
      "Iteration: 0113 Loss: 0.69221 Time: 0.19892\n",
      "Iteration: 0114 Loss: 0.69101 Time: 0.19246\n",
      "Iteration: 0115 Loss: 0.69018 Time: 0.18831\n",
      "Iteration: 0116 Loss: 0.68373 Time: 0.19160\n",
      "Iteration: 0117 Loss: 0.68141 Time: 0.18725\n",
      "Iteration: 0118 Loss: 0.67721 Time: 0.19321\n",
      "Iteration: 0119 Loss: 0.67559 Time: 0.19517\n",
      "Iteration: 0120 Loss: 0.67395 Time: 0.18744\n",
      "Iteration: 0121 Loss: 0.66585 Time: 0.18949\n",
      "Iteration: 0122 Loss: 0.66973 Time: 0.18399\n",
      "Iteration: 0123 Loss: 0.66328 Time: 0.18723\n",
      "Iteration: 0124 Loss: 0.66256 Time: 0.19202\n",
      "Iteration: 0125 Loss: 0.66601 Time: 0.18496\n",
      "Iteration: 0126 Loss: 0.65962 Time: 0.19150\n",
      "Iteration: 0127 Loss: 0.65985 Time: 0.18927\n",
      "Iteration: 0128 Loss: 0.65659 Time: 0.19390\n",
      "Iteration: 0129 Loss: 0.65511 Time: 0.19081\n",
      "Iteration: 0130 Loss: 0.64750 Time: 0.18998\n",
      "Iteration: 0131 Loss: 0.64873 Time: 0.18637\n",
      "Iteration: 0132 Loss: 0.65120 Time: 0.18861\n",
      "Iteration: 0133 Loss: 0.64693 Time: 0.18195\n",
      "Iteration: 0134 Loss: 0.64559 Time: 0.19573\n",
      "Iteration: 0135 Loss: 0.64627 Time: 0.19400\n",
      "Iteration: 0136 Loss: 0.64233 Time: 0.18795\n",
      "Iteration: 0137 Loss: 0.64116 Time: 0.18786\n",
      "Iteration: 0138 Loss: 0.63983 Time: 0.19405\n",
      "Iteration: 0139 Loss: 0.63857 Time: 0.19041\n",
      "Iteration: 0140 Loss: 0.63278 Time: 0.18533\n",
      "Iteration: 0141 Loss: 0.63676 Time: 0.19339\n",
      "Iteration: 0142 Loss: 0.63349 Time: 0.19103\n",
      "Iteration: 0143 Loss: 0.62942 Time: 0.18408\n",
      "Iteration: 0144 Loss: 0.62717 Time: 0.18933\n",
      "Iteration: 0145 Loss: 0.62798 Time: 0.18800\n",
      "Iteration: 0146 Loss: 0.62517 Time: 0.19198\n",
      "Iteration: 0147 Loss: 0.62665 Time: 0.18853\n",
      "Iteration: 0148 Loss: 0.62095 Time: 0.19526\n",
      "Iteration: 0149 Loss: 0.62139 Time: 0.19269\n",
      "Iteration: 0150 Loss: 0.62149 Time: 0.18706\n",
      "Iteration: 0151 Loss: 0.61559 Time: 0.18400\n",
      "Iteration: 0152 Loss: 0.61835 Time: 0.18952\n",
      "Iteration: 0153 Loss: 0.61395 Time: 0.18184\n",
      "Iteration: 0154 Loss: 0.61383 Time: 0.18339\n",
      "Iteration: 0155 Loss: 0.61297 Time: 0.18566\n",
      "Iteration: 0156 Loss: 0.61497 Time: 0.18897\n",
      "Iteration: 0157 Loss: 0.61559 Time: 0.19592\n",
      "Iteration: 0158 Loss: 0.61074 Time: 0.18677\n",
      "Iteration: 0159 Loss: 0.60645 Time: 0.18760\n",
      "Iteration: 0160 Loss: 0.60754 Time: 0.19395\n",
      "Iteration: 0161 Loss: 0.60686 Time: 0.18870\n",
      "Iteration: 0162 Loss: 0.60328 Time: 0.19125\n",
      "Iteration: 0163 Loss: 0.59813 Time: 0.19238\n",
      "Iteration: 0164 Loss: 0.60238 Time: 0.18297\n",
      "Iteration: 0165 Loss: 0.59785 Time: 0.18321\n",
      "Iteration: 0166 Loss: 0.59552 Time: 0.19200\n",
      "Iteration: 0167 Loss: 0.59281 Time: 0.19288\n",
      "Iteration: 0168 Loss: 0.59405 Time: 0.19559\n",
      "Iteration: 0169 Loss: 0.59254 Time: 0.18838\n",
      "Iteration: 0170 Loss: 0.59461 Time: 0.19021\n",
      "Iteration: 0171 Loss: 0.59323 Time: 0.19239\n",
      "Iteration: 0172 Loss: 0.59737 Time: 0.19124\n",
      "Iteration: 0173 Loss: 0.59010 Time: 0.19167\n",
      "Iteration: 0174 Loss: 0.58941 Time: 0.18872\n",
      "Iteration: 0175 Loss: 0.58910 Time: 0.18503\n",
      "Iteration: 0176 Loss: 0.58417 Time: 0.18997\n",
      "Iteration: 0177 Loss: 0.58398 Time: 0.18600\n",
      "Iteration: 0178 Loss: 0.58619 Time: 0.19983\n",
      "Iteration: 0179 Loss: 0.59043 Time: 0.19378\n",
      "Iteration: 0180 Loss: 0.58397 Time: 0.19287\n",
      "Iteration: 0181 Loss: 0.58373 Time: 0.19276\n",
      "Iteration: 0182 Loss: 0.58261 Time: 0.18719\n",
      "Iteration: 0183 Loss: 0.58034 Time: 0.18982\n",
      "Iteration: 0184 Loss: 0.57908 Time: 0.18956\n",
      "Iteration: 0185 Loss: 0.57891 Time: 0.18568\n",
      "Iteration: 0186 Loss: 0.57903 Time: 0.18211\n",
      "Iteration: 0187 Loss: 0.57853 Time: 0.18635\n",
      "Iteration: 0188 Loss: 0.57352 Time: 0.18805\n",
      "Iteration: 0189 Loss: 0.57755 Time: 0.19554\n",
      "Iteration: 0190 Loss: 0.57256 Time: 0.19628\n",
      "Iteration: 0191 Loss: 0.57208 Time: 0.19188\n",
      "Iteration: 0192 Loss: 0.57249 Time: 0.18834\n",
      "Iteration: 0193 Loss: 0.56691 Time: 0.18808\n",
      "Iteration: 0194 Loss: 0.56956 Time: 0.18386\n",
      "Iteration: 0195 Loss: 0.56680 Time: 0.19566\n",
      "Iteration: 0196 Loss: 0.56705 Time: 0.18644\n",
      "Iteration: 0197 Loss: 0.56557 Time: 0.18536\n",
      "Iteration: 0198 Loss: 0.56634 Time: 0.18774\n",
      "Iteration: 0199 Loss: 0.56493 Time: 0.18639\n",
      "Iteration: 0200 Loss: 0.56373 Time: 0.18715\n",
      "Done! \n",
      "\n",
      "STEP 3/3 - MODEL EVALUATION \n",
      "\n",
      "Computing the final embedding vectors, for evaluation\n",
      "Done! \n",
      "\n",
      "Testing: link prediction\n",
      "Done! \n",
      "\n",
      "EXPERIMENTS ON MODEL 7 / 10 \n",
      "\n",
      "STEP 1/3 - PREPROCESSING STEPS \n",
      "\n",
      "Masking some edges from the training graph, for link prediction\n",
      "(validation set: 1.0 % of edges - test set: 10.0 % of edges)\n",
      "Done! \n",
      "\n",
      "Preprocessing node features\n",
      "Done! \n",
      "\n",
      "Running the Louvain algorithm for community detection\n",
      "as a preprocessing step for the encoder\n",
      "Done! Louvain has found 87 communities \n",
      "\n",
      "Setting up the model and the optimizer\n",
      "Done! \n",
      "\n",
      "Preprocessing on message passing matrices\n",
      "Done! \n",
      "\n",
      "Initializing TF session\n",
      "Done! \n",
      "\n",
      "STEP 2/3 - MODEL TRAINING \n",
      "\n",
      "Starting training\n",
      "Iteration: 0001 Loss: 1.78655 Time: 0.43591\n",
      "Iteration: 0002 Loss: 1.67404 Time: 0.19124\n",
      "Iteration: 0003 Loss: 1.68863 Time: 0.19033\n",
      "Iteration: 0004 Loss: 1.64215 Time: 0.18963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0005 Loss: 1.61220 Time: 0.19478\n",
      "Iteration: 0006 Loss: 1.59296 Time: 0.18077\n",
      "Iteration: 0007 Loss: 1.56966 Time: 0.18371\n",
      "Iteration: 0008 Loss: 1.56354 Time: 0.18688\n",
      "Iteration: 0009 Loss: 1.50766 Time: 0.18960\n",
      "Iteration: 0010 Loss: 1.50727 Time: 0.18874\n",
      "Iteration: 0011 Loss: 1.47381 Time: 0.19174\n",
      "Iteration: 0012 Loss: 1.49963 Time: 0.18693\n",
      "Iteration: 0013 Loss: 1.46042 Time: 0.19280\n",
      "Iteration: 0014 Loss: 1.41855 Time: 0.19100\n",
      "Iteration: 0015 Loss: 1.41456 Time: 0.19084\n",
      "Iteration: 0016 Loss: 1.40125 Time: 0.18555\n",
      "Iteration: 0017 Loss: 1.39765 Time: 0.18504\n",
      "Iteration: 0018 Loss: 1.37547 Time: 0.18623\n",
      "Iteration: 0019 Loss: 1.37574 Time: 0.18387\n",
      "Iteration: 0020 Loss: 1.33655 Time: 0.19763\n",
      "Iteration: 0021 Loss: 1.32086 Time: 0.18944\n",
      "Iteration: 0022 Loss: 1.29362 Time: 0.18594\n",
      "Iteration: 0023 Loss: 1.28779 Time: 0.19409\n",
      "Iteration: 0024 Loss: 1.28090 Time: 0.18501\n",
      "Iteration: 0025 Loss: 1.26687 Time: 0.19234\n",
      "Iteration: 0026 Loss: 1.25140 Time: 0.19237\n",
      "Iteration: 0027 Loss: 1.24521 Time: 0.18989\n",
      "Iteration: 0028 Loss: 1.21454 Time: 0.18134\n",
      "Iteration: 0029 Loss: 1.22132 Time: 0.18981\n",
      "Iteration: 0030 Loss: 1.21405 Time: 0.19001\n",
      "Iteration: 0031 Loss: 1.19228 Time: 0.18976\n",
      "Iteration: 0032 Loss: 1.19012 Time: 0.19451\n",
      "Iteration: 0033 Loss: 1.18311 Time: 0.19125\n",
      "Iteration: 0034 Loss: 1.15919 Time: 0.18995\n",
      "Iteration: 0035 Loss: 1.14225 Time: 0.18799\n",
      "Iteration: 0036 Loss: 1.14572 Time: 0.18449\n",
      "Iteration: 0037 Loss: 1.14207 Time: 0.18714\n",
      "Iteration: 0038 Loss: 1.12605 Time: 0.18528\n",
      "Iteration: 0039 Loss: 1.12138 Time: 0.18903\n",
      "Iteration: 0040 Loss: 1.09649 Time: 0.19057\n",
      "Iteration: 0041 Loss: 1.11737 Time: 0.18490\n",
      "Iteration: 0042 Loss: 1.07509 Time: 0.18931\n",
      "Iteration: 0043 Loss: 1.07328 Time: 0.19280\n",
      "Iteration: 0044 Loss: 1.06047 Time: 0.18798\n",
      "Iteration: 0045 Loss: 1.06427 Time: 0.19058\n",
      "Iteration: 0046 Loss: 1.05395 Time: 0.18484\n",
      "Iteration: 0047 Loss: 1.04135 Time: 0.18724\n",
      "Iteration: 0048 Loss: 1.04988 Time: 0.18453\n",
      "Iteration: 0049 Loss: 1.02685 Time: 0.18294\n",
      "Iteration: 0050 Loss: 1.02987 Time: 0.19164\n",
      "Iteration: 0051 Loss: 1.00940 Time: 0.18623\n",
      "Iteration: 0052 Loss: 1.02008 Time: 0.18806\n",
      "Iteration: 0053 Loss: 0.99929 Time: 0.18084\n",
      "Iteration: 0054 Loss: 0.99168 Time: 0.19317\n",
      "Iteration: 0055 Loss: 0.98276 Time: 0.19149\n",
      "Iteration: 0056 Loss: 0.97231 Time: 0.18885\n",
      "Iteration: 0057 Loss: 0.96614 Time: 0.18831\n",
      "Iteration: 0058 Loss: 0.94642 Time: 0.19122\n",
      "Iteration: 0059 Loss: 0.95171 Time: 0.18606\n",
      "Iteration: 0060 Loss: 0.94456 Time: 0.18185\n",
      "Iteration: 0061 Loss: 0.93236 Time: 0.18883\n",
      "Iteration: 0062 Loss: 0.92903 Time: 0.19204\n",
      "Iteration: 0063 Loss: 0.91793 Time: 0.19060\n",
      "Iteration: 0064 Loss: 0.91239 Time: 0.19026\n",
      "Iteration: 0065 Loss: 0.91202 Time: 0.18709\n",
      "Iteration: 0066 Loss: 0.90505 Time: 0.18922\n",
      "Iteration: 0067 Loss: 0.89631 Time: 0.19041\n",
      "Iteration: 0068 Loss: 0.88379 Time: 0.18982\n",
      "Iteration: 0069 Loss: 0.88846 Time: 0.18769\n",
      "Iteration: 0070 Loss: 0.88174 Time: 0.18137\n",
      "Iteration: 0071 Loss: 0.87117 Time: 0.18434\n",
      "Iteration: 0072 Loss: 0.86307 Time: 0.19039\n",
      "Iteration: 0073 Loss: 0.85167 Time: 0.19409\n",
      "Iteration: 0074 Loss: 0.85159 Time: 0.18914\n",
      "Iteration: 0075 Loss: 0.85491 Time: 0.19011\n",
      "Iteration: 0076 Loss: 0.83268 Time: 0.18970\n",
      "Iteration: 0077 Loss: 0.83382 Time: 0.19532\n",
      "Iteration: 0078 Loss: 0.82038 Time: 0.18835\n",
      "Iteration: 0079 Loss: 0.83929 Time: 0.18474\n",
      "Iteration: 0080 Loss: 0.80654 Time: 0.18668\n",
      "Iteration: 0081 Loss: 0.80630 Time: 0.18658\n",
      "Iteration: 0082 Loss: 0.80741 Time: 0.19045\n",
      "Iteration: 0083 Loss: 0.80534 Time: 0.18499\n",
      "Iteration: 0084 Loss: 0.80629 Time: 0.19232\n",
      "Iteration: 0085 Loss: 0.78547 Time: 0.19204\n",
      "Iteration: 0086 Loss: 0.78221 Time: 0.19244\n",
      "Iteration: 0087 Loss: 0.78160 Time: 0.19118\n",
      "Iteration: 0088 Loss: 0.77207 Time: 0.18741\n",
      "Iteration: 0089 Loss: 0.77287 Time: 0.19006\n",
      "Iteration: 0090 Loss: 0.76949 Time: 0.18997\n",
      "Iteration: 0091 Loss: 0.76137 Time: 0.18688\n",
      "Iteration: 0092 Loss: 0.76565 Time: 0.18751\n",
      "Iteration: 0093 Loss: 0.75284 Time: 0.18700\n",
      "Iteration: 0094 Loss: 0.74913 Time: 0.18762\n",
      "Iteration: 0095 Loss: 0.75075 Time: 0.18568\n",
      "Iteration: 0096 Loss: 0.74650 Time: 0.19159\n",
      "Iteration: 0097 Loss: 0.73602 Time: 0.18931\n",
      "Iteration: 0098 Loss: 0.73195 Time: 0.19544\n",
      "Iteration: 0099 Loss: 0.73227 Time: 0.18571\n",
      "Iteration: 0100 Loss: 0.73450 Time: 0.19295\n",
      "Iteration: 0101 Loss: 0.72076 Time: 0.18496\n",
      "Iteration: 0102 Loss: 0.72066 Time: 0.17948\n",
      "Iteration: 0103 Loss: 0.71188 Time: 0.18839\n",
      "Iteration: 0104 Loss: 0.71397 Time: 0.19751\n",
      "Iteration: 0105 Loss: 0.71493 Time: 0.18810\n",
      "Iteration: 0106 Loss: 0.70851 Time: 0.19069\n",
      "Iteration: 0107 Loss: 0.70893 Time: 0.19035\n",
      "Iteration: 0108 Loss: 0.70781 Time: 0.19203\n",
      "Iteration: 0109 Loss: 0.70918 Time: 0.18604\n",
      "Iteration: 0110 Loss: 0.69475 Time: 0.19244\n",
      "Iteration: 0111 Loss: 0.69238 Time: 0.18805\n",
      "Iteration: 0112 Loss: 0.69477 Time: 0.18157\n",
      "Iteration: 0113 Loss: 0.69222 Time: 0.19204\n",
      "Iteration: 0114 Loss: 0.69160 Time: 0.18757\n",
      "Iteration: 0115 Loss: 0.68975 Time: 0.18678\n",
      "Iteration: 0116 Loss: 0.68472 Time: 0.19662\n",
      "Iteration: 0117 Loss: 0.68072 Time: 0.19023\n",
      "Iteration: 0118 Loss: 0.68161 Time: 0.18900\n",
      "Iteration: 0119 Loss: 0.67592 Time: 0.19238\n",
      "Iteration: 0120 Loss: 0.67321 Time: 0.18626\n",
      "Iteration: 0121 Loss: 0.67092 Time: 0.18518\n",
      "Iteration: 0122 Loss: 0.67066 Time: 0.19081\n",
      "Iteration: 0123 Loss: 0.66837 Time: 0.18396\n",
      "Iteration: 0124 Loss: 0.66254 Time: 0.18869\n",
      "Iteration: 0125 Loss: 0.66361 Time: 0.18945\n",
      "Iteration: 0126 Loss: 0.66347 Time: 0.18108\n",
      "Iteration: 0127 Loss: 0.66605 Time: 0.19183\n",
      "Iteration: 0128 Loss: 0.65783 Time: 0.18964\n",
      "Iteration: 0129 Loss: 0.65430 Time: 0.19077\n",
      "Iteration: 0130 Loss: 0.65336 Time: 0.19183\n",
      "Iteration: 0131 Loss: 0.65083 Time: 0.18346\n",
      "Iteration: 0132 Loss: 0.65069 Time: 0.19301\n",
      "Iteration: 0133 Loss: 0.64580 Time: 0.18699\n",
      "Iteration: 0134 Loss: 0.64332 Time: 0.18892\n",
      "Iteration: 0135 Loss: 0.64143 Time: 0.19157\n",
      "Iteration: 0136 Loss: 0.64138 Time: 0.19028\n",
      "Iteration: 0137 Loss: 0.64235 Time: 0.18408\n",
      "Iteration: 0138 Loss: 0.63647 Time: 0.18186\n",
      "Iteration: 0139 Loss: 0.63993 Time: 0.19154\n",
      "Iteration: 0140 Loss: 0.63768 Time: 0.19951\n",
      "Iteration: 0141 Loss: 0.63463 Time: 0.19123\n",
      "Iteration: 0142 Loss: 0.63598 Time: 0.18673\n",
      "Iteration: 0143 Loss: 0.62986 Time: 0.18990\n",
      "Iteration: 0144 Loss: 0.63227 Time: 0.18703\n",
      "Iteration: 0145 Loss: 0.62826 Time: 0.19116\n",
      "Iteration: 0146 Loss: 0.62697 Time: 0.19001\n",
      "Iteration: 0147 Loss: 0.62520 Time: 0.19242\n",
      "Iteration: 0148 Loss: 0.62292 Time: 0.18683\n",
      "Iteration: 0149 Loss: 0.62074 Time: 0.18878\n",
      "Iteration: 0150 Loss: 0.61608 Time: 0.19173\n",
      "Iteration: 0151 Loss: 0.62213 Time: 0.18807\n",
      "Iteration: 0152 Loss: 0.61766 Time: 0.18988\n",
      "Iteration: 0153 Loss: 0.61603 Time: 0.18437\n",
      "Iteration: 0154 Loss: 0.61379 Time: 0.18318\n",
      "Iteration: 0155 Loss: 0.60852 Time: 0.17855\n",
      "Iteration: 0156 Loss: 0.61291 Time: 0.19051\n",
      "Iteration: 0157 Loss: 0.61420 Time: 0.19079\n",
      "Iteration: 0158 Loss: 0.61384 Time: 0.19020\n",
      "Iteration: 0159 Loss: 0.61031 Time: 0.18982\n",
      "Iteration: 0160 Loss: 0.60683 Time: 0.19656\n",
      "Iteration: 0161 Loss: 0.60811 Time: 0.19550\n",
      "Iteration: 0162 Loss: 0.60703 Time: 0.19514\n",
      "Iteration: 0163 Loss: 0.60425 Time: 0.18768\n",
      "Iteration: 0164 Loss: 0.60401 Time: 0.19438\n",
      "Iteration: 0165 Loss: 0.59699 Time: 0.18014\n",
      "Iteration: 0166 Loss: 0.60116 Time: 0.19546\n",
      "Iteration: 0167 Loss: 0.59894 Time: 0.18903\n",
      "Iteration: 0168 Loss: 0.59695 Time: 0.18957\n",
      "Iteration: 0169 Loss: 0.59905 Time: 0.19194\n",
      "Iteration: 0170 Loss: 0.59087 Time: 0.19142\n",
      "Iteration: 0171 Loss: 0.60009 Time: 0.18640\n",
      "Iteration: 0172 Loss: 0.59371 Time: 0.19311\n",
      "Iteration: 0173 Loss: 0.59416 Time: 0.18880\n",
      "Iteration: 0174 Loss: 0.59380 Time: 0.18712\n",
      "Iteration: 0175 Loss: 0.58930 Time: 0.18075\n",
      "Iteration: 0176 Loss: 0.59165 Time: 0.18480\n",
      "Iteration: 0177 Loss: 0.58826 Time: 0.19227\n",
      "Iteration: 0178 Loss: 0.58482 Time: 0.18560\n",
      "Iteration: 0179 Loss: 0.58932 Time: 0.18914\n",
      "Iteration: 0180 Loss: 0.58589 Time: 0.19099\n",
      "Iteration: 0181 Loss: 0.58396 Time: 0.18703\n",
      "Iteration: 0182 Loss: 0.58285 Time: 0.19226\n",
      "Iteration: 0183 Loss: 0.58392 Time: 0.18983\n",
      "Iteration: 0184 Loss: 0.58499 Time: 0.19178\n",
      "Iteration: 0185 Loss: 0.57709 Time: 0.18892\n",
      "Iteration: 0186 Loss: 0.57856 Time: 0.18142\n",
      "Iteration: 0187 Loss: 0.58112 Time: 0.19115\n",
      "Iteration: 0188 Loss: 0.57716 Time: 0.18701\n",
      "Iteration: 0189 Loss: 0.57697 Time: 0.18346\n",
      "Iteration: 0190 Loss: 0.57423 Time: 0.19847\n",
      "Iteration: 0191 Loss: 0.57501 Time: 0.18982\n",
      "Iteration: 0192 Loss: 0.57545 Time: 0.18948\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0193 Loss: 0.57104 Time: 0.19075\n",
      "Iteration: 0194 Loss: 0.57283 Time: 0.19004\n",
      "Iteration: 0195 Loss: 0.57322 Time: 0.19044\n",
      "Iteration: 0196 Loss: 0.57032 Time: 0.18993\n",
      "Iteration: 0197 Loss: 0.56717 Time: 0.18713\n",
      "Iteration: 0198 Loss: 0.56982 Time: 0.18971\n",
      "Iteration: 0199 Loss: 0.56407 Time: 0.19072\n",
      "Iteration: 0200 Loss: 0.56697 Time: 0.19437\n",
      "Done! \n",
      "\n",
      "STEP 3/3 - MODEL EVALUATION \n",
      "\n",
      "Computing the final embedding vectors, for evaluation\n",
      "Done! \n",
      "\n",
      "Testing: link prediction\n",
      "Done! \n",
      "\n",
      "EXPERIMENTS ON MODEL 8 / 10 \n",
      "\n",
      "STEP 1/3 - PREPROCESSING STEPS \n",
      "\n",
      "Masking some edges from the training graph, for link prediction\n",
      "(validation set: 1.0 % of edges - test set: 10.0 % of edges)\n",
      "Done! \n",
      "\n",
      "Preprocessing node features\n",
      "Done! \n",
      "\n",
      "Running the Louvain algorithm for community detection\n",
      "as a preprocessing step for the encoder\n",
      "Done! Louvain has found 95 communities \n",
      "\n",
      "Setting up the model and the optimizer\n",
      "Done! \n",
      "\n",
      "Preprocessing on message passing matrices\n",
      "Done! \n",
      "\n",
      "Initializing TF session\n",
      "Done! \n",
      "\n",
      "STEP 2/3 - MODEL TRAINING \n",
      "\n",
      "Starting training\n",
      "Iteration: 0001 Loss: 1.73368 Time: 0.44480\n",
      "Iteration: 0002 Loss: 1.72055 Time: 0.19220\n",
      "Iteration: 0003 Loss: 1.66481 Time: 0.19171\n",
      "Iteration: 0004 Loss: 1.62448 Time: 0.19025\n",
      "Iteration: 0005 Loss: 1.60542 Time: 0.19135\n",
      "Iteration: 0006 Loss: 1.63020 Time: 0.18609\n",
      "Iteration: 0007 Loss: 1.56907 Time: 0.18958\n",
      "Iteration: 0008 Loss: 1.53500 Time: 0.19238\n",
      "Iteration: 0009 Loss: 1.56471 Time: 0.19073\n",
      "Iteration: 0010 Loss: 1.51033 Time: 0.18223\n",
      "Iteration: 0011 Loss: 1.48525 Time: 0.19053\n",
      "Iteration: 0012 Loss: 1.49289 Time: 0.19368\n",
      "Iteration: 0013 Loss: 1.47224 Time: 0.18718\n",
      "Iteration: 0014 Loss: 1.43400 Time: 0.19549\n",
      "Iteration: 0015 Loss: 1.42010 Time: 0.18883\n",
      "Iteration: 0016 Loss: 1.41887 Time: 0.19354\n",
      "Iteration: 0017 Loss: 1.38352 Time: 0.18354\n",
      "Iteration: 0018 Loss: 1.36552 Time: 0.18505\n",
      "Iteration: 0019 Loss: 1.36454 Time: 0.19321\n",
      "Iteration: 0020 Loss: 1.33619 Time: 0.19236\n",
      "Iteration: 0021 Loss: 1.30872 Time: 0.18665\n",
      "Iteration: 0022 Loss: 1.30977 Time: 0.18613\n",
      "Iteration: 0023 Loss: 1.27912 Time: 0.18741\n",
      "Iteration: 0024 Loss: 1.28448 Time: 0.19091\n",
      "Iteration: 0025 Loss: 1.28246 Time: 0.19275\n",
      "Iteration: 0026 Loss: 1.25123 Time: 0.18684\n",
      "Iteration: 0027 Loss: 1.24674 Time: 0.19032\n",
      "Iteration: 0028 Loss: 1.23367 Time: 0.17907\n",
      "Iteration: 0029 Loss: 1.21216 Time: 0.18314\n",
      "Iteration: 0030 Loss: 1.21610 Time: 0.18800\n",
      "Iteration: 0031 Loss: 1.20130 Time: 0.19012\n",
      "Iteration: 0032 Loss: 1.17913 Time: 0.19000\n",
      "Iteration: 0033 Loss: 1.18208 Time: 0.18784\n",
      "Iteration: 0034 Loss: 1.17064 Time: 0.18924\n",
      "Iteration: 0035 Loss: 1.15892 Time: 0.19039\n",
      "Iteration: 0036 Loss: 1.13884 Time: 0.18721\n",
      "Iteration: 0037 Loss: 1.11098 Time: 0.18907\n",
      "Iteration: 0038 Loss: 1.11266 Time: 0.18588\n",
      "Iteration: 0039 Loss: 1.11513 Time: 0.18473\n",
      "Iteration: 0040 Loss: 1.09891 Time: 0.18520\n",
      "Iteration: 0041 Loss: 1.09512 Time: 0.18692\n",
      "Iteration: 0042 Loss: 1.10198 Time: 0.19035\n",
      "Iteration: 0043 Loss: 1.06966 Time: 0.19047\n",
      "Iteration: 0044 Loss: 1.06466 Time: 0.19162\n",
      "Iteration: 0045 Loss: 1.04868 Time: 0.18936\n",
      "Iteration: 0046 Loss: 1.06054 Time: 0.18877\n",
      "Iteration: 0047 Loss: 1.04816 Time: 0.18502\n",
      "Iteration: 0048 Loss: 1.03115 Time: 0.18177\n",
      "Iteration: 0049 Loss: 1.03166 Time: 0.18782\n",
      "Iteration: 0050 Loss: 1.01042 Time: 0.19520\n",
      "Iteration: 0051 Loss: 1.01671 Time: 0.18723\n",
      "Iteration: 0052 Loss: 1.00460 Time: 0.18787\n",
      "Iteration: 0053 Loss: 0.99453 Time: 0.18800\n",
      "Iteration: 0054 Loss: 0.98094 Time: 0.18661\n",
      "Iteration: 0055 Loss: 0.98183 Time: 0.19121\n",
      "Iteration: 0056 Loss: 0.97548 Time: 0.18471\n",
      "Iteration: 0057 Loss: 0.95777 Time: 0.19592\n",
      "Iteration: 0058 Loss: 0.95886 Time: 0.18427\n",
      "Iteration: 0059 Loss: 0.94322 Time: 0.18612\n",
      "Iteration: 0060 Loss: 0.95248 Time: 0.18188\n",
      "Iteration: 0061 Loss: 0.94559 Time: 0.18623\n",
      "Iteration: 0062 Loss: 0.93300 Time: 0.19162\n",
      "Iteration: 0063 Loss: 0.92030 Time: 0.18966\n",
      "Iteration: 0064 Loss: 0.91013 Time: 0.19099\n",
      "Iteration: 0065 Loss: 0.90551 Time: 0.18536\n",
      "Iteration: 0066 Loss: 0.89914 Time: 0.19215\n",
      "Iteration: 0067 Loss: 0.89519 Time: 0.19057\n",
      "Iteration: 0068 Loss: 0.89569 Time: 0.18883\n",
      "Iteration: 0069 Loss: 0.87440 Time: 0.18565\n",
      "Iteration: 0070 Loss: 0.88348 Time: 0.19101\n",
      "Iteration: 0071 Loss: 0.87289 Time: 0.18541\n",
      "Iteration: 0072 Loss: 0.84774 Time: 0.18901\n",
      "Iteration: 0073 Loss: 0.85625 Time: 0.18606\n",
      "Iteration: 0074 Loss: 0.85165 Time: 0.18815\n",
      "Iteration: 0075 Loss: 0.85357 Time: 0.18649\n",
      "Iteration: 0076 Loss: 0.83236 Time: 0.18223\n",
      "Iteration: 0077 Loss: 0.83206 Time: 0.19575\n",
      "Iteration: 0078 Loss: 0.83307 Time: 0.18225\n",
      "Iteration: 0079 Loss: 0.82977 Time: 0.19517\n",
      "Iteration: 0080 Loss: 0.81583 Time: 0.18830\n",
      "Iteration: 0081 Loss: 0.81116 Time: 0.17976\n",
      "Iteration: 0082 Loss: 0.80362 Time: 0.18839\n",
      "Iteration: 0083 Loss: 0.79605 Time: 0.18771\n",
      "Iteration: 0084 Loss: 0.80127 Time: 0.19141\n",
      "Iteration: 0085 Loss: 0.79614 Time: 0.19153\n",
      "Iteration: 0086 Loss: 0.79701 Time: 0.19284\n",
      "Iteration: 0087 Loss: 0.77876 Time: 0.19180\n",
      "Iteration: 0088 Loss: 0.77892 Time: 0.18978\n",
      "Iteration: 0089 Loss: 0.77270 Time: 0.18130\n",
      "Iteration: 0090 Loss: 0.76490 Time: 0.19053\n",
      "Iteration: 0091 Loss: 0.76563 Time: 0.18812\n",
      "Iteration: 0092 Loss: 0.77094 Time: 0.19127\n",
      "Iteration: 0093 Loss: 0.76104 Time: 0.18572\n",
      "Iteration: 0094 Loss: 0.75178 Time: 0.19129\n",
      "Iteration: 0095 Loss: 0.74231 Time: 0.19337\n",
      "Iteration: 0096 Loss: 0.74679 Time: 0.18753\n",
      "Iteration: 0097 Loss: 0.73623 Time: 0.19404\n",
      "Iteration: 0098 Loss: 0.74248 Time: 0.18851\n",
      "Iteration: 0099 Loss: 0.73977 Time: 0.19263\n",
      "Iteration: 0100 Loss: 0.72966 Time: 0.18465\n",
      "Iteration: 0101 Loss: 0.72772 Time: 0.19217\n",
      "Iteration: 0102 Loss: 0.73141 Time: 0.17850\n",
      "Iteration: 0103 Loss: 0.72100 Time: 0.18753\n",
      "Iteration: 0104 Loss: 0.71913 Time: 0.18961\n",
      "Iteration: 0105 Loss: 0.71123 Time: 0.18991\n",
      "Iteration: 0106 Loss: 0.70730 Time: 0.18150\n",
      "Iteration: 0107 Loss: 0.70749 Time: 0.19552\n",
      "Iteration: 0108 Loss: 0.69379 Time: 0.19299\n",
      "Iteration: 0109 Loss: 0.69907 Time: 0.19063\n",
      "Iteration: 0110 Loss: 0.69822 Time: 0.18780\n",
      "Iteration: 0111 Loss: 0.70471 Time: 0.19228\n",
      "Iteration: 0112 Loss: 0.69887 Time: 0.19073\n",
      "Iteration: 0113 Loss: 0.69667 Time: 0.18496\n",
      "Iteration: 0114 Loss: 0.69403 Time: 0.19260\n",
      "Iteration: 0115 Loss: 0.68883 Time: 0.18799\n",
      "Iteration: 0116 Loss: 0.68413 Time: 0.18592\n",
      "Iteration: 0117 Loss: 0.68686 Time: 0.19112\n",
      "Iteration: 0118 Loss: 0.67409 Time: 0.19100\n",
      "Iteration: 0119 Loss: 0.68013 Time: 0.18914\n",
      "Iteration: 0120 Loss: 0.68301 Time: 0.18975\n",
      "Iteration: 0121 Loss: 0.67332 Time: 0.18962\n",
      "Iteration: 0122 Loss: 0.66582 Time: 0.19066\n",
      "Iteration: 0123 Loss: 0.66640 Time: 0.18399\n",
      "Iteration: 0124 Loss: 0.66293 Time: 0.18433\n",
      "Iteration: 0125 Loss: 0.65669 Time: 0.18904\n",
      "Iteration: 0126 Loss: 0.65404 Time: 0.18894\n",
      "Iteration: 0127 Loss: 0.65665 Time: 0.18890\n",
      "Iteration: 0128 Loss: 0.65622 Time: 0.18761\n",
      "Iteration: 0129 Loss: 0.65765 Time: 0.18922\n",
      "Iteration: 0130 Loss: 0.66152 Time: 0.18760\n",
      "Iteration: 0131 Loss: 0.65201 Time: 0.19035\n",
      "Iteration: 0132 Loss: 0.64812 Time: 0.19148\n",
      "Iteration: 0133 Loss: 0.64805 Time: 0.19350\n",
      "Iteration: 0134 Loss: 0.64630 Time: 0.18432\n",
      "Iteration: 0135 Loss: 0.64393 Time: 0.18529\n",
      "Iteration: 0136 Loss: 0.64377 Time: 0.18874\n",
      "Iteration: 0137 Loss: 0.64438 Time: 0.19160\n",
      "Iteration: 0138 Loss: 0.63945 Time: 0.18824\n",
      "Iteration: 0139 Loss: 0.63524 Time: 0.19026\n",
      "Iteration: 0140 Loss: 0.63615 Time: 0.18684\n",
      "Iteration: 0141 Loss: 0.63584 Time: 0.19190\n",
      "Iteration: 0142 Loss: 0.63441 Time: 0.19320\n",
      "Iteration: 0143 Loss: 0.63216 Time: 0.18813\n",
      "Iteration: 0144 Loss: 0.63164 Time: 0.18646\n",
      "Iteration: 0145 Loss: 0.62985 Time: 0.18559\n",
      "Iteration: 0146 Loss: 0.62321 Time: 0.18555\n",
      "Iteration: 0147 Loss: 0.61997 Time: 0.18296\n",
      "Iteration: 0148 Loss: 0.62793 Time: 0.18868\n",
      "Iteration: 0149 Loss: 0.62142 Time: 0.19103\n",
      "Iteration: 0150 Loss: 0.62295 Time: 0.18789\n",
      "Iteration: 0151 Loss: 0.62092 Time: 0.19607\n",
      "Iteration: 0152 Loss: 0.62101 Time: 0.18392\n",
      "Iteration: 0153 Loss: 0.62077 Time: 0.18920\n",
      "Iteration: 0154 Loss: 0.61877 Time: 0.19029\n",
      "Iteration: 0155 Loss: 0.61304 Time: 0.18349\n",
      "Iteration: 0156 Loss: 0.61451 Time: 0.18884\n",
      "Iteration: 0157 Loss: 0.61453 Time: 0.18950\n",
      "Iteration: 0158 Loss: 0.60941 Time: 0.18767\n",
      "Iteration: 0159 Loss: 0.61049 Time: 0.18590\n",
      "Iteration: 0160 Loss: 0.60913 Time: 0.19343\n",
      "Iteration: 0161 Loss: 0.60312 Time: 0.18527\n",
      "Iteration: 0162 Loss: 0.60212 Time: 0.18896\n",
      "Iteration: 0163 Loss: 0.60524 Time: 0.19358\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0164 Loss: 0.60350 Time: 0.18932\n",
      "Iteration: 0165 Loss: 0.59925 Time: 0.18549\n",
      "Iteration: 0166 Loss: 0.60028 Time: 0.18565\n",
      "Iteration: 0167 Loss: 0.59855 Time: 0.19528\n",
      "Iteration: 0168 Loss: 0.60165 Time: 0.18450\n",
      "Iteration: 0169 Loss: 0.59809 Time: 0.19517\n",
      "Iteration: 0170 Loss: 0.59526 Time: 0.19198\n",
      "Iteration: 0171 Loss: 0.59001 Time: 0.18574\n",
      "Iteration: 0172 Loss: 0.60045 Time: 0.18481\n",
      "Iteration: 0173 Loss: 0.59566 Time: 0.18718\n",
      "Iteration: 0174 Loss: 0.58854 Time: 0.18943\n",
      "Iteration: 0175 Loss: 0.59092 Time: 0.19119\n",
      "Iteration: 0176 Loss: 0.58973 Time: 0.18806\n",
      "Iteration: 0177 Loss: 0.58885 Time: 0.18654\n",
      "Iteration: 0178 Loss: 0.58367 Time: 0.18281\n",
      "Iteration: 0179 Loss: 0.58823 Time: 0.18991\n",
      "Iteration: 0180 Loss: 0.58318 Time: 0.18314\n",
      "Iteration: 0181 Loss: 0.58229 Time: 0.19282\n",
      "Iteration: 0182 Loss: 0.58297 Time: 0.18706\n",
      "Iteration: 0183 Loss: 0.58507 Time: 0.19222\n",
      "Iteration: 0184 Loss: 0.58034 Time: 0.18465\n",
      "Iteration: 0185 Loss: 0.58226 Time: 0.18825\n",
      "Iteration: 0186 Loss: 0.57946 Time: 0.19053\n",
      "Iteration: 0187 Loss: 0.57661 Time: 0.18170\n",
      "Iteration: 0188 Loss: 0.58031 Time: 0.18837\n",
      "Iteration: 0189 Loss: 0.57662 Time: 0.19000\n",
      "Iteration: 0190 Loss: 0.57667 Time: 0.18700\n",
      "Iteration: 0191 Loss: 0.57311 Time: 0.18894\n",
      "Iteration: 0192 Loss: 0.57176 Time: 0.18624\n",
      "Iteration: 0193 Loss: 0.57444 Time: 0.18922\n",
      "Iteration: 0194 Loss: 0.57107 Time: 0.18801\n",
      "Iteration: 0195 Loss: 0.57005 Time: 0.18927\n",
      "Iteration: 0196 Loss: 0.56744 Time: 0.18581\n",
      "Iteration: 0197 Loss: 0.56738 Time: 0.19145\n",
      "Iteration: 0198 Loss: 0.57216 Time: 0.18267\n",
      "Iteration: 0199 Loss: 0.56379 Time: 0.19171\n",
      "Iteration: 0200 Loss: 0.57010 Time: 0.18801\n",
      "Done! \n",
      "\n",
      "STEP 3/3 - MODEL EVALUATION \n",
      "\n",
      "Computing the final embedding vectors, for evaluation\n",
      "Done! \n",
      "\n",
      "Testing: link prediction\n",
      "Done! \n",
      "\n",
      "EXPERIMENTS ON MODEL 9 / 10 \n",
      "\n",
      "STEP 1/3 - PREPROCESSING STEPS \n",
      "\n",
      "Masking some edges from the training graph, for link prediction\n",
      "(validation set: 1.0 % of edges - test set: 10.0 % of edges)\n",
      "Done! \n",
      "\n",
      "Preprocessing node features\n",
      "Done! \n",
      "\n",
      "Running the Louvain algorithm for community detection\n",
      "as a preprocessing step for the encoder\n",
      "Done! Louvain has found 87 communities \n",
      "\n",
      "Setting up the model and the optimizer\n",
      "Done! \n",
      "\n",
      "Preprocessing on message passing matrices\n",
      "Done! \n",
      "\n",
      "Initializing TF session\n",
      "Done! \n",
      "\n",
      "STEP 2/3 - MODEL TRAINING \n",
      "\n",
      "Starting training\n",
      "Iteration: 0001 Loss: 1.73695 Time: 0.44214\n",
      "Iteration: 0002 Loss: 1.69385 Time: 0.19240\n",
      "Iteration: 0003 Loss: 1.67692 Time: 0.19317\n",
      "Iteration: 0004 Loss: 1.66160 Time: 0.19638\n",
      "Iteration: 0005 Loss: 1.64038 Time: 0.18741\n",
      "Iteration: 0006 Loss: 1.58315 Time: 0.19454\n",
      "Iteration: 0007 Loss: 1.57845 Time: 0.18331\n",
      "Iteration: 0008 Loss: 1.54438 Time: 0.19766\n",
      "Iteration: 0009 Loss: 1.52291 Time: 0.19412\n",
      "Iteration: 0010 Loss: 1.51042 Time: 0.19182\n",
      "Iteration: 0011 Loss: 1.47297 Time: 0.18900\n",
      "Iteration: 0012 Loss: 1.45658 Time: 0.18925\n",
      "Iteration: 0013 Loss: 1.47082 Time: 0.19369\n",
      "Iteration: 0014 Loss: 1.43542 Time: 0.18351\n",
      "Iteration: 0015 Loss: 1.41762 Time: 0.19119\n",
      "Iteration: 0016 Loss: 1.39203 Time: 0.19768\n",
      "Iteration: 0017 Loss: 1.36478 Time: 0.19125\n",
      "Iteration: 0018 Loss: 1.37502 Time: 0.18625\n",
      "Iteration: 0019 Loss: 1.36022 Time: 0.19028\n",
      "Iteration: 0020 Loss: 1.34623 Time: 0.19088\n",
      "Iteration: 0021 Loss: 1.32611 Time: 0.19112\n",
      "Iteration: 0022 Loss: 1.31547 Time: 0.19450\n",
      "Iteration: 0023 Loss: 1.28446 Time: 0.18661\n",
      "Iteration: 0024 Loss: 1.28016 Time: 0.19500\n",
      "Iteration: 0025 Loss: 1.25655 Time: 0.18908\n",
      "Iteration: 0026 Loss: 1.26950 Time: 0.18860\n",
      "Iteration: 0027 Loss: 1.24634 Time: 0.18505\n",
      "Iteration: 0028 Loss: 1.22278 Time: 0.19253\n",
      "Iteration: 0029 Loss: 1.22079 Time: 0.18584\n",
      "Iteration: 0030 Loss: 1.20721 Time: 0.19430\n",
      "Iteration: 0031 Loss: 1.20265 Time: 0.19402\n",
      "Iteration: 0032 Loss: 1.19471 Time: 0.19572\n",
      "Iteration: 0033 Loss: 1.16238 Time: 0.18675\n",
      "Iteration: 0034 Loss: 1.16997 Time: 0.19492\n",
      "Iteration: 0035 Loss: 1.13921 Time: 0.18747\n",
      "Iteration: 0036 Loss: 1.15317 Time: 0.19321\n",
      "Iteration: 0037 Loss: 1.14091 Time: 0.18773\n",
      "Iteration: 0038 Loss: 1.11803 Time: 0.18648\n",
      "Iteration: 0039 Loss: 1.11233 Time: 0.18720\n",
      "Iteration: 0040 Loss: 1.10620 Time: 0.19018\n",
      "Iteration: 0041 Loss: 1.08749 Time: 0.19017\n",
      "Iteration: 0042 Loss: 1.09098 Time: 0.18931\n",
      "Iteration: 0043 Loss: 1.07003 Time: 0.19198\n",
      "Iteration: 0044 Loss: 1.06879 Time: 0.18857\n",
      "Iteration: 0045 Loss: 1.04431 Time: 0.18681\n",
      "Iteration: 0046 Loss: 1.04564 Time: 0.19272\n",
      "Iteration: 0047 Loss: 1.03227 Time: 0.18968\n",
      "Iteration: 0048 Loss: 1.02362 Time: 0.19424\n",
      "Iteration: 0049 Loss: 1.02157 Time: 0.18755\n",
      "Iteration: 0050 Loss: 1.01150 Time: 0.18698\n",
      "Iteration: 0051 Loss: 0.99679 Time: 0.19085\n",
      "Iteration: 0052 Loss: 0.99185 Time: 0.19634\n",
      "Iteration: 0053 Loss: 0.99467 Time: 0.19137\n",
      "Iteration: 0054 Loss: 0.98142 Time: 0.19529\n",
      "Iteration: 0055 Loss: 0.97339 Time: 0.19119\n",
      "Iteration: 0056 Loss: 0.97686 Time: 0.19167\n",
      "Iteration: 0057 Loss: 0.96146 Time: 0.19563\n",
      "Iteration: 0058 Loss: 0.95350 Time: 0.19721\n",
      "Iteration: 0059 Loss: 0.94592 Time: 0.18810\n",
      "Iteration: 0060 Loss: 0.92698 Time: 0.18463\n",
      "Iteration: 0061 Loss: 0.93598 Time: 0.19031\n",
      "Iteration: 0062 Loss: 0.91399 Time: 0.19094\n",
      "Iteration: 0063 Loss: 0.91763 Time: 0.18601\n",
      "Iteration: 0064 Loss: 0.90904 Time: 0.19563\n",
      "Iteration: 0065 Loss: 0.90473 Time: 0.18830\n",
      "Iteration: 0066 Loss: 0.89254 Time: 0.19295\n",
      "Iteration: 0067 Loss: 0.88538 Time: 0.18915\n",
      "Iteration: 0068 Loss: 0.88600 Time: 0.19079\n",
      "Iteration: 0069 Loss: 0.87499 Time: 0.19521\n",
      "Iteration: 0070 Loss: 0.86914 Time: 0.18777\n",
      "Iteration: 0071 Loss: 0.86348 Time: 0.19067\n",
      "Iteration: 0072 Loss: 0.85420 Time: 0.18744\n",
      "Iteration: 0073 Loss: 0.85341 Time: 0.19115\n",
      "Iteration: 0074 Loss: 0.84478 Time: 0.18672\n",
      "Iteration: 0075 Loss: 0.83265 Time: 0.19205\n",
      "Iteration: 0076 Loss: 0.84209 Time: 0.18518\n",
      "Iteration: 0077 Loss: 0.82599 Time: 0.18936\n",
      "Iteration: 0078 Loss: 0.82083 Time: 0.18401\n",
      "Iteration: 0079 Loss: 0.81780 Time: 0.18706\n",
      "Iteration: 0080 Loss: 0.80775 Time: 0.19296\n",
      "Iteration: 0081 Loss: 0.80648 Time: 0.19046\n",
      "Iteration: 0082 Loss: 0.79986 Time: 0.18100\n",
      "Iteration: 0083 Loss: 0.79673 Time: 0.19483\n",
      "Iteration: 0084 Loss: 0.79379 Time: 0.18930\n",
      "Iteration: 0085 Loss: 0.78508 Time: 0.18217\n",
      "Iteration: 0086 Loss: 0.77895 Time: 0.19980\n",
      "Iteration: 0087 Loss: 0.77952 Time: 0.18094\n",
      "Iteration: 0088 Loss: 0.77576 Time: 0.19842\n",
      "Iteration: 0089 Loss: 0.76904 Time: 0.19524\n",
      "Iteration: 0090 Loss: 0.76748 Time: 0.18900\n",
      "Iteration: 0091 Loss: 0.75761 Time: 0.18990\n",
      "Iteration: 0092 Loss: 0.75399 Time: 0.18542\n",
      "Iteration: 0093 Loss: 0.74865 Time: 0.19100\n",
      "Iteration: 0094 Loss: 0.74201 Time: 0.19613\n",
      "Iteration: 0095 Loss: 0.74682 Time: 0.19781\n",
      "Iteration: 0096 Loss: 0.73684 Time: 0.19872\n",
      "Iteration: 0097 Loss: 0.73467 Time: 0.20436\n",
      "Iteration: 0098 Loss: 0.73089 Time: 0.19062\n",
      "Iteration: 0099 Loss: 0.73039 Time: 0.18597\n",
      "Iteration: 0100 Loss: 0.72570 Time: 0.19464\n",
      "Iteration: 0101 Loss: 0.72082 Time: 0.18569\n",
      "Iteration: 0102 Loss: 0.71873 Time: 0.18699\n",
      "Iteration: 0103 Loss: 0.72239 Time: 0.19311\n",
      "Iteration: 0104 Loss: 0.70769 Time: 0.19228\n",
      "Iteration: 0105 Loss: 0.70863 Time: 0.18620\n",
      "Iteration: 0106 Loss: 0.70385 Time: 0.19273\n",
      "Iteration: 0107 Loss: 0.70573 Time: 0.19122\n",
      "Iteration: 0108 Loss: 0.69922 Time: 0.19880\n",
      "Iteration: 0109 Loss: 0.69639 Time: 0.18885\n",
      "Iteration: 0110 Loss: 0.69372 Time: 0.19331\n",
      "Iteration: 0111 Loss: 0.69119 Time: 0.18778\n",
      "Iteration: 0112 Loss: 0.68688 Time: 0.18481\n",
      "Iteration: 0113 Loss: 0.68586 Time: 0.18623\n",
      "Iteration: 0114 Loss: 0.68587 Time: 0.19663\n",
      "Iteration: 0115 Loss: 0.68077 Time: 0.19278\n",
      "Iteration: 0116 Loss: 0.68324 Time: 0.18952\n",
      "Iteration: 0117 Loss: 0.67717 Time: 0.19062\n",
      "Iteration: 0118 Loss: 0.67433 Time: 0.19796\n",
      "Iteration: 0119 Loss: 0.67269 Time: 0.19569\n",
      "Iteration: 0120 Loss: 0.67146 Time: 0.19323\n",
      "Iteration: 0121 Loss: 0.66319 Time: 0.19168\n",
      "Iteration: 0122 Loss: 0.67226 Time: 0.18656\n",
      "Iteration: 0123 Loss: 0.66325 Time: 0.18632\n",
      "Iteration: 0124 Loss: 0.66194 Time: 0.19484\n",
      "Iteration: 0125 Loss: 0.66786 Time: 0.19588\n",
      "Iteration: 0126 Loss: 0.65854 Time: 0.18813\n",
      "Iteration: 0127 Loss: 0.65895 Time: 0.19600\n",
      "Iteration: 0128 Loss: 0.65786 Time: 0.19231\n",
      "Iteration: 0129 Loss: 0.65254 Time: 0.19399\n",
      "Iteration: 0130 Loss: 0.65556 Time: 0.19405\n",
      "Iteration: 0131 Loss: 0.64780 Time: 0.18869\n",
      "Iteration: 0132 Loss: 0.64706 Time: 0.18865\n",
      "Iteration: 0133 Loss: 0.64727 Time: 0.18702\n",
      "Iteration: 0134 Loss: 0.63875 Time: 0.18766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0135 Loss: 0.64140 Time: 0.19374\n",
      "Iteration: 0136 Loss: 0.64791 Time: 0.18784\n",
      "Iteration: 0137 Loss: 0.63747 Time: 0.19159\n",
      "Iteration: 0138 Loss: 0.63586 Time: 0.18835\n",
      "Iteration: 0139 Loss: 0.63145 Time: 0.18738\n",
      "Iteration: 0140 Loss: 0.63769 Time: 0.19275\n",
      "Iteration: 0141 Loss: 0.63405 Time: 0.19099\n",
      "Iteration: 0142 Loss: 0.63396 Time: 0.18476\n",
      "Iteration: 0143 Loss: 0.63072 Time: 0.18785\n",
      "Iteration: 0144 Loss: 0.62697 Time: 0.18657\n",
      "Iteration: 0145 Loss: 0.62955 Time: 0.18767\n",
      "Iteration: 0146 Loss: 0.62003 Time: 0.19131\n",
      "Iteration: 0147 Loss: 0.62581 Time: 0.18315\n",
      "Iteration: 0148 Loss: 0.62522 Time: 0.19491\n",
      "Iteration: 0149 Loss: 0.62180 Time: 0.19660\n",
      "Iteration: 0150 Loss: 0.62161 Time: 0.19212\n",
      "Iteration: 0151 Loss: 0.61935 Time: 0.18340\n",
      "Iteration: 0152 Loss: 0.62192 Time: 0.18863\n",
      "Iteration: 0153 Loss: 0.61633 Time: 0.18771\n",
      "Iteration: 0154 Loss: 0.61283 Time: 0.19291\n",
      "Iteration: 0155 Loss: 0.61668 Time: 0.18366\n",
      "Iteration: 0156 Loss: 0.61046 Time: 0.18299\n",
      "Iteration: 0157 Loss: 0.60899 Time: 0.19490\n",
      "Iteration: 0158 Loss: 0.60908 Time: 0.18635\n",
      "Iteration: 0159 Loss: 0.60523 Time: 0.18575\n",
      "Iteration: 0160 Loss: 0.60699 Time: 0.19332\n",
      "Iteration: 0161 Loss: 0.60859 Time: 0.19521\n",
      "Iteration: 0162 Loss: 0.60387 Time: 0.19582\n",
      "Iteration: 0163 Loss: 0.60247 Time: 0.19457\n",
      "Iteration: 0164 Loss: 0.60121 Time: 0.18838\n",
      "Iteration: 0165 Loss: 0.60062 Time: 0.18326\n",
      "Iteration: 0166 Loss: 0.59602 Time: 0.19422\n",
      "Iteration: 0167 Loss: 0.59765 Time: 0.19053\n",
      "Iteration: 0168 Loss: 0.59987 Time: 0.19279\n",
      "Iteration: 0169 Loss: 0.59454 Time: 0.18968\n",
      "Iteration: 0170 Loss: 0.59115 Time: 0.19098\n",
      "Iteration: 0171 Loss: 0.59694 Time: 0.19059\n",
      "Iteration: 0172 Loss: 0.59056 Time: 0.19158\n",
      "Iteration: 0173 Loss: 0.59388 Time: 0.18692\n",
      "Iteration: 0174 Loss: 0.58927 Time: 0.19135\n",
      "Iteration: 0175 Loss: 0.58989 Time: 0.19133\n",
      "Iteration: 0176 Loss: 0.58644 Time: 0.18945\n",
      "Iteration: 0177 Loss: 0.58684 Time: 0.19009\n",
      "Iteration: 0178 Loss: 0.58482 Time: 0.19403\n",
      "Iteration: 0179 Loss: 0.58574 Time: 0.19144\n",
      "Iteration: 0180 Loss: 0.58400 Time: 0.18736\n",
      "Iteration: 0181 Loss: 0.58530 Time: 0.19246\n",
      "Iteration: 0182 Loss: 0.58066 Time: 0.19201\n",
      "Iteration: 0183 Loss: 0.57845 Time: 0.19070\n",
      "Iteration: 0184 Loss: 0.57832 Time: 0.18698\n",
      "Iteration: 0185 Loss: 0.57970 Time: 0.19014\n",
      "Iteration: 0186 Loss: 0.57871 Time: 0.18903\n",
      "Iteration: 0187 Loss: 0.57793 Time: 0.18834\n",
      "Iteration: 0188 Loss: 0.57343 Time: 0.18619\n",
      "Iteration: 0189 Loss: 0.57707 Time: 0.19322\n",
      "Iteration: 0190 Loss: 0.57422 Time: 0.18851\n",
      "Iteration: 0191 Loss: 0.57521 Time: 0.19233\n",
      "Iteration: 0192 Loss: 0.57726 Time: 0.19417\n",
      "Iteration: 0193 Loss: 0.57293 Time: 0.18730\n",
      "Iteration: 0194 Loss: 0.57433 Time: 0.19077\n",
      "Iteration: 0195 Loss: 0.56900 Time: 0.19081\n",
      "Iteration: 0196 Loss: 0.57254 Time: 0.18779\n",
      "Iteration: 0197 Loss: 0.56796 Time: 0.18981\n",
      "Iteration: 0198 Loss: 0.56870 Time: 0.19304\n",
      "Iteration: 0199 Loss: 0.56552 Time: 0.19416\n",
      "Iteration: 0200 Loss: 0.56642 Time: 0.18902\n",
      "Done! \n",
      "\n",
      "STEP 3/3 - MODEL EVALUATION \n",
      "\n",
      "Computing the final embedding vectors, for evaluation\n",
      "Done! \n",
      "\n",
      "Testing: link prediction\n",
      "Done! \n",
      "\n",
      "EXPERIMENTS ON MODEL 10 / 10 \n",
      "\n",
      "STEP 1/3 - PREPROCESSING STEPS \n",
      "\n",
      "Masking some edges from the training graph, for link prediction\n",
      "(validation set: 1.0 % of edges - test set: 10.0 % of edges)\n",
      "Done! \n",
      "\n",
      "Preprocessing node features\n",
      "Done! \n",
      "\n",
      "Running the Louvain algorithm for community detection\n",
      "as a preprocessing step for the encoder\n",
      "Done! Louvain has found 84 communities \n",
      "\n",
      "Setting up the model and the optimizer\n",
      "Done! \n",
      "\n",
      "Preprocessing on message passing matrices\n",
      "Done! \n",
      "\n",
      "Initializing TF session\n",
      "Done! \n",
      "\n",
      "STEP 2/3 - MODEL TRAINING \n",
      "\n",
      "Starting training\n",
      "Iteration: 0001 Loss: 1.70772 Time: 0.46018\n",
      "Iteration: 0002 Loss: 1.69986 Time: 0.19799\n",
      "Iteration: 0003 Loss: 1.65476 Time: 0.19220\n",
      "Iteration: 0004 Loss: 1.63894 Time: 0.18945\n",
      "Iteration: 0005 Loss: 1.60828 Time: 0.18619\n",
      "Iteration: 0006 Loss: 1.59946 Time: 0.19500\n",
      "Iteration: 0007 Loss: 1.57571 Time: 0.19959\n",
      "Iteration: 0008 Loss: 1.57319 Time: 0.19877\n",
      "Iteration: 0009 Loss: 1.54738 Time: 0.19023\n",
      "Iteration: 0010 Loss: 1.53230 Time: 0.19430\n",
      "Iteration: 0011 Loss: 1.47486 Time: 0.18856\n",
      "Iteration: 0012 Loss: 1.46614 Time: 0.19778\n",
      "Iteration: 0013 Loss: 1.46625 Time: 0.19459\n",
      "Iteration: 0014 Loss: 1.45290 Time: 0.20127\n",
      "Iteration: 0015 Loss: 1.40741 Time: 0.18500\n",
      "Iteration: 0016 Loss: 1.41133 Time: 0.19586\n",
      "Iteration: 0017 Loss: 1.39764 Time: 0.18931\n",
      "Iteration: 0018 Loss: 1.36541 Time: 0.19300\n",
      "Iteration: 0019 Loss: 1.33994 Time: 0.19528\n",
      "Iteration: 0020 Loss: 1.34961 Time: 0.18879\n",
      "Iteration: 0021 Loss: 1.32443 Time: 0.19304\n",
      "Iteration: 0022 Loss: 1.31826 Time: 0.18953\n",
      "Iteration: 0023 Loss: 1.29091 Time: 0.19135\n",
      "Iteration: 0024 Loss: 1.28397 Time: 0.19115\n",
      "Iteration: 0025 Loss: 1.27178 Time: 0.19335\n",
      "Iteration: 0026 Loss: 1.23087 Time: 0.18565\n",
      "Iteration: 0027 Loss: 1.23060 Time: 0.18732\n",
      "Iteration: 0028 Loss: 1.23768 Time: 0.18948\n",
      "Iteration: 0029 Loss: 1.22713 Time: 0.18673\n",
      "Iteration: 0030 Loss: 1.20046 Time: 0.18971\n",
      "Iteration: 0031 Loss: 1.20372 Time: 0.19436\n",
      "Iteration: 0032 Loss: 1.18911 Time: 0.18950\n",
      "Iteration: 0033 Loss: 1.16845 Time: 0.19101\n",
      "Iteration: 0034 Loss: 1.15690 Time: 0.19347\n",
      "Iteration: 0035 Loss: 1.15126 Time: 0.19791\n",
      "Iteration: 0036 Loss: 1.15832 Time: 0.19275\n",
      "Iteration: 0037 Loss: 1.13099 Time: 0.19313\n",
      "Iteration: 0038 Loss: 1.12947 Time: 0.19537\n",
      "Iteration: 0039 Loss: 1.12189 Time: 0.19132\n",
      "Iteration: 0040 Loss: 1.11944 Time: 0.18720\n",
      "Iteration: 0041 Loss: 1.09574 Time: 0.18816\n",
      "Iteration: 0042 Loss: 1.09184 Time: 0.20327\n",
      "Iteration: 0043 Loss: 1.07784 Time: 0.20328\n",
      "Iteration: 0044 Loss: 1.05990 Time: 0.18716\n",
      "Iteration: 0045 Loss: 1.06054 Time: 0.19830\n",
      "Iteration: 0046 Loss: 1.05398 Time: 0.19136\n",
      "Iteration: 0047 Loss: 1.04301 Time: 0.18463\n",
      "Iteration: 0048 Loss: 1.03267 Time: 0.18839\n",
      "Iteration: 0049 Loss: 1.01537 Time: 0.19037\n",
      "Iteration: 0050 Loss: 1.02589 Time: 0.19067\n",
      "Iteration: 0051 Loss: 1.01669 Time: 0.18720\n",
      "Iteration: 0052 Loss: 0.99510 Time: 0.18674\n",
      "Iteration: 0053 Loss: 0.99395 Time: 0.18754\n",
      "Iteration: 0054 Loss: 0.98731 Time: 0.19488\n",
      "Iteration: 0055 Loss: 0.97050 Time: 0.19475\n",
      "Iteration: 0056 Loss: 0.97272 Time: 0.18804\n",
      "Iteration: 0057 Loss: 0.96980 Time: 0.18446\n",
      "Iteration: 0058 Loss: 0.95764 Time: 0.18111\n",
      "Iteration: 0059 Loss: 0.95659 Time: 0.18822\n",
      "Iteration: 0060 Loss: 0.93643 Time: 0.19591\n",
      "Iteration: 0061 Loss: 0.92977 Time: 0.18839\n",
      "Iteration: 0062 Loss: 0.92708 Time: 0.19261\n",
      "Iteration: 0063 Loss: 0.91795 Time: 0.18917\n",
      "Iteration: 0064 Loss: 0.91957 Time: 0.18568\n",
      "Iteration: 0065 Loss: 0.90071 Time: 0.19274\n",
      "Iteration: 0066 Loss: 0.90821 Time: 0.19391\n",
      "Iteration: 0067 Loss: 0.90087 Time: 0.19281\n",
      "Iteration: 0068 Loss: 0.88418 Time: 0.18686\n",
      "Iteration: 0069 Loss: 0.87595 Time: 0.18740\n",
      "Iteration: 0070 Loss: 0.87501 Time: 0.18885\n",
      "Iteration: 0071 Loss: 0.87361 Time: 0.19496\n",
      "Iteration: 0072 Loss: 0.86042 Time: 0.18148\n",
      "Iteration: 0073 Loss: 0.85826 Time: 0.19527\n",
      "Iteration: 0074 Loss: 0.85613 Time: 0.19099\n",
      "Iteration: 0075 Loss: 0.84792 Time: 0.18884\n",
      "Iteration: 0076 Loss: 0.85235 Time: 0.19257\n",
      "Iteration: 0077 Loss: 0.84015 Time: 0.18917\n",
      "Iteration: 0078 Loss: 0.82621 Time: 0.19064\n",
      "Iteration: 0079 Loss: 0.82258 Time: 0.18794\n",
      "Iteration: 0080 Loss: 0.81798 Time: 0.18906\n",
      "Iteration: 0081 Loss: 0.82135 Time: 0.18800\n",
      "Iteration: 0082 Loss: 0.80696 Time: 0.19226\n",
      "Iteration: 0083 Loss: 0.80104 Time: 0.18938\n",
      "Iteration: 0084 Loss: 0.80175 Time: 0.19141\n",
      "Iteration: 0085 Loss: 0.79597 Time: 0.18994\n",
      "Iteration: 0086 Loss: 0.79092 Time: 0.18743\n",
      "Iteration: 0087 Loss: 0.78135 Time: 0.19271\n",
      "Iteration: 0088 Loss: 0.78365 Time: 0.18736\n",
      "Iteration: 0089 Loss: 0.78370 Time: 0.18414\n",
      "Iteration: 0090 Loss: 0.77163 Time: 0.19473\n",
      "Iteration: 0091 Loss: 0.76008 Time: 0.19101\n",
      "Iteration: 0092 Loss: 0.76743 Time: 0.18837\n",
      "Iteration: 0093 Loss: 0.75745 Time: 0.19175\n",
      "Iteration: 0094 Loss: 0.75493 Time: 0.18710\n",
      "Iteration: 0095 Loss: 0.75433 Time: 0.19096\n",
      "Iteration: 0096 Loss: 0.74965 Time: 0.18439\n",
      "Iteration: 0097 Loss: 0.74710 Time: 0.19392\n",
      "Iteration: 0098 Loss: 0.74430 Time: 0.19216\n",
      "Iteration: 0099 Loss: 0.73552 Time: 0.18824\n",
      "Iteration: 0100 Loss: 0.72988 Time: 0.19115\n",
      "Iteration: 0101 Loss: 0.72637 Time: 0.18533\n",
      "Iteration: 0102 Loss: 0.72360 Time: 0.19300\n",
      "Iteration: 0103 Loss: 0.72348 Time: 0.19276\n",
      "Iteration: 0104 Loss: 0.72323 Time: 0.18853\n",
      "Iteration: 0105 Loss: 0.71880 Time: 0.19268\n",
      "Iteration: 0106 Loss: 0.71338 Time: 0.18733\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0107 Loss: 0.71392 Time: 0.19182\n",
      "Iteration: 0108 Loss: 0.70788 Time: 0.19293\n",
      "Iteration: 0109 Loss: 0.69806 Time: 0.19015\n",
      "Iteration: 0110 Loss: 0.70667 Time: 0.19052\n",
      "Iteration: 0111 Loss: 0.70393 Time: 0.18546\n",
      "Iteration: 0112 Loss: 0.70105 Time: 0.19000\n",
      "Iteration: 0113 Loss: 0.69146 Time: 0.18943\n",
      "Iteration: 0114 Loss: 0.69331 Time: 0.18859\n",
      "Iteration: 0115 Loss: 0.68935 Time: 0.18978\n",
      "Iteration: 0116 Loss: 0.68479 Time: 0.19009\n",
      "Iteration: 0117 Loss: 0.68080 Time: 0.18527\n",
      "Iteration: 0118 Loss: 0.67792 Time: 0.18799\n",
      "Iteration: 0119 Loss: 0.68095 Time: 0.19072\n",
      "Iteration: 0120 Loss: 0.67843 Time: 0.19393\n",
      "Iteration: 0121 Loss: 0.67618 Time: 0.18599\n",
      "Iteration: 0122 Loss: 0.67728 Time: 0.18886\n",
      "Iteration: 0123 Loss: 0.66405 Time: 0.18581\n",
      "Iteration: 0124 Loss: 0.67188 Time: 0.18953\n",
      "Iteration: 0125 Loss: 0.66269 Time: 0.18197\n",
      "Iteration: 0126 Loss: 0.66607 Time: 0.18570\n",
      "Iteration: 0127 Loss: 0.65915 Time: 0.20073\n",
      "Iteration: 0128 Loss: 0.65529 Time: 0.18994\n",
      "Iteration: 0129 Loss: 0.65532 Time: 0.20383\n",
      "Iteration: 0130 Loss: 0.65537 Time: 0.19109\n",
      "Iteration: 0131 Loss: 0.65358 Time: 0.18677\n",
      "Iteration: 0132 Loss: 0.65422 Time: 0.18543\n",
      "Iteration: 0133 Loss: 0.65070 Time: 0.19415\n",
      "Iteration: 0134 Loss: 0.64665 Time: 0.18800\n",
      "Iteration: 0135 Loss: 0.64675 Time: 0.19300\n",
      "Iteration: 0136 Loss: 0.64277 Time: 0.18399\n",
      "Iteration: 0137 Loss: 0.65137 Time: 0.18715\n",
      "Iteration: 0138 Loss: 0.64903 Time: 0.19361\n",
      "Iteration: 0139 Loss: 0.63877 Time: 0.19418\n",
      "Iteration: 0140 Loss: 0.63691 Time: 0.18702\n",
      "Iteration: 0141 Loss: 0.63056 Time: 0.18654\n",
      "Iteration: 0142 Loss: 0.63455 Time: 0.18872\n",
      "Iteration: 0143 Loss: 0.63192 Time: 0.19275\n",
      "Iteration: 0144 Loss: 0.62883 Time: 0.18529\n",
      "Iteration: 0145 Loss: 0.62895 Time: 0.19272\n",
      "Iteration: 0146 Loss: 0.62812 Time: 0.19111\n",
      "Iteration: 0147 Loss: 0.62587 Time: 0.19129\n",
      "Iteration: 0148 Loss: 0.62536 Time: 0.19537\n",
      "Iteration: 0149 Loss: 0.62128 Time: 0.18333\n",
      "Iteration: 0150 Loss: 0.61946 Time: 0.18873\n",
      "Iteration: 0151 Loss: 0.62225 Time: 0.19244\n",
      "Iteration: 0152 Loss: 0.61988 Time: 0.18192\n",
      "Iteration: 0153 Loss: 0.61565 Time: 0.18136\n",
      "Iteration: 0154 Loss: 0.61752 Time: 0.19301\n",
      "Iteration: 0155 Loss: 0.61557 Time: 0.18788\n",
      "Iteration: 0156 Loss: 0.60991 Time: 0.18894\n",
      "Iteration: 0157 Loss: 0.61132 Time: 0.19684\n",
      "Iteration: 0158 Loss: 0.60999 Time: 0.18689\n",
      "Iteration: 0159 Loss: 0.60894 Time: 0.19262\n",
      "Iteration: 0160 Loss: 0.60667 Time: 0.18885\n",
      "Iteration: 0161 Loss: 0.60935 Time: 0.19294\n",
      "Iteration: 0162 Loss: 0.60492 Time: 0.18957\n",
      "Iteration: 0163 Loss: 0.60757 Time: 0.18722\n",
      "Iteration: 0164 Loss: 0.59839 Time: 0.18791\n",
      "Iteration: 0165 Loss: 0.60217 Time: 0.18822\n",
      "Iteration: 0166 Loss: 0.60347 Time: 0.19251\n",
      "Iteration: 0167 Loss: 0.59994 Time: 0.18960\n",
      "Iteration: 0168 Loss: 0.60018 Time: 0.18699\n",
      "Iteration: 0169 Loss: 0.59720 Time: 0.19065\n",
      "Iteration: 0170 Loss: 0.59557 Time: 0.19076\n",
      "Iteration: 0171 Loss: 0.59408 Time: 0.18633\n",
      "Iteration: 0172 Loss: 0.59372 Time: 0.18762\n",
      "Iteration: 0173 Loss: 0.59054 Time: 0.18673\n",
      "Iteration: 0174 Loss: 0.59570 Time: 0.18575\n",
      "Iteration: 0175 Loss: 0.59256 Time: 0.19385\n",
      "Iteration: 0176 Loss: 0.59059 Time: 0.18647\n",
      "Iteration: 0177 Loss: 0.59063 Time: 0.19100\n",
      "Iteration: 0178 Loss: 0.58768 Time: 0.19300\n",
      "Iteration: 0179 Loss: 0.58585 Time: 0.18511\n",
      "Iteration: 0180 Loss: 0.58201 Time: 0.18737\n",
      "Iteration: 0181 Loss: 0.58408 Time: 0.19840\n",
      "Iteration: 0182 Loss: 0.58268 Time: 0.18565\n",
      "Iteration: 0183 Loss: 0.57924 Time: 0.19031\n",
      "Iteration: 0184 Loss: 0.58391 Time: 0.18283\n",
      "Iteration: 0185 Loss: 0.58406 Time: 0.18562\n",
      "Iteration: 0186 Loss: 0.57969 Time: 0.18900\n",
      "Iteration: 0187 Loss: 0.57908 Time: 0.18741\n",
      "Iteration: 0188 Loss: 0.57844 Time: 0.18698\n",
      "Iteration: 0189 Loss: 0.57742 Time: 0.19178\n",
      "Iteration: 0190 Loss: 0.57447 Time: 0.18790\n",
      "Iteration: 0191 Loss: 0.57453 Time: 0.19673\n",
      "Iteration: 0192 Loss: 0.57432 Time: 0.18952\n",
      "Iteration: 0193 Loss: 0.56922 Time: 0.19455\n",
      "Iteration: 0194 Loss: 0.57076 Time: 0.18739\n",
      "Iteration: 0195 Loss: 0.56897 Time: 0.18669\n",
      "Iteration: 0196 Loss: 0.57228 Time: 0.19108\n",
      "Iteration: 0197 Loss: 0.57092 Time: 0.19518\n",
      "Iteration: 0198 Loss: 0.56840 Time: 0.19246\n",
      "Iteration: 0199 Loss: 0.57072 Time: 0.18988\n",
      "Iteration: 0200 Loss: 0.56653 Time: 0.18979\n",
      "Done! \n",
      "\n",
      "STEP 3/3 - MODEL EVALUATION \n",
      "\n",
      "Computing the final embedding vectors, for evaluation\n",
      "Done! \n",
      "\n",
      "Testing: link prediction\n",
      "Done! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We repeat the entire training+test process FLAGS.nb_run times\n",
    "for i in range(FLAGS.nb_run):\n",
    "    if FLAGS.verbose:\n",
    "        print(\"EXPERIMENTS ON MODEL\", i + 1, \"/\", FLAGS.nb_run, \"\\n\")\n",
    "        print(\"STEP 1/3 - PREPROCESSING STEPS \\n\")\n",
    "\n",
    "\n",
    "    # Edge masking for Link Prediction:\n",
    "    if FLAGS.task == 'task_2':\n",
    "        # Compute Train/Validation/Test sets\n",
    "        if FLAGS.verbose:\n",
    "            print(\"Masking some edges from the training graph, for link prediction\")\n",
    "            print(\"(validation set:\", FLAGS.prop_val, \"% of edges - test set:\",\n",
    "                  FLAGS.prop_test, \"% of edges)\")\n",
    "        #adj, val_edges, val_edges_false, test_edges, test_edges_false = mask_test_edges(adj_init, FLAGS.prop_test, FLAGS.prop_val)\n",
    "        adj, test_edges, test_edges_false = mask_test_edges(adj_init, FLAGS.prop_test)\n",
    "        if FLAGS.verbose:\n",
    "            print(\"Done! \\n\")\n",
    "    else:\n",
    "        adj = adj_init\n",
    "\n",
    "    # Compute the number of nodes\n",
    "    num_nodes = adj.shape[0]\n",
    "\n",
    "    # Preprocessing on node features\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Preprocessing node features\")\n",
    "    if FLAGS.features:\n",
    "        features = features_init\n",
    "    else:\n",
    "        # If features are not used, replace feature matrix by identity matrix\n",
    "        features = sp.identity(num_nodes)\n",
    "    features = sparse_to_tuple(features)\n",
    "    num_features = features[2][1]\n",
    "    features_nonzero = features[1].shape[0]\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Done! \\n\")\n",
    "\n",
    "\n",
    "    # Community detection using Louvain, as a preprocessing step\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Running the Louvain algorithm for community detection\")\n",
    "        print(\"as a preprocessing step for the encoder\")\n",
    "    # Get binary community matrix (adj_louvain_init[i,j] = 1 if nodes i and j are\n",
    "    # in the same community) as well as number of communities found by Louvain\n",
    "    adj_louvain_init, nb_communities_louvain, partition = louvain_clustering(adj, FLAGS.s_reg)\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Done! Louvain has found\", nb_communities_louvain, \"communities \\n\")\n",
    "\n",
    "    # FastGAE: node sampling for stochastic subgraph decoding - used when facing scalability issues\n",
    "    if FLAGS.fastgae:\n",
    "        if FLAGS.verbose:\n",
    "            print(\"Preprocessing operations related to the FastGAE method\")\n",
    "        # Node-level p_i degree-based, core-based or uniform distribution\n",
    "        node_distribution = get_distribution(FLAGS.measure, FLAGS.alpha, adj)\n",
    "        # Node sampling for initializations\n",
    "        sampled_nodes, adj_label, adj_sampled_sparse = node_sampling(adj, node_distribution, FLAGS.nb_node_samples, FLAGS.replace)\n",
    "        if FLAGS.verbose:\n",
    "            print(\"Done! \\n\")\n",
    "    else:\n",
    "        sampled_nodes = np.array(range(FLAGS.nb_node_samples))\n",
    "\n",
    "\n",
    "    # Placeholders\n",
    "    if FLAGS.verbose:\n",
    "        print('Setting up the model and the optimizer')\n",
    "    placeholders = {\n",
    "        'features': tf.sparse_placeholder(tf.float32),\n",
    "        'adj': tf.sparse_placeholder(tf.float32),\n",
    "        'adj_layer2': tf.sparse_placeholder(tf.float32), # Only used for 2-layer GCN encoders\n",
    "        'degree_matrix': tf.sparse_placeholder(tf.float32),\n",
    "        'adj_orig': tf.sparse_placeholder(tf.float32),\n",
    "        'dropout': tf.placeholder_with_default(0., shape = ()),\n",
    "        'sampled_nodes': tf.placeholder_with_default(sampled_nodes, shape = [FLAGS.nb_node_samples])\n",
    "    }\n",
    "\n",
    "\n",
    "    # Create model\n",
    "    if FLAGS.model == 'linear_ae':\n",
    "        # Linear Graph Autoencoder\n",
    "        model = LinearModelAE(placeholders, num_features, features_nonzero)\n",
    "    elif FLAGS.model == 'linear_vae':\n",
    "        # Linear Graph Variational Autoencoder\n",
    "        model = LinearModelVAE(placeholders, num_features, num_nodes, features_nonzero)\n",
    "    elif FLAGS.model == 'gcn_ae':\n",
    "        # 2-layer GCN Graph Autoencoder\n",
    "        model = GCNModelAE(placeholders, num_features, features_nonzero)\n",
    "    elif FLAGS.model == 'gcn_vae':\n",
    "        # 2-layer GCN Graph Variational Autoencoder\n",
    "        model = GCNModelVAE(placeholders, num_features, num_nodes, features_nonzero)\n",
    "    else:\n",
    "        raise ValueError('Undefined model!')\n",
    "\n",
    "\n",
    "    # Optimizer\n",
    "    if FLAGS.fastgae:\n",
    "        num_sampled = adj_sampled_sparse.shape[0]\n",
    "        sum_sampled = adj_sampled_sparse.sum()\n",
    "        pos_weight = float(num_sampled * num_sampled - sum_sampled) / sum_sampled\n",
    "        norm = num_sampled * num_sampled / float((num_sampled * num_sampled - sum_sampled) * 2)\n",
    "    else:\n",
    "        pos_weight = float(num_nodes * num_nodes - adj.sum()) / adj.sum()\n",
    "        norm = num_nodes * num_nodes / float((num_nodes * num_nodes - adj.sum()) * 2)\n",
    "\n",
    "    if FLAGS.model in ('gcn_ae', 'linear_ae'):\n",
    "        opt = OptimizerAE(preds = model.reconstructions,\n",
    "                          labels = tf.reshape(tf.sparse_tensor_to_dense(placeholders['adj_orig'], validate_indices = False), [-1]),\n",
    "                          degree_matrix = tf.reshape(tf.sparse_tensor_to_dense(placeholders['degree_matrix'], validate_indices = False), [-1]),\n",
    "                          num_nodes = num_nodes,\n",
    "                          pos_weight = pos_weight,\n",
    "                          norm = norm,\n",
    "                          clusters_distance = model.clusters)\n",
    "    else:\n",
    "        opt = OptimizerVAE(preds = model.reconstructions,\n",
    "                           labels = tf.reshape(tf.sparse_tensor_to_dense(placeholders['adj_orig'], validate_indices = False), [-1]),\n",
    "                           degree_matrix = tf.reshape(tf.sparse_tensor_to_dense(placeholders['degree_matrix'], validate_indices = False), [-1]),\n",
    "                           model = model,\n",
    "                           num_nodes = num_nodes,\n",
    "                           pos_weight = pos_weight,\n",
    "                           norm = norm,\n",
    "                           clusters_distance = model.clusters)\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Done! \\n\")\n",
    "\n",
    "\n",
    "    # Symmetrically normalized \"message passing\" matrices\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Preprocessing on message passing matrices\")\n",
    "    adj_norm = preprocess_graph(adj + FLAGS.lamb*adj_louvain_init)\n",
    "    adj_norm_layer2 = preprocess_graph(adj)\n",
    "    if not FLAGS.fastgae:\n",
    "        adj_label = sparse_to_tuple(adj + sp.eye(num_nodes))\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Done! \\n\")\n",
    "\n",
    "\n",
    "    # Degree matrices\n",
    "    deg_matrix, deg_matrix_init = preprocess_degree(adj, FLAGS.simple)\n",
    "\n",
    "\n",
    "    # Initialize TF session\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Initializing TF session\")\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Done! \\n\")\n",
    "\n",
    "\n",
    "    # Model training\n",
    "    if FLAGS.verbose:\n",
    "        print(\"STEP 2/3 - MODEL TRAINING \\n\")\n",
    "        print(\"Starting training\")\n",
    "\n",
    "    for iter in range(FLAGS.iterations):\n",
    "\n",
    "        # Flag to compute running time for each iteration\n",
    "        t = time.time()\n",
    "\n",
    "        # Construct feed dictionary\n",
    "        feed_dict = construct_feed_dict(adj_norm, adj_norm_layer2, adj_label, features, deg_matrix, placeholders)\n",
    "        if FLAGS.fastgae:\n",
    "            # Update sampled subgraph and sampled Louvain matrix\n",
    "            feed_dict.update({placeholders['sampled_nodes']: sampled_nodes})\n",
    "            # New node sampling\n",
    "            sampled_nodes, adj_label, adj_label_sparse, = node_sampling(adj, node_distribution, FLAGS.nb_node_samples, FLAGS.replace)\n",
    "\n",
    "        # Weights update\n",
    "        outs = sess.run([opt.opt_op, opt.cost, opt.cost_adj, opt.cost_mod],\n",
    "                        feed_dict = feed_dict)\n",
    "\n",
    "        # Compute average loss\n",
    "        avg_cost = outs[1]\n",
    "        if FLAGS.verbose:\n",
    "            # Display information on the iteration\n",
    "            print(\"Iteration:\", '%04d' % (iter + 1), \"Loss:\", \"{:.5f}\".format(avg_cost),\n",
    "                  \"Time:\", \"{:.5f}\".format(time.time() - t))\n",
    "\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Done! \\n\")\n",
    "\n",
    "    # Compute embedding vectors, for evaluation\n",
    "    if FLAGS.verbose:\n",
    "        print(\"STEP 3/3 - MODEL EVALUATION \\n\")\n",
    "        print(\"Computing the final embedding vectors, for evaluation\")\n",
    "    emb = sess.run(model.z_mean, feed_dict = feed_dict)\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Done! \\n\")\n",
    "\n",
    "    # Test model: link prediction (classification edges/non-edges)\n",
    "\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Testing: link prediction\")\n",
    "    # Get ROC and AP scores\n",
    "    roc_score, ap_score = link_prediction(test_edges, test_edges_false, emb)\n",
    "    mean_roc.append(roc_score)\n",
    "    mean_ap.append(ap_score)\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Done! \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bef29d76",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-28T09:06:07.930070Z",
     "start_time": "2022-10-28T09:06:07.916170Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL RESULTS \n",
      "\n",
      "Recall: the selected task was \"Task 2\", i.e., joint community detection and link prediction, on wiki\n",
      "All scores reported below are computed over the 10 run(s) \n",
      "\n",
      "Link prediction:\n",
      "\n",
      "Mean AUC score:  0.9111491192088623\n",
      "Std of AUC scores:  0.005777872555465969 \n",
      "\n",
      "Mean AP score:  0.9344324138944022\n",
      "Std of AP scores:  0.0036641922082108836 \n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Report final results\n",
    "print(\"FINAL RESULTS \\n\")\n",
    "\n",
    "print('Recall: the selected task was \"Task 2\", i.e., joint community detection and link prediction, on', FLAGS.dataset)\n",
    "\n",
    "print(\"All scores reported below are computed over the\", FLAGS.nb_run, \"run(s) \\n\")\n",
    "\n",
    "print(\"Link prediction:\\n\")\n",
    "print(\"Mean AUC score: \", np.mean(mean_roc))\n",
    "print(\"Std of AUC scores: \", np.std(mean_roc), \"\\n\")\n",
    "\n",
    "print(\"Mean AP score: \", np.mean(mean_ap))\n",
    "print(\"Std of AP scores: \", np.std(mean_ap), \"\\n \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553e6eb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf1.15",
   "language": "python",
   "name": "tf1.15"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
