{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67dbcca0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-28T09:15:01.440054Z",
     "start_time": "2022-10-28T09:14:59.340064Z"
    }
   },
   "outputs": [],
   "source": [
    "from modularity_aware_gae.evaluation import community_detection, link_prediction\n",
    "from modularity_aware_gae.input_data import load_data, load_labels\n",
    "from modularity_aware_gae.louvain import louvain_clustering\n",
    "from modularity_aware_gae.model import  GCNModelAE, GCNModelVAE, LinearModelAE, LinearModelVAE\n",
    "from modularity_aware_gae.optimizer import OptimizerAE, OptimizerVAE\n",
    "from modularity_aware_gae.preprocessing import *\n",
    "from modularity_aware_gae.sampling import get_distribution, node_sampling\n",
    "import numpy as np\n",
    "import os\n",
    "import scipy.sparse as sp\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel')#添加的，不报错\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14ffd620",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-28T09:15:01.504027Z",
     "start_time": "2022-10-28T09:15:01.475148Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " \n",
      " \n",
      "[MODULARITY-AWARE GRAPH AUTOENCODERS]\n",
      " \n",
      " \n",
      " \n",
      "\n",
      "EXPERIMENTAL SETTING \n",
      "\n",
      "- Graph dataset: texas\n",
      "- Mode name: linear_vae\n",
      "- Number of models to train: 10\n",
      "- Number of training iterations for each model: 200\n",
      "- Learning rate: 0.01\n",
      "- Dropout rate: 0.0\n",
      "- Use of node features in the input layer: False\n",
      "- Dimension of the output layer: 16\n",
      "- lambda: 0.0\n",
      "- beta: 0.0\n",
      "- gamma: 1.0\n",
      "- s: 2\n",
      "- FastGAE: no \n",
      "\n",
      "Final embedding vectors will be evaluated on:\n",
      "- Task 2, i.e., joint community detection and link prediction\n",
      "\n",
      " \n",
      " \n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "# Parameters related to the input data\n",
    "flags.DEFINE_string('dataset', 'texas', 'Graph dataset, among: cora, citeseer, wiki, celegans, email, polbooks, texas, wisconsin')\n",
    "\n",
    "flags.DEFINE_boolean('features', False, 'Whether to include node features')\n",
    "\n",
    "\n",
    "# Parameters related to the Modularity-Aware GAE/VGAE model to train\n",
    "\n",
    "# 1/3 - General parameters associated with GAE/VGAE\n",
    "flags.DEFINE_string('model', 'linear_vae', 'Model to train, among: gcn_ae, gcn_vae, \\\n",
    "                                            linear_ae, linear_vae')\n",
    "flags.DEFINE_float('dropout', 0., 'Dropout rate')\n",
    "flags.DEFINE_integer('iterations', 200, 'Number of iterations in training')\n",
    "flags.DEFINE_float('learning_rate', 0.01, 'Initial learning rate (Adam)')\n",
    "flags.DEFINE_integer('hidden', 32, 'Dimension of the GCN hidden layer')\n",
    "flags.DEFINE_integer('dimension', 16, 'Dimension of the output layer, i.e., \\\n",
    "                                       dimension of the embedding space')\n",
    "\n",
    "# 2/3 - Additional parameters, specific to Modularity-Aware models\n",
    "flags.DEFINE_float('beta', 0.0, 'Beta hyperparameter of Mod.-Aware models')\n",
    "flags.DEFINE_float('lamb', 0.0, 'Lambda hyperparameter of Mod.-Aware models')\n",
    "flags.DEFINE_float('gamma', 1.0, 'Gamma hyperparameter of Mod.-Aware models')\n",
    "flags.DEFINE_integer('s_reg', 2, 's hyperparameter of Mod.-Aware models')\n",
    "\n",
    "# 3/3 - Additional parameters, aiming to improve scalability\n",
    "flags.DEFINE_boolean('fastgae', False, 'Whether to use the FastGAE framework')\n",
    "flags.DEFINE_integer('nb_node_samples', 1000, 'In FastGAE, number of nodes to \\\n",
    "                                               sample at each iteration, i.e., \\\n",
    "                                               size of decoded subgraph')\n",
    "flags.DEFINE_string('measure', 'degree', 'In FastGAE, node importance measure used \\\n",
    "                                          for sampling: degree, core, or uniform')\n",
    "flags.DEFINE_float('alpha', 1.0, 'alpha hyperparameter associated with the FastGAE sampling')\n",
    "flags.DEFINE_boolean('replace', False, 'Sample nodes with or without replacement')\n",
    "flags.DEFINE_boolean('simple', True, 'Use simpler (and faster) modularity in optimizers')\n",
    "\n",
    "\n",
    "# Parameters related to the experimental setup\n",
    "flags.DEFINE_string('task', 'task_2', 'task_1: pure community detection \\\n",
    "                                       task_2: joint link prediction and \\\n",
    "                                               community detection')\n",
    "flags.DEFINE_integer('nb_run', 10, 'Number of model run + test')\n",
    "flags.DEFINE_float('prop_val', 1., 'Proportion of edges in validation set \\\n",
    "                                    for the link prediction task')\n",
    "flags.DEFINE_float('prop_test', 10., 'Proportion of edges in test set \\\n",
    "                                      for the link prediction task')\n",
    "flags.DEFINE_boolean('validation', False, 'Whether to compute validation \\\n",
    "                                           results at each iteration, for \\\n",
    "                                           the link prediction task')\n",
    "flags.DEFINE_boolean('verbose', True, 'Whether to print all comments details')\n",
    "\n",
    "\n",
    "# Introductory message\n",
    "if FLAGS.verbose:\n",
    "    introductory_message()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ce16ae4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-28T09:15:01.552094Z",
     "start_time": "2022-10-28T09:15:01.537959Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADING DATA\n",
      "\n",
      "Loading the texas graph\n",
      "- Number of nodes: 183\n",
      "- Use of node features: False\n",
      "Done! \n",
      " \n",
      " \n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize lists to collect final results\n",
    "mean_ami = []\n",
    "mean_ari = []\n",
    "\n",
    "mean_roc = []\n",
    "mean_ap = []\n",
    "\n",
    "# Load data\n",
    "if FLAGS.verbose:\n",
    "    print(\"LOADING DATA\\n\")\n",
    "    print(\"Loading the\", FLAGS.dataset, \"graph\")\n",
    "adj_init, features_init = load_data(FLAGS.dataset)\n",
    "#labels = load_labels(adj_init.shape[0])\n",
    "if FLAGS.verbose:\n",
    "    print(\"- Number of nodes:\", adj_init.shape[0])\n",
    "    #print(\"- Number of communities:\", len(np.unique(labels)))\n",
    "    print(\"- Use of node features:\", FLAGS.features)\n",
    "    print(\"Done! \\n \\n \\n \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd19b933",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-28T09:15:14.452102Z",
     "start_time": "2022-10-28T09:15:01.583926Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENTS ON MODEL 1 / 10 \n",
      "\n",
      "STEP 1/3 - PREPROCESSING STEPS \n",
      "\n",
      "Masking some edges from the training graph, for link prediction\n",
      "(validation set: 1.0 % of edges - test set: 10.0 % of edges)\n",
      "Done! \n",
      "\n",
      "Preprocessing node features\n",
      "Done! \n",
      "\n",
      "Running the Louvain algorithm for community detection\n",
      "as a preprocessing step for the encoder\n",
      "Done! Louvain has found 25 communities \n",
      "\n",
      "Setting up the model and the optimizer\n",
      "Done! \n",
      "\n",
      "Preprocessing on message passing matrices\n",
      "Done! \n",
      "\n",
      "Initializing TF session\n",
      "Done! \n",
      "\n",
      "STEP 2/3 - MODEL TRAINING \n",
      "\n",
      "Starting training\n",
      "Iteration: 0001 Loss: 1.65854 Time: 0.09930\n",
      "Iteration: 0002 Loss: 1.69740 Time: 0.00298\n",
      "Iteration: 0003 Loss: 1.75439 Time: 0.00301\n",
      "Iteration: 0004 Loss: 1.61392 Time: 0.00201\n",
      "Iteration: 0005 Loss: 1.56671 Time: 0.00200\n",
      "Iteration: 0006 Loss: 1.58254 Time: 0.00200\n",
      "Iteration: 0007 Loss: 1.58077 Time: 0.00300\n",
      "Iteration: 0008 Loss: 1.42423 Time: 0.00401\n",
      "Iteration: 0009 Loss: 1.54077 Time: 0.00200\n",
      "Iteration: 0010 Loss: 1.64633 Time: 0.00194\n",
      "Iteration: 0011 Loss: 1.63529 Time: 0.00300\n",
      "Iteration: 0012 Loss: 1.37727 Time: 0.00304\n",
      "Iteration: 0013 Loss: 1.51143 Time: 0.00297\n",
      "Iteration: 0014 Loss: 1.54007 Time: 0.00305\n",
      "Iteration: 0015 Loss: 1.31630 Time: 0.00302\n",
      "Iteration: 0016 Loss: 1.47443 Time: 0.00195\n",
      "Iteration: 0017 Loss: 1.30288 Time: 0.00196\n",
      "Iteration: 0018 Loss: 1.51202 Time: 0.00304\n",
      "Iteration: 0019 Loss: 1.43251 Time: 0.00301\n",
      "Iteration: 0020 Loss: 1.43745 Time: 0.00203\n",
      "Iteration: 0021 Loss: 1.37902 Time: 0.00399\n",
      "Iteration: 0022 Loss: 1.38049 Time: 0.00195\n",
      "Iteration: 0023 Loss: 1.44219 Time: 0.00104\n",
      "Iteration: 0024 Loss: 1.34586 Time: 0.00200\n",
      "Iteration: 0025 Loss: 1.32132 Time: 0.00201\n",
      "Iteration: 0026 Loss: 1.31602 Time: 0.00408\n",
      "Iteration: 0027 Loss: 1.25600 Time: 0.00290\n",
      "Iteration: 0028 Loss: 1.33448 Time: 0.00201\n",
      "Iteration: 0029 Loss: 1.24717 Time: 0.00411\n",
      "Iteration: 0030 Loss: 1.22569 Time: 0.00402\n",
      "Iteration: 0031 Loss: 1.21302 Time: 0.00393\n",
      "Iteration: 0032 Loss: 1.30190 Time: 0.00194\n",
      "Iteration: 0033 Loss: 1.24604 Time: 0.00307\n",
      "Iteration: 0034 Loss: 1.26831 Time: 0.00215\n",
      "Iteration: 0035 Loss: 1.26685 Time: 0.00380\n",
      "Iteration: 0036 Loss: 1.26136 Time: 0.00206\n",
      "Iteration: 0037 Loss: 1.16587 Time: 0.00398\n",
      "Iteration: 0038 Loss: 1.18694 Time: 0.00200\n",
      "Iteration: 0039 Loss: 1.24089 Time: 0.00196\n",
      "Iteration: 0040 Loss: 1.21679 Time: 0.00397\n",
      "Iteration: 0041 Loss: 1.21544 Time: 0.00301\n",
      "Iteration: 0042 Loss: 1.16998 Time: 0.00298\n",
      "Iteration: 0043 Loss: 1.15495 Time: 0.00400\n",
      "Iteration: 0044 Loss: 1.14627 Time: 0.00397\n",
      "Iteration: 0045 Loss: 1.11668 Time: 0.00307\n",
      "Iteration: 0046 Loss: 1.10126 Time: 0.00199\n",
      "Iteration: 0047 Loss: 1.08761 Time: 0.00203\n",
      "Iteration: 0048 Loss: 1.05699 Time: 0.00199\n",
      "Iteration: 0049 Loss: 1.08990 Time: 0.00303\n",
      "Iteration: 0050 Loss: 1.08851 Time: 0.00294\n",
      "Iteration: 0051 Loss: 1.14899 Time: 0.00398\n",
      "Iteration: 0052 Loss: 1.07241 Time: 0.00205\n",
      "Iteration: 0053 Loss: 1.10513 Time: 0.00196\n",
      "Iteration: 0054 Loss: 1.10243 Time: 0.00214\n",
      "Iteration: 0055 Loss: 1.07424 Time: 0.00198\n",
      "Iteration: 0056 Loss: 0.99231 Time: 0.00304\n",
      "Iteration: 0057 Loss: 1.07082 Time: 0.00201\n",
      "Iteration: 0058 Loss: 1.14788 Time: 0.00299\n",
      "Iteration: 0059 Loss: 1.06264 Time: 0.00344\n",
      "Iteration: 0060 Loss: 1.05281 Time: 0.00189\n",
      "Iteration: 0061 Loss: 1.12302 Time: 0.00299\n",
      "Iteration: 0062 Loss: 0.99630 Time: 0.00202\n",
      "Iteration: 0063 Loss: 1.05756 Time: 0.00206\n",
      "Iteration: 0064 Loss: 1.07632 Time: 0.00205\n",
      "Iteration: 0065 Loss: 1.03011 Time: 0.00299\n",
      "Iteration: 0066 Loss: 1.04708 Time: 0.00296\n",
      "Iteration: 0067 Loss: 1.03026 Time: 0.00380\n",
      "Iteration: 0068 Loss: 1.03348 Time: 0.00200\n",
      "Iteration: 0069 Loss: 1.11027 Time: 0.00303\n",
      "Iteration: 0070 Loss: 1.00526 Time: 0.00193\n",
      "Iteration: 0071 Loss: 0.99581 Time: 0.00301\n",
      "Iteration: 0072 Loss: 1.00415 Time: 0.00400\n",
      "Iteration: 0073 Loss: 1.01433 Time: 0.00200\n",
      "Iteration: 0074 Loss: 0.99074 Time: 0.00196\n",
      "Iteration: 0075 Loss: 1.01036 Time: 0.00304\n",
      "Iteration: 0076 Loss: 0.98767 Time: 0.00198\n",
      "Iteration: 0077 Loss: 1.02652 Time: 0.00217\n",
      "Iteration: 0078 Loss: 1.00141 Time: 0.00384\n",
      "Iteration: 0079 Loss: 0.93746 Time: 0.00299\n",
      "Iteration: 0080 Loss: 0.95293 Time: 0.00301\n",
      "Iteration: 0081 Loss: 0.97553 Time: 0.00300\n",
      "Iteration: 0082 Loss: 0.98655 Time: 0.00207\n",
      "Iteration: 0083 Loss: 0.96619 Time: 0.00508\n",
      "Iteration: 0084 Loss: 1.01054 Time: 0.00287\n",
      "Iteration: 0085 Loss: 1.02113 Time: 0.00312\n",
      "Iteration: 0086 Loss: 0.94481 Time: 0.00202\n",
      "Iteration: 0087 Loss: 0.95789 Time: 0.00199\n",
      "Iteration: 0088 Loss: 0.94415 Time: 0.00299\n",
      "Iteration: 0089 Loss: 0.95426 Time: 0.00200\n",
      "Iteration: 0090 Loss: 0.96077 Time: 0.00212\n",
      "Iteration: 0091 Loss: 0.92306 Time: 0.00205\n",
      "Iteration: 0092 Loss: 0.99028 Time: 0.00294\n",
      "Iteration: 0093 Loss: 0.92838 Time: 0.00200\n",
      "Iteration: 0094 Loss: 0.96050 Time: 0.00200\n",
      "Iteration: 0095 Loss: 0.97025 Time: 0.00300\n",
      "Iteration: 0096 Loss: 0.91607 Time: 0.00301\n",
      "Iteration: 0097 Loss: 0.93687 Time: 0.00199\n",
      "Iteration: 0098 Loss: 0.92487 Time: 0.00199\n",
      "Iteration: 0099 Loss: 0.93068 Time: 0.00206\n",
      "Iteration: 0100 Loss: 0.95688 Time: 0.00200\n",
      "Iteration: 0101 Loss: 0.92435 Time: 0.00300\n",
      "Iteration: 0102 Loss: 0.90540 Time: 0.00200\n",
      "Iteration: 0103 Loss: 0.91131 Time: 0.00296\n",
      "Iteration: 0104 Loss: 0.91806 Time: 0.00318\n",
      "Iteration: 0105 Loss: 0.90810 Time: 0.00335\n",
      "Iteration: 0106 Loss: 0.91145 Time: 0.00247\n",
      "Iteration: 0107 Loss: 0.90846 Time: 0.00300\n",
      "Iteration: 0108 Loss: 0.90926 Time: 0.00200\n",
      "Iteration: 0109 Loss: 0.86909 Time: 0.00400\n",
      "Iteration: 0110 Loss: 0.90198 Time: 0.00200\n",
      "Iteration: 0111 Loss: 0.83964 Time: 0.00301\n",
      "Iteration: 0112 Loss: 0.83931 Time: 0.00314\n",
      "Iteration: 0113 Loss: 0.88901 Time: 0.00181\n",
      "Iteration: 0114 Loss: 0.83920 Time: 0.00298\n",
      "Iteration: 0115 Loss: 0.89349 Time: 0.00307\n",
      "Iteration: 0116 Loss: 0.86298 Time: 0.00297\n",
      "Iteration: 0117 Loss: 0.84129 Time: 0.00294\n",
      "Iteration: 0118 Loss: 0.86182 Time: 0.00302\n",
      "Iteration: 0119 Loss: 0.82832 Time: 0.00295\n",
      "Iteration: 0120 Loss: 0.88472 Time: 0.00314\n",
      "Iteration: 0121 Loss: 0.82154 Time: 0.00201\n",
      "Iteration: 0122 Loss: 0.86488 Time: 0.00299\n",
      "Iteration: 0123 Loss: 0.83005 Time: 0.00199\n",
      "Iteration: 0124 Loss: 0.85002 Time: 0.00401\n",
      "Iteration: 0125 Loss: 0.83797 Time: 0.00196\n",
      "Iteration: 0126 Loss: 0.82376 Time: 0.00302\n",
      "Iteration: 0127 Loss: 0.83261 Time: 0.00297\n",
      "Iteration: 0128 Loss: 0.82891 Time: 0.00200\n",
      "Iteration: 0129 Loss: 0.84193 Time: 0.00200\n",
      "Iteration: 0130 Loss: 0.81139 Time: 0.00304\n",
      "Iteration: 0131 Loss: 0.82690 Time: 0.00194\n",
      "Iteration: 0132 Loss: 0.83535 Time: 0.00296\n",
      "Iteration: 0133 Loss: 0.81154 Time: 0.00300\n",
      "Iteration: 0134 Loss: 0.83510 Time: 0.00301\n",
      "Iteration: 0135 Loss: 0.81421 Time: 0.00203\n",
      "Iteration: 0136 Loss: 0.83453 Time: 0.00205\n",
      "Iteration: 0137 Loss: 0.82233 Time: 0.00208\n",
      "Iteration: 0138 Loss: 0.80302 Time: 0.00319\n",
      "Iteration: 0139 Loss: 0.77476 Time: 0.00289\n",
      "Iteration: 0140 Loss: 0.83075 Time: 0.00292\n",
      "Iteration: 0141 Loss: 0.87096 Time: 0.00307\n",
      "Iteration: 0142 Loss: 0.79281 Time: 0.00298\n",
      "Iteration: 0143 Loss: 0.78513 Time: 0.00195\n",
      "Iteration: 0144 Loss: 0.79638 Time: 0.00198\n",
      "Iteration: 0145 Loss: 0.82166 Time: 0.00210\n",
      "Iteration: 0146 Loss: 0.75341 Time: 0.00211\n",
      "Iteration: 0147 Loss: 0.78240 Time: 0.00202\n",
      "Iteration: 0148 Loss: 0.76485 Time: 0.00196\n",
      "Iteration: 0149 Loss: 0.78821 Time: 0.00199\n",
      "Iteration: 0150 Loss: 0.78217 Time: 0.00415\n",
      "Iteration: 0151 Loss: 0.76152 Time: 0.00186\n",
      "Iteration: 0152 Loss: 0.78383 Time: 0.00199\n",
      "Iteration: 0153 Loss: 0.75425 Time: 0.00199\n",
      "Iteration: 0154 Loss: 0.77705 Time: 0.00298\n",
      "Iteration: 0155 Loss: 0.77523 Time: 0.00300\n",
      "Iteration: 0156 Loss: 0.75216 Time: 0.00399\n",
      "Iteration: 0157 Loss: 0.74000 Time: 0.00200\n",
      "Iteration: 0158 Loss: 0.77550 Time: 0.00296\n",
      "Iteration: 0159 Loss: 0.75898 Time: 0.00200\n",
      "Iteration: 0160 Loss: 0.77494 Time: 0.00295\n",
      "Iteration: 0161 Loss: 0.73349 Time: 0.00207\n",
      "Iteration: 0162 Loss: 0.73101 Time: 0.00207\n",
      "Iteration: 0163 Loss: 0.74466 Time: 0.00398\n",
      "Iteration: 0164 Loss: 0.74779 Time: 0.00195\n",
      "Iteration: 0165 Loss: 0.75858 Time: 0.00304\n",
      "Iteration: 0166 Loss: 0.74483 Time: 0.00299\n",
      "Iteration: 0167 Loss: 0.72172 Time: 0.00298\n",
      "Iteration: 0168 Loss: 0.75008 Time: 0.00502\n",
      "Iteration: 0169 Loss: 0.74533 Time: 0.00298\n",
      "Iteration: 0170 Loss: 0.74384 Time: 0.00300\n",
      "Iteration: 0171 Loss: 0.70528 Time: 0.00300\n",
      "Iteration: 0172 Loss: 0.75242 Time: 0.00200\n",
      "Iteration: 0173 Loss: 0.70482 Time: 0.00301\n",
      "Iteration: 0174 Loss: 0.73514 Time: 0.00099\n",
      "Iteration: 0175 Loss: 0.73153 Time: 0.00201\n",
      "Iteration: 0176 Loss: 0.68896 Time: 0.00198\n",
      "Iteration: 0177 Loss: 0.70652 Time: 0.00199\n",
      "Iteration: 0178 Loss: 0.69209 Time: 0.00201\n",
      "Iteration: 0179 Loss: 0.71360 Time: 0.00300\n",
      "Iteration: 0180 Loss: 0.73580 Time: 0.00201\n",
      "Iteration: 0181 Loss: 0.70415 Time: 0.00200\n",
      "Iteration: 0182 Loss: 0.68744 Time: 0.00213\n",
      "Iteration: 0183 Loss: 0.70840 Time: 0.00300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0184 Loss: 0.72165 Time: 0.00200\n",
      "Iteration: 0185 Loss: 0.71146 Time: 0.00307\n",
      "Iteration: 0186 Loss: 0.71918 Time: 0.00206\n",
      "Iteration: 0187 Loss: 0.73290 Time: 0.00311\n",
      "Iteration: 0188 Loss: 0.71026 Time: 0.00196\n",
      "Iteration: 0189 Loss: 0.71552 Time: 0.00300\n",
      "Iteration: 0190 Loss: 0.72700 Time: 0.00207\n",
      "Iteration: 0191 Loss: 0.69100 Time: 0.00327\n",
      "Iteration: 0192 Loss: 0.70494 Time: 0.00173\n",
      "Iteration: 0193 Loss: 0.68002 Time: 0.00199\n",
      "Iteration: 0194 Loss: 0.69987 Time: 0.00296\n",
      "Iteration: 0195 Loss: 0.70660 Time: 0.00300\n",
      "Iteration: 0196 Loss: 0.71590 Time: 0.00206\n",
      "Iteration: 0197 Loss: 0.68276 Time: 0.00296\n",
      "Iteration: 0198 Loss: 0.68130 Time: 0.00200\n",
      "Iteration: 0199 Loss: 0.68808 Time: 0.00304\n",
      "Iteration: 0200 Loss: 0.68349 Time: 0.00309\n",
      "Done! \n",
      "\n",
      "STEP 3/3 - MODEL EVALUATION \n",
      "\n",
      "Computing the final embedding vectors, for evaluation\n",
      "Done! \n",
      "\n",
      "Testing: link prediction\n",
      "Done! \n",
      "\n",
      "EXPERIMENTS ON MODEL 2 / 10 \n",
      "\n",
      "STEP 1/3 - PREPROCESSING STEPS \n",
      "\n",
      "Masking some edges from the training graph, for link prediction\n",
      "(validation set: 1.0 % of edges - test set: 10.0 % of edges)\n",
      "Done! \n",
      "\n",
      "Preprocessing node features\n",
      "Done! \n",
      "\n",
      "Running the Louvain algorithm for community detection\n",
      "as a preprocessing step for the encoder\n",
      "Done! Louvain has found 23 communities \n",
      "\n",
      "Setting up the model and the optimizer\n",
      "Done! \n",
      "\n",
      "Preprocessing on message passing matrices\n",
      "Done! \n",
      "\n",
      "Initializing TF session\n",
      "Done! \n",
      "\n",
      "STEP 2/3 - MODEL TRAINING \n",
      "\n",
      "Starting training\n",
      "Iteration: 0001 Loss: 1.54267 Time: 0.08100\n",
      "Iteration: 0002 Loss: 1.67156 Time: 0.00301\n",
      "Iteration: 0003 Loss: 1.71096 Time: 0.00201\n",
      "Iteration: 0004 Loss: 1.69735 Time: 0.00300\n",
      "Iteration: 0005 Loss: 1.60060 Time: 0.00313\n",
      "Iteration: 0006 Loss: 1.62707 Time: 0.00288\n",
      "Iteration: 0007 Loss: 1.58791 Time: 0.00300\n",
      "Iteration: 0008 Loss: 1.78736 Time: 0.00400\n",
      "Iteration: 0009 Loss: 1.72962 Time: 0.00299\n",
      "Iteration: 0010 Loss: 1.51005 Time: 0.00300\n",
      "Iteration: 0011 Loss: 1.58782 Time: 0.00500\n",
      "Iteration: 0012 Loss: 1.46169 Time: 0.00299\n",
      "Iteration: 0013 Loss: 1.45674 Time: 0.00332\n",
      "Iteration: 0014 Loss: 1.40829 Time: 0.00270\n",
      "Iteration: 0015 Loss: 1.41320 Time: 0.00299\n",
      "Iteration: 0016 Loss: 1.48118 Time: 0.00403\n",
      "Iteration: 0017 Loss: 1.44442 Time: 0.00310\n",
      "Iteration: 0018 Loss: 1.32494 Time: 0.00285\n",
      "Iteration: 0019 Loss: 1.47216 Time: 0.00301\n",
      "Iteration: 0020 Loss: 1.37441 Time: 0.00300\n",
      "Iteration: 0021 Loss: 1.45568 Time: 0.00401\n",
      "Iteration: 0022 Loss: 1.28363 Time: 0.00300\n",
      "Iteration: 0023 Loss: 1.38490 Time: 0.00399\n",
      "Iteration: 0024 Loss: 1.36453 Time: 0.00202\n",
      "Iteration: 0025 Loss: 1.30538 Time: 0.00400\n",
      "Iteration: 0026 Loss: 1.25806 Time: 0.00292\n",
      "Iteration: 0027 Loss: 1.31289 Time: 0.00208\n",
      "Iteration: 0028 Loss: 1.25493 Time: 0.00300\n",
      "Iteration: 0029 Loss: 1.19437 Time: 0.00295\n",
      "Iteration: 0030 Loss: 1.24010 Time: 0.00297\n",
      "Iteration: 0031 Loss: 1.18776 Time: 0.00301\n",
      "Iteration: 0032 Loss: 1.28166 Time: 0.00205\n",
      "Iteration: 0033 Loss: 1.27982 Time: 0.00203\n",
      "Iteration: 0034 Loss: 1.26196 Time: 0.00200\n",
      "Iteration: 0035 Loss: 1.26516 Time: 0.00300\n",
      "Iteration: 0036 Loss: 1.23469 Time: 0.00200\n",
      "Iteration: 0037 Loss: 1.21159 Time: 0.00198\n",
      "Iteration: 0038 Loss: 1.16689 Time: 0.00190\n",
      "Iteration: 0039 Loss: 1.21164 Time: 0.00195\n",
      "Iteration: 0040 Loss: 1.18797 Time: 0.00305\n",
      "Iteration: 0041 Loss: 1.15257 Time: 0.00195\n",
      "Iteration: 0042 Loss: 1.10964 Time: 0.00203\n",
      "Iteration: 0043 Loss: 1.08844 Time: 0.00296\n",
      "Iteration: 0044 Loss: 1.12771 Time: 0.00203\n",
      "Iteration: 0045 Loss: 1.18523 Time: 0.00213\n",
      "Iteration: 0046 Loss: 1.11838 Time: 0.00305\n",
      "Iteration: 0047 Loss: 1.13886 Time: 0.00289\n",
      "Iteration: 0048 Loss: 1.15922 Time: 0.00207\n",
      "Iteration: 0049 Loss: 1.13350 Time: 0.00303\n",
      "Iteration: 0050 Loss: 1.17235 Time: 0.00201\n",
      "Iteration: 0051 Loss: 1.14275 Time: 0.00302\n",
      "Iteration: 0052 Loss: 1.10962 Time: 0.00198\n",
      "Iteration: 0053 Loss: 1.08786 Time: 0.00305\n",
      "Iteration: 0054 Loss: 1.08629 Time: 0.00294\n",
      "Iteration: 0055 Loss: 1.21019 Time: 0.00300\n",
      "Iteration: 0056 Loss: 1.15671 Time: 0.00300\n",
      "Iteration: 0057 Loss: 1.03935 Time: 0.00199\n",
      "Iteration: 0058 Loss: 1.14858 Time: 0.00205\n",
      "Iteration: 0059 Loss: 1.10599 Time: 0.00316\n",
      "Iteration: 0060 Loss: 1.11574 Time: 0.00184\n",
      "Iteration: 0061 Loss: 1.06489 Time: 0.00199\n",
      "Iteration: 0062 Loss: 1.01506 Time: 0.00301\n",
      "Iteration: 0063 Loss: 1.05628 Time: 0.00195\n",
      "Iteration: 0064 Loss: 1.04366 Time: 0.00307\n",
      "Iteration: 0065 Loss: 1.02955 Time: 0.00191\n",
      "Iteration: 0066 Loss: 1.04585 Time: 0.00303\n",
      "Iteration: 0067 Loss: 1.09983 Time: 0.00197\n",
      "Iteration: 0068 Loss: 1.03342 Time: 0.00347\n",
      "Iteration: 0069 Loss: 1.00189 Time: 0.00200\n",
      "Iteration: 0070 Loss: 1.06502 Time: 0.00296\n",
      "Iteration: 0071 Loss: 0.98248 Time: 0.00298\n",
      "Iteration: 0072 Loss: 1.03797 Time: 0.00207\n",
      "Iteration: 0073 Loss: 1.03121 Time: 0.00303\n",
      "Iteration: 0074 Loss: 1.00459 Time: 0.00200\n",
      "Iteration: 0075 Loss: 1.00807 Time: 0.00200\n",
      "Iteration: 0076 Loss: 1.02649 Time: 0.00296\n",
      "Iteration: 0077 Loss: 0.96610 Time: 0.00314\n",
      "Iteration: 0078 Loss: 0.97397 Time: 0.00395\n",
      "Iteration: 0079 Loss: 0.99097 Time: 0.00291\n",
      "Iteration: 0080 Loss: 1.02404 Time: 0.00305\n",
      "Iteration: 0081 Loss: 1.01675 Time: 0.00296\n",
      "Iteration: 0082 Loss: 0.99920 Time: 0.00211\n",
      "Iteration: 0083 Loss: 1.00630 Time: 0.00201\n",
      "Iteration: 0084 Loss: 1.00149 Time: 0.00301\n",
      "Iteration: 0085 Loss: 0.94622 Time: 0.00300\n",
      "Iteration: 0086 Loss: 0.95166 Time: 0.00308\n",
      "Iteration: 0087 Loss: 0.95206 Time: 0.00188\n",
      "Iteration: 0088 Loss: 0.94866 Time: 0.00318\n",
      "Iteration: 0089 Loss: 0.95901 Time: 0.00290\n",
      "Iteration: 0090 Loss: 0.99285 Time: 0.00297\n",
      "Iteration: 0091 Loss: 0.94889 Time: 0.00341\n",
      "Iteration: 0092 Loss: 0.97689 Time: 0.00260\n",
      "Iteration: 0093 Loss: 0.94467 Time: 0.00195\n",
      "Iteration: 0094 Loss: 0.94850 Time: 0.00400\n",
      "Iteration: 0095 Loss: 0.99909 Time: 0.00301\n",
      "Iteration: 0096 Loss: 0.93300 Time: 0.00199\n",
      "Iteration: 0097 Loss: 0.96199 Time: 0.00202\n",
      "Iteration: 0098 Loss: 0.91392 Time: 0.00297\n",
      "Iteration: 0099 Loss: 0.94984 Time: 0.00198\n",
      "Iteration: 0100 Loss: 0.92055 Time: 0.00308\n",
      "Iteration: 0101 Loss: 0.94622 Time: 0.00200\n",
      "Iteration: 0102 Loss: 0.93218 Time: 0.00304\n",
      "Iteration: 0103 Loss: 0.94031 Time: 0.00297\n",
      "Iteration: 0104 Loss: 0.92688 Time: 0.00199\n",
      "Iteration: 0105 Loss: 0.91279 Time: 0.00203\n",
      "Iteration: 0106 Loss: 0.96878 Time: 0.00318\n",
      "Iteration: 0107 Loss: 0.95633 Time: 0.00277\n",
      "Iteration: 0108 Loss: 0.93596 Time: 0.00299\n",
      "Iteration: 0109 Loss: 0.93288 Time: 0.00207\n",
      "Iteration: 0110 Loss: 0.87123 Time: 0.00200\n",
      "Iteration: 0111 Loss: 0.93681 Time: 0.00196\n",
      "Iteration: 0112 Loss: 0.88486 Time: 0.00198\n",
      "Iteration: 0113 Loss: 0.89732 Time: 0.00203\n",
      "Iteration: 0114 Loss: 0.87000 Time: 0.00304\n",
      "Iteration: 0115 Loss: 0.89881 Time: 0.00204\n",
      "Iteration: 0116 Loss: 0.88909 Time: 0.00207\n",
      "Iteration: 0117 Loss: 0.89723 Time: 0.00300\n",
      "Iteration: 0118 Loss: 0.86392 Time: 0.00308\n",
      "Iteration: 0119 Loss: 0.85870 Time: 0.00292\n",
      "Iteration: 0120 Loss: 0.86276 Time: 0.00200\n",
      "Iteration: 0121 Loss: 0.84840 Time: 0.00305\n",
      "Iteration: 0122 Loss: 0.81586 Time: 0.00297\n",
      "Iteration: 0123 Loss: 0.87999 Time: 0.00263\n",
      "Iteration: 0124 Loss: 0.84371 Time: 0.00308\n",
      "Iteration: 0125 Loss: 0.90072 Time: 0.00290\n",
      "Iteration: 0126 Loss: 0.81321 Time: 0.00307\n",
      "Iteration: 0127 Loss: 0.82742 Time: 0.00297\n",
      "Iteration: 0128 Loss: 0.83854 Time: 0.00392\n",
      "Iteration: 0129 Loss: 0.83143 Time: 0.00209\n",
      "Iteration: 0130 Loss: 0.83301 Time: 0.00305\n",
      "Iteration: 0131 Loss: 0.83095 Time: 0.00200\n",
      "Iteration: 0132 Loss: 0.82576 Time: 0.00197\n",
      "Iteration: 0133 Loss: 0.83585 Time: 0.00339\n",
      "Iteration: 0134 Loss: 0.81644 Time: 0.00297\n",
      "Iteration: 0135 Loss: 0.82386 Time: 0.00299\n",
      "Iteration: 0136 Loss: 0.82662 Time: 0.00205\n",
      "Iteration: 0137 Loss: 0.84883 Time: 0.00195\n",
      "Iteration: 0138 Loss: 0.80755 Time: 0.00304\n",
      "Iteration: 0139 Loss: 0.79991 Time: 0.00210\n",
      "Iteration: 0140 Loss: 0.84418 Time: 0.00209\n",
      "Iteration: 0141 Loss: 0.83055 Time: 0.00202\n",
      "Iteration: 0142 Loss: 0.84252 Time: 0.00196\n",
      "Iteration: 0143 Loss: 0.80076 Time: 0.00307\n",
      "Iteration: 0144 Loss: 0.78035 Time: 0.00304\n",
      "Iteration: 0145 Loss: 0.78044 Time: 0.00253\n",
      "Iteration: 0146 Loss: 0.78815 Time: 0.00205\n",
      "Iteration: 0147 Loss: 0.79111 Time: 0.00304\n",
      "Iteration: 0148 Loss: 0.78688 Time: 0.00201\n",
      "Iteration: 0149 Loss: 0.77855 Time: 0.00395\n",
      "Iteration: 0150 Loss: 0.78110 Time: 0.00200\n",
      "Iteration: 0151 Loss: 0.81073 Time: 0.00275\n",
      "Iteration: 0152 Loss: 0.75637 Time: 0.00207\n",
      "Iteration: 0153 Loss: 0.78487 Time: 0.00205\n",
      "Iteration: 0154 Loss: 0.80015 Time: 0.00200\n",
      "Iteration: 0155 Loss: 0.77077 Time: 0.00305\n",
      "Iteration: 0156 Loss: 0.75512 Time: 0.00304\n",
      "Iteration: 0157 Loss: 0.76927 Time: 0.00299\n",
      "Iteration: 0158 Loss: 0.75017 Time: 0.00295\n",
      "Iteration: 0159 Loss: 0.76727 Time: 0.00302\n",
      "Iteration: 0160 Loss: 0.75714 Time: 0.00297\n",
      "Iteration: 0161 Loss: 0.73726 Time: 0.00312\n",
      "Iteration: 0162 Loss: 0.78590 Time: 0.00394\n",
      "Iteration: 0163 Loss: 0.74322 Time: 0.00200\n",
      "Iteration: 0164 Loss: 0.75423 Time: 0.00204\n",
      "Iteration: 0165 Loss: 0.77353 Time: 0.00296\n",
      "Iteration: 0166 Loss: 0.72738 Time: 0.00295\n",
      "Iteration: 0167 Loss: 0.73918 Time: 0.00400\n",
      "Iteration: 0168 Loss: 0.75428 Time: 0.00300\n",
      "Iteration: 0169 Loss: 0.76046 Time: 0.00200\n",
      "Iteration: 0170 Loss: 0.73080 Time: 0.00307\n",
      "Iteration: 0171 Loss: 0.72585 Time: 0.00193\n",
      "Iteration: 0172 Loss: 0.76236 Time: 0.00202\n",
      "Iteration: 0173 Loss: 0.75311 Time: 0.00296\n",
      "Iteration: 0174 Loss: 0.74837 Time: 0.00200\n",
      "Iteration: 0175 Loss: 0.70135 Time: 0.00207\n",
      "Iteration: 0176 Loss: 0.70457 Time: 0.00299\n",
      "Iteration: 0177 Loss: 0.72393 Time: 0.00298\n",
      "Iteration: 0178 Loss: 0.72149 Time: 0.00196\n",
      "Iteration: 0179 Loss: 0.71117 Time: 0.00203\n",
      "Iteration: 0180 Loss: 0.72866 Time: 0.00196\n",
      "Iteration: 0181 Loss: 0.68786 Time: 0.00206\n",
      "Iteration: 0182 Loss: 0.70269 Time: 0.00303\n",
      "Iteration: 0183 Loss: 0.72129 Time: 0.00198\n",
      "Iteration: 0184 Loss: 0.69461 Time: 0.00205\n",
      "Iteration: 0185 Loss: 0.72885 Time: 0.00300\n",
      "Iteration: 0186 Loss: 0.68511 Time: 0.00298\n",
      "Iteration: 0187 Loss: 0.69720 Time: 0.00295\n",
      "Iteration: 0188 Loss: 0.69604 Time: 0.00200\n",
      "Iteration: 0189 Loss: 0.72629 Time: 0.00199\n",
      "Iteration: 0190 Loss: 0.68979 Time: 0.00197\n",
      "Iteration: 0191 Loss: 0.69866 Time: 0.00303\n",
      "Iteration: 0192 Loss: 0.70347 Time: 0.00196\n",
      "Iteration: 0193 Loss: 0.68533 Time: 0.00202\n",
      "Iteration: 0194 Loss: 0.69662 Time: 0.00303\n",
      "Iteration: 0195 Loss: 0.68871 Time: 0.00300\n",
      "Iteration: 0196 Loss: 0.68030 Time: 0.00199\n",
      "Iteration: 0197 Loss: 0.71235 Time: 0.00204\n",
      "Iteration: 0198 Loss: 0.72331 Time: 0.00412\n",
      "Iteration: 0199 Loss: 0.69086 Time: 0.00397\n",
      "Iteration: 0200 Loss: 0.68294 Time: 0.00336\n",
      "Done! \n",
      "\n",
      "STEP 3/3 - MODEL EVALUATION \n",
      "\n",
      "Computing the final embedding vectors, for evaluation\n",
      "Done! \n",
      "\n",
      "Testing: link prediction\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! \n",
      "\n",
      "EXPERIMENTS ON MODEL 3 / 10 \n",
      "\n",
      "STEP 1/3 - PREPROCESSING STEPS \n",
      "\n",
      "Masking some edges from the training graph, for link prediction\n",
      "(validation set: 1.0 % of edges - test set: 10.0 % of edges)\n",
      "Done! \n",
      "\n",
      "Preprocessing node features\n",
      "Done! \n",
      "\n",
      "Running the Louvain algorithm for community detection\n",
      "as a preprocessing step for the encoder\n",
      "Done! Louvain has found 22 communities \n",
      "\n",
      "Setting up the model and the optimizer\n",
      "Done! \n",
      "\n",
      "Preprocessing on message passing matrices\n",
      "Done! \n",
      "\n",
      "Initializing TF session\n",
      "Done! \n",
      "\n",
      "STEP 2/3 - MODEL TRAINING \n",
      "\n",
      "Starting training\n",
      "Iteration: 0001 Loss: 1.92446 Time: 0.10001\n",
      "Iteration: 0002 Loss: 1.72953 Time: 0.00537\n",
      "Iteration: 0003 Loss: 1.56105 Time: 0.00263\n",
      "Iteration: 0004 Loss: 1.61497 Time: 0.00200\n",
      "Iteration: 0005 Loss: 1.73039 Time: 0.00301\n",
      "Iteration: 0006 Loss: 1.68240 Time: 0.00299\n",
      "Iteration: 0007 Loss: 1.55037 Time: 0.00200\n",
      "Iteration: 0008 Loss: 1.53294 Time: 0.00401\n",
      "Iteration: 0009 Loss: 1.64136 Time: 0.00300\n",
      "Iteration: 0010 Loss: 1.61865 Time: 0.00300\n",
      "Iteration: 0011 Loss: 1.44692 Time: 0.00299\n",
      "Iteration: 0012 Loss: 1.52705 Time: 0.00301\n",
      "Iteration: 0013 Loss: 1.56840 Time: 0.00299\n",
      "Iteration: 0014 Loss: 1.47799 Time: 0.00300\n",
      "Iteration: 0015 Loss: 1.41041 Time: 0.00200\n",
      "Iteration: 0016 Loss: 1.42765 Time: 0.00204\n",
      "Iteration: 0017 Loss: 1.41153 Time: 0.00297\n",
      "Iteration: 0018 Loss: 1.44323 Time: 0.00203\n",
      "Iteration: 0019 Loss: 1.34694 Time: 0.00410\n",
      "Iteration: 0020 Loss: 1.41806 Time: 0.00287\n",
      "Iteration: 0021 Loss: 1.36619 Time: 0.00196\n",
      "Iteration: 0022 Loss: 1.42196 Time: 0.00395\n",
      "Iteration: 0023 Loss: 1.37690 Time: 0.00201\n",
      "Iteration: 0024 Loss: 1.35401 Time: 0.00300\n",
      "Iteration: 0025 Loss: 1.21660 Time: 0.00196\n",
      "Iteration: 0026 Loss: 1.20463 Time: 0.00306\n",
      "Iteration: 0027 Loss: 1.31569 Time: 0.00296\n",
      "Iteration: 0028 Loss: 1.27414 Time: 0.00296\n",
      "Iteration: 0029 Loss: 1.28224 Time: 0.00198\n",
      "Iteration: 0030 Loss: 1.30151 Time: 0.00306\n",
      "Iteration: 0031 Loss: 1.18022 Time: 0.00198\n",
      "Iteration: 0032 Loss: 1.23436 Time: 0.00207\n",
      "Iteration: 0033 Loss: 1.26419 Time: 0.00298\n",
      "Iteration: 0034 Loss: 1.28625 Time: 0.00397\n",
      "Iteration: 0035 Loss: 1.18544 Time: 0.00303\n",
      "Iteration: 0036 Loss: 1.21399 Time: 0.00298\n",
      "Iteration: 0037 Loss: 1.16733 Time: 0.00303\n",
      "Iteration: 0038 Loss: 1.25849 Time: 0.00193\n",
      "Iteration: 0039 Loss: 1.16086 Time: 0.00298\n",
      "Iteration: 0040 Loss: 1.23757 Time: 0.00192\n",
      "Iteration: 0041 Loss: 1.10435 Time: 0.00197\n",
      "Iteration: 0042 Loss: 1.22697 Time: 0.00204\n",
      "Iteration: 0043 Loss: 1.16550 Time: 0.00201\n",
      "Iteration: 0044 Loss: 1.17463 Time: 0.00301\n",
      "Iteration: 0045 Loss: 1.21854 Time: 0.00597\n",
      "Iteration: 0046 Loss: 1.12034 Time: 0.00203\n",
      "Iteration: 0047 Loss: 1.19869 Time: 0.00398\n",
      "Iteration: 0048 Loss: 1.11945 Time: 0.00306\n",
      "Iteration: 0049 Loss: 1.15651 Time: 0.00289\n",
      "Iteration: 0050 Loss: 1.17043 Time: 0.00315\n",
      "Iteration: 0051 Loss: 1.06791 Time: 0.00297\n",
      "Iteration: 0052 Loss: 1.15567 Time: 0.00205\n",
      "Iteration: 0053 Loss: 1.08665 Time: 0.00197\n",
      "Iteration: 0054 Loss: 1.14098 Time: 0.00303\n",
      "Iteration: 0055 Loss: 1.13841 Time: 0.00297\n",
      "Iteration: 0056 Loss: 1.09183 Time: 0.00307\n",
      "Iteration: 0057 Loss: 1.04210 Time: 0.00193\n",
      "Iteration: 0058 Loss: 1.08415 Time: 0.00308\n",
      "Iteration: 0059 Loss: 1.12129 Time: 0.00293\n",
      "Iteration: 0060 Loss: 1.14204 Time: 0.00399\n",
      "Iteration: 0061 Loss: 1.04250 Time: 0.00300\n",
      "Iteration: 0062 Loss: 1.02216 Time: 0.00195\n",
      "Iteration: 0063 Loss: 1.09794 Time: 0.00209\n",
      "Iteration: 0064 Loss: 1.02401 Time: 0.00304\n",
      "Iteration: 0065 Loss: 1.01227 Time: 0.00302\n",
      "Iteration: 0066 Loss: 1.05422 Time: 0.00398\n",
      "Iteration: 0067 Loss: 1.10271 Time: 0.00301\n",
      "Iteration: 0068 Loss: 1.04306 Time: 0.00339\n",
      "Iteration: 0069 Loss: 1.04116 Time: 0.00297\n",
      "Iteration: 0070 Loss: 1.05891 Time: 0.00302\n",
      "Iteration: 0071 Loss: 1.03547 Time: 0.00398\n",
      "Iteration: 0072 Loss: 1.05535 Time: 0.00213\n",
      "Iteration: 0073 Loss: 0.99923 Time: 0.00204\n",
      "Iteration: 0074 Loss: 1.01091 Time: 0.00299\n",
      "Iteration: 0075 Loss: 0.99636 Time: 0.00194\n",
      "Iteration: 0076 Loss: 1.00567 Time: 0.00299\n",
      "Iteration: 0077 Loss: 0.97787 Time: 0.00301\n",
      "Iteration: 0078 Loss: 1.00445 Time: 0.00199\n",
      "Iteration: 0079 Loss: 1.00484 Time: 0.00300\n",
      "Iteration: 0080 Loss: 0.99543 Time: 0.00202\n",
      "Iteration: 0081 Loss: 1.02251 Time: 0.00294\n",
      "Iteration: 0082 Loss: 1.01124 Time: 0.00203\n",
      "Iteration: 0083 Loss: 1.02762 Time: 0.00197\n",
      "Iteration: 0084 Loss: 0.99535 Time: 0.00308\n",
      "Iteration: 0085 Loss: 0.95371 Time: 0.00297\n",
      "Iteration: 0086 Loss: 1.03656 Time: 0.00206\n",
      "Iteration: 0087 Loss: 1.00264 Time: 0.00290\n",
      "Iteration: 0088 Loss: 1.02257 Time: 0.00209\n",
      "Iteration: 0089 Loss: 0.95702 Time: 0.00201\n",
      "Iteration: 0090 Loss: 0.99416 Time: 0.00207\n",
      "Iteration: 0091 Loss: 0.94976 Time: 0.00193\n",
      "Iteration: 0092 Loss: 0.95986 Time: 0.00301\n",
      "Iteration: 0093 Loss: 0.97990 Time: 0.00300\n",
      "Iteration: 0094 Loss: 0.99806 Time: 0.00410\n",
      "Iteration: 0095 Loss: 0.96655 Time: 0.00193\n",
      "Iteration: 0096 Loss: 1.01570 Time: 0.00299\n",
      "Iteration: 0097 Loss: 0.91528 Time: 0.00301\n",
      "Iteration: 0098 Loss: 0.97090 Time: 0.00408\n",
      "Iteration: 0099 Loss: 0.97289 Time: 0.00204\n",
      "Iteration: 0100 Loss: 0.94794 Time: 0.00296\n",
      "Iteration: 0101 Loss: 0.93102 Time: 0.00306\n",
      "Iteration: 0102 Loss: 0.92751 Time: 0.00199\n",
      "Iteration: 0103 Loss: 0.93739 Time: 0.00304\n",
      "Iteration: 0104 Loss: 0.99312 Time: 0.00199\n",
      "Iteration: 0105 Loss: 0.89591 Time: 0.00306\n",
      "Iteration: 0106 Loss: 0.90269 Time: 0.00202\n",
      "Iteration: 0107 Loss: 0.94157 Time: 0.00202\n",
      "Iteration: 0108 Loss: 0.89431 Time: 0.00197\n",
      "Iteration: 0109 Loss: 0.92467 Time: 0.00305\n",
      "Iteration: 0110 Loss: 0.97621 Time: 0.00195\n",
      "Iteration: 0111 Loss: 0.87318 Time: 0.00296\n",
      "Iteration: 0112 Loss: 0.89671 Time: 0.00306\n",
      "Iteration: 0113 Loss: 0.93172 Time: 0.00206\n",
      "Iteration: 0114 Loss: 0.86025 Time: 0.00303\n",
      "Iteration: 0115 Loss: 0.88408 Time: 0.00195\n",
      "Iteration: 0116 Loss: 0.84415 Time: 0.00305\n",
      "Iteration: 0117 Loss: 0.84456 Time: 0.00295\n",
      "Iteration: 0118 Loss: 0.87899 Time: 0.00200\n",
      "Iteration: 0119 Loss: 0.85694 Time: 0.00211\n",
      "Iteration: 0120 Loss: 0.87216 Time: 0.00297\n",
      "Iteration: 0121 Loss: 0.84806 Time: 0.00200\n",
      "Iteration: 0122 Loss: 0.86391 Time: 0.00304\n",
      "Iteration: 0123 Loss: 0.85099 Time: 0.00195\n",
      "Iteration: 0124 Loss: 0.87671 Time: 0.00200\n",
      "Iteration: 0125 Loss: 0.89581 Time: 0.00196\n",
      "Iteration: 0126 Loss: 0.84215 Time: 0.00308\n",
      "Iteration: 0127 Loss: 0.85526 Time: 0.00300\n",
      "Iteration: 0128 Loss: 0.86443 Time: 0.00294\n",
      "Iteration: 0129 Loss: 0.86326 Time: 0.00306\n",
      "Iteration: 0130 Loss: 0.84023 Time: 0.00193\n",
      "Iteration: 0131 Loss: 0.86160 Time: 0.00316\n",
      "Iteration: 0132 Loss: 0.81134 Time: 0.00207\n",
      "Iteration: 0133 Loss: 0.83660 Time: 0.00190\n",
      "Iteration: 0134 Loss: 0.84802 Time: 0.00200\n",
      "Iteration: 0135 Loss: 0.81625 Time: 0.00448\n",
      "Iteration: 0136 Loss: 0.84693 Time: 0.00249\n",
      "Iteration: 0137 Loss: 0.85204 Time: 0.00506\n",
      "Iteration: 0138 Loss: 0.82148 Time: 0.00198\n",
      "Iteration: 0139 Loss: 0.81168 Time: 0.00205\n",
      "Iteration: 0140 Loss: 0.81667 Time: 0.00208\n",
      "Iteration: 0141 Loss: 0.79460 Time: 0.00103\n",
      "Iteration: 0142 Loss: 0.81641 Time: 0.00204\n",
      "Iteration: 0143 Loss: 0.78933 Time: 0.00200\n",
      "Iteration: 0144 Loss: 0.77982 Time: 0.00197\n",
      "Iteration: 0145 Loss: 0.79516 Time: 0.00306\n",
      "Iteration: 0146 Loss: 0.78708 Time: 0.00294\n",
      "Iteration: 0147 Loss: 0.79346 Time: 0.00302\n",
      "Iteration: 0148 Loss: 0.78046 Time: 0.00301\n",
      "Iteration: 0149 Loss: 0.79767 Time: 0.00404\n",
      "Iteration: 0150 Loss: 0.78983 Time: 0.00296\n",
      "Iteration: 0151 Loss: 0.82571 Time: 0.00205\n",
      "Iteration: 0152 Loss: 0.78439 Time: 0.00297\n",
      "Iteration: 0153 Loss: 0.75130 Time: 0.00295\n",
      "Iteration: 0154 Loss: 0.81535 Time: 0.00404\n",
      "Iteration: 0155 Loss: 0.78356 Time: 0.00196\n",
      "Iteration: 0156 Loss: 0.76257 Time: 0.00405\n",
      "Iteration: 0157 Loss: 0.82166 Time: 0.00308\n",
      "Iteration: 0158 Loss: 0.77100 Time: 0.00304\n",
      "Iteration: 0159 Loss: 0.77385 Time: 0.00200\n",
      "Iteration: 0160 Loss: 0.80519 Time: 0.00300\n",
      "Iteration: 0161 Loss: 0.78149 Time: 0.00203\n",
      "Iteration: 0162 Loss: 0.76580 Time: 0.00203\n",
      "Iteration: 0163 Loss: 0.77858 Time: 0.00398\n",
      "Iteration: 0164 Loss: 0.75949 Time: 0.00196\n",
      "Iteration: 0165 Loss: 0.78874 Time: 0.00404\n",
      "Iteration: 0166 Loss: 0.76694 Time: 0.00297\n",
      "Iteration: 0167 Loss: 0.73340 Time: 0.00357\n",
      "Iteration: 0168 Loss: 0.74559 Time: 0.00142\n",
      "Iteration: 0169 Loss: 0.74638 Time: 0.00209\n",
      "Iteration: 0170 Loss: 0.74330 Time: 0.00303\n",
      "Iteration: 0171 Loss: 0.74758 Time: 0.00301\n",
      "Iteration: 0172 Loss: 0.72815 Time: 0.00192\n",
      "Iteration: 0173 Loss: 0.72696 Time: 0.00195\n",
      "Iteration: 0174 Loss: 0.77252 Time: 0.00305\n",
      "Iteration: 0175 Loss: 0.74428 Time: 0.00204\n",
      "Iteration: 0176 Loss: 0.73831 Time: 0.00305\n",
      "Iteration: 0177 Loss: 0.75562 Time: 0.00291\n",
      "Iteration: 0178 Loss: 0.75489 Time: 0.00206\n",
      "Iteration: 0179 Loss: 0.75235 Time: 0.00204\n",
      "Iteration: 0180 Loss: 0.74649 Time: 0.00300\n",
      "Iteration: 0181 Loss: 0.72756 Time: 0.00197\n",
      "Iteration: 0182 Loss: 0.74501 Time: 0.00307\n",
      "Iteration: 0183 Loss: 0.75636 Time: 0.00393\n",
      "Iteration: 0184 Loss: 0.73431 Time: 0.00295\n",
      "Iteration: 0185 Loss: 0.71589 Time: 0.00203\n",
      "Iteration: 0186 Loss: 0.74358 Time: 0.00200\n",
      "Iteration: 0187 Loss: 0.73610 Time: 0.00293\n",
      "Iteration: 0188 Loss: 0.73420 Time: 0.00366\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0189 Loss: 0.73662 Time: 0.00238\n",
      "Iteration: 0190 Loss: 0.71249 Time: 0.00309\n",
      "Iteration: 0191 Loss: 0.68752 Time: 0.00405\n",
      "Iteration: 0192 Loss: 0.68741 Time: 0.00183\n",
      "Iteration: 0193 Loss: 0.72620 Time: 0.00305\n",
      "Iteration: 0194 Loss: 0.70120 Time: 0.00309\n",
      "Iteration: 0195 Loss: 0.72462 Time: 0.00286\n",
      "Iteration: 0196 Loss: 0.71257 Time: 0.00304\n",
      "Iteration: 0197 Loss: 0.68916 Time: 0.00195\n",
      "Iteration: 0198 Loss: 0.70338 Time: 0.00201\n",
      "Iteration: 0199 Loss: 0.73562 Time: 0.00303\n",
      "Iteration: 0200 Loss: 0.71911 Time: 0.00210\n",
      "Done! \n",
      "\n",
      "STEP 3/3 - MODEL EVALUATION \n",
      "\n",
      "Computing the final embedding vectors, for evaluation\n",
      "Done! \n",
      "\n",
      "Testing: link prediction\n",
      "Done! \n",
      "\n",
      "EXPERIMENTS ON MODEL 4 / 10 \n",
      "\n",
      "STEP 1/3 - PREPROCESSING STEPS \n",
      "\n",
      "Masking some edges from the training graph, for link prediction\n",
      "(validation set: 1.0 % of edges - test set: 10.0 % of edges)\n",
      "Done! \n",
      "\n",
      "Preprocessing node features\n",
      "Done! \n",
      "\n",
      "Running the Louvain algorithm for community detection\n",
      "as a preprocessing step for the encoder\n",
      "Done! Louvain has found 20 communities \n",
      "\n",
      "Setting up the model and the optimizer\n",
      "Done! \n",
      "\n",
      "Preprocessing on message passing matrices\n",
      "Done! \n",
      "\n",
      "Initializing TF session\n",
      "Done! \n",
      "\n",
      "STEP 2/3 - MODEL TRAINING \n",
      "\n",
      "Starting training\n",
      "Iteration: 0001 Loss: 1.81489 Time: 0.10806\n",
      "Iteration: 0002 Loss: 1.75943 Time: 0.00478\n",
      "Iteration: 0003 Loss: 1.67349 Time: 0.00310\n",
      "Iteration: 0004 Loss: 1.60485 Time: 0.00294\n",
      "Iteration: 0005 Loss: 1.71912 Time: 0.00204\n",
      "Iteration: 0006 Loss: 1.69818 Time: 0.00297\n",
      "Iteration: 0007 Loss: 1.61440 Time: 0.00302\n",
      "Iteration: 0008 Loss: 1.47837 Time: 0.00398\n",
      "Iteration: 0009 Loss: 1.55300 Time: 0.00300\n",
      "Iteration: 0010 Loss: 1.45943 Time: 0.00308\n",
      "Iteration: 0011 Loss: 1.44156 Time: 0.00388\n",
      "Iteration: 0012 Loss: 1.46560 Time: 0.00314\n",
      "Iteration: 0013 Loss: 1.40159 Time: 0.00290\n",
      "Iteration: 0014 Loss: 1.39638 Time: 0.00295\n",
      "Iteration: 0015 Loss: 1.50457 Time: 0.00200\n",
      "Iteration: 0016 Loss: 1.43292 Time: 0.00198\n",
      "Iteration: 0017 Loss: 1.45048 Time: 0.00297\n",
      "Iteration: 0018 Loss: 1.53804 Time: 0.00205\n",
      "Iteration: 0019 Loss: 1.29894 Time: 0.00300\n",
      "Iteration: 0020 Loss: 1.45643 Time: 0.00247\n",
      "Iteration: 0021 Loss: 1.42088 Time: 0.00198\n",
      "Iteration: 0022 Loss: 1.33257 Time: 0.00197\n",
      "Iteration: 0023 Loss: 1.37169 Time: 0.00301\n",
      "Iteration: 0024 Loss: 1.23145 Time: 0.00196\n",
      "Iteration: 0025 Loss: 1.23461 Time: 0.00306\n",
      "Iteration: 0026 Loss: 1.32876 Time: 0.00299\n",
      "Iteration: 0027 Loss: 1.32446 Time: 0.00303\n",
      "Iteration: 0028 Loss: 1.28694 Time: 0.00202\n",
      "Iteration: 0029 Loss: 1.29401 Time: 0.00208\n",
      "Iteration: 0030 Loss: 1.23005 Time: 0.00302\n",
      "Iteration: 0031 Loss: 1.28063 Time: 0.00190\n",
      "Iteration: 0032 Loss: 1.16165 Time: 0.00199\n",
      "Iteration: 0033 Loss: 1.26775 Time: 0.00296\n",
      "Iteration: 0034 Loss: 1.28166 Time: 0.00200\n",
      "Iteration: 0035 Loss: 1.24841 Time: 0.00203\n",
      "Iteration: 0036 Loss: 1.19634 Time: 0.00299\n",
      "Iteration: 0037 Loss: 1.16387 Time: 0.00202\n",
      "Iteration: 0038 Loss: 1.26633 Time: 0.00200\n",
      "Iteration: 0039 Loss: 1.24597 Time: 0.00208\n",
      "Iteration: 0040 Loss: 1.22367 Time: 0.00194\n",
      "Iteration: 0041 Loss: 1.13362 Time: 0.00297\n",
      "Iteration: 0042 Loss: 1.21762 Time: 0.00333\n",
      "Iteration: 0043 Loss: 1.18287 Time: 0.00265\n",
      "Iteration: 0044 Loss: 1.14946 Time: 0.00303\n",
      "Iteration: 0045 Loss: 1.14973 Time: 0.00300\n",
      "Iteration: 0046 Loss: 1.18265 Time: 0.00196\n",
      "Iteration: 0047 Loss: 1.10894 Time: 0.00402\n",
      "Iteration: 0048 Loss: 1.13344 Time: 0.00202\n",
      "Iteration: 0049 Loss: 1.20048 Time: 0.00301\n",
      "Iteration: 0050 Loss: 1.10186 Time: 0.00199\n",
      "Iteration: 0051 Loss: 1.07170 Time: 0.00302\n",
      "Iteration: 0052 Loss: 1.11069 Time: 0.00192\n",
      "Iteration: 0053 Loss: 1.17119 Time: 0.00299\n",
      "Iteration: 0054 Loss: 1.10510 Time: 0.00301\n",
      "Iteration: 0055 Loss: 1.13048 Time: 0.00299\n",
      "Iteration: 0056 Loss: 1.08847 Time: 0.00300\n",
      "Iteration: 0057 Loss: 1.09721 Time: 0.00200\n",
      "Iteration: 0058 Loss: 1.08507 Time: 0.00411\n",
      "Iteration: 0059 Loss: 1.05946 Time: 0.00293\n",
      "Iteration: 0060 Loss: 1.12682 Time: 0.00196\n",
      "Iteration: 0061 Loss: 1.09863 Time: 0.00317\n",
      "Iteration: 0062 Loss: 1.09826 Time: 0.00196\n",
      "Iteration: 0063 Loss: 1.04373 Time: 0.00302\n",
      "Iteration: 0064 Loss: 1.09032 Time: 0.00299\n",
      "Iteration: 0065 Loss: 1.07849 Time: 0.00206\n",
      "Iteration: 0066 Loss: 1.02593 Time: 0.00294\n",
      "Iteration: 0067 Loss: 1.06787 Time: 0.00199\n",
      "Iteration: 0068 Loss: 1.04334 Time: 0.00204\n",
      "Iteration: 0069 Loss: 1.03593 Time: 0.00104\n",
      "Iteration: 0070 Loss: 1.10274 Time: 0.00200\n",
      "Iteration: 0071 Loss: 1.05333 Time: 0.00196\n",
      "Iteration: 0072 Loss: 1.09934 Time: 0.00200\n",
      "Iteration: 0073 Loss: 1.05011 Time: 0.00203\n",
      "Iteration: 0074 Loss: 1.00709 Time: 0.00201\n",
      "Iteration: 0075 Loss: 1.00347 Time: 0.00295\n",
      "Iteration: 0076 Loss: 1.00745 Time: 0.00207\n",
      "Iteration: 0077 Loss: 0.95476 Time: 0.00307\n",
      "Iteration: 0078 Loss: 1.04775 Time: 0.00291\n",
      "Iteration: 0079 Loss: 0.99659 Time: 0.00202\n",
      "Iteration: 0080 Loss: 1.08220 Time: 0.00197\n",
      "Iteration: 0081 Loss: 0.97933 Time: 0.00309\n",
      "Iteration: 0082 Loss: 1.01475 Time: 0.00398\n",
      "Iteration: 0083 Loss: 1.00042 Time: 0.00311\n",
      "Iteration: 0084 Loss: 0.92717 Time: 0.00303\n",
      "Iteration: 0085 Loss: 0.99428 Time: 0.00201\n",
      "Iteration: 0086 Loss: 0.96707 Time: 0.00349\n",
      "Iteration: 0087 Loss: 0.95787 Time: 0.00193\n",
      "Iteration: 0088 Loss: 0.99905 Time: 0.00405\n",
      "Iteration: 0089 Loss: 0.99446 Time: 0.00195\n",
      "Iteration: 0090 Loss: 0.95165 Time: 0.00211\n",
      "Iteration: 0091 Loss: 0.93725 Time: 0.00204\n",
      "Iteration: 0092 Loss: 0.93689 Time: 0.00305\n",
      "Iteration: 0093 Loss: 0.95515 Time: 0.00337\n",
      "Iteration: 0094 Loss: 0.94988 Time: 0.00264\n",
      "Iteration: 0095 Loss: 0.97678 Time: 0.00308\n",
      "Iteration: 0096 Loss: 0.92494 Time: 0.00308\n",
      "Iteration: 0097 Loss: 0.93498 Time: 0.00252\n",
      "Iteration: 0098 Loss: 0.91692 Time: 0.00293\n",
      "Iteration: 0099 Loss: 0.91344 Time: 0.00297\n",
      "Iteration: 0100 Loss: 0.97347 Time: 0.00301\n",
      "Iteration: 0101 Loss: 0.90475 Time: 0.00310\n",
      "Iteration: 0102 Loss: 0.92362 Time: 0.00296\n",
      "Iteration: 0103 Loss: 0.90602 Time: 0.00296\n",
      "Iteration: 0104 Loss: 0.92265 Time: 0.00302\n",
      "Iteration: 0105 Loss: 0.89962 Time: 0.00293\n",
      "Iteration: 0106 Loss: 0.90504 Time: 0.00304\n",
      "Iteration: 0107 Loss: 0.88045 Time: 0.00295\n",
      "Iteration: 0108 Loss: 0.93901 Time: 0.00300\n",
      "Iteration: 0109 Loss: 0.88827 Time: 0.00304\n",
      "Iteration: 0110 Loss: 0.88261 Time: 0.00337\n",
      "Iteration: 0111 Loss: 0.90897 Time: 0.00261\n",
      "Iteration: 0112 Loss: 0.86810 Time: 0.00298\n",
      "Iteration: 0113 Loss: 0.92357 Time: 0.00208\n",
      "Iteration: 0114 Loss: 0.88837 Time: 0.00399\n",
      "Iteration: 0115 Loss: 0.87987 Time: 0.00304\n",
      "Iteration: 0116 Loss: 0.85044 Time: 0.00298\n",
      "Iteration: 0117 Loss: 0.85239 Time: 0.00291\n",
      "Iteration: 0118 Loss: 0.86872 Time: 0.00306\n",
      "Iteration: 0119 Loss: 0.84177 Time: 0.00403\n",
      "Iteration: 0120 Loss: 0.86052 Time: 0.00290\n",
      "Iteration: 0121 Loss: 0.81565 Time: 0.00301\n",
      "Iteration: 0122 Loss: 0.84412 Time: 0.00316\n",
      "Iteration: 0123 Loss: 0.88780 Time: 0.00202\n",
      "Iteration: 0124 Loss: 0.86853 Time: 0.00306\n",
      "Iteration: 0125 Loss: 0.85976 Time: 0.00291\n",
      "Iteration: 0126 Loss: 0.82790 Time: 0.00208\n",
      "Iteration: 0127 Loss: 0.81691 Time: 0.00392\n",
      "Iteration: 0128 Loss: 0.79861 Time: 0.00200\n",
      "Iteration: 0129 Loss: 0.79044 Time: 0.00304\n",
      "Iteration: 0130 Loss: 0.83516 Time: 0.00198\n",
      "Iteration: 0131 Loss: 0.84142 Time: 0.00308\n",
      "Iteration: 0132 Loss: 0.84363 Time: 0.00406\n",
      "Iteration: 0133 Loss: 0.84075 Time: 0.00301\n",
      "Iteration: 0134 Loss: 0.81861 Time: 0.00294\n",
      "Iteration: 0135 Loss: 0.80866 Time: 0.00399\n",
      "Iteration: 0136 Loss: 0.81823 Time: 0.00396\n",
      "Iteration: 0137 Loss: 0.80936 Time: 0.00201\n",
      "Iteration: 0138 Loss: 0.81006 Time: 0.00212\n",
      "Iteration: 0139 Loss: 0.76988 Time: 0.00208\n",
      "Iteration: 0140 Loss: 0.75895 Time: 0.00303\n",
      "Iteration: 0141 Loss: 0.81451 Time: 0.00203\n",
      "Iteration: 0142 Loss: 0.77270 Time: 0.00197\n",
      "Iteration: 0143 Loss: 0.79459 Time: 0.00313\n",
      "Iteration: 0144 Loss: 0.80079 Time: 0.00304\n",
      "Iteration: 0145 Loss: 0.77176 Time: 0.00393\n",
      "Iteration: 0146 Loss: 0.81401 Time: 0.00301\n",
      "Iteration: 0147 Loss: 0.80237 Time: 0.00302\n",
      "Iteration: 0148 Loss: 0.80710 Time: 0.00297\n",
      "Iteration: 0149 Loss: 0.79861 Time: 0.00199\n",
      "Iteration: 0150 Loss: 0.79010 Time: 0.00400\n",
      "Iteration: 0151 Loss: 0.81305 Time: 0.00306\n",
      "Iteration: 0152 Loss: 0.79562 Time: 0.00305\n",
      "Iteration: 0153 Loss: 0.76019 Time: 0.00292\n",
      "Iteration: 0154 Loss: 0.76808 Time: 0.00297\n",
      "Iteration: 0155 Loss: 0.74021 Time: 0.00343\n",
      "Iteration: 0156 Loss: 0.75590 Time: 0.00163\n",
      "Iteration: 0157 Loss: 0.76920 Time: 0.00203\n",
      "Iteration: 0158 Loss: 0.77744 Time: 0.00200\n",
      "Iteration: 0159 Loss: 0.74796 Time: 0.00307\n",
      "Iteration: 0160 Loss: 0.74589 Time: 0.00191\n",
      "Iteration: 0161 Loss: 0.76086 Time: 0.00200\n",
      "Iteration: 0162 Loss: 0.76409 Time: 0.00401\n",
      "Iteration: 0163 Loss: 0.73074 Time: 0.00200\n",
      "Iteration: 0164 Loss: 0.74040 Time: 0.00404\n",
      "Iteration: 0165 Loss: 0.73964 Time: 0.00299\n",
      "Iteration: 0166 Loss: 0.77842 Time: 0.00300\n",
      "Iteration: 0167 Loss: 0.75724 Time: 0.00296\n",
      "Iteration: 0168 Loss: 0.71460 Time: 0.00301\n",
      "Iteration: 0169 Loss: 0.75424 Time: 0.00300\n",
      "Iteration: 0170 Loss: 0.73424 Time: 0.00300\n",
      "Iteration: 0171 Loss: 0.71953 Time: 0.00292\n",
      "Iteration: 0172 Loss: 0.74056 Time: 0.00309\n",
      "Iteration: 0173 Loss: 0.73677 Time: 0.00291\n",
      "Iteration: 0174 Loss: 0.73164 Time: 0.00303\n",
      "Iteration: 0175 Loss: 0.70743 Time: 0.00196\n",
      "Iteration: 0176 Loss: 0.69704 Time: 0.00202\n",
      "Iteration: 0177 Loss: 0.70470 Time: 0.00202\n",
      "Iteration: 0178 Loss: 0.69010 Time: 0.00297\n",
      "Iteration: 0179 Loss: 0.71416 Time: 0.00402\n",
      "Iteration: 0180 Loss: 0.72506 Time: 0.00198\n",
      "Iteration: 0181 Loss: 0.72984 Time: 0.00308\n",
      "Iteration: 0182 Loss: 0.72738 Time: 0.00293\n",
      "Iteration: 0183 Loss: 0.71134 Time: 0.00402\n",
      "Iteration: 0184 Loss: 0.71017 Time: 0.00297\n",
      "Iteration: 0185 Loss: 0.69641 Time: 0.00300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0186 Loss: 0.74327 Time: 0.00200\n",
      "Iteration: 0187 Loss: 0.70866 Time: 0.00297\n",
      "Iteration: 0188 Loss: 0.72229 Time: 0.00305\n",
      "Iteration: 0189 Loss: 0.72385 Time: 0.00194\n",
      "Iteration: 0190 Loss: 0.70084 Time: 0.00400\n",
      "Iteration: 0191 Loss: 0.72353 Time: 0.00307\n",
      "Iteration: 0192 Loss: 0.71176 Time: 0.00397\n",
      "Iteration: 0193 Loss: 0.71661 Time: 0.00297\n",
      "Iteration: 0194 Loss: 0.69975 Time: 0.00302\n",
      "Iteration: 0195 Loss: 0.70087 Time: 0.00198\n",
      "Iteration: 0196 Loss: 0.70438 Time: 0.00300\n",
      "Iteration: 0197 Loss: 0.70397 Time: 0.00311\n",
      "Iteration: 0198 Loss: 0.68303 Time: 0.00303\n",
      "Iteration: 0199 Loss: 0.69585 Time: 0.00298\n",
      "Iteration: 0200 Loss: 0.68607 Time: 0.00308\n",
      "Done! \n",
      "\n",
      "STEP 3/3 - MODEL EVALUATION \n",
      "\n",
      "Computing the final embedding vectors, for evaluation\n",
      "Done! \n",
      "\n",
      "Testing: link prediction\n",
      "Done! \n",
      "\n",
      "EXPERIMENTS ON MODEL 5 / 10 \n",
      "\n",
      "STEP 1/3 - PREPROCESSING STEPS \n",
      "\n",
      "Masking some edges from the training graph, for link prediction\n",
      "(validation set: 1.0 % of edges - test set: 10.0 % of edges)\n",
      "Done! \n",
      "\n",
      "Preprocessing node features\n",
      "Done! \n",
      "\n",
      "Running the Louvain algorithm for community detection\n",
      "as a preprocessing step for the encoder\n",
      "Done! Louvain has found 19 communities \n",
      "\n",
      "Setting up the model and the optimizer\n",
      "Done! \n",
      "\n",
      "Preprocessing on message passing matrices\n",
      "Done! \n",
      "\n",
      "Initializing TF session\n",
      "Done! \n",
      "\n",
      "STEP 2/3 - MODEL TRAINING \n",
      "\n",
      "Starting training\n",
      "Iteration: 0001 Loss: 1.82345 Time: 0.10841\n",
      "Iteration: 0002 Loss: 1.68769 Time: 0.00260\n",
      "Iteration: 0003 Loss: 1.76204 Time: 0.00400\n",
      "Iteration: 0004 Loss: 1.56490 Time: 0.00200\n",
      "Iteration: 0005 Loss: 1.57358 Time: 0.00201\n",
      "Iteration: 0006 Loss: 1.61723 Time: 0.00299\n",
      "Iteration: 0007 Loss: 1.69222 Time: 0.00300\n",
      "Iteration: 0008 Loss: 1.65156 Time: 0.00201\n",
      "Iteration: 0009 Loss: 1.54886 Time: 0.00201\n",
      "Iteration: 0010 Loss: 1.40964 Time: 0.00301\n",
      "Iteration: 0011 Loss: 1.45533 Time: 0.00199\n",
      "Iteration: 0012 Loss: 1.58510 Time: 0.00299\n",
      "Iteration: 0013 Loss: 1.42444 Time: 0.00301\n",
      "Iteration: 0014 Loss: 1.38562 Time: 0.00199\n",
      "Iteration: 0015 Loss: 1.40287 Time: 0.00300\n",
      "Iteration: 0016 Loss: 1.37534 Time: 0.00201\n",
      "Iteration: 0017 Loss: 1.41984 Time: 0.00200\n",
      "Iteration: 0018 Loss: 1.30289 Time: 0.00406\n",
      "Iteration: 0019 Loss: 1.49010 Time: 0.00397\n",
      "Iteration: 0020 Loss: 1.40858 Time: 0.00297\n",
      "Iteration: 0021 Loss: 1.41669 Time: 0.00303\n",
      "Iteration: 0022 Loss: 1.30839 Time: 0.00197\n",
      "Iteration: 0023 Loss: 1.32192 Time: 0.00299\n",
      "Iteration: 0024 Loss: 1.29508 Time: 0.00201\n",
      "Iteration: 0025 Loss: 1.24239 Time: 0.00200\n",
      "Iteration: 0026 Loss: 1.41613 Time: 0.00299\n",
      "Iteration: 0027 Loss: 1.26656 Time: 0.00195\n",
      "Iteration: 0028 Loss: 1.21021 Time: 0.00304\n",
      "Iteration: 0029 Loss: 1.27532 Time: 0.00374\n",
      "Iteration: 0030 Loss: 1.22437 Time: 0.00427\n",
      "Iteration: 0031 Loss: 1.29635 Time: 0.00296\n",
      "Iteration: 0032 Loss: 1.28464 Time: 0.00203\n",
      "Iteration: 0033 Loss: 1.21667 Time: 0.00197\n",
      "Iteration: 0034 Loss: 1.31860 Time: 0.00305\n",
      "Iteration: 0035 Loss: 1.22486 Time: 0.00299\n",
      "Iteration: 0036 Loss: 1.24350 Time: 0.00306\n",
      "Iteration: 0037 Loss: 1.23848 Time: 0.00191\n",
      "Iteration: 0038 Loss: 1.25440 Time: 0.00305\n",
      "Iteration: 0039 Loss: 1.21261 Time: 0.00203\n",
      "Iteration: 0040 Loss: 1.19563 Time: 0.00302\n",
      "Iteration: 0041 Loss: 1.16194 Time: 0.00297\n",
      "Iteration: 0042 Loss: 1.18463 Time: 0.00304\n",
      "Iteration: 0043 Loss: 1.21369 Time: 0.00303\n",
      "Iteration: 0044 Loss: 1.17788 Time: 0.00312\n",
      "Iteration: 0045 Loss: 1.22081 Time: 0.00385\n",
      "Iteration: 0046 Loss: 1.19141 Time: 0.00304\n",
      "Iteration: 0047 Loss: 1.13474 Time: 0.00300\n",
      "Iteration: 0048 Loss: 1.16276 Time: 0.00196\n",
      "Iteration: 0049 Loss: 1.10024 Time: 0.00298\n",
      "Iteration: 0050 Loss: 1.10103 Time: 0.00299\n",
      "Iteration: 0051 Loss: 1.14933 Time: 0.00302\n",
      "Iteration: 0052 Loss: 1.27006 Time: 0.00405\n",
      "Iteration: 0053 Loss: 1.16878 Time: 0.00403\n",
      "Iteration: 0054 Loss: 1.17056 Time: 0.00195\n",
      "Iteration: 0055 Loss: 1.07935 Time: 0.00310\n",
      "Iteration: 0056 Loss: 1.11715 Time: 0.00195\n",
      "Iteration: 0057 Loss: 1.08035 Time: 0.00401\n",
      "Iteration: 0058 Loss: 1.06392 Time: 0.00303\n",
      "Iteration: 0059 Loss: 1.04116 Time: 0.00302\n",
      "Iteration: 0060 Loss: 1.03252 Time: 0.00295\n",
      "Iteration: 0061 Loss: 1.06213 Time: 0.00316\n",
      "Iteration: 0062 Loss: 1.03765 Time: 0.00200\n",
      "Iteration: 0063 Loss: 1.04053 Time: 0.00197\n",
      "Iteration: 0064 Loss: 1.08101 Time: 0.00200\n",
      "Iteration: 0065 Loss: 1.08712 Time: 0.00304\n",
      "Iteration: 0066 Loss: 1.11595 Time: 0.00300\n",
      "Iteration: 0067 Loss: 1.06135 Time: 0.00299\n",
      "Iteration: 0068 Loss: 1.00685 Time: 0.00395\n",
      "Iteration: 0069 Loss: 1.08173 Time: 0.00204\n",
      "Iteration: 0070 Loss: 1.02814 Time: 0.00305\n",
      "Iteration: 0071 Loss: 1.03412 Time: 0.00297\n",
      "Iteration: 0072 Loss: 1.04178 Time: 0.00202\n",
      "Iteration: 0073 Loss: 0.92519 Time: 0.00305\n",
      "Iteration: 0074 Loss: 0.97211 Time: 0.00194\n",
      "Iteration: 0075 Loss: 1.04046 Time: 0.00208\n",
      "Iteration: 0076 Loss: 0.97400 Time: 0.00287\n",
      "Iteration: 0077 Loss: 1.05197 Time: 0.00302\n",
      "Iteration: 0078 Loss: 0.97843 Time: 0.00194\n",
      "Iteration: 0079 Loss: 1.07029 Time: 0.00302\n",
      "Iteration: 0080 Loss: 0.94466 Time: 0.00404\n",
      "Iteration: 0081 Loss: 0.99809 Time: 0.00299\n",
      "Iteration: 0082 Loss: 0.96958 Time: 0.00204\n",
      "Iteration: 0083 Loss: 0.99425 Time: 0.00304\n",
      "Iteration: 0084 Loss: 0.99914 Time: 0.00292\n",
      "Iteration: 0085 Loss: 0.99712 Time: 0.00208\n",
      "Iteration: 0086 Loss: 0.97014 Time: 0.00297\n",
      "Iteration: 0087 Loss: 1.00634 Time: 0.00299\n",
      "Iteration: 0088 Loss: 0.97021 Time: 0.00203\n",
      "Iteration: 0089 Loss: 0.97752 Time: 0.00203\n",
      "Iteration: 0090 Loss: 0.95897 Time: 0.00407\n",
      "Iteration: 0091 Loss: 0.95234 Time: 0.00285\n",
      "Iteration: 0092 Loss: 0.95854 Time: 0.00301\n",
      "Iteration: 0093 Loss: 0.92251 Time: 0.00300\n",
      "Iteration: 0094 Loss: 0.96154 Time: 0.00302\n",
      "Iteration: 0095 Loss: 0.99517 Time: 0.00201\n",
      "Iteration: 0096 Loss: 0.99928 Time: 0.00300\n",
      "Iteration: 0097 Loss: 0.90472 Time: 0.00200\n",
      "Iteration: 0098 Loss: 0.90656 Time: 0.00197\n",
      "Iteration: 0099 Loss: 0.94149 Time: 0.00203\n",
      "Iteration: 0100 Loss: 0.94744 Time: 0.00300\n",
      "Iteration: 0101 Loss: 0.92886 Time: 0.00401\n",
      "Iteration: 0102 Loss: 0.90758 Time: 0.00302\n",
      "Iteration: 0103 Loss: 0.92643 Time: 0.00303\n",
      "Iteration: 0104 Loss: 0.93105 Time: 0.00197\n",
      "Iteration: 0105 Loss: 0.89485 Time: 0.00306\n",
      "Iteration: 0106 Loss: 0.90622 Time: 0.00202\n",
      "Iteration: 0107 Loss: 0.88545 Time: 0.00311\n",
      "Iteration: 0108 Loss: 0.88691 Time: 0.00343\n",
      "Iteration: 0109 Loss: 0.91137 Time: 0.00193\n",
      "Iteration: 0110 Loss: 0.89990 Time: 0.00300\n",
      "Iteration: 0111 Loss: 0.91218 Time: 0.00300\n",
      "Iteration: 0112 Loss: 0.96481 Time: 0.00394\n",
      "Iteration: 0113 Loss: 0.89712 Time: 0.00306\n",
      "Iteration: 0114 Loss: 0.87706 Time: 0.00193\n",
      "Iteration: 0115 Loss: 0.87213 Time: 0.00206\n",
      "Iteration: 0116 Loss: 0.89479 Time: 0.00201\n",
      "Iteration: 0117 Loss: 0.84732 Time: 0.00303\n",
      "Iteration: 0118 Loss: 0.89355 Time: 0.00298\n",
      "Iteration: 0119 Loss: 0.88112 Time: 0.00306\n",
      "Iteration: 0120 Loss: 0.88750 Time: 0.00300\n",
      "Iteration: 0121 Loss: 0.87929 Time: 0.00196\n",
      "Iteration: 0122 Loss: 0.88170 Time: 0.00301\n",
      "Iteration: 0123 Loss: 0.86779 Time: 0.00297\n",
      "Iteration: 0124 Loss: 0.89298 Time: 0.00299\n",
      "Iteration: 0125 Loss: 0.85492 Time: 0.00301\n",
      "Iteration: 0126 Loss: 0.88332 Time: 0.00201\n",
      "Iteration: 0127 Loss: 0.86085 Time: 0.00195\n",
      "Iteration: 0128 Loss: 0.86474 Time: 0.00307\n",
      "Iteration: 0129 Loss: 0.83661 Time: 0.00195\n",
      "Iteration: 0130 Loss: 0.85919 Time: 0.00307\n",
      "Iteration: 0131 Loss: 0.86064 Time: 0.00197\n",
      "Iteration: 0132 Loss: 0.86077 Time: 0.00311\n",
      "Iteration: 0133 Loss: 0.82306 Time: 0.00294\n",
      "Iteration: 0134 Loss: 0.83277 Time: 0.00298\n",
      "Iteration: 0135 Loss: 0.84830 Time: 0.00295\n",
      "Iteration: 0136 Loss: 0.82325 Time: 0.00401\n",
      "Iteration: 0137 Loss: 0.77937 Time: 0.00452\n",
      "Iteration: 0138 Loss: 0.85804 Time: 0.00250\n",
      "Iteration: 0139 Loss: 0.80704 Time: 0.00201\n",
      "Iteration: 0140 Loss: 0.84238 Time: 0.00316\n",
      "Iteration: 0141 Loss: 0.82825 Time: 0.00200\n",
      "Iteration: 0142 Loss: 0.81951 Time: 0.00300\n",
      "Iteration: 0143 Loss: 0.78494 Time: 0.00301\n",
      "Iteration: 0144 Loss: 0.78642 Time: 0.00306\n",
      "Iteration: 0145 Loss: 0.81254 Time: 0.00197\n",
      "Iteration: 0146 Loss: 0.81432 Time: 0.00307\n",
      "Iteration: 0147 Loss: 0.81244 Time: 0.00331\n",
      "Iteration: 0148 Loss: 0.81365 Time: 0.00300\n",
      "Iteration: 0149 Loss: 0.78093 Time: 0.00305\n",
      "Iteration: 0150 Loss: 0.76068 Time: 0.00295\n",
      "Iteration: 0151 Loss: 0.81816 Time: 0.00406\n",
      "Iteration: 0152 Loss: 0.79251 Time: 0.00299\n",
      "Iteration: 0153 Loss: 0.80684 Time: 0.00297\n",
      "Iteration: 0154 Loss: 0.76630 Time: 0.00199\n",
      "Iteration: 0155 Loss: 0.81214 Time: 0.00205\n",
      "Iteration: 0156 Loss: 0.76864 Time: 0.00306\n",
      "Iteration: 0157 Loss: 0.79398 Time: 0.00303\n",
      "Iteration: 0158 Loss: 0.76637 Time: 0.00201\n",
      "Iteration: 0159 Loss: 0.78795 Time: 0.00296\n",
      "Iteration: 0160 Loss: 0.80290 Time: 0.00303\n",
      "Iteration: 0161 Loss: 0.76302 Time: 0.00293\n",
      "Iteration: 0162 Loss: 0.76004 Time: 0.00305\n",
      "Iteration: 0163 Loss: 0.73265 Time: 0.00302\n",
      "Iteration: 0164 Loss: 0.78741 Time: 0.00294\n",
      "Iteration: 0165 Loss: 0.77480 Time: 0.00305\n",
      "Iteration: 0166 Loss: 0.77733 Time: 0.00301\n",
      "Iteration: 0167 Loss: 0.78243 Time: 0.00290\n",
      "Iteration: 0168 Loss: 0.73583 Time: 0.00311\n",
      "Iteration: 0169 Loss: 0.72170 Time: 0.00207\n",
      "Iteration: 0170 Loss: 0.74428 Time: 0.00398\n",
      "Iteration: 0171 Loss: 0.76543 Time: 0.00398\n",
      "Iteration: 0172 Loss: 0.73526 Time: 0.00300\n",
      "Iteration: 0173 Loss: 0.76972 Time: 0.00303\n",
      "Iteration: 0174 Loss: 0.74159 Time: 0.00294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0175 Loss: 0.73543 Time: 0.00220\n",
      "Iteration: 0176 Loss: 0.74207 Time: 0.00194\n",
      "Iteration: 0177 Loss: 0.75559 Time: 0.00409\n",
      "Iteration: 0178 Loss: 0.71724 Time: 0.00291\n",
      "Iteration: 0179 Loss: 0.73937 Time: 0.00312\n",
      "Iteration: 0180 Loss: 0.70777 Time: 0.00404\n",
      "Iteration: 0181 Loss: 0.74477 Time: 0.00284\n",
      "Iteration: 0182 Loss: 0.72412 Time: 0.00203\n",
      "Iteration: 0183 Loss: 0.72267 Time: 0.00398\n",
      "Iteration: 0184 Loss: 0.72382 Time: 0.00301\n",
      "Iteration: 0185 Loss: 0.72903 Time: 0.00299\n",
      "Iteration: 0186 Loss: 0.75234 Time: 0.00210\n",
      "Iteration: 0187 Loss: 0.71873 Time: 0.00290\n",
      "Iteration: 0188 Loss: 0.69953 Time: 0.00298\n",
      "Iteration: 0189 Loss: 0.70772 Time: 0.00349\n",
      "Iteration: 0190 Loss: 0.73573 Time: 0.00262\n",
      "Iteration: 0191 Loss: 0.70263 Time: 0.00315\n",
      "Iteration: 0192 Loss: 0.71237 Time: 0.00287\n",
      "Iteration: 0193 Loss: 0.73507 Time: 0.00195\n",
      "Iteration: 0194 Loss: 0.68800 Time: 0.00306\n",
      "Iteration: 0195 Loss: 0.74457 Time: 0.00199\n",
      "Iteration: 0196 Loss: 0.69869 Time: 0.00300\n",
      "Iteration: 0197 Loss: 0.70299 Time: 0.00300\n",
      "Iteration: 0198 Loss: 0.69988 Time: 0.00394\n",
      "Iteration: 0199 Loss: 0.70156 Time: 0.00300\n",
      "Iteration: 0200 Loss: 0.68053 Time: 0.00200\n",
      "Done! \n",
      "\n",
      "STEP 3/3 - MODEL EVALUATION \n",
      "\n",
      "Computing the final embedding vectors, for evaluation\n",
      "Done! \n",
      "\n",
      "Testing: link prediction\n",
      "Done! \n",
      "\n",
      "EXPERIMENTS ON MODEL 6 / 10 \n",
      "\n",
      "STEP 1/3 - PREPROCESSING STEPS \n",
      "\n",
      "Masking some edges from the training graph, for link prediction\n",
      "(validation set: 1.0 % of edges - test set: 10.0 % of edges)\n",
      "Done! \n",
      "\n",
      "Preprocessing node features\n",
      "Done! \n",
      "\n",
      "Running the Louvain algorithm for community detection\n",
      "as a preprocessing step for the encoder\n",
      "Done! Louvain has found 24 communities \n",
      "\n",
      "Setting up the model and the optimizer\n",
      "Done! \n",
      "\n",
      "Preprocessing on message passing matrices\n",
      "Done! \n",
      "\n",
      "Initializing TF session\n",
      "Done! \n",
      "\n",
      "STEP 2/3 - MODEL TRAINING \n",
      "\n",
      "Starting training\n",
      "Iteration: 0001 Loss: 1.64618 Time: 0.11855\n",
      "Iteration: 0002 Loss: 1.69474 Time: 0.00360\n",
      "Iteration: 0003 Loss: 1.71215 Time: 0.00395\n",
      "Iteration: 0004 Loss: 1.69301 Time: 0.00189\n",
      "Iteration: 0005 Loss: 1.67240 Time: 0.00196\n",
      "Iteration: 0006 Loss: 1.56188 Time: 0.00392\n",
      "Iteration: 0007 Loss: 1.51785 Time: 0.00204\n",
      "Iteration: 0008 Loss: 1.49626 Time: 0.00198\n",
      "Iteration: 0009 Loss: 1.63244 Time: 0.00193\n",
      "Iteration: 0010 Loss: 1.49261 Time: 0.00312\n",
      "Iteration: 0011 Loss: 1.45283 Time: 0.00291\n",
      "Iteration: 0012 Loss: 1.40422 Time: 0.00206\n",
      "Iteration: 0013 Loss: 1.50165 Time: 0.00200\n",
      "Iteration: 0014 Loss: 1.48748 Time: 0.00301\n",
      "Iteration: 0015 Loss: 1.34616 Time: 0.00206\n",
      "Iteration: 0016 Loss: 1.49314 Time: 0.00300\n",
      "Iteration: 0017 Loss: 1.46602 Time: 0.00300\n",
      "Iteration: 0018 Loss: 1.39726 Time: 0.00206\n",
      "Iteration: 0019 Loss: 1.41310 Time: 0.00195\n",
      "Iteration: 0020 Loss: 1.37637 Time: 0.00305\n",
      "Iteration: 0021 Loss: 1.34854 Time: 0.00295\n",
      "Iteration: 0022 Loss: 1.34487 Time: 0.00301\n",
      "Iteration: 0023 Loss: 1.45591 Time: 0.00306\n",
      "Iteration: 0024 Loss: 1.29712 Time: 0.00300\n",
      "Iteration: 0025 Loss: 1.26688 Time: 0.00294\n",
      "Iteration: 0026 Loss: 1.29508 Time: 0.00302\n",
      "Iteration: 0027 Loss: 1.30395 Time: 0.00294\n",
      "Iteration: 0028 Loss: 1.36259 Time: 0.00305\n",
      "Iteration: 0029 Loss: 1.28052 Time: 0.00204\n",
      "Iteration: 0030 Loss: 1.23707 Time: 0.00301\n",
      "Iteration: 0031 Loss: 1.29369 Time: 0.00313\n",
      "Iteration: 0032 Loss: 1.28808 Time: 0.00201\n",
      "Iteration: 0033 Loss: 1.25332 Time: 0.00200\n",
      "Iteration: 0034 Loss: 1.24881 Time: 0.00302\n",
      "Iteration: 0035 Loss: 1.22083 Time: 0.00295\n",
      "Iteration: 0036 Loss: 1.20534 Time: 0.00399\n",
      "Iteration: 0037 Loss: 1.18329 Time: 0.00206\n",
      "Iteration: 0038 Loss: 1.26352 Time: 0.00295\n",
      "Iteration: 0039 Loss: 1.19893 Time: 0.00206\n",
      "Iteration: 0040 Loss: 1.20699 Time: 0.00200\n",
      "Iteration: 0041 Loss: 1.16835 Time: 0.00304\n",
      "Iteration: 0042 Loss: 1.15750 Time: 0.00201\n",
      "Iteration: 0043 Loss: 1.17659 Time: 0.00204\n",
      "Iteration: 0044 Loss: 1.19743 Time: 0.00295\n",
      "Iteration: 0045 Loss: 1.14190 Time: 0.00306\n",
      "Iteration: 0046 Loss: 1.10636 Time: 0.00403\n",
      "Iteration: 0047 Loss: 1.12328 Time: 0.00296\n",
      "Iteration: 0048 Loss: 1.11308 Time: 0.00300\n",
      "Iteration: 0049 Loss: 1.11488 Time: 0.00202\n",
      "Iteration: 0050 Loss: 1.15280 Time: 0.00296\n",
      "Iteration: 0051 Loss: 1.05833 Time: 0.00307\n",
      "Iteration: 0052 Loss: 1.14148 Time: 0.00207\n",
      "Iteration: 0053 Loss: 1.08080 Time: 0.00197\n",
      "Iteration: 0054 Loss: 1.12214 Time: 0.00205\n",
      "Iteration: 0055 Loss: 1.08951 Time: 0.00195\n",
      "Iteration: 0056 Loss: 1.07912 Time: 0.00207\n",
      "Iteration: 0057 Loss: 1.06979 Time: 0.00295\n",
      "Iteration: 0058 Loss: 1.06838 Time: 0.00200\n",
      "Iteration: 0059 Loss: 1.11945 Time: 0.00307\n",
      "Iteration: 0060 Loss: 1.03308 Time: 0.00300\n",
      "Iteration: 0061 Loss: 1.05544 Time: 0.00302\n",
      "Iteration: 0062 Loss: 1.15570 Time: 0.00291\n",
      "Iteration: 0063 Loss: 1.10890 Time: 0.00408\n",
      "Iteration: 0064 Loss: 1.08241 Time: 0.00195\n",
      "Iteration: 0065 Loss: 1.03618 Time: 0.00200\n",
      "Iteration: 0066 Loss: 1.04286 Time: 0.00305\n",
      "Iteration: 0067 Loss: 1.06922 Time: 0.00196\n",
      "Iteration: 0068 Loss: 1.03429 Time: 0.00193\n",
      "Iteration: 0069 Loss: 1.02202 Time: 0.00204\n",
      "Iteration: 0070 Loss: 0.99774 Time: 0.00196\n",
      "Iteration: 0071 Loss: 0.99526 Time: 0.00201\n",
      "Iteration: 0072 Loss: 0.97535 Time: 0.00298\n",
      "Iteration: 0073 Loss: 1.02018 Time: 0.00297\n",
      "Iteration: 0074 Loss: 1.07581 Time: 0.00308\n",
      "Iteration: 0075 Loss: 0.98612 Time: 0.00392\n",
      "Iteration: 0076 Loss: 1.02310 Time: 0.00400\n",
      "Iteration: 0077 Loss: 0.99831 Time: 0.00201\n",
      "Iteration: 0078 Loss: 1.01062 Time: 0.00390\n",
      "Iteration: 0079 Loss: 1.00758 Time: 0.00306\n",
      "Iteration: 0080 Loss: 0.96003 Time: 0.00301\n",
      "Iteration: 0081 Loss: 0.93924 Time: 0.00296\n",
      "Iteration: 0082 Loss: 1.00783 Time: 0.00200\n",
      "Iteration: 0083 Loss: 1.04013 Time: 0.00202\n",
      "Iteration: 0084 Loss: 1.00173 Time: 0.00401\n",
      "Iteration: 0085 Loss: 1.03245 Time: 0.00190\n",
      "Iteration: 0086 Loss: 1.01609 Time: 0.00306\n",
      "Iteration: 0087 Loss: 0.99755 Time: 0.00196\n",
      "Iteration: 0088 Loss: 1.00985 Time: 0.00399\n",
      "Iteration: 0089 Loss: 0.93455 Time: 0.00300\n",
      "Iteration: 0090 Loss: 0.91071 Time: 0.00196\n",
      "Iteration: 0091 Loss: 0.93267 Time: 0.00204\n",
      "Iteration: 0092 Loss: 0.95868 Time: 0.00394\n",
      "Iteration: 0093 Loss: 0.97393 Time: 0.00303\n",
      "Iteration: 0094 Loss: 0.94367 Time: 0.00306\n",
      "Iteration: 0095 Loss: 0.96235 Time: 0.00309\n",
      "Iteration: 0096 Loss: 0.96626 Time: 0.00289\n",
      "Iteration: 0097 Loss: 0.95917 Time: 0.00303\n",
      "Iteration: 0098 Loss: 0.94092 Time: 0.00197\n",
      "Iteration: 0099 Loss: 0.90271 Time: 0.00200\n",
      "Iteration: 0100 Loss: 0.90857 Time: 0.00203\n",
      "Iteration: 0101 Loss: 0.92868 Time: 0.00401\n",
      "Iteration: 0102 Loss: 0.95905 Time: 0.00298\n",
      "Iteration: 0103 Loss: 0.91610 Time: 0.00401\n",
      "Iteration: 0104 Loss: 0.90987 Time: 0.00307\n",
      "Iteration: 0105 Loss: 0.90715 Time: 0.00291\n",
      "Iteration: 0106 Loss: 0.92278 Time: 0.00299\n",
      "Iteration: 0107 Loss: 0.88092 Time: 0.00196\n",
      "Iteration: 0108 Loss: 0.90059 Time: 0.00300\n",
      "Iteration: 0109 Loss: 0.93773 Time: 0.00308\n",
      "Iteration: 0110 Loss: 0.85886 Time: 0.00300\n",
      "Iteration: 0111 Loss: 0.87290 Time: 0.00302\n",
      "Iteration: 0112 Loss: 0.91204 Time: 0.00295\n",
      "Iteration: 0113 Loss: 0.90960 Time: 0.00200\n",
      "Iteration: 0114 Loss: 0.89138 Time: 0.00410\n",
      "Iteration: 0115 Loss: 0.91729 Time: 0.00290\n",
      "Iteration: 0116 Loss: 0.91616 Time: 0.00201\n",
      "Iteration: 0117 Loss: 0.90881 Time: 0.00195\n",
      "Iteration: 0118 Loss: 0.85662 Time: 0.00307\n",
      "Iteration: 0119 Loss: 0.86523 Time: 0.00294\n",
      "Iteration: 0120 Loss: 0.90760 Time: 0.00201\n",
      "Iteration: 0121 Loss: 0.85817 Time: 0.00295\n",
      "Iteration: 0122 Loss: 0.85611 Time: 0.00293\n",
      "Iteration: 0123 Loss: 0.86327 Time: 0.00357\n",
      "Iteration: 0124 Loss: 0.86904 Time: 0.00199\n",
      "Iteration: 0125 Loss: 0.87328 Time: 0.00302\n",
      "Iteration: 0126 Loss: 0.86567 Time: 0.00300\n",
      "Iteration: 0127 Loss: 0.87046 Time: 0.00308\n",
      "Iteration: 0128 Loss: 0.84398 Time: 0.00297\n",
      "Iteration: 0129 Loss: 0.87454 Time: 0.00295\n",
      "Iteration: 0130 Loss: 0.85968 Time: 0.00313\n",
      "Iteration: 0131 Loss: 0.85382 Time: 0.00291\n",
      "Iteration: 0132 Loss: 0.82898 Time: 0.00297\n",
      "Iteration: 0133 Loss: 0.83519 Time: 0.00294\n",
      "Iteration: 0134 Loss: 0.83026 Time: 0.00201\n",
      "Iteration: 0135 Loss: 0.81201 Time: 0.00411\n",
      "Iteration: 0136 Loss: 0.82126 Time: 0.00188\n",
      "Iteration: 0137 Loss: 0.80788 Time: 0.00301\n",
      "Iteration: 0138 Loss: 0.83437 Time: 0.00306\n",
      "Iteration: 0139 Loss: 0.84200 Time: 0.00301\n",
      "Iteration: 0140 Loss: 0.84053 Time: 0.00294\n",
      "Iteration: 0141 Loss: 0.78520 Time: 0.00300\n",
      "Iteration: 0142 Loss: 0.81254 Time: 0.00211\n",
      "Iteration: 0143 Loss: 0.80255 Time: 0.00300\n",
      "Iteration: 0144 Loss: 0.81159 Time: 0.00199\n",
      "Iteration: 0145 Loss: 0.79218 Time: 0.00204\n",
      "Iteration: 0146 Loss: 0.79289 Time: 0.00200\n",
      "Iteration: 0147 Loss: 0.81044 Time: 0.00304\n",
      "Iteration: 0148 Loss: 0.78902 Time: 0.00299\n",
      "Iteration: 0149 Loss: 0.77119 Time: 0.00197\n",
      "Iteration: 0150 Loss: 0.78180 Time: 0.00200\n",
      "Iteration: 0151 Loss: 0.77946 Time: 0.00205\n",
      "Iteration: 0152 Loss: 0.78303 Time: 0.00302\n",
      "Iteration: 0153 Loss: 0.76181 Time: 0.00196\n",
      "Iteration: 0154 Loss: 0.75974 Time: 0.00444\n",
      "Iteration: 0155 Loss: 0.77235 Time: 0.00259\n",
      "Iteration: 0156 Loss: 0.78596 Time: 0.00299\n",
      "Iteration: 0157 Loss: 0.77206 Time: 0.00300\n",
      "Iteration: 0158 Loss: 0.77498 Time: 0.00396\n",
      "Iteration: 0159 Loss: 0.76055 Time: 0.00309\n",
      "Iteration: 0160 Loss: 0.74803 Time: 0.00295\n",
      "Iteration: 0161 Loss: 0.82810 Time: 0.00296\n",
      "Iteration: 0162 Loss: 0.75111 Time: 0.00200\n",
      "Iteration: 0163 Loss: 0.76747 Time: 0.00302\n",
      "Iteration: 0164 Loss: 0.72325 Time: 0.00292\n",
      "Iteration: 0165 Loss: 0.75318 Time: 0.00404\n",
      "Iteration: 0166 Loss: 0.74041 Time: 0.00199\n",
      "Iteration: 0167 Loss: 0.73752 Time: 0.00306\n",
      "Iteration: 0168 Loss: 0.75168 Time: 0.00299\n",
      "Iteration: 0169 Loss: 0.76077 Time: 0.00296\n",
      "Iteration: 0170 Loss: 0.76426 Time: 0.00299\n",
      "Iteration: 0171 Loss: 0.73556 Time: 0.00197\n",
      "Iteration: 0172 Loss: 0.75657 Time: 0.00207\n",
      "Iteration: 0173 Loss: 0.75749 Time: 0.00204\n",
      "Iteration: 0174 Loss: 0.75513 Time: 0.00300\n",
      "Iteration: 0175 Loss: 0.72211 Time: 0.00196\n",
      "Iteration: 0176 Loss: 0.72144 Time: 0.00211\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0177 Loss: 0.75718 Time: 0.00198\n",
      "Iteration: 0178 Loss: 0.74130 Time: 0.00282\n",
      "Iteration: 0179 Loss: 0.74160 Time: 0.00399\n",
      "Iteration: 0180 Loss: 0.75854 Time: 0.00300\n",
      "Iteration: 0181 Loss: 0.71754 Time: 0.00300\n",
      "Iteration: 0182 Loss: 0.73558 Time: 0.00201\n",
      "Iteration: 0183 Loss: 0.69687 Time: 0.00305\n",
      "Iteration: 0184 Loss: 0.73860 Time: 0.00195\n",
      "Iteration: 0185 Loss: 0.75353 Time: 0.00300\n",
      "Iteration: 0186 Loss: 0.73743 Time: 0.00301\n",
      "Iteration: 0187 Loss: 0.74912 Time: 0.00205\n",
      "Iteration: 0188 Loss: 0.71804 Time: 0.00201\n",
      "Iteration: 0189 Loss: 0.72313 Time: 0.00297\n",
      "Iteration: 0190 Loss: 0.75072 Time: 0.00206\n",
      "Iteration: 0191 Loss: 0.74308 Time: 0.00436\n",
      "Iteration: 0192 Loss: 0.70360 Time: 0.00259\n",
      "Iteration: 0193 Loss: 0.73585 Time: 0.00197\n",
      "Iteration: 0194 Loss: 0.69278 Time: 0.00300\n",
      "Iteration: 0195 Loss: 0.70805 Time: 0.00297\n",
      "Iteration: 0196 Loss: 0.69067 Time: 0.00400\n",
      "Iteration: 0197 Loss: 0.69165 Time: 0.00200\n",
      "Iteration: 0198 Loss: 0.70411 Time: 0.00199\n",
      "Iteration: 0199 Loss: 0.70767 Time: 0.00397\n",
      "Iteration: 0200 Loss: 0.69787 Time: 0.00405\n",
      "Done! \n",
      "\n",
      "STEP 3/3 - MODEL EVALUATION \n",
      "\n",
      "Computing the final embedding vectors, for evaluation\n",
      "Done! \n",
      "\n",
      "Testing: link prediction\n",
      "Done! \n",
      "\n",
      "EXPERIMENTS ON MODEL 7 / 10 \n",
      "\n",
      "STEP 1/3 - PREPROCESSING STEPS \n",
      "\n",
      "Masking some edges from the training graph, for link prediction\n",
      "(validation set: 1.0 % of edges - test set: 10.0 % of edges)\n",
      "Done! \n",
      "\n",
      "Preprocessing node features\n",
      "Done! \n",
      "\n",
      "Running the Louvain algorithm for community detection\n",
      "as a preprocessing step for the encoder\n",
      "Done! Louvain has found 20 communities \n",
      "\n",
      "Setting up the model and the optimizer\n",
      "Done! \n",
      "\n",
      "Preprocessing on message passing matrices\n",
      "Done! \n",
      "\n",
      "Initializing TF session\n",
      "Done! \n",
      "\n",
      "STEP 2/3 - MODEL TRAINING \n",
      "\n",
      "Starting training\n",
      "Iteration: 0001 Loss: 1.74161 Time: 0.13700\n",
      "Iteration: 0002 Loss: 1.72232 Time: 0.00199\n",
      "Iteration: 0003 Loss: 1.72679 Time: 0.00400\n",
      "Iteration: 0004 Loss: 1.51852 Time: 0.00200\n",
      "Iteration: 0005 Loss: 1.60338 Time: 0.00199\n",
      "Iteration: 0006 Loss: 1.58558 Time: 0.00201\n",
      "Iteration: 0007 Loss: 1.73541 Time: 0.00300\n",
      "Iteration: 0008 Loss: 1.62723 Time: 0.00399\n",
      "Iteration: 0009 Loss: 1.68597 Time: 0.00301\n",
      "Iteration: 0010 Loss: 1.52879 Time: 0.00200\n",
      "Iteration: 0011 Loss: 1.59836 Time: 0.00301\n",
      "Iteration: 0012 Loss: 1.48394 Time: 0.00299\n",
      "Iteration: 0013 Loss: 1.46692 Time: 0.00300\n",
      "Iteration: 0014 Loss: 1.51396 Time: 0.00302\n",
      "Iteration: 0015 Loss: 1.55489 Time: 0.00199\n",
      "Iteration: 0016 Loss: 1.50883 Time: 0.00301\n",
      "Iteration: 0017 Loss: 1.33894 Time: 0.00313\n",
      "Iteration: 0018 Loss: 1.53343 Time: 0.00200\n",
      "Iteration: 0019 Loss: 1.37213 Time: 0.00328\n",
      "Iteration: 0020 Loss: 1.33587 Time: 0.00309\n",
      "Iteration: 0021 Loss: 1.38946 Time: 0.00201\n",
      "Iteration: 0022 Loss: 1.46393 Time: 0.00200\n",
      "Iteration: 0023 Loss: 1.37850 Time: 0.00406\n",
      "Iteration: 0024 Loss: 1.40337 Time: 0.00296\n",
      "Iteration: 0025 Loss: 1.27700 Time: 0.00200\n",
      "Iteration: 0026 Loss: 1.36195 Time: 0.00303\n",
      "Iteration: 0027 Loss: 1.39714 Time: 0.00201\n",
      "Iteration: 0028 Loss: 1.23950 Time: 0.00300\n",
      "Iteration: 0029 Loss: 1.31602 Time: 0.00204\n",
      "Iteration: 0030 Loss: 1.33397 Time: 0.00308\n",
      "Iteration: 0031 Loss: 1.20708 Time: 0.00198\n",
      "Iteration: 0032 Loss: 1.30596 Time: 0.00194\n",
      "Iteration: 0033 Loss: 1.18052 Time: 0.00204\n",
      "Iteration: 0034 Loss: 1.16985 Time: 0.00300\n",
      "Iteration: 0035 Loss: 1.29646 Time: 0.00235\n",
      "Iteration: 0036 Loss: 1.15736 Time: 0.00305\n",
      "Iteration: 0037 Loss: 1.14045 Time: 0.00304\n",
      "Iteration: 0038 Loss: 1.22637 Time: 0.00301\n",
      "Iteration: 0039 Loss: 1.17095 Time: 0.00213\n",
      "Iteration: 0040 Loss: 1.31821 Time: 0.00281\n",
      "Iteration: 0041 Loss: 1.19834 Time: 0.00307\n",
      "Iteration: 0042 Loss: 1.15537 Time: 0.00200\n",
      "Iteration: 0043 Loss: 1.23358 Time: 0.00202\n",
      "Iteration: 0044 Loss: 1.19145 Time: 0.00200\n",
      "Iteration: 0045 Loss: 1.12713 Time: 0.00301\n",
      "Iteration: 0046 Loss: 1.08056 Time: 0.00203\n",
      "Iteration: 0047 Loss: 1.16391 Time: 0.00298\n",
      "Iteration: 0048 Loss: 1.22375 Time: 0.00200\n",
      "Iteration: 0049 Loss: 1.17761 Time: 0.00450\n",
      "Iteration: 0050 Loss: 1.13113 Time: 0.00206\n",
      "Iteration: 0051 Loss: 1.13904 Time: 0.00300\n",
      "Iteration: 0052 Loss: 1.11017 Time: 0.00209\n",
      "Iteration: 0053 Loss: 1.08074 Time: 0.00311\n",
      "Iteration: 0054 Loss: 1.12179 Time: 0.00403\n",
      "Iteration: 0055 Loss: 1.12256 Time: 0.00196\n",
      "Iteration: 0056 Loss: 1.16636 Time: 0.00201\n",
      "Iteration: 0057 Loss: 1.08756 Time: 0.00307\n",
      "Iteration: 0058 Loss: 1.11380 Time: 0.00293\n",
      "Iteration: 0059 Loss: 1.11488 Time: 0.00303\n",
      "Iteration: 0060 Loss: 1.07863 Time: 0.00202\n",
      "Iteration: 0061 Loss: 1.14102 Time: 0.00298\n",
      "Iteration: 0062 Loss: 1.08764 Time: 0.00308\n",
      "Iteration: 0063 Loss: 1.08165 Time: 0.00299\n",
      "Iteration: 0064 Loss: 1.11800 Time: 0.00306\n",
      "Iteration: 0065 Loss: 1.08928 Time: 0.00195\n",
      "Iteration: 0066 Loss: 1.01609 Time: 0.00303\n",
      "Iteration: 0067 Loss: 1.06541 Time: 0.00296\n",
      "Iteration: 0068 Loss: 1.00091 Time: 0.00300\n",
      "Iteration: 0069 Loss: 1.08194 Time: 0.00201\n",
      "Iteration: 0070 Loss: 1.04266 Time: 0.00295\n",
      "Iteration: 0071 Loss: 1.03149 Time: 0.00209\n",
      "Iteration: 0072 Loss: 1.05683 Time: 0.00391\n",
      "Iteration: 0073 Loss: 1.09074 Time: 0.00303\n",
      "Iteration: 0074 Loss: 1.05171 Time: 0.00400\n",
      "Iteration: 0075 Loss: 1.00593 Time: 0.00310\n",
      "Iteration: 0076 Loss: 1.03657 Time: 0.00197\n",
      "Iteration: 0077 Loss: 1.01893 Time: 0.00299\n",
      "Iteration: 0078 Loss: 1.05406 Time: 0.00399\n",
      "Iteration: 0079 Loss: 1.01483 Time: 0.00301\n",
      "Iteration: 0080 Loss: 1.04289 Time: 0.00300\n",
      "Iteration: 0081 Loss: 0.98947 Time: 0.00396\n",
      "Iteration: 0082 Loss: 0.99609 Time: 0.00206\n",
      "Iteration: 0083 Loss: 1.00687 Time: 0.00292\n",
      "Iteration: 0084 Loss: 0.99161 Time: 0.00337\n",
      "Iteration: 0085 Loss: 0.99007 Time: 0.00206\n",
      "Iteration: 0086 Loss: 0.97479 Time: 0.00394\n",
      "Iteration: 0087 Loss: 0.99545 Time: 0.00200\n",
      "Iteration: 0088 Loss: 0.94708 Time: 0.00192\n",
      "Iteration: 0089 Loss: 0.96644 Time: 0.00407\n",
      "Iteration: 0090 Loss: 0.93922 Time: 0.00297\n",
      "Iteration: 0091 Loss: 0.96695 Time: 0.00298\n",
      "Iteration: 0092 Loss: 0.93118 Time: 0.00197\n",
      "Iteration: 0093 Loss: 0.94504 Time: 0.00302\n",
      "Iteration: 0094 Loss: 0.98429 Time: 0.00295\n",
      "Iteration: 0095 Loss: 0.98293 Time: 0.00199\n",
      "Iteration: 0096 Loss: 0.91328 Time: 0.00304\n",
      "Iteration: 0097 Loss: 0.94788 Time: 0.00294\n",
      "Iteration: 0098 Loss: 0.93884 Time: 0.00208\n",
      "Iteration: 0099 Loss: 1.00482 Time: 0.00392\n",
      "Iteration: 0100 Loss: 0.90468 Time: 0.00203\n",
      "Iteration: 0101 Loss: 0.90665 Time: 0.00300\n",
      "Iteration: 0102 Loss: 0.93044 Time: 0.00405\n",
      "Iteration: 0103 Loss: 0.89362 Time: 0.00207\n",
      "Iteration: 0104 Loss: 0.89065 Time: 0.00299\n",
      "Iteration: 0105 Loss: 0.92490 Time: 0.00401\n",
      "Iteration: 0106 Loss: 0.89955 Time: 0.00304\n",
      "Iteration: 0107 Loss: 0.91443 Time: 0.00197\n",
      "Iteration: 0108 Loss: 0.92590 Time: 0.00300\n",
      "Iteration: 0109 Loss: 0.93998 Time: 0.00197\n",
      "Iteration: 0110 Loss: 0.87484 Time: 0.00305\n",
      "Iteration: 0111 Loss: 0.92826 Time: 0.00299\n",
      "Iteration: 0112 Loss: 0.90787 Time: 0.00201\n",
      "Iteration: 0113 Loss: 0.88369 Time: 0.00302\n",
      "Iteration: 0114 Loss: 0.90368 Time: 0.00294\n",
      "Iteration: 0115 Loss: 0.88258 Time: 0.00400\n",
      "Iteration: 0116 Loss: 0.85971 Time: 0.00308\n",
      "Iteration: 0117 Loss: 0.89527 Time: 0.00198\n",
      "Iteration: 0118 Loss: 0.86763 Time: 0.00295\n",
      "Iteration: 0119 Loss: 0.90174 Time: 0.00305\n",
      "Iteration: 0120 Loss: 0.86316 Time: 0.00204\n",
      "Iteration: 0121 Loss: 0.85325 Time: 0.00296\n",
      "Iteration: 0122 Loss: 0.88185 Time: 0.00295\n",
      "Iteration: 0123 Loss: 0.89440 Time: 0.00206\n",
      "Iteration: 0124 Loss: 0.87550 Time: 0.00194\n",
      "Iteration: 0125 Loss: 0.88714 Time: 0.00407\n",
      "Iteration: 0126 Loss: 0.87038 Time: 0.00292\n",
      "Iteration: 0127 Loss: 0.85112 Time: 0.00305\n",
      "Iteration: 0128 Loss: 0.83636 Time: 0.00303\n",
      "Iteration: 0129 Loss: 0.88834 Time: 0.00299\n",
      "Iteration: 0130 Loss: 0.84509 Time: 0.00298\n",
      "Iteration: 0131 Loss: 0.86240 Time: 0.00300\n",
      "Iteration: 0132 Loss: 0.87791 Time: 0.00301\n",
      "Iteration: 0133 Loss: 0.82416 Time: 0.00200\n",
      "Iteration: 0134 Loss: 0.83093 Time: 0.00397\n",
      "Iteration: 0135 Loss: 0.85312 Time: 0.00200\n",
      "Iteration: 0136 Loss: 0.84649 Time: 0.00207\n",
      "Iteration: 0137 Loss: 0.82128 Time: 0.00199\n",
      "Iteration: 0138 Loss: 0.83550 Time: 0.00396\n",
      "Iteration: 0139 Loss: 0.82150 Time: 0.00304\n",
      "Iteration: 0140 Loss: 0.83192 Time: 0.00402\n",
      "Iteration: 0141 Loss: 0.80862 Time: 0.00194\n",
      "Iteration: 0142 Loss: 0.81920 Time: 0.00312\n",
      "Iteration: 0143 Loss: 0.80922 Time: 0.00397\n",
      "Iteration: 0144 Loss: 0.80510 Time: 0.00626\n",
      "Iteration: 0145 Loss: 0.83732 Time: 0.00268\n",
      "Iteration: 0146 Loss: 0.83017 Time: 0.00313\n",
      "Iteration: 0147 Loss: 0.81074 Time: 0.00290\n",
      "Iteration: 0148 Loss: 0.83197 Time: 0.00303\n",
      "Iteration: 0149 Loss: 0.79153 Time: 0.00453\n",
      "Iteration: 0150 Loss: 0.80303 Time: 0.00200\n",
      "Iteration: 0151 Loss: 0.79337 Time: 0.00345\n",
      "Iteration: 0152 Loss: 0.78495 Time: 0.00260\n",
      "Iteration: 0153 Loss: 0.82849 Time: 0.00296\n",
      "Iteration: 0154 Loss: 0.81330 Time: 0.00195\n",
      "Iteration: 0155 Loss: 0.79582 Time: 0.00301\n",
      "Iteration: 0156 Loss: 0.78161 Time: 0.00399\n",
      "Iteration: 0157 Loss: 0.76923 Time: 0.00310\n",
      "Iteration: 0158 Loss: 0.80729 Time: 0.00190\n",
      "Iteration: 0159 Loss: 0.77810 Time: 0.00201\n",
      "Iteration: 0160 Loss: 0.77455 Time: 0.00209\n",
      "Iteration: 0161 Loss: 0.79428 Time: 0.00310\n",
      "Iteration: 0162 Loss: 0.78864 Time: 0.00288\n",
      "Iteration: 0163 Loss: 0.78703 Time: 0.00294\n",
      "Iteration: 0164 Loss: 0.75568 Time: 0.00204\n",
      "Iteration: 0165 Loss: 0.76537 Time: 0.00296\n",
      "Iteration: 0166 Loss: 0.78093 Time: 0.00304\n",
      "Iteration: 0167 Loss: 0.76692 Time: 0.00202\n",
      "Iteration: 0168 Loss: 0.74574 Time: 0.00303\n",
      "Iteration: 0169 Loss: 0.75327 Time: 0.00291\n",
      "Iteration: 0170 Loss: 0.79457 Time: 0.00200\n",
      "Iteration: 0171 Loss: 0.73609 Time: 0.00306\n",
      "Iteration: 0172 Loss: 0.73274 Time: 0.00294\n",
      "Iteration: 0173 Loss: 0.76066 Time: 0.00303\n",
      "Iteration: 0174 Loss: 0.75407 Time: 0.00306\n",
      "Iteration: 0175 Loss: 0.76666 Time: 0.00294\n",
      "Iteration: 0176 Loss: 0.74131 Time: 0.00206\n",
      "Iteration: 0177 Loss: 0.75868 Time: 0.00297\n",
      "Iteration: 0178 Loss: 0.72685 Time: 0.00197\n",
      "Iteration: 0179 Loss: 0.72031 Time: 0.00304\n",
      "Iteration: 0180 Loss: 0.74214 Time: 0.00321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0181 Loss: 0.72231 Time: 0.00285\n",
      "Iteration: 0182 Loss: 0.70142 Time: 0.00317\n",
      "Iteration: 0183 Loss: 0.73262 Time: 0.00287\n",
      "Iteration: 0184 Loss: 0.73095 Time: 0.00197\n",
      "Iteration: 0185 Loss: 0.74335 Time: 0.00297\n",
      "Iteration: 0186 Loss: 0.72442 Time: 0.00305\n",
      "Iteration: 0187 Loss: 0.72104 Time: 0.00301\n",
      "Iteration: 0188 Loss: 0.68962 Time: 0.00293\n",
      "Iteration: 0189 Loss: 0.73893 Time: 0.00312\n",
      "Iteration: 0190 Loss: 0.71384 Time: 0.00301\n",
      "Iteration: 0191 Loss: 0.68970 Time: 0.00396\n",
      "Iteration: 0192 Loss: 0.72232 Time: 0.00199\n",
      "Iteration: 0193 Loss: 0.72008 Time: 0.00300\n",
      "Iteration: 0194 Loss: 0.70220 Time: 0.00383\n",
      "Iteration: 0195 Loss: 0.69502 Time: 0.00400\n",
      "Iteration: 0196 Loss: 0.71945 Time: 0.00306\n",
      "Iteration: 0197 Loss: 0.72278 Time: 0.00194\n",
      "Iteration: 0198 Loss: 0.71162 Time: 0.00300\n",
      "Iteration: 0199 Loss: 0.70973 Time: 0.00299\n",
      "Iteration: 0200 Loss: 0.68190 Time: 0.00304\n",
      "Done! \n",
      "\n",
      "STEP 3/3 - MODEL EVALUATION \n",
      "\n",
      "Computing the final embedding vectors, for evaluation\n",
      "Done! \n",
      "\n",
      "Testing: link prediction\n",
      "Done! \n",
      "\n",
      "EXPERIMENTS ON MODEL 8 / 10 \n",
      "\n",
      "STEP 1/3 - PREPROCESSING STEPS \n",
      "\n",
      "Masking some edges from the training graph, for link prediction\n",
      "(validation set: 1.0 % of edges - test set: 10.0 % of edges)\n",
      "Done! \n",
      "\n",
      "Preprocessing node features\n",
      "Done! \n",
      "\n",
      "Running the Louvain algorithm for community detection\n",
      "as a preprocessing step for the encoder\n",
      "Done! Louvain has found 16 communities \n",
      "\n",
      "Setting up the model and the optimizer\n",
      "Done! \n",
      "\n",
      "Preprocessing on message passing matrices\n",
      "Done! \n",
      "\n",
      "Initializing TF session\n",
      "Done! \n",
      "\n",
      "STEP 2/3 - MODEL TRAINING \n",
      "\n",
      "Starting training\n",
      "Iteration: 0001 Loss: 1.90095 Time: 0.13899\n",
      "Iteration: 0002 Loss: 1.69152 Time: 0.00401\n",
      "Iteration: 0003 Loss: 1.79382 Time: 0.00201\n",
      "Iteration: 0004 Loss: 1.47595 Time: 0.00299\n",
      "Iteration: 0005 Loss: 1.77316 Time: 0.00299\n",
      "Iteration: 0006 Loss: 1.53707 Time: 0.00303\n",
      "Iteration: 0007 Loss: 1.52319 Time: 0.00397\n",
      "Iteration: 0008 Loss: 1.71013 Time: 0.00400\n",
      "Iteration: 0009 Loss: 1.59401 Time: 0.00300\n",
      "Iteration: 0010 Loss: 1.52370 Time: 0.00199\n",
      "Iteration: 0011 Loss: 1.59839 Time: 0.00400\n",
      "Iteration: 0012 Loss: 1.34571 Time: 0.00300\n",
      "Iteration: 0013 Loss: 1.52024 Time: 0.00300\n",
      "Iteration: 0014 Loss: 1.60752 Time: 0.00201\n",
      "Iteration: 0015 Loss: 1.55663 Time: 0.00300\n",
      "Iteration: 0016 Loss: 1.43814 Time: 0.00305\n",
      "Iteration: 0017 Loss: 1.38781 Time: 0.00304\n",
      "Iteration: 0018 Loss: 1.42780 Time: 0.00309\n",
      "Iteration: 0019 Loss: 1.43191 Time: 0.00198\n",
      "Iteration: 0020 Loss: 1.29167 Time: 0.00305\n",
      "Iteration: 0021 Loss: 1.27969 Time: 0.00197\n",
      "Iteration: 0022 Loss: 1.29495 Time: 0.00300\n",
      "Iteration: 0023 Loss: 1.37797 Time: 0.00297\n",
      "Iteration: 0024 Loss: 1.30060 Time: 0.00201\n",
      "Iteration: 0025 Loss: 1.38071 Time: 0.00297\n",
      "Iteration: 0026 Loss: 1.35695 Time: 0.00305\n",
      "Iteration: 0027 Loss: 1.36373 Time: 0.00395\n",
      "Iteration: 0028 Loss: 1.30388 Time: 0.00201\n",
      "Iteration: 0029 Loss: 1.32340 Time: 0.00207\n",
      "Iteration: 0030 Loss: 1.34566 Time: 0.00397\n",
      "Iteration: 0031 Loss: 1.29045 Time: 0.00297\n",
      "Iteration: 0032 Loss: 1.26042 Time: 0.00405\n",
      "Iteration: 0033 Loss: 1.20766 Time: 0.00158\n",
      "Iteration: 0034 Loss: 1.22082 Time: 0.00310\n",
      "Iteration: 0035 Loss: 1.24745 Time: 0.00286\n",
      "Iteration: 0036 Loss: 1.28910 Time: 0.00211\n",
      "Iteration: 0037 Loss: 1.19260 Time: 0.00391\n",
      "Iteration: 0038 Loss: 1.39254 Time: 0.00404\n",
      "Iteration: 0039 Loss: 1.16315 Time: 0.00291\n",
      "Iteration: 0040 Loss: 1.20707 Time: 0.00306\n",
      "Iteration: 0041 Loss: 1.14875 Time: 0.00286\n",
      "Iteration: 0042 Loss: 1.20405 Time: 0.00208\n",
      "Iteration: 0043 Loss: 1.20739 Time: 0.00345\n",
      "Iteration: 0044 Loss: 1.14949 Time: 0.00307\n",
      "Iteration: 0045 Loss: 1.18650 Time: 0.00293\n",
      "Iteration: 0046 Loss: 1.13930 Time: 0.00300\n",
      "Iteration: 0047 Loss: 1.20903 Time: 0.00201\n",
      "Iteration: 0048 Loss: 1.12454 Time: 0.00201\n",
      "Iteration: 0049 Loss: 1.22776 Time: 0.00302\n",
      "Iteration: 0050 Loss: 1.10451 Time: 0.00196\n",
      "Iteration: 0051 Loss: 1.17885 Time: 0.00511\n",
      "Iteration: 0052 Loss: 1.13624 Time: 0.00291\n",
      "Iteration: 0053 Loss: 1.15047 Time: 0.00297\n",
      "Iteration: 0054 Loss: 1.06473 Time: 0.00302\n",
      "Iteration: 0055 Loss: 1.09489 Time: 0.00193\n",
      "Iteration: 0056 Loss: 1.16565 Time: 0.00306\n",
      "Iteration: 0057 Loss: 1.06025 Time: 0.00293\n",
      "Iteration: 0058 Loss: 1.07564 Time: 0.00402\n",
      "Iteration: 0059 Loss: 1.12265 Time: 0.00203\n",
      "Iteration: 0060 Loss: 1.12206 Time: 0.00299\n",
      "Iteration: 0061 Loss: 1.05801 Time: 0.00200\n",
      "Iteration: 0062 Loss: 1.01340 Time: 0.00301\n",
      "Iteration: 0063 Loss: 1.09585 Time: 0.00298\n",
      "Iteration: 0064 Loss: 1.01136 Time: 0.00298\n",
      "Iteration: 0065 Loss: 1.07665 Time: 0.00303\n",
      "Iteration: 0066 Loss: 1.04966 Time: 0.00202\n",
      "Iteration: 0067 Loss: 1.07659 Time: 0.00402\n",
      "Iteration: 0068 Loss: 1.04758 Time: 0.00210\n",
      "Iteration: 0069 Loss: 1.04921 Time: 0.00345\n",
      "Iteration: 0070 Loss: 1.08175 Time: 0.00200\n",
      "Iteration: 0071 Loss: 1.00932 Time: 0.00296\n",
      "Iteration: 0072 Loss: 1.07287 Time: 0.00304\n",
      "Iteration: 0073 Loss: 1.03536 Time: 0.00396\n",
      "Iteration: 0074 Loss: 1.04169 Time: 0.00299\n",
      "Iteration: 0075 Loss: 0.99042 Time: 0.00202\n",
      "Iteration: 0076 Loss: 0.98415 Time: 0.00406\n",
      "Iteration: 0077 Loss: 1.03890 Time: 0.00292\n",
      "Iteration: 0078 Loss: 1.07221 Time: 0.00404\n",
      "Iteration: 0079 Loss: 1.07121 Time: 0.00302\n",
      "Iteration: 0080 Loss: 0.95917 Time: 0.00309\n",
      "Iteration: 0081 Loss: 1.00367 Time: 0.00387\n",
      "Iteration: 0082 Loss: 0.99397 Time: 0.00401\n",
      "Iteration: 0083 Loss: 0.95870 Time: 0.00302\n",
      "Iteration: 0084 Loss: 0.98605 Time: 0.00307\n",
      "Iteration: 0085 Loss: 0.95371 Time: 0.00297\n",
      "Iteration: 0086 Loss: 0.94763 Time: 0.00300\n",
      "Iteration: 0087 Loss: 0.97124 Time: 0.00401\n",
      "Iteration: 0088 Loss: 1.00602 Time: 0.00201\n",
      "Iteration: 0089 Loss: 1.00432 Time: 0.00211\n",
      "Iteration: 0090 Loss: 0.96436 Time: 0.00207\n",
      "Iteration: 0091 Loss: 0.91679 Time: 0.00205\n",
      "Iteration: 0092 Loss: 0.95874 Time: 0.00196\n",
      "Iteration: 0093 Loss: 0.99155 Time: 0.00198\n",
      "Iteration: 0094 Loss: 0.94048 Time: 0.00095\n",
      "Iteration: 0095 Loss: 1.00973 Time: 0.00300\n",
      "Iteration: 0096 Loss: 0.90509 Time: 0.00300\n",
      "Iteration: 0097 Loss: 0.98421 Time: 0.00205\n",
      "Iteration: 0098 Loss: 0.95404 Time: 0.00299\n",
      "Iteration: 0099 Loss: 0.94876 Time: 0.00195\n",
      "Iteration: 0100 Loss: 0.96074 Time: 0.00211\n",
      "Iteration: 0101 Loss: 0.88641 Time: 0.00181\n",
      "Iteration: 0102 Loss: 0.88275 Time: 0.00310\n",
      "Iteration: 0103 Loss: 0.88630 Time: 0.00190\n",
      "Iteration: 0104 Loss: 0.92267 Time: 0.00300\n",
      "Iteration: 0105 Loss: 0.92113 Time: 0.00202\n",
      "Iteration: 0106 Loss: 0.87845 Time: 0.00295\n",
      "Iteration: 0107 Loss: 0.84055 Time: 0.00302\n",
      "Iteration: 0108 Loss: 0.93740 Time: 0.00294\n",
      "Iteration: 0109 Loss: 0.88688 Time: 0.00318\n",
      "Iteration: 0110 Loss: 0.94812 Time: 0.00292\n",
      "Iteration: 0111 Loss: 0.91652 Time: 0.00395\n",
      "Iteration: 0112 Loss: 0.92710 Time: 0.00196\n",
      "Iteration: 0113 Loss: 0.88797 Time: 0.00317\n",
      "Iteration: 0114 Loss: 0.90705 Time: 0.00283\n",
      "Iteration: 0115 Loss: 0.86857 Time: 0.00297\n",
      "Iteration: 0116 Loss: 0.90781 Time: 0.00200\n",
      "Iteration: 0117 Loss: 0.86324 Time: 0.00304\n",
      "Iteration: 0118 Loss: 0.88790 Time: 0.00300\n",
      "Iteration: 0119 Loss: 0.89066 Time: 0.00294\n",
      "Iteration: 0120 Loss: 0.85899 Time: 0.00301\n",
      "Iteration: 0121 Loss: 0.86667 Time: 0.00299\n",
      "Iteration: 0122 Loss: 0.85720 Time: 0.00200\n",
      "Iteration: 0123 Loss: 0.86049 Time: 0.00300\n",
      "Iteration: 0124 Loss: 0.85380 Time: 0.00300\n",
      "Iteration: 0125 Loss: 0.82227 Time: 0.00304\n",
      "Iteration: 0126 Loss: 0.85899 Time: 0.00196\n",
      "Iteration: 0127 Loss: 0.85804 Time: 0.00206\n",
      "Iteration: 0128 Loss: 0.84574 Time: 0.00299\n",
      "Iteration: 0129 Loss: 0.85239 Time: 0.00300\n",
      "Iteration: 0130 Loss: 0.84907 Time: 0.00308\n",
      "Iteration: 0131 Loss: 0.83337 Time: 0.00398\n",
      "Iteration: 0132 Loss: 0.83093 Time: 0.00195\n",
      "Iteration: 0133 Loss: 0.81334 Time: 0.00306\n",
      "Iteration: 0134 Loss: 0.85384 Time: 0.00302\n",
      "Iteration: 0135 Loss: 0.86708 Time: 0.00393\n",
      "Iteration: 0136 Loss: 0.83120 Time: 0.00300\n",
      "Iteration: 0137 Loss: 0.81803 Time: 0.00200\n",
      "Iteration: 0138 Loss: 0.82296 Time: 0.00300\n",
      "Iteration: 0139 Loss: 0.84166 Time: 0.00309\n",
      "Iteration: 0140 Loss: 0.81716 Time: 0.00285\n",
      "Iteration: 0141 Loss: 0.82921 Time: 0.00307\n",
      "Iteration: 0142 Loss: 0.80578 Time: 0.00198\n",
      "Iteration: 0143 Loss: 0.80826 Time: 0.00297\n",
      "Iteration: 0144 Loss: 0.78785 Time: 0.00213\n",
      "Iteration: 0145 Loss: 0.81955 Time: 0.00196\n",
      "Iteration: 0146 Loss: 0.81350 Time: 0.00300\n",
      "Iteration: 0147 Loss: 0.79542 Time: 0.00308\n",
      "Iteration: 0148 Loss: 0.81659 Time: 0.00297\n",
      "Iteration: 0149 Loss: 0.76262 Time: 0.00204\n",
      "Iteration: 0150 Loss: 0.79992 Time: 0.00197\n",
      "Iteration: 0151 Loss: 0.79694 Time: 0.00210\n",
      "Iteration: 0152 Loss: 0.76769 Time: 0.00302\n",
      "Iteration: 0153 Loss: 0.79917 Time: 0.00200\n",
      "Iteration: 0154 Loss: 0.79161 Time: 0.00309\n",
      "Iteration: 0155 Loss: 0.77786 Time: 0.00191\n",
      "Iteration: 0156 Loss: 0.76865 Time: 0.00206\n",
      "Iteration: 0157 Loss: 0.80754 Time: 0.00294\n",
      "Iteration: 0158 Loss: 0.76112 Time: 0.00304\n",
      "Iteration: 0159 Loss: 0.74360 Time: 0.00221\n",
      "Iteration: 0160 Loss: 0.79317 Time: 0.00201\n",
      "Iteration: 0161 Loss: 0.79144 Time: 0.00387\n",
      "Iteration: 0162 Loss: 0.75214 Time: 0.00295\n",
      "Iteration: 0163 Loss: 0.74748 Time: 0.00301\n",
      "Iteration: 0164 Loss: 0.76964 Time: 0.00303\n",
      "Iteration: 0165 Loss: 0.75833 Time: 0.00397\n",
      "Iteration: 0166 Loss: 0.74598 Time: 0.00199\n",
      "Iteration: 0167 Loss: 0.75025 Time: 0.00403\n",
      "Iteration: 0168 Loss: 0.74773 Time: 0.00204\n",
      "Iteration: 0169 Loss: 0.74287 Time: 0.00296\n",
      "Iteration: 0170 Loss: 0.76075 Time: 0.00202\n",
      "Iteration: 0171 Loss: 0.74355 Time: 0.00303\n",
      "Iteration: 0172 Loss: 0.77156 Time: 0.00311\n",
      "Iteration: 0173 Loss: 0.73539 Time: 0.00294\n",
      "Iteration: 0174 Loss: 0.74209 Time: 0.00294\n",
      "Iteration: 0175 Loss: 0.77324 Time: 0.00395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0176 Loss: 0.73310 Time: 0.00428\n",
      "Iteration: 0177 Loss: 0.71985 Time: 0.00280\n",
      "Iteration: 0178 Loss: 0.72867 Time: 0.00192\n",
      "Iteration: 0179 Loss: 0.76013 Time: 0.00308\n",
      "Iteration: 0180 Loss: 0.74826 Time: 0.00306\n",
      "Iteration: 0181 Loss: 0.72062 Time: 0.00289\n",
      "Iteration: 0182 Loss: 0.75945 Time: 0.00203\n",
      "Iteration: 0183 Loss: 0.73624 Time: 0.00294\n",
      "Iteration: 0184 Loss: 0.75099 Time: 0.00304\n",
      "Iteration: 0185 Loss: 0.68449 Time: 0.00205\n",
      "Iteration: 0186 Loss: 0.72454 Time: 0.00296\n",
      "Iteration: 0187 Loss: 0.68869 Time: 0.00199\n",
      "Iteration: 0188 Loss: 0.70648 Time: 0.00409\n",
      "Iteration: 0189 Loss: 0.73235 Time: 0.00295\n",
      "Iteration: 0190 Loss: 0.73075 Time: 0.00306\n",
      "Iteration: 0191 Loss: 0.72213 Time: 0.00302\n",
      "Iteration: 0192 Loss: 0.69125 Time: 0.00289\n",
      "Iteration: 0193 Loss: 0.73352 Time: 0.00401\n",
      "Iteration: 0194 Loss: 0.71462 Time: 0.00299\n",
      "Iteration: 0195 Loss: 0.70609 Time: 0.00305\n",
      "Iteration: 0196 Loss: 0.71480 Time: 0.00297\n",
      "Iteration: 0197 Loss: 0.70958 Time: 0.00209\n",
      "Iteration: 0198 Loss: 0.73691 Time: 0.00303\n",
      "Iteration: 0199 Loss: 0.69065 Time: 0.00292\n",
      "Iteration: 0200 Loss: 0.70538 Time: 0.00299\n",
      "Done! \n",
      "\n",
      "STEP 3/3 - MODEL EVALUATION \n",
      "\n",
      "Computing the final embedding vectors, for evaluation\n",
      "Done! \n",
      "\n",
      "Testing: link prediction\n",
      "Done! \n",
      "\n",
      "EXPERIMENTS ON MODEL 9 / 10 \n",
      "\n",
      "STEP 1/3 - PREPROCESSING STEPS \n",
      "\n",
      "Masking some edges from the training graph, for link prediction\n",
      "(validation set: 1.0 % of edges - test set: 10.0 % of edges)\n",
      "Done! \n",
      "\n",
      "Preprocessing node features\n",
      "Done! \n",
      "\n",
      "Running the Louvain algorithm for community detection\n",
      "as a preprocessing step for the encoder\n",
      "Done! Louvain has found 21 communities \n",
      "\n",
      "Setting up the model and the optimizer\n",
      "Done! \n",
      "\n",
      "Preprocessing on message passing matrices\n",
      "Done! \n",
      "\n",
      "Initializing TF session\n",
      "Done! \n",
      "\n",
      "STEP 2/3 - MODEL TRAINING \n",
      "\n",
      "Starting training\n",
      "Iteration: 0001 Loss: 1.80177 Time: 0.13586\n",
      "Iteration: 0002 Loss: 1.68409 Time: 0.00406\n",
      "Iteration: 0003 Loss: 1.65002 Time: 0.00398\n",
      "Iteration: 0004 Loss: 1.74671 Time: 0.00305\n",
      "Iteration: 0005 Loss: 1.67947 Time: 0.00297\n",
      "Iteration: 0006 Loss: 1.47257 Time: 0.00291\n",
      "Iteration: 0007 Loss: 1.57862 Time: 0.00415\n",
      "Iteration: 0008 Loss: 1.66394 Time: 0.00384\n",
      "Iteration: 0009 Loss: 1.52487 Time: 0.00203\n",
      "Iteration: 0010 Loss: 1.53953 Time: 0.00206\n",
      "Iteration: 0011 Loss: 1.52037 Time: 0.00296\n",
      "Iteration: 0012 Loss: 1.50259 Time: 0.00305\n",
      "Iteration: 0013 Loss: 1.46623 Time: 0.00304\n",
      "Iteration: 0014 Loss: 1.40104 Time: 0.00194\n",
      "Iteration: 0015 Loss: 1.47257 Time: 0.00302\n",
      "Iteration: 0016 Loss: 1.39000 Time: 0.00202\n",
      "Iteration: 0017 Loss: 1.44965 Time: 0.00203\n",
      "Iteration: 0018 Loss: 1.29473 Time: 0.00397\n",
      "Iteration: 0019 Loss: 1.46887 Time: 0.00343\n",
      "Iteration: 0020 Loss: 1.35380 Time: 0.00357\n",
      "Iteration: 0021 Loss: 1.34427 Time: 0.00295\n",
      "Iteration: 0022 Loss: 1.48146 Time: 0.00301\n",
      "Iteration: 0023 Loss: 1.28752 Time: 0.00306\n",
      "Iteration: 0024 Loss: 1.31441 Time: 0.00300\n",
      "Iteration: 0025 Loss: 1.36273 Time: 0.00298\n",
      "Iteration: 0026 Loss: 1.25634 Time: 0.00298\n",
      "Iteration: 0027 Loss: 1.35356 Time: 0.00397\n",
      "Iteration: 0028 Loss: 1.34590 Time: 0.00304\n",
      "Iteration: 0029 Loss: 1.19307 Time: 0.00198\n",
      "Iteration: 0030 Loss: 1.30380 Time: 0.00205\n",
      "Iteration: 0031 Loss: 1.28466 Time: 0.00203\n",
      "Iteration: 0032 Loss: 1.25674 Time: 0.00294\n",
      "Iteration: 0033 Loss: 1.23032 Time: 0.00461\n",
      "Iteration: 0034 Loss: 1.18430 Time: 0.00200\n",
      "Iteration: 0035 Loss: 1.29551 Time: 0.00406\n",
      "Iteration: 0036 Loss: 1.35786 Time: 0.00194\n",
      "Iteration: 0037 Loss: 1.28568 Time: 0.00401\n",
      "Iteration: 0038 Loss: 1.10735 Time: 0.00299\n",
      "Iteration: 0039 Loss: 1.17880 Time: 0.00304\n",
      "Iteration: 0040 Loss: 1.18136 Time: 0.00300\n",
      "Iteration: 0041 Loss: 1.21303 Time: 0.00329\n",
      "Iteration: 0042 Loss: 1.20815 Time: 0.00272\n",
      "Iteration: 0043 Loss: 1.21548 Time: 0.00303\n",
      "Iteration: 0044 Loss: 1.11544 Time: 0.00297\n",
      "Iteration: 0045 Loss: 1.18071 Time: 0.00211\n",
      "Iteration: 0046 Loss: 1.11614 Time: 0.00293\n",
      "Iteration: 0047 Loss: 1.06841 Time: 0.00292\n",
      "Iteration: 0048 Loss: 1.17273 Time: 0.00307\n",
      "Iteration: 0049 Loss: 1.14206 Time: 0.00294\n",
      "Iteration: 0050 Loss: 1.10535 Time: 0.00306\n",
      "Iteration: 0051 Loss: 1.14779 Time: 0.00201\n",
      "Iteration: 0052 Loss: 1.11939 Time: 0.00300\n",
      "Iteration: 0053 Loss: 1.12175 Time: 0.00299\n",
      "Iteration: 0054 Loss: 1.12192 Time: 0.00295\n",
      "Iteration: 0055 Loss: 1.09580 Time: 0.00305\n",
      "Iteration: 0056 Loss: 1.13409 Time: 0.00202\n",
      "Iteration: 0057 Loss: 1.05042 Time: 0.00304\n",
      "Iteration: 0058 Loss: 1.04182 Time: 0.00198\n",
      "Iteration: 0059 Loss: 1.13335 Time: 0.00297\n",
      "Iteration: 0060 Loss: 1.03003 Time: 0.00404\n",
      "Iteration: 0061 Loss: 1.06135 Time: 0.00299\n",
      "Iteration: 0062 Loss: 1.01924 Time: 0.00295\n",
      "Iteration: 0063 Loss: 1.06537 Time: 0.00403\n",
      "Iteration: 0064 Loss: 1.09126 Time: 0.00298\n",
      "Iteration: 0065 Loss: 1.07917 Time: 0.00198\n",
      "Iteration: 0066 Loss: 1.05255 Time: 0.00300\n",
      "Iteration: 0067 Loss: 1.09294 Time: 0.00207\n",
      "Iteration: 0068 Loss: 1.01276 Time: 0.00300\n",
      "Iteration: 0069 Loss: 1.08581 Time: 0.00305\n",
      "Iteration: 0070 Loss: 1.04470 Time: 0.00407\n",
      "Iteration: 0071 Loss: 1.02125 Time: 0.00384\n",
      "Iteration: 0072 Loss: 1.00112 Time: 0.00306\n",
      "Iteration: 0073 Loss: 0.99225 Time: 0.00301\n",
      "Iteration: 0074 Loss: 1.02180 Time: 0.00293\n",
      "Iteration: 0075 Loss: 1.01809 Time: 0.00402\n",
      "Iteration: 0076 Loss: 0.94519 Time: 0.00398\n",
      "Iteration: 0077 Loss: 0.98773 Time: 0.00299\n",
      "Iteration: 0078 Loss: 1.00783 Time: 0.00304\n",
      "Iteration: 0079 Loss: 0.99388 Time: 0.00401\n",
      "Iteration: 0080 Loss: 0.96134 Time: 0.00395\n",
      "Iteration: 0081 Loss: 1.01101 Time: 0.00200\n",
      "Iteration: 0082 Loss: 0.95186 Time: 0.00200\n",
      "Iteration: 0083 Loss: 0.95642 Time: 0.00204\n",
      "Iteration: 0084 Loss: 0.98146 Time: 0.00296\n",
      "Iteration: 0085 Loss: 0.94402 Time: 0.00304\n",
      "Iteration: 0086 Loss: 0.92949 Time: 0.00299\n",
      "Iteration: 0087 Loss: 0.96649 Time: 0.00301\n",
      "Iteration: 0088 Loss: 0.97557 Time: 0.00401\n",
      "Iteration: 0089 Loss: 0.98090 Time: 0.00300\n",
      "Iteration: 0090 Loss: 0.93371 Time: 0.00395\n",
      "Iteration: 0091 Loss: 0.95797 Time: 0.00210\n",
      "Iteration: 0092 Loss: 0.98560 Time: 0.00206\n",
      "Iteration: 0093 Loss: 0.93257 Time: 0.00296\n",
      "Iteration: 0094 Loss: 0.92052 Time: 0.00304\n",
      "Iteration: 0095 Loss: 0.95751 Time: 0.00295\n",
      "Iteration: 0096 Loss: 0.90862 Time: 0.00200\n",
      "Iteration: 0097 Loss: 0.96446 Time: 0.00311\n",
      "Iteration: 0098 Loss: 0.90297 Time: 0.00296\n",
      "Iteration: 0099 Loss: 0.90600 Time: 0.00208\n",
      "Iteration: 0100 Loss: 0.92397 Time: 0.00387\n",
      "Iteration: 0101 Loss: 0.94657 Time: 0.00305\n",
      "Iteration: 0102 Loss: 0.92837 Time: 0.00197\n",
      "Iteration: 0103 Loss: 0.92610 Time: 0.00316\n",
      "Iteration: 0104 Loss: 0.91794 Time: 0.00301\n",
      "Iteration: 0105 Loss: 0.90885 Time: 0.00199\n",
      "Iteration: 0106 Loss: 0.94099 Time: 0.00301\n",
      "Iteration: 0107 Loss: 0.88830 Time: 0.00392\n",
      "Iteration: 0108 Loss: 0.95067 Time: 0.00403\n",
      "Iteration: 0109 Loss: 0.90566 Time: 0.00200\n",
      "Iteration: 0110 Loss: 0.87780 Time: 0.00400\n",
      "Iteration: 0111 Loss: 0.93571 Time: 0.00298\n",
      "Iteration: 0112 Loss: 0.88332 Time: 0.00212\n",
      "Iteration: 0113 Loss: 0.89703 Time: 0.00200\n",
      "Iteration: 0114 Loss: 0.91278 Time: 0.00397\n",
      "Iteration: 0115 Loss: 0.87896 Time: 0.00200\n",
      "Iteration: 0116 Loss: 0.91188 Time: 0.00296\n",
      "Iteration: 0117 Loss: 0.87872 Time: 0.00309\n",
      "Iteration: 0118 Loss: 0.84812 Time: 0.00296\n",
      "Iteration: 0119 Loss: 0.88103 Time: 0.00300\n",
      "Iteration: 0120 Loss: 0.83765 Time: 0.00401\n",
      "Iteration: 0121 Loss: 0.85642 Time: 0.00294\n",
      "Iteration: 0122 Loss: 0.83737 Time: 0.00306\n",
      "Iteration: 0123 Loss: 0.86247 Time: 0.00297\n",
      "Iteration: 0124 Loss: 0.82082 Time: 0.00337\n",
      "Iteration: 0125 Loss: 0.87241 Time: 0.00261\n",
      "Iteration: 0126 Loss: 0.82963 Time: 0.00408\n",
      "Iteration: 0127 Loss: 0.83953 Time: 0.00292\n",
      "Iteration: 0128 Loss: 0.84233 Time: 0.00306\n",
      "Iteration: 0129 Loss: 0.88046 Time: 0.00394\n",
      "Iteration: 0130 Loss: 0.84535 Time: 0.00310\n",
      "Iteration: 0131 Loss: 0.82290 Time: 0.00390\n",
      "Iteration: 0132 Loss: 0.86097 Time: 0.00302\n",
      "Iteration: 0133 Loss: 0.88090 Time: 0.00300\n",
      "Iteration: 0134 Loss: 0.85622 Time: 0.00297\n",
      "Iteration: 0135 Loss: 0.83594 Time: 0.00203\n",
      "Iteration: 0136 Loss: 0.83688 Time: 0.00296\n",
      "Iteration: 0137 Loss: 0.82668 Time: 0.00200\n",
      "Iteration: 0138 Loss: 0.83653 Time: 0.00201\n",
      "Iteration: 0139 Loss: 0.82653 Time: 0.00198\n",
      "Iteration: 0140 Loss: 0.88004 Time: 0.00399\n",
      "Iteration: 0141 Loss: 0.79322 Time: 0.00410\n",
      "Iteration: 0142 Loss: 0.81371 Time: 0.00295\n",
      "Iteration: 0143 Loss: 0.81371 Time: 0.00296\n",
      "Iteration: 0144 Loss: 0.79991 Time: 0.00295\n",
      "Iteration: 0145 Loss: 0.80390 Time: 0.00303\n",
      "Iteration: 0146 Loss: 0.77971 Time: 0.00306\n",
      "Iteration: 0147 Loss: 0.77593 Time: 0.00341\n",
      "Iteration: 0148 Loss: 0.80560 Time: 0.00200\n",
      "Iteration: 0149 Loss: 0.81022 Time: 0.00197\n",
      "Iteration: 0150 Loss: 0.77986 Time: 0.00205\n",
      "Iteration: 0151 Loss: 0.77674 Time: 0.00299\n",
      "Iteration: 0152 Loss: 0.80788 Time: 0.00295\n",
      "Iteration: 0153 Loss: 0.78176 Time: 0.00298\n",
      "Iteration: 0154 Loss: 0.79276 Time: 0.00307\n",
      "Iteration: 0155 Loss: 0.76184 Time: 0.00194\n",
      "Iteration: 0156 Loss: 0.78680 Time: 0.00308\n",
      "Iteration: 0157 Loss: 0.80126 Time: 0.00402\n",
      "Iteration: 0158 Loss: 0.76612 Time: 0.00304\n",
      "Iteration: 0159 Loss: 0.76515 Time: 0.00295\n",
      "Iteration: 0160 Loss: 0.80135 Time: 0.00302\n",
      "Iteration: 0161 Loss: 0.77266 Time: 0.00303\n",
      "Iteration: 0162 Loss: 0.75539 Time: 0.00299\n",
      "Iteration: 0163 Loss: 0.76104 Time: 0.00203\n",
      "Iteration: 0164 Loss: 0.75654 Time: 0.00399\n",
      "Iteration: 0165 Loss: 0.73979 Time: 0.00198\n",
      "Iteration: 0166 Loss: 0.73282 Time: 0.00295\n",
      "Iteration: 0167 Loss: 0.74033 Time: 0.00303\n",
      "Iteration: 0168 Loss: 0.73938 Time: 0.00199\n",
      "Iteration: 0169 Loss: 0.73583 Time: 0.00202\n",
      "Iteration: 0170 Loss: 0.78324 Time: 0.00207\n",
      "Iteration: 0171 Loss: 0.75077 Time: 0.00302\n",
      "Iteration: 0172 Loss: 0.72630 Time: 0.00294\n",
      "Iteration: 0173 Loss: 0.73059 Time: 0.00301\n",
      "Iteration: 0174 Loss: 0.72305 Time: 0.00202\n",
      "Iteration: 0175 Loss: 0.72275 Time: 0.00200\n",
      "Iteration: 0176 Loss: 0.73004 Time: 0.00210\n",
      "Iteration: 0177 Loss: 0.73330 Time: 0.00201\n",
      "Iteration: 0178 Loss: 0.73248 Time: 0.00394\n",
      "Iteration: 0179 Loss: 0.74024 Time: 0.00210\n",
      "Iteration: 0180 Loss: 0.76182 Time: 0.00197\n",
      "Iteration: 0181 Loss: 0.73627 Time: 0.00300\n",
      "Iteration: 0182 Loss: 0.69394 Time: 0.00306\n",
      "Iteration: 0183 Loss: 0.74280 Time: 0.00300\n",
      "Iteration: 0184 Loss: 0.72754 Time: 0.00403\n",
      "Iteration: 0185 Loss: 0.68353 Time: 0.00292\n",
      "Iteration: 0186 Loss: 0.73238 Time: 0.00199\n",
      "Iteration: 0187 Loss: 0.72196 Time: 0.00201\n",
      "Iteration: 0188 Loss: 0.71300 Time: 0.00205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0189 Loss: 0.72202 Time: 0.00401\n",
      "Iteration: 0190 Loss: 0.71298 Time: 0.00299\n",
      "Iteration: 0191 Loss: 0.71877 Time: 0.00300\n",
      "Iteration: 0192 Loss: 0.70165 Time: 0.00396\n",
      "Iteration: 0193 Loss: 0.70255 Time: 0.00400\n",
      "Iteration: 0194 Loss: 0.69724 Time: 0.00405\n",
      "Iteration: 0195 Loss: 0.73090 Time: 0.00300\n",
      "Iteration: 0196 Loss: 0.71330 Time: 0.00201\n",
      "Iteration: 0197 Loss: 0.68572 Time: 0.00313\n",
      "Iteration: 0198 Loss: 0.73875 Time: 0.00186\n",
      "Iteration: 0199 Loss: 0.71818 Time: 0.00303\n",
      "Iteration: 0200 Loss: 0.68695 Time: 0.00300\n",
      "Done! \n",
      "\n",
      "STEP 3/3 - MODEL EVALUATION \n",
      "\n",
      "Computing the final embedding vectors, for evaluation\n",
      "Done! \n",
      "\n",
      "Testing: link prediction\n",
      "Done! \n",
      "\n",
      "EXPERIMENTS ON MODEL 10 / 10 \n",
      "\n",
      "STEP 1/3 - PREPROCESSING STEPS \n",
      "\n",
      "Masking some edges from the training graph, for link prediction\n",
      "(validation set: 1.0 % of edges - test set: 10.0 % of edges)\n",
      "Done! \n",
      "\n",
      "Preprocessing node features\n",
      "Done! \n",
      "\n",
      "Running the Louvain algorithm for community detection\n",
      "as a preprocessing step for the encoder\n",
      "Done! Louvain has found 23 communities \n",
      "\n",
      "Setting up the model and the optimizer\n",
      "Done! \n",
      "\n",
      "Preprocessing on message passing matrices\n",
      "Done! \n",
      "\n",
      "Initializing TF session\n",
      "Done! \n",
      "\n",
      "STEP 2/3 - MODEL TRAINING \n",
      "\n",
      "Starting training\n",
      "Iteration: 0001 Loss: 1.65129 Time: 0.14308\n",
      "Iteration: 0002 Loss: 1.72833 Time: 0.00400\n",
      "Iteration: 0003 Loss: 1.65518 Time: 0.00300\n",
      "Iteration: 0004 Loss: 1.80298 Time: 0.00300\n",
      "Iteration: 0005 Loss: 1.64970 Time: 0.00399\n",
      "Iteration: 0006 Loss: 1.63501 Time: 0.00301\n",
      "Iteration: 0007 Loss: 1.56963 Time: 0.00300\n",
      "Iteration: 0008 Loss: 1.58632 Time: 0.00301\n",
      "Iteration: 0009 Loss: 1.58363 Time: 0.00299\n",
      "Iteration: 0010 Loss: 1.56323 Time: 0.00301\n",
      "Iteration: 0011 Loss: 1.53329 Time: 0.00399\n",
      "Iteration: 0012 Loss: 1.48607 Time: 0.00301\n",
      "Iteration: 0013 Loss: 1.53298 Time: 0.00299\n",
      "Iteration: 0014 Loss: 1.38379 Time: 0.00300\n",
      "Iteration: 0015 Loss: 1.51619 Time: 0.00300\n",
      "Iteration: 0016 Loss: 1.53508 Time: 0.00403\n",
      "Iteration: 0017 Loss: 1.48145 Time: 0.00313\n",
      "Iteration: 0018 Loss: 1.35922 Time: 0.00203\n",
      "Iteration: 0019 Loss: 1.51501 Time: 0.00204\n",
      "Iteration: 0020 Loss: 1.34034 Time: 0.00297\n",
      "Iteration: 0021 Loss: 1.40289 Time: 0.00301\n",
      "Iteration: 0022 Loss: 1.38774 Time: 0.00296\n",
      "Iteration: 0023 Loss: 1.49222 Time: 0.00407\n",
      "Iteration: 0024 Loss: 1.22342 Time: 0.00292\n",
      "Iteration: 0025 Loss: 1.33890 Time: 0.00200\n",
      "Iteration: 0026 Loss: 1.21574 Time: 0.00207\n",
      "Iteration: 0027 Loss: 1.31734 Time: 0.00201\n",
      "Iteration: 0028 Loss: 1.19500 Time: 0.00293\n",
      "Iteration: 0029 Loss: 1.28217 Time: 0.00201\n",
      "Iteration: 0030 Loss: 1.29532 Time: 0.00309\n",
      "Iteration: 0031 Loss: 1.26286 Time: 0.00206\n",
      "Iteration: 0032 Loss: 1.19582 Time: 0.00404\n",
      "Iteration: 0033 Loss: 1.28787 Time: 0.00298\n",
      "Iteration: 0034 Loss: 1.21849 Time: 0.00299\n",
      "Iteration: 0035 Loss: 1.16938 Time: 0.00396\n",
      "Iteration: 0036 Loss: 1.30162 Time: 0.00197\n",
      "Iteration: 0037 Loss: 1.25596 Time: 0.00301\n",
      "Iteration: 0038 Loss: 1.24792 Time: 0.00302\n",
      "Iteration: 0039 Loss: 1.25294 Time: 0.00345\n",
      "Iteration: 0040 Loss: 1.24287 Time: 0.00255\n",
      "Iteration: 0041 Loss: 1.16751 Time: 0.00305\n",
      "Iteration: 0042 Loss: 1.19182 Time: 0.00287\n",
      "Iteration: 0043 Loss: 1.16584 Time: 0.00313\n",
      "Iteration: 0044 Loss: 1.17751 Time: 0.00199\n",
      "Iteration: 0045 Loss: 1.13219 Time: 0.00208\n",
      "Iteration: 0046 Loss: 1.20746 Time: 0.00198\n",
      "Iteration: 0047 Loss: 1.19012 Time: 0.00200\n",
      "Iteration: 0048 Loss: 1.17044 Time: 0.00299\n",
      "Iteration: 0049 Loss: 1.14315 Time: 0.00200\n",
      "Iteration: 0050 Loss: 1.18185 Time: 0.00196\n",
      "Iteration: 0051 Loss: 1.06302 Time: 0.00199\n",
      "Iteration: 0052 Loss: 1.14182 Time: 0.00198\n",
      "Iteration: 0053 Loss: 1.13852 Time: 0.00200\n",
      "Iteration: 0054 Loss: 1.13775 Time: 0.00301\n",
      "Iteration: 0055 Loss: 1.11722 Time: 0.00294\n",
      "Iteration: 0056 Loss: 1.12731 Time: 0.00311\n",
      "Iteration: 0057 Loss: 1.05673 Time: 0.00293\n",
      "Iteration: 0058 Loss: 1.11440 Time: 0.00304\n",
      "Iteration: 0059 Loss: 1.06534 Time: 0.00195\n",
      "Iteration: 0060 Loss: 1.11877 Time: 0.00197\n",
      "Iteration: 0061 Loss: 1.10199 Time: 0.00406\n",
      "Iteration: 0062 Loss: 1.02020 Time: 0.00294\n",
      "Iteration: 0063 Loss: 1.05160 Time: 0.00305\n",
      "Iteration: 0064 Loss: 1.03419 Time: 0.00196\n",
      "Iteration: 0065 Loss: 1.12204 Time: 0.00300\n",
      "Iteration: 0066 Loss: 1.06230 Time: 0.00304\n",
      "Iteration: 0067 Loss: 1.05145 Time: 0.00295\n",
      "Iteration: 0068 Loss: 1.07188 Time: 0.00308\n",
      "Iteration: 0069 Loss: 1.05248 Time: 0.00297\n",
      "Iteration: 0070 Loss: 1.04384 Time: 0.00404\n",
      "Iteration: 0071 Loss: 1.06399 Time: 0.00395\n",
      "Iteration: 0072 Loss: 1.06869 Time: 0.00297\n",
      "Iteration: 0073 Loss: 1.00884 Time: 0.00200\n",
      "Iteration: 0074 Loss: 1.06641 Time: 0.00304\n",
      "Iteration: 0075 Loss: 1.04766 Time: 0.00296\n",
      "Iteration: 0076 Loss: 0.96544 Time: 0.00303\n",
      "Iteration: 0077 Loss: 1.03866 Time: 0.00296\n",
      "Iteration: 0078 Loss: 0.95295 Time: 0.00300\n",
      "Iteration: 0079 Loss: 0.99727 Time: 0.00286\n",
      "Iteration: 0080 Loss: 0.98048 Time: 0.00296\n",
      "Iteration: 0081 Loss: 1.00014 Time: 0.00300\n",
      "Iteration: 0082 Loss: 0.98806 Time: 0.00299\n",
      "Iteration: 0083 Loss: 0.97039 Time: 0.00311\n",
      "Iteration: 0084 Loss: 0.99941 Time: 0.00200\n",
      "Iteration: 0085 Loss: 1.04167 Time: 0.00298\n",
      "Iteration: 0086 Loss: 1.03451 Time: 0.00200\n",
      "Iteration: 0087 Loss: 0.98946 Time: 0.00204\n",
      "Iteration: 0088 Loss: 0.97461 Time: 0.00301\n",
      "Iteration: 0089 Loss: 0.95932 Time: 0.00205\n",
      "Iteration: 0090 Loss: 0.96397 Time: 0.00295\n",
      "Iteration: 0091 Loss: 0.96282 Time: 0.00200\n",
      "Iteration: 0092 Loss: 1.00245 Time: 0.00300\n",
      "Iteration: 0093 Loss: 0.96838 Time: 0.00405\n",
      "Iteration: 0094 Loss: 0.99239 Time: 0.00209\n",
      "Iteration: 0095 Loss: 0.97534 Time: 0.00203\n",
      "Iteration: 0096 Loss: 0.93696 Time: 0.00301\n",
      "Iteration: 0097 Loss: 0.92913 Time: 0.00303\n",
      "Iteration: 0098 Loss: 0.95467 Time: 0.00301\n",
      "Iteration: 0099 Loss: 0.96397 Time: 0.00204\n",
      "Iteration: 0100 Loss: 0.93435 Time: 0.00299\n",
      "Iteration: 0101 Loss: 0.92228 Time: 0.00193\n",
      "Iteration: 0102 Loss: 0.93963 Time: 0.00306\n",
      "Iteration: 0103 Loss: 0.93766 Time: 0.00195\n",
      "Iteration: 0104 Loss: 0.96649 Time: 0.00205\n",
      "Iteration: 0105 Loss: 0.91268 Time: 0.00201\n",
      "Iteration: 0106 Loss: 0.88034 Time: 0.00400\n",
      "Iteration: 0107 Loss: 0.87831 Time: 0.00293\n",
      "Iteration: 0108 Loss: 0.90638 Time: 0.00206\n",
      "Iteration: 0109 Loss: 0.93622 Time: 0.00397\n",
      "Iteration: 0110 Loss: 0.87236 Time: 0.00199\n",
      "Iteration: 0111 Loss: 0.88653 Time: 0.00314\n",
      "Iteration: 0112 Loss: 0.90253 Time: 0.00285\n",
      "Iteration: 0113 Loss: 0.88042 Time: 0.00198\n",
      "Iteration: 0114 Loss: 0.88624 Time: 0.00298\n",
      "Iteration: 0115 Loss: 0.85434 Time: 0.00400\n",
      "Iteration: 0116 Loss: 0.87141 Time: 0.00200\n",
      "Iteration: 0117 Loss: 0.93941 Time: 0.00302\n",
      "Iteration: 0118 Loss: 0.86543 Time: 0.00295\n",
      "Iteration: 0119 Loss: 0.87007 Time: 0.00304\n",
      "Iteration: 0120 Loss: 0.87529 Time: 0.00201\n",
      "Iteration: 0121 Loss: 0.87317 Time: 0.00295\n",
      "Iteration: 0122 Loss: 0.89032 Time: 0.00205\n",
      "Iteration: 0123 Loss: 0.87035 Time: 0.00199\n",
      "Iteration: 0124 Loss: 0.85001 Time: 0.00295\n",
      "Iteration: 0125 Loss: 0.89927 Time: 0.00309\n",
      "Iteration: 0126 Loss: 0.87926 Time: 0.00303\n",
      "Iteration: 0127 Loss: 0.83094 Time: 0.00295\n",
      "Iteration: 0128 Loss: 0.84812 Time: 0.00215\n",
      "Iteration: 0129 Loss: 0.87905 Time: 0.00300\n",
      "Iteration: 0130 Loss: 0.86779 Time: 0.00404\n",
      "Iteration: 0131 Loss: 0.82206 Time: 0.00303\n",
      "Iteration: 0132 Loss: 0.83019 Time: 0.00299\n",
      "Iteration: 0133 Loss: 0.78929 Time: 0.00301\n",
      "Iteration: 0134 Loss: 0.84409 Time: 0.00434\n",
      "Iteration: 0135 Loss: 0.86056 Time: 0.00266\n",
      "Iteration: 0136 Loss: 0.81229 Time: 0.00213\n",
      "Iteration: 0137 Loss: 0.83758 Time: 0.00285\n",
      "Iteration: 0138 Loss: 0.78524 Time: 0.00302\n",
      "Iteration: 0139 Loss: 0.87200 Time: 0.00394\n",
      "Iteration: 0140 Loss: 0.83075 Time: 0.00302\n",
      "Iteration: 0141 Loss: 0.86733 Time: 0.00297\n",
      "Iteration: 0142 Loss: 0.81339 Time: 0.00301\n",
      "Iteration: 0143 Loss: 0.84053 Time: 0.00320\n",
      "Iteration: 0144 Loss: 0.79120 Time: 0.00198\n",
      "Iteration: 0145 Loss: 0.81693 Time: 0.00299\n",
      "Iteration: 0146 Loss: 0.79244 Time: 0.00298\n",
      "Iteration: 0147 Loss: 0.82243 Time: 0.00309\n",
      "Iteration: 0148 Loss: 0.79459 Time: 0.00292\n",
      "Iteration: 0149 Loss: 0.78840 Time: 0.00304\n",
      "Iteration: 0150 Loss: 0.81977 Time: 0.00197\n",
      "Iteration: 0151 Loss: 0.80891 Time: 0.00290\n",
      "Iteration: 0152 Loss: 0.80579 Time: 0.00196\n",
      "Iteration: 0153 Loss: 0.77720 Time: 0.00201\n",
      "Iteration: 0154 Loss: 0.79315 Time: 0.00305\n",
      "Iteration: 0155 Loss: 0.76637 Time: 0.00301\n",
      "Iteration: 0156 Loss: 0.79045 Time: 0.00295\n",
      "Iteration: 0157 Loss: 0.77699 Time: 0.00298\n",
      "Iteration: 0158 Loss: 0.77246 Time: 0.00199\n",
      "Iteration: 0159 Loss: 0.75398 Time: 0.00445\n",
      "Iteration: 0160 Loss: 0.79912 Time: 0.00255\n",
      "Iteration: 0161 Loss: 0.76006 Time: 0.00301\n",
      "Iteration: 0162 Loss: 0.77506 Time: 0.00304\n",
      "Iteration: 0163 Loss: 0.77659 Time: 0.00208\n",
      "Iteration: 0164 Loss: 0.77447 Time: 0.00199\n",
      "Iteration: 0165 Loss: 0.73228 Time: 0.00305\n",
      "Iteration: 0166 Loss: 0.74869 Time: 0.00296\n",
      "Iteration: 0167 Loss: 0.76411 Time: 0.00302\n",
      "Iteration: 0168 Loss: 0.74749 Time: 0.00201\n",
      "Iteration: 0169 Loss: 0.79347 Time: 0.00296\n",
      "Iteration: 0170 Loss: 0.77200 Time: 0.00306\n",
      "Iteration: 0171 Loss: 0.72419 Time: 0.00362\n",
      "Iteration: 0172 Loss: 0.75631 Time: 0.00197\n",
      "Iteration: 0173 Loss: 0.74777 Time: 0.00203\n",
      "Iteration: 0174 Loss: 0.74159 Time: 0.00197\n",
      "Iteration: 0175 Loss: 0.73402 Time: 0.00302\n",
      "Iteration: 0176 Loss: 0.75462 Time: 0.00306\n",
      "Iteration: 0177 Loss: 0.70741 Time: 0.00296\n",
      "Iteration: 0178 Loss: 0.74687 Time: 0.00300\n",
      "Iteration: 0179 Loss: 0.75578 Time: 0.00198\n",
      "Iteration: 0180 Loss: 0.73563 Time: 0.00400\n",
      "Iteration: 0181 Loss: 0.75517 Time: 0.00309\n",
      "Iteration: 0182 Loss: 0.75636 Time: 0.00411\n",
      "Iteration: 0183 Loss: 0.73885 Time: 0.00284\n",
      "Iteration: 0184 Loss: 0.72836 Time: 0.00299\n",
      "Iteration: 0185 Loss: 0.71193 Time: 0.00298\n",
      "Iteration: 0186 Loss: 0.72703 Time: 0.00303\n",
      "Iteration: 0187 Loss: 0.72973 Time: 0.00209\n",
      "Iteration: 0188 Loss: 0.71058 Time: 0.00307\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0189 Loss: 0.73101 Time: 0.00403\n",
      "Iteration: 0190 Loss: 0.72718 Time: 0.00291\n",
      "Iteration: 0191 Loss: 0.72066 Time: 0.00400\n",
      "Iteration: 0192 Loss: 0.74184 Time: 0.00299\n",
      "Iteration: 0193 Loss: 0.70633 Time: 0.00304\n",
      "Iteration: 0194 Loss: 0.71564 Time: 0.00298\n",
      "Iteration: 0195 Loss: 0.70416 Time: 0.00305\n",
      "Iteration: 0196 Loss: 0.74607 Time: 0.00300\n",
      "Iteration: 0197 Loss: 0.69688 Time: 0.00193\n",
      "Iteration: 0198 Loss: 0.72911 Time: 0.00305\n",
      "Iteration: 0199 Loss: 0.70876 Time: 0.00295\n",
      "Iteration: 0200 Loss: 0.70091 Time: 0.00301\n",
      "Done! \n",
      "\n",
      "STEP 3/3 - MODEL EVALUATION \n",
      "\n",
      "Computing the final embedding vectors, for evaluation\n",
      "Done! \n",
      "\n",
      "Testing: link prediction\n",
      "Done! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We repeat the entire training+test process FLAGS.nb_run times\n",
    "for i in range(FLAGS.nb_run):\n",
    "    if FLAGS.verbose:\n",
    "        print(\"EXPERIMENTS ON MODEL\", i + 1, \"/\", FLAGS.nb_run, \"\\n\")\n",
    "        print(\"STEP 1/3 - PREPROCESSING STEPS \\n\")\n",
    "\n",
    "\n",
    "    # Edge masking for Link Prediction:\n",
    "    if FLAGS.task == 'task_2':\n",
    "        # Compute Train/Validation/Test sets\n",
    "        if FLAGS.verbose:\n",
    "            print(\"Masking some edges from the training graph, for link prediction\")\n",
    "            print(\"(validation set:\", FLAGS.prop_val, \"% of edges - test set:\",\n",
    "                  FLAGS.prop_test, \"% of edges)\")\n",
    "        #adj, val_edges, val_edges_false, test_edges, test_edges_false = mask_test_edges(adj_init, FLAGS.prop_test, FLAGS.prop_val)\n",
    "        adj, test_edges, test_edges_false = mask_test_edges(adj_init, FLAGS.prop_test)\n",
    "        if FLAGS.verbose:\n",
    "            print(\"Done! \\n\")\n",
    "    else:\n",
    "        adj = adj_init\n",
    "\n",
    "    # Compute the number of nodes\n",
    "    num_nodes = adj.shape[0]\n",
    "\n",
    "    # Preprocessing on node features\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Preprocessing node features\")\n",
    "    if FLAGS.features:\n",
    "        features = features_init\n",
    "    else:\n",
    "        # If features are not used, replace feature matrix by identity matrix\n",
    "        features = sp.identity(num_nodes)\n",
    "    features = sparse_to_tuple(features)\n",
    "    num_features = features[2][1]\n",
    "    features_nonzero = features[1].shape[0]\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Done! \\n\")\n",
    "\n",
    "\n",
    "    # Community detection using Louvain, as a preprocessing step\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Running the Louvain algorithm for community detection\")\n",
    "        print(\"as a preprocessing step for the encoder\")\n",
    "    # Get binary community matrix (adj_louvain_init[i,j] = 1 if nodes i and j are\n",
    "    # in the same community) as well as number of communities found by Louvain\n",
    "    adj_louvain_init, nb_communities_louvain, partition = louvain_clustering(adj, FLAGS.s_reg)\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Done! Louvain has found\", nb_communities_louvain, \"communities \\n\")\n",
    "\n",
    "    # FastGAE: node sampling for stochastic subgraph decoding - used when facing scalability issues\n",
    "    if FLAGS.fastgae:\n",
    "        if FLAGS.verbose:\n",
    "            print(\"Preprocessing operations related to the FastGAE method\")\n",
    "        # Node-level p_i degree-based, core-based or uniform distribution\n",
    "        node_distribution = get_distribution(FLAGS.measure, FLAGS.alpha, adj)\n",
    "        # Node sampling for initializations\n",
    "        sampled_nodes, adj_label, adj_sampled_sparse = node_sampling(adj, node_distribution, FLAGS.nb_node_samples, FLAGS.replace)\n",
    "        if FLAGS.verbose:\n",
    "            print(\"Done! \\n\")\n",
    "    else:\n",
    "        sampled_nodes = np.array(range(FLAGS.nb_node_samples))\n",
    "\n",
    "\n",
    "    # Placeholders\n",
    "    if FLAGS.verbose:\n",
    "        print('Setting up the model and the optimizer')\n",
    "    placeholders = {\n",
    "        'features': tf.sparse_placeholder(tf.float32),\n",
    "        'adj': tf.sparse_placeholder(tf.float32),\n",
    "        'adj_layer2': tf.sparse_placeholder(tf.float32), # Only used for 2-layer GCN encoders\n",
    "        'degree_matrix': tf.sparse_placeholder(tf.float32),\n",
    "        'adj_orig': tf.sparse_placeholder(tf.float32),\n",
    "        'dropout': tf.placeholder_with_default(0., shape = ()),\n",
    "        'sampled_nodes': tf.placeholder_with_default(sampled_nodes, shape = [FLAGS.nb_node_samples])\n",
    "    }\n",
    "\n",
    "\n",
    "    # Create model\n",
    "    if FLAGS.model == 'linear_ae':\n",
    "        # Linear Graph Autoencoder\n",
    "        model = LinearModelAE(placeholders, num_features, features_nonzero)\n",
    "    elif FLAGS.model == 'linear_vae':\n",
    "        # Linear Graph Variational Autoencoder\n",
    "        model = LinearModelVAE(placeholders, num_features, num_nodes, features_nonzero)\n",
    "    elif FLAGS.model == 'gcn_ae':\n",
    "        # 2-layer GCN Graph Autoencoder\n",
    "        model = GCNModelAE(placeholders, num_features, features_nonzero)\n",
    "    elif FLAGS.model == 'gcn_vae':\n",
    "        # 2-layer GCN Graph Variational Autoencoder\n",
    "        model = GCNModelVAE(placeholders, num_features, num_nodes, features_nonzero)\n",
    "    else:\n",
    "        raise ValueError('Undefined model!')\n",
    "\n",
    "\n",
    "    # Optimizer\n",
    "    if FLAGS.fastgae:\n",
    "        num_sampled = adj_sampled_sparse.shape[0]\n",
    "        sum_sampled = adj_sampled_sparse.sum()\n",
    "        pos_weight = float(num_sampled * num_sampled - sum_sampled) / sum_sampled\n",
    "        norm = num_sampled * num_sampled / float((num_sampled * num_sampled - sum_sampled) * 2)\n",
    "    else:\n",
    "        pos_weight = float(num_nodes * num_nodes - adj.sum()) / adj.sum()\n",
    "        norm = num_nodes * num_nodes / float((num_nodes * num_nodes - adj.sum()) * 2)\n",
    "\n",
    "    if FLAGS.model in ('gcn_ae', 'linear_ae'):\n",
    "        opt = OptimizerAE(preds = model.reconstructions,\n",
    "                          labels = tf.reshape(tf.sparse_tensor_to_dense(placeholders['adj_orig'], validate_indices = False), [-1]),\n",
    "                          degree_matrix = tf.reshape(tf.sparse_tensor_to_dense(placeholders['degree_matrix'], validate_indices = False), [-1]),\n",
    "                          num_nodes = num_nodes,\n",
    "                          pos_weight = pos_weight,\n",
    "                          norm = norm,\n",
    "                          clusters_distance = model.clusters)\n",
    "    else:\n",
    "        opt = OptimizerVAE(preds = model.reconstructions,\n",
    "                           labels = tf.reshape(tf.sparse_tensor_to_dense(placeholders['adj_orig'], validate_indices = False), [-1]),\n",
    "                           degree_matrix = tf.reshape(tf.sparse_tensor_to_dense(placeholders['degree_matrix'], validate_indices = False), [-1]),\n",
    "                           model = model,\n",
    "                           num_nodes = num_nodes,\n",
    "                           pos_weight = pos_weight,\n",
    "                           norm = norm,\n",
    "                           clusters_distance = model.clusters)\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Done! \\n\")\n",
    "\n",
    "\n",
    "    # Symmetrically normalized \"message passing\" matrices\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Preprocessing on message passing matrices\")\n",
    "    adj_norm = preprocess_graph(adj + FLAGS.lamb*adj_louvain_init)\n",
    "    adj_norm_layer2 = preprocess_graph(adj)\n",
    "    if not FLAGS.fastgae:\n",
    "        adj_label = sparse_to_tuple(adj + sp.eye(num_nodes))\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Done! \\n\")\n",
    "\n",
    "\n",
    "    # Degree matrices\n",
    "    deg_matrix, deg_matrix_init = preprocess_degree(adj, FLAGS.simple)\n",
    "\n",
    "\n",
    "    # Initialize TF session\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Initializing TF session\")\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Done! \\n\")\n",
    "\n",
    "\n",
    "    # Model training\n",
    "    if FLAGS.verbose:\n",
    "        print(\"STEP 2/3 - MODEL TRAINING \\n\")\n",
    "        print(\"Starting training\")\n",
    "\n",
    "    for iter in range(FLAGS.iterations):\n",
    "\n",
    "        # Flag to compute running time for each iteration\n",
    "        t = time.time()\n",
    "\n",
    "        # Construct feed dictionary\n",
    "        feed_dict = construct_feed_dict(adj_norm, adj_norm_layer2, adj_label, features, deg_matrix, placeholders)\n",
    "        if FLAGS.fastgae:\n",
    "            # Update sampled subgraph and sampled Louvain matrix\n",
    "            feed_dict.update({placeholders['sampled_nodes']: sampled_nodes})\n",
    "            # New node sampling\n",
    "            sampled_nodes, adj_label, adj_label_sparse, = node_sampling(adj, node_distribution, FLAGS.nb_node_samples, FLAGS.replace)\n",
    "\n",
    "        # Weights update\n",
    "        outs = sess.run([opt.opt_op, opt.cost, opt.cost_adj, opt.cost_mod],\n",
    "                        feed_dict = feed_dict)\n",
    "\n",
    "        # Compute average loss\n",
    "        avg_cost = outs[1]\n",
    "        if FLAGS.verbose:\n",
    "            # Display information on the iteration\n",
    "            print(\"Iteration:\", '%04d' % (iter + 1), \"Loss:\", \"{:.5f}\".format(avg_cost),\n",
    "                  \"Time:\", \"{:.5f}\".format(time.time() - t))\n",
    "\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Done! \\n\")\n",
    "\n",
    "    # Compute embedding vectors, for evaluation\n",
    "    if FLAGS.verbose:\n",
    "        print(\"STEP 3/3 - MODEL EVALUATION \\n\")\n",
    "        print(\"Computing the final embedding vectors, for evaluation\")\n",
    "    emb = sess.run(model.z_mean, feed_dict = feed_dict)\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Done! \\n\")\n",
    "\n",
    "    # Test model: link prediction (classification edges/non-edges)\n",
    "\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Testing: link prediction\")\n",
    "    # Get ROC and AP scores\n",
    "    roc_score, ap_score = link_prediction(test_edges, test_edges_false, emb)\n",
    "    mean_roc.append(roc_score)\n",
    "    mean_ap.append(ap_score)\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Done! \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bef29d76",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-28T09:15:14.559870Z",
     "start_time": "2022-10-28T09:15:14.546823Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL RESULTS \n",
      "\n",
      "Recall: the selected task was \"Task 2\", i.e., joint community detection and link prediction, on texas\n",
      "All scores reported below are computed over the 10 run(s) \n",
      "\n",
      "Link prediction:\n",
      "\n",
      "Mean AUC score:  0.48587105624142657\n",
      "Std of AUC scores:  0.051755958970461695 \n",
      "\n",
      "Mean AP score:  0.6239076996269487\n",
      "Std of AP scores:  0.0575596967144286 \n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Report final results\n",
    "print(\"FINAL RESULTS \\n\")\n",
    "\n",
    "print('Recall: the selected task was \"Task 2\", i.e., joint community detection and link prediction, on', FLAGS.dataset)\n",
    "\n",
    "print(\"All scores reported below are computed over the\", FLAGS.nb_run, \"run(s) \\n\")\n",
    "\n",
    "print(\"Link prediction:\\n\")\n",
    "print(\"Mean AUC score: \", np.mean(mean_roc))\n",
    "print(\"Std of AUC scores: \", np.std(mean_roc), \"\\n\")\n",
    "\n",
    "print(\"Mean AP score: \", np.mean(mean_ap))\n",
    "print(\"Std of AP scores: \", np.std(mean_ap), \"\\n \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553e6eb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf1.15",
   "language": "python",
   "name": "tf1.15"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
