{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67dbcca0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-28T09:11:32.494003Z",
     "start_time": "2022-10-28T09:11:30.361214Z"
    }
   },
   "outputs": [],
   "source": [
    "from modularity_aware_gae.evaluation import community_detection, link_prediction\n",
    "from modularity_aware_gae.input_data import load_data, load_labels\n",
    "from modularity_aware_gae.louvain import louvain_clustering\n",
    "from modularity_aware_gae.model import  GCNModelAE, GCNModelVAE, LinearModelAE, LinearModelVAE\n",
    "from modularity_aware_gae.optimizer import OptimizerAE, OptimizerVAE\n",
    "from modularity_aware_gae.preprocessing import *\n",
    "from modularity_aware_gae.sampling import get_distribution, node_sampling\n",
    "import numpy as np\n",
    "import os\n",
    "import scipy.sparse as sp\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel')#添加的，不报错\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14ffd620",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-28T09:11:32.541282Z",
     "start_time": "2022-10-28T09:11:32.527678Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " \n",
      " \n",
      "[MODULARITY-AWARE GRAPH AUTOENCODERS]\n",
      " \n",
      " \n",
      " \n",
      "\n",
      "EXPERIMENTAL SETTING \n",
      "\n",
      "- Graph dataset: email\n",
      "- Mode name: linear_vae\n",
      "- Number of models to train: 10\n",
      "- Number of training iterations for each model: 200\n",
      "- Learning rate: 0.01\n",
      "- Dropout rate: 0.0\n",
      "- Use of node features in the input layer: False\n",
      "- Dimension of the output layer: 16\n",
      "- lambda: 0.0\n",
      "- beta: 0.0\n",
      "- gamma: 1.0\n",
      "- s: 2\n",
      "- FastGAE: no \n",
      "\n",
      "Final embedding vectors will be evaluated on:\n",
      "- Task 2, i.e., joint community detection and link prediction\n",
      "\n",
      " \n",
      " \n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "# Parameters related to the input data\n",
    "flags.DEFINE_string('dataset', 'email', 'Graph dataset, among: cora, citeseer, wiki, celegans, email, polbooks, texas, wisconsin')\n",
    "\n",
    "flags.DEFINE_boolean('features', False, 'Whether to include node features')\n",
    "\n",
    "\n",
    "# Parameters related to the Modularity-Aware GAE/VGAE model to train\n",
    "\n",
    "# 1/3 - General parameters associated with GAE/VGAE\n",
    "flags.DEFINE_string('model', 'linear_vae', 'Model to train, among: gcn_ae, gcn_vae, \\\n",
    "                                            linear_ae, linear_vae')\n",
    "flags.DEFINE_float('dropout', 0., 'Dropout rate')\n",
    "flags.DEFINE_integer('iterations', 200, 'Number of iterations in training')\n",
    "flags.DEFINE_float('learning_rate', 0.01, 'Initial learning rate (Adam)')\n",
    "flags.DEFINE_integer('hidden', 32, 'Dimension of the GCN hidden layer')\n",
    "flags.DEFINE_integer('dimension', 16, 'Dimension of the output layer, i.e., \\\n",
    "                                       dimension of the embedding space')\n",
    "\n",
    "# 2/3 - Additional parameters, specific to Modularity-Aware models\n",
    "flags.DEFINE_float('beta', 0.0, 'Beta hyperparameter of Mod.-Aware models')\n",
    "flags.DEFINE_float('lamb', 0.0, 'Lambda hyperparameter of Mod.-Aware models')\n",
    "flags.DEFINE_float('gamma', 1.0, 'Gamma hyperparameter of Mod.-Aware models')\n",
    "flags.DEFINE_integer('s_reg', 2, 's hyperparameter of Mod.-Aware models')\n",
    "\n",
    "# 3/3 - Additional parameters, aiming to improve scalability\n",
    "flags.DEFINE_boolean('fastgae', False, 'Whether to use the FastGAE framework')\n",
    "flags.DEFINE_integer('nb_node_samples', 1000, 'In FastGAE, number of nodes to \\\n",
    "                                               sample at each iteration, i.e., \\\n",
    "                                               size of decoded subgraph')\n",
    "flags.DEFINE_string('measure', 'degree', 'In FastGAE, node importance measure used \\\n",
    "                                          for sampling: degree, core, or uniform')\n",
    "flags.DEFINE_float('alpha', 1.0, 'alpha hyperparameter associated with the FastGAE sampling')\n",
    "flags.DEFINE_boolean('replace', False, 'Sample nodes with or without replacement')\n",
    "flags.DEFINE_boolean('simple', True, 'Use simpler (and faster) modularity in optimizers')\n",
    "\n",
    "\n",
    "# Parameters related to the experimental setup\n",
    "flags.DEFINE_string('task', 'task_2', 'task_1: pure community detection \\\n",
    "                                       task_2: joint link prediction and \\\n",
    "                                               community detection')\n",
    "flags.DEFINE_integer('nb_run', 10, 'Number of model run + test')\n",
    "flags.DEFINE_float('prop_val', 1., 'Proportion of edges in validation set \\\n",
    "                                    for the link prediction task')\n",
    "flags.DEFINE_float('prop_test', 10., 'Proportion of edges in test set \\\n",
    "                                      for the link prediction task')\n",
    "flags.DEFINE_boolean('validation', False, 'Whether to compute validation \\\n",
    "                                           results at each iteration, for \\\n",
    "                                           the link prediction task')\n",
    "flags.DEFINE_boolean('verbose', True, 'Whether to print all comments details')\n",
    "\n",
    "\n",
    "# Introductory message\n",
    "if FLAGS.verbose:\n",
    "    introductory_message()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ce16ae4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-28T09:11:32.698103Z",
     "start_time": "2022-10-28T09:11:32.572724Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADING DATA\n",
      "\n",
      "Loading the email graph\n",
      "- Number of nodes: 986\n",
      "- Use of node features: False\n",
      "Done! \n",
      " \n",
      " \n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize lists to collect final results\n",
    "mean_ami = []\n",
    "mean_ari = []\n",
    "\n",
    "mean_roc = []\n",
    "mean_ap = []\n",
    "\n",
    "# Load data\n",
    "if FLAGS.verbose:\n",
    "    print(\"LOADING DATA\\n\")\n",
    "    print(\"Loading the\", FLAGS.dataset, \"graph\")\n",
    "adj_init, features_init = load_data(FLAGS.dataset)\n",
    "#labels = load_labels(adj_init.shape[0])\n",
    "if FLAGS.verbose:\n",
    "    print(\"- Number of nodes:\", adj_init.shape[0])\n",
    "    #print(\"- Number of communities:\", len(np.unique(labels)))\n",
    "    print(\"- Use of node features:\", FLAGS.features)\n",
    "    print(\"Done! \\n \\n \\n \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd19b933",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-28T09:13:09.959514Z",
     "start_time": "2022-10-28T09:11:32.731726Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENTS ON MODEL 1 / 10 \n",
      "\n",
      "STEP 1/3 - PREPROCESSING STEPS \n",
      "\n",
      "Masking some edges from the training graph, for link prediction\n",
      "(validation set: 1.0 % of edges - test set: 10.0 % of edges)\n",
      "Done! \n",
      "\n",
      "Preprocessing node features\n",
      "Done! \n",
      "\n",
      "Running the Louvain algorithm for community detection\n",
      "as a preprocessing step for the encoder\n",
      "Done! Louvain has found 20 communities \n",
      "\n",
      "Setting up the model and the optimizer\n",
      "Done! \n",
      "\n",
      "Preprocessing on message passing matrices\n",
      "Done! \n",
      "\n",
      "Initializing TF session\n",
      "Done! \n",
      "\n",
      "STEP 2/3 - MODEL TRAINING \n",
      "\n",
      "Starting training\n",
      "Iteration: 0001 Loss: 1.71865 Time: 0.25320\n",
      "Iteration: 0002 Loss: 1.69200 Time: 0.04097\n",
      "Iteration: 0003 Loss: 1.65148 Time: 0.04131\n",
      "Iteration: 0004 Loss: 1.67516 Time: 0.04212\n",
      "Iteration: 0005 Loss: 1.63269 Time: 0.04170\n",
      "Iteration: 0006 Loss: 1.60129 Time: 0.04296\n",
      "Iteration: 0007 Loss: 1.56575 Time: 0.04234\n",
      "Iteration: 0008 Loss: 1.56472 Time: 0.04196\n",
      "Iteration: 0009 Loss: 1.51188 Time: 0.04437\n",
      "Iteration: 0010 Loss: 1.49677 Time: 0.04091\n",
      "Iteration: 0011 Loss: 1.50936 Time: 0.04294\n",
      "Iteration: 0012 Loss: 1.48207 Time: 0.04124\n",
      "Iteration: 0013 Loss: 1.40801 Time: 0.04146\n",
      "Iteration: 0014 Loss: 1.39619 Time: 0.04241\n",
      "Iteration: 0015 Loss: 1.41240 Time: 0.04007\n",
      "Iteration: 0016 Loss: 1.39666 Time: 0.03895\n",
      "Iteration: 0017 Loss: 1.36630 Time: 0.04238\n",
      "Iteration: 0018 Loss: 1.37291 Time: 0.03888\n",
      "Iteration: 0019 Loss: 1.33941 Time: 0.03840\n",
      "Iteration: 0020 Loss: 1.33207 Time: 0.04030\n",
      "Iteration: 0021 Loss: 1.31018 Time: 0.04154\n",
      "Iteration: 0022 Loss: 1.30578 Time: 0.04440\n",
      "Iteration: 0023 Loss: 1.26743 Time: 0.04011\n",
      "Iteration: 0024 Loss: 1.26689 Time: 0.04337\n",
      "Iteration: 0025 Loss: 1.25480 Time: 0.04479\n",
      "Iteration: 0026 Loss: 1.26358 Time: 0.04092\n",
      "Iteration: 0027 Loss: 1.20760 Time: 0.04296\n",
      "Iteration: 0028 Loss: 1.20094 Time: 0.04204\n",
      "Iteration: 0029 Loss: 1.19558 Time: 0.04399\n",
      "Iteration: 0030 Loss: 1.17774 Time: 0.04265\n",
      "Iteration: 0031 Loss: 1.17259 Time: 0.04127\n",
      "Iteration: 0032 Loss: 1.13678 Time: 0.04300\n",
      "Iteration: 0033 Loss: 1.11910 Time: 0.04195\n",
      "Iteration: 0034 Loss: 1.11774 Time: 0.04399\n",
      "Iteration: 0035 Loss: 1.12742 Time: 0.04056\n",
      "Iteration: 0036 Loss: 1.10172 Time: 0.04132\n",
      "Iteration: 0037 Loss: 1.08763 Time: 0.04086\n",
      "Iteration: 0038 Loss: 1.08166 Time: 0.04163\n",
      "Iteration: 0039 Loss: 1.08614 Time: 0.04039\n",
      "Iteration: 0040 Loss: 1.04075 Time: 0.04296\n",
      "Iteration: 0041 Loss: 1.02503 Time: 0.04035\n",
      "Iteration: 0042 Loss: 1.05368 Time: 0.04207\n",
      "Iteration: 0043 Loss: 1.01748 Time: 0.04339\n",
      "Iteration: 0044 Loss: 1.02514 Time: 0.04125\n",
      "Iteration: 0045 Loss: 1.00876 Time: 0.04155\n",
      "Iteration: 0046 Loss: 1.00300 Time: 0.04199\n",
      "Iteration: 0047 Loss: 0.99192 Time: 0.04295\n",
      "Iteration: 0048 Loss: 0.99577 Time: 0.03962\n",
      "Iteration: 0049 Loss: 0.99661 Time: 0.04318\n",
      "Iteration: 0050 Loss: 0.96575 Time: 0.04071\n",
      "Iteration: 0051 Loss: 0.95921 Time: 0.04361\n",
      "Iteration: 0052 Loss: 0.95578 Time: 0.04249\n",
      "Iteration: 0053 Loss: 0.94531 Time: 0.04042\n",
      "Iteration: 0054 Loss: 0.93274 Time: 0.04199\n",
      "Iteration: 0055 Loss: 0.94888 Time: 0.04250\n",
      "Iteration: 0056 Loss: 0.93999 Time: 0.04250\n",
      "Iteration: 0057 Loss: 0.92546 Time: 0.04321\n",
      "Iteration: 0058 Loss: 0.92731 Time: 0.04179\n",
      "Iteration: 0059 Loss: 0.92234 Time: 0.04300\n",
      "Iteration: 0060 Loss: 0.90611 Time: 0.04207\n",
      "Iteration: 0061 Loss: 0.90049 Time: 0.04100\n",
      "Iteration: 0062 Loss: 0.90581 Time: 0.04200\n",
      "Iteration: 0063 Loss: 0.88371 Time: 0.04075\n",
      "Iteration: 0064 Loss: 0.88867 Time: 0.03847\n",
      "Iteration: 0065 Loss: 0.88681 Time: 0.03864\n",
      "Iteration: 0066 Loss: 0.89028 Time: 0.04073\n",
      "Iteration: 0067 Loss: 0.87686 Time: 0.03926\n",
      "Iteration: 0068 Loss: 0.88237 Time: 0.03986\n",
      "Iteration: 0069 Loss: 0.85873 Time: 0.03915\n",
      "Iteration: 0070 Loss: 0.85558 Time: 0.04376\n",
      "Iteration: 0071 Loss: 0.85172 Time: 0.04023\n",
      "Iteration: 0072 Loss: 0.86918 Time: 0.04196\n",
      "Iteration: 0073 Loss: 0.84654 Time: 0.04124\n",
      "Iteration: 0074 Loss: 0.85233 Time: 0.04256\n",
      "Iteration: 0075 Loss: 0.84441 Time: 0.04060\n",
      "Iteration: 0076 Loss: 0.84135 Time: 0.04237\n",
      "Iteration: 0077 Loss: 0.83783 Time: 0.04396\n",
      "Iteration: 0078 Loss: 0.82765 Time: 0.04100\n",
      "Iteration: 0079 Loss: 0.82532 Time: 0.04104\n",
      "Iteration: 0080 Loss: 0.81131 Time: 0.04024\n",
      "Iteration: 0081 Loss: 0.81479 Time: 0.04157\n",
      "Iteration: 0082 Loss: 0.81046 Time: 0.04239\n",
      "Iteration: 0083 Loss: 0.81269 Time: 0.04200\n",
      "Iteration: 0084 Loss: 0.79877 Time: 0.04355\n",
      "Iteration: 0085 Loss: 0.79723 Time: 0.04233\n",
      "Iteration: 0086 Loss: 0.79012 Time: 0.04032\n",
      "Iteration: 0087 Loss: 0.78278 Time: 0.04196\n",
      "Iteration: 0088 Loss: 0.78347 Time: 0.04066\n",
      "Iteration: 0089 Loss: 0.79298 Time: 0.04104\n",
      "Iteration: 0090 Loss: 0.78412 Time: 0.04193\n",
      "Iteration: 0091 Loss: 0.78350 Time: 0.04062\n",
      "Iteration: 0092 Loss: 0.78578 Time: 0.04195\n",
      "Iteration: 0093 Loss: 0.77139 Time: 0.04157\n",
      "Iteration: 0094 Loss: 0.76897 Time: 0.04200\n",
      "Iteration: 0095 Loss: 0.77902 Time: 0.04174\n",
      "Iteration: 0096 Loss: 0.76565 Time: 0.04252\n",
      "Iteration: 0097 Loss: 0.75978 Time: 0.04296\n",
      "Iteration: 0098 Loss: 0.75626 Time: 0.04200\n",
      "Iteration: 0099 Loss: 0.76141 Time: 0.04236\n",
      "Iteration: 0100 Loss: 0.75213 Time: 0.04296\n",
      "Iteration: 0101 Loss: 0.75881 Time: 0.04166\n",
      "Iteration: 0102 Loss: 0.75180 Time: 0.04100\n",
      "Iteration: 0103 Loss: 0.74241 Time: 0.04197\n",
      "Iteration: 0104 Loss: 0.74977 Time: 0.04096\n",
      "Iteration: 0105 Loss: 0.74053 Time: 0.04065\n",
      "Iteration: 0106 Loss: 0.73977 Time: 0.04028\n",
      "Iteration: 0107 Loss: 0.73181 Time: 0.04257\n",
      "Iteration: 0108 Loss: 0.73514 Time: 0.04300\n",
      "Iteration: 0109 Loss: 0.73911 Time: 0.04099\n",
      "Iteration: 0110 Loss: 0.72578 Time: 0.04054\n",
      "Iteration: 0111 Loss: 0.72998 Time: 0.03828\n",
      "Iteration: 0112 Loss: 0.72825 Time: 0.04223\n",
      "Iteration: 0113 Loss: 0.72393 Time: 0.04071\n",
      "Iteration: 0114 Loss: 0.73181 Time: 0.03888\n",
      "Iteration: 0115 Loss: 0.71830 Time: 0.04023\n",
      "Iteration: 0116 Loss: 0.71882 Time: 0.04008\n",
      "Iteration: 0117 Loss: 0.71512 Time: 0.04108\n",
      "Iteration: 0118 Loss: 0.71662 Time: 0.04163\n",
      "Iteration: 0119 Loss: 0.71079 Time: 0.04196\n",
      "Iteration: 0120 Loss: 0.71150 Time: 0.04166\n",
      "Iteration: 0121 Loss: 0.70876 Time: 0.04130\n",
      "Iteration: 0122 Loss: 0.70742 Time: 0.04187\n",
      "Iteration: 0123 Loss: 0.70622 Time: 0.04085\n",
      "Iteration: 0124 Loss: 0.70682 Time: 0.04201\n",
      "Iteration: 0125 Loss: 0.69646 Time: 0.04184\n",
      "Iteration: 0126 Loss: 0.69920 Time: 0.04155\n",
      "Iteration: 0127 Loss: 0.69790 Time: 0.03995\n",
      "Iteration: 0128 Loss: 0.69326 Time: 0.04241\n",
      "Iteration: 0129 Loss: 0.69255 Time: 0.04106\n",
      "Iteration: 0130 Loss: 0.69143 Time: 0.04389\n",
      "Iteration: 0131 Loss: 0.70036 Time: 0.04030\n",
      "Iteration: 0132 Loss: 0.69146 Time: 0.04195\n",
      "Iteration: 0133 Loss: 0.68516 Time: 0.04304\n",
      "Iteration: 0134 Loss: 0.68649 Time: 0.04191\n",
      "Iteration: 0135 Loss: 0.69063 Time: 0.04399\n",
      "Iteration: 0136 Loss: 0.68228 Time: 0.04241\n",
      "Iteration: 0137 Loss: 0.67442 Time: 0.04157\n",
      "Iteration: 0138 Loss: 0.67854 Time: 0.04050\n",
      "Iteration: 0139 Loss: 0.67506 Time: 0.04146\n",
      "Iteration: 0140 Loss: 0.67514 Time: 0.04228\n",
      "Iteration: 0141 Loss: 0.67358 Time: 0.04169\n",
      "Iteration: 0142 Loss: 0.68136 Time: 0.04031\n",
      "Iteration: 0143 Loss: 0.67214 Time: 0.04387\n",
      "Iteration: 0144 Loss: 0.67068 Time: 0.04167\n",
      "Iteration: 0145 Loss: 0.67515 Time: 0.04177\n",
      "Iteration: 0146 Loss: 0.67059 Time: 0.04253\n",
      "Iteration: 0147 Loss: 0.66752 Time: 0.04109\n",
      "Iteration: 0148 Loss: 0.65880 Time: 0.04129\n",
      "Iteration: 0149 Loss: 0.65731 Time: 0.03967\n",
      "Iteration: 0150 Loss: 0.66355 Time: 0.04315\n",
      "Iteration: 0151 Loss: 0.66376 Time: 0.04073\n",
      "Iteration: 0152 Loss: 0.65938 Time: 0.03965\n",
      "Iteration: 0153 Loss: 0.65740 Time: 0.04017\n",
      "Iteration: 0154 Loss: 0.66265 Time: 0.04299\n",
      "Iteration: 0155 Loss: 0.65701 Time: 0.04040\n",
      "Iteration: 0156 Loss: 0.65175 Time: 0.04216\n",
      "Iteration: 0157 Loss: 0.65626 Time: 0.03893\n",
      "Iteration: 0158 Loss: 0.65787 Time: 0.04350\n",
      "Iteration: 0159 Loss: 0.65796 Time: 0.04023\n",
      "Iteration: 0160 Loss: 0.65796 Time: 0.03906\n",
      "Iteration: 0161 Loss: 0.64948 Time: 0.04079\n",
      "Iteration: 0162 Loss: 0.64828 Time: 0.04082\n",
      "Iteration: 0163 Loss: 0.64885 Time: 0.04062\n",
      "Iteration: 0164 Loss: 0.66040 Time: 0.04012\n",
      "Iteration: 0165 Loss: 0.64432 Time: 0.04065\n",
      "Iteration: 0166 Loss: 0.64985 Time: 0.04300\n",
      "Iteration: 0167 Loss: 0.64665 Time: 0.04018\n",
      "Iteration: 0168 Loss: 0.64872 Time: 0.04099\n",
      "Iteration: 0169 Loss: 0.64623 Time: 0.04100\n",
      "Iteration: 0170 Loss: 0.65028 Time: 0.04107\n",
      "Iteration: 0171 Loss: 0.64297 Time: 0.04114\n",
      "Iteration: 0172 Loss: 0.64157 Time: 0.04145\n",
      "Iteration: 0173 Loss: 0.63948 Time: 0.04263\n",
      "Iteration: 0174 Loss: 0.63618 Time: 0.04145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0175 Loss: 0.63867 Time: 0.04119\n",
      "Iteration: 0176 Loss: 0.63405 Time: 0.04119\n",
      "Iteration: 0177 Loss: 0.63420 Time: 0.04057\n",
      "Iteration: 0178 Loss: 0.63739 Time: 0.04200\n",
      "Iteration: 0179 Loss: 0.63709 Time: 0.04056\n",
      "Iteration: 0180 Loss: 0.62903 Time: 0.04200\n",
      "Iteration: 0181 Loss: 0.63223 Time: 0.04026\n",
      "Iteration: 0182 Loss: 0.63827 Time: 0.04203\n",
      "Iteration: 0183 Loss: 0.62976 Time: 0.04240\n",
      "Iteration: 0184 Loss: 0.62710 Time: 0.04354\n",
      "Iteration: 0185 Loss: 0.62544 Time: 0.03961\n",
      "Iteration: 0186 Loss: 0.62476 Time: 0.04084\n",
      "Iteration: 0187 Loss: 0.62543 Time: 0.03901\n",
      "Iteration: 0188 Loss: 0.63251 Time: 0.04194\n",
      "Iteration: 0189 Loss: 0.62302 Time: 0.04008\n",
      "Iteration: 0190 Loss: 0.62902 Time: 0.03993\n",
      "Iteration: 0191 Loss: 0.62527 Time: 0.04200\n",
      "Iteration: 0192 Loss: 0.62482 Time: 0.04009\n",
      "Iteration: 0193 Loss: 0.63103 Time: 0.04113\n",
      "Iteration: 0194 Loss: 0.62607 Time: 0.04194\n",
      "Iteration: 0195 Loss: 0.62174 Time: 0.04092\n",
      "Iteration: 0196 Loss: 0.62687 Time: 0.04135\n",
      "Iteration: 0197 Loss: 0.61918 Time: 0.04069\n",
      "Iteration: 0198 Loss: 0.62001 Time: 0.04184\n",
      "Iteration: 0199 Loss: 0.61852 Time: 0.04098\n",
      "Iteration: 0200 Loss: 0.62460 Time: 0.04163\n",
      "Done! \n",
      "\n",
      "STEP 3/3 - MODEL EVALUATION \n",
      "\n",
      "Computing the final embedding vectors, for evaluation\n",
      "Done! \n",
      "\n",
      "Testing: link prediction\n",
      "Done! \n",
      "\n",
      "EXPERIMENTS ON MODEL 2 / 10 \n",
      "\n",
      "STEP 1/3 - PREPROCESSING STEPS \n",
      "\n",
      "Masking some edges from the training graph, for link prediction\n",
      "(validation set: 1.0 % of edges - test set: 10.0 % of edges)\n",
      "Done! \n",
      "\n",
      "Preprocessing node features\n",
      "Done! \n",
      "\n",
      "Running the Louvain algorithm for community detection\n",
      "as a preprocessing step for the encoder\n",
      "Done! Louvain has found 21 communities \n",
      "\n",
      "Setting up the model and the optimizer\n",
      "Done! \n",
      "\n",
      "Preprocessing on message passing matrices\n",
      "Done! \n",
      "\n",
      "Initializing TF session\n",
      "Done! \n",
      "\n",
      "STEP 2/3 - MODEL TRAINING \n",
      "\n",
      "Starting training\n",
      "Iteration: 0001 Loss: 1.75403 Time: 0.23414\n",
      "Iteration: 0002 Loss: 1.71790 Time: 0.04122\n",
      "Iteration: 0003 Loss: 1.69017 Time: 0.03999\n",
      "Iteration: 0004 Loss: 1.64800 Time: 0.04399\n",
      "Iteration: 0005 Loss: 1.60617 Time: 0.03999\n",
      "Iteration: 0006 Loss: 1.61142 Time: 0.04100\n",
      "Iteration: 0007 Loss: 1.59977 Time: 0.04200\n",
      "Iteration: 0008 Loss: 1.57249 Time: 0.03946\n",
      "Iteration: 0009 Loss: 1.50687 Time: 0.04189\n",
      "Iteration: 0010 Loss: 1.49966 Time: 0.04127\n",
      "Iteration: 0011 Loss: 1.49503 Time: 0.04100\n",
      "Iteration: 0012 Loss: 1.47055 Time: 0.04099\n",
      "Iteration: 0013 Loss: 1.42761 Time: 0.04200\n",
      "Iteration: 0014 Loss: 1.40482 Time: 0.04100\n",
      "Iteration: 0015 Loss: 1.42325 Time: 0.04101\n",
      "Iteration: 0016 Loss: 1.38530 Time: 0.04163\n",
      "Iteration: 0017 Loss: 1.37589 Time: 0.04155\n",
      "Iteration: 0018 Loss: 1.33710 Time: 0.03999\n",
      "Iteration: 0019 Loss: 1.32997 Time: 0.04073\n",
      "Iteration: 0020 Loss: 1.34222 Time: 0.03985\n",
      "Iteration: 0021 Loss: 1.30187 Time: 0.03967\n",
      "Iteration: 0022 Loss: 1.30045 Time: 0.04324\n",
      "Iteration: 0023 Loss: 1.26587 Time: 0.04075\n",
      "Iteration: 0024 Loss: 1.27587 Time: 0.04217\n",
      "Iteration: 0025 Loss: 1.25314 Time: 0.03918\n",
      "Iteration: 0026 Loss: 1.25425 Time: 0.03860\n",
      "Iteration: 0027 Loss: 1.23334 Time: 0.03882\n",
      "Iteration: 0028 Loss: 1.24054 Time: 0.04111\n",
      "Iteration: 0029 Loss: 1.20374 Time: 0.04280\n",
      "Iteration: 0030 Loss: 1.20685 Time: 0.03954\n",
      "Iteration: 0031 Loss: 1.18058 Time: 0.03983\n",
      "Iteration: 0032 Loss: 1.13213 Time: 0.03998\n",
      "Iteration: 0033 Loss: 1.15379 Time: 0.04201\n",
      "Iteration: 0034 Loss: 1.14089 Time: 0.04540\n",
      "Iteration: 0035 Loss: 1.11979 Time: 0.04060\n",
      "Iteration: 0036 Loss: 1.10276 Time: 0.04199\n",
      "Iteration: 0037 Loss: 1.10404 Time: 0.04245\n",
      "Iteration: 0038 Loss: 1.09338 Time: 0.04049\n",
      "Iteration: 0039 Loss: 1.07641 Time: 0.04184\n",
      "Iteration: 0040 Loss: 1.07734 Time: 0.04088\n",
      "Iteration: 0041 Loss: 1.07130 Time: 0.04040\n",
      "Iteration: 0042 Loss: 1.04068 Time: 0.04051\n",
      "Iteration: 0043 Loss: 1.04115 Time: 0.04072\n",
      "Iteration: 0044 Loss: 1.02644 Time: 0.04203\n",
      "Iteration: 0045 Loss: 1.00914 Time: 0.04086\n",
      "Iteration: 0046 Loss: 1.01203 Time: 0.04166\n",
      "Iteration: 0047 Loss: 1.00301 Time: 0.03986\n",
      "Iteration: 0048 Loss: 0.98450 Time: 0.04040\n",
      "Iteration: 0049 Loss: 0.98179 Time: 0.04065\n",
      "Iteration: 0050 Loss: 0.97427 Time: 0.04100\n",
      "Iteration: 0051 Loss: 0.95867 Time: 0.04076\n",
      "Iteration: 0052 Loss: 0.96097 Time: 0.04057\n",
      "Iteration: 0053 Loss: 0.95701 Time: 0.04050\n",
      "Iteration: 0054 Loss: 0.94790 Time: 0.04149\n",
      "Iteration: 0055 Loss: 0.94498 Time: 0.04199\n",
      "Iteration: 0056 Loss: 0.93572 Time: 0.04429\n",
      "Iteration: 0057 Loss: 0.92239 Time: 0.04116\n",
      "Iteration: 0058 Loss: 0.91287 Time: 0.04108\n",
      "Iteration: 0059 Loss: 0.90428 Time: 0.03991\n",
      "Iteration: 0060 Loss: 0.90587 Time: 0.04198\n",
      "Iteration: 0061 Loss: 0.90803 Time: 0.04043\n",
      "Iteration: 0062 Loss: 0.91689 Time: 0.04031\n",
      "Iteration: 0063 Loss: 0.89469 Time: 0.03976\n",
      "Iteration: 0064 Loss: 0.88638 Time: 0.04121\n",
      "Iteration: 0065 Loss: 0.89106 Time: 0.04112\n",
      "Iteration: 0066 Loss: 0.88640 Time: 0.04108\n",
      "Iteration: 0067 Loss: 0.88377 Time: 0.04147\n",
      "Iteration: 0068 Loss: 0.85754 Time: 0.04093\n",
      "Iteration: 0069 Loss: 0.87011 Time: 0.04051\n",
      "Iteration: 0070 Loss: 0.85835 Time: 0.04062\n",
      "Iteration: 0071 Loss: 0.86256 Time: 0.04290\n",
      "Iteration: 0072 Loss: 0.84446 Time: 0.04000\n",
      "Iteration: 0073 Loss: 0.85184 Time: 0.03984\n",
      "Iteration: 0074 Loss: 0.84750 Time: 0.03891\n",
      "Iteration: 0075 Loss: 0.83952 Time: 0.03819\n",
      "Iteration: 0076 Loss: 0.84635 Time: 0.03809\n",
      "Iteration: 0077 Loss: 0.84210 Time: 0.04079\n",
      "Iteration: 0078 Loss: 0.83844 Time: 0.04051\n",
      "Iteration: 0079 Loss: 0.83302 Time: 0.03803\n",
      "Iteration: 0080 Loss: 0.82511 Time: 0.04307\n",
      "Iteration: 0081 Loss: 0.81405 Time: 0.04097\n",
      "Iteration: 0082 Loss: 0.82024 Time: 0.04018\n",
      "Iteration: 0083 Loss: 0.82658 Time: 0.04199\n",
      "Iteration: 0084 Loss: 0.80394 Time: 0.04100\n",
      "Iteration: 0085 Loss: 0.81781 Time: 0.04104\n",
      "Iteration: 0086 Loss: 0.80294 Time: 0.03925\n",
      "Iteration: 0087 Loss: 0.79395 Time: 0.04047\n",
      "Iteration: 0088 Loss: 0.80044 Time: 0.04113\n",
      "Iteration: 0089 Loss: 0.78177 Time: 0.04085\n",
      "Iteration: 0090 Loss: 0.78549 Time: 0.03984\n",
      "Iteration: 0091 Loss: 0.78600 Time: 0.04100\n",
      "Iteration: 0092 Loss: 0.78224 Time: 0.03898\n",
      "Iteration: 0093 Loss: 0.78411 Time: 0.04082\n",
      "Iteration: 0094 Loss: 0.77253 Time: 0.03922\n",
      "Iteration: 0095 Loss: 0.77807 Time: 0.03858\n",
      "Iteration: 0096 Loss: 0.77215 Time: 0.04199\n",
      "Iteration: 0097 Loss: 0.76418 Time: 0.04058\n",
      "Iteration: 0098 Loss: 0.75929 Time: 0.04211\n",
      "Iteration: 0099 Loss: 0.75638 Time: 0.04004\n",
      "Iteration: 0100 Loss: 0.75777 Time: 0.04099\n",
      "Iteration: 0101 Loss: 0.75527 Time: 0.04132\n",
      "Iteration: 0102 Loss: 0.74750 Time: 0.04120\n",
      "Iteration: 0103 Loss: 0.74918 Time: 0.04032\n",
      "Iteration: 0104 Loss: 0.75291 Time: 0.04122\n",
      "Iteration: 0105 Loss: 0.74514 Time: 0.04392\n",
      "Iteration: 0106 Loss: 0.74616 Time: 0.04242\n",
      "Iteration: 0107 Loss: 0.74917 Time: 0.04016\n",
      "Iteration: 0108 Loss: 0.74082 Time: 0.04023\n",
      "Iteration: 0109 Loss: 0.73040 Time: 0.04254\n",
      "Iteration: 0110 Loss: 0.72519 Time: 0.04189\n",
      "Iteration: 0111 Loss: 0.72310 Time: 0.03973\n",
      "Iteration: 0112 Loss: 0.73147 Time: 0.04100\n",
      "Iteration: 0113 Loss: 0.72499 Time: 0.04008\n",
      "Iteration: 0114 Loss: 0.72123 Time: 0.04094\n",
      "Iteration: 0115 Loss: 0.72230 Time: 0.03947\n",
      "Iteration: 0116 Loss: 0.72942 Time: 0.04223\n",
      "Iteration: 0117 Loss: 0.72554 Time: 0.04029\n",
      "Iteration: 0118 Loss: 0.70984 Time: 0.04271\n",
      "Iteration: 0119 Loss: 0.71173 Time: 0.04208\n",
      "Iteration: 0120 Loss: 0.71317 Time: 0.04059\n",
      "Iteration: 0121 Loss: 0.70747 Time: 0.04085\n",
      "Iteration: 0122 Loss: 0.71289 Time: 0.04100\n",
      "Iteration: 0123 Loss: 0.71301 Time: 0.03929\n",
      "Iteration: 0124 Loss: 0.70205 Time: 0.03998\n",
      "Iteration: 0125 Loss: 0.70567 Time: 0.03847\n",
      "Iteration: 0126 Loss: 0.70326 Time: 0.03928\n",
      "Iteration: 0127 Loss: 0.70170 Time: 0.03917\n",
      "Iteration: 0128 Loss: 0.68998 Time: 0.03863\n",
      "Iteration: 0129 Loss: 0.70194 Time: 0.04109\n",
      "Iteration: 0130 Loss: 0.69003 Time: 0.04171\n",
      "Iteration: 0131 Loss: 0.68601 Time: 0.04165\n",
      "Iteration: 0132 Loss: 0.68654 Time: 0.03902\n",
      "Iteration: 0133 Loss: 0.68738 Time: 0.04300\n",
      "Iteration: 0134 Loss: 0.68887 Time: 0.03946\n",
      "Iteration: 0135 Loss: 0.68532 Time: 0.04101\n",
      "Iteration: 0136 Loss: 0.68683 Time: 0.04099\n",
      "Iteration: 0137 Loss: 0.68403 Time: 0.04103\n",
      "Iteration: 0138 Loss: 0.68846 Time: 0.03987\n",
      "Iteration: 0139 Loss: 0.67650 Time: 0.04090\n",
      "Iteration: 0140 Loss: 0.67554 Time: 0.04062\n",
      "Iteration: 0141 Loss: 0.67468 Time: 0.04105\n",
      "Iteration: 0142 Loss: 0.67613 Time: 0.03980\n",
      "Iteration: 0143 Loss: 0.67698 Time: 0.03939\n",
      "Iteration: 0144 Loss: 0.66512 Time: 0.03973\n",
      "Iteration: 0145 Loss: 0.67561 Time: 0.03941\n",
      "Iteration: 0146 Loss: 0.66130 Time: 0.04128\n",
      "Iteration: 0147 Loss: 0.66636 Time: 0.04300\n",
      "Iteration: 0148 Loss: 0.66306 Time: 0.04102\n",
      "Iteration: 0149 Loss: 0.66706 Time: 0.04168\n",
      "Iteration: 0150 Loss: 0.65859 Time: 0.04016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0151 Loss: 0.66087 Time: 0.04233\n",
      "Iteration: 0152 Loss: 0.66176 Time: 0.03976\n",
      "Iteration: 0153 Loss: 0.66346 Time: 0.04272\n",
      "Iteration: 0154 Loss: 0.65712 Time: 0.04000\n",
      "Iteration: 0155 Loss: 0.66658 Time: 0.04100\n",
      "Iteration: 0156 Loss: 0.66317 Time: 0.04299\n",
      "Iteration: 0157 Loss: 0.66273 Time: 0.04379\n",
      "Iteration: 0158 Loss: 0.65622 Time: 0.04037\n",
      "Iteration: 0159 Loss: 0.66253 Time: 0.04113\n",
      "Iteration: 0160 Loss: 0.66008 Time: 0.04058\n",
      "Iteration: 0161 Loss: 0.64794 Time: 0.04082\n",
      "Iteration: 0162 Loss: 0.65864 Time: 0.04037\n",
      "Iteration: 0163 Loss: 0.65722 Time: 0.04116\n",
      "Iteration: 0164 Loss: 0.64814 Time: 0.03999\n",
      "Iteration: 0165 Loss: 0.64733 Time: 0.04100\n",
      "Iteration: 0166 Loss: 0.65259 Time: 0.04158\n",
      "Iteration: 0167 Loss: 0.64971 Time: 0.04020\n",
      "Iteration: 0168 Loss: 0.65077 Time: 0.04285\n",
      "Iteration: 0169 Loss: 0.64785 Time: 0.04036\n",
      "Iteration: 0170 Loss: 0.64419 Time: 0.04116\n",
      "Iteration: 0171 Loss: 0.64206 Time: 0.03912\n",
      "Iteration: 0172 Loss: 0.63978 Time: 0.03948\n",
      "Iteration: 0173 Loss: 0.63985 Time: 0.04150\n",
      "Iteration: 0174 Loss: 0.64226 Time: 0.03960\n",
      "Iteration: 0175 Loss: 0.64251 Time: 0.04003\n",
      "Iteration: 0176 Loss: 0.64240 Time: 0.03773\n",
      "Iteration: 0177 Loss: 0.64139 Time: 0.04020\n",
      "Iteration: 0178 Loss: 0.63783 Time: 0.04193\n",
      "Iteration: 0179 Loss: 0.63774 Time: 0.04121\n",
      "Iteration: 0180 Loss: 0.63125 Time: 0.04107\n",
      "Iteration: 0181 Loss: 0.63733 Time: 0.04281\n",
      "Iteration: 0182 Loss: 0.63876 Time: 0.04362\n",
      "Iteration: 0183 Loss: 0.63109 Time: 0.04228\n",
      "Iteration: 0184 Loss: 0.63177 Time: 0.04146\n",
      "Iteration: 0185 Loss: 0.63170 Time: 0.04013\n",
      "Iteration: 0186 Loss: 0.63095 Time: 0.04100\n",
      "Iteration: 0187 Loss: 0.63144 Time: 0.04288\n",
      "Iteration: 0188 Loss: 0.63335 Time: 0.04065\n",
      "Iteration: 0189 Loss: 0.63540 Time: 0.04214\n",
      "Iteration: 0190 Loss: 0.62571 Time: 0.04035\n",
      "Iteration: 0191 Loss: 0.62517 Time: 0.04068\n",
      "Iteration: 0192 Loss: 0.63004 Time: 0.04029\n",
      "Iteration: 0193 Loss: 0.62073 Time: 0.04091\n",
      "Iteration: 0194 Loss: 0.62249 Time: 0.04234\n",
      "Iteration: 0195 Loss: 0.62421 Time: 0.04303\n",
      "Iteration: 0196 Loss: 0.61665 Time: 0.04095\n",
      "Iteration: 0197 Loss: 0.61736 Time: 0.04087\n",
      "Iteration: 0198 Loss: 0.62269 Time: 0.04168\n",
      "Iteration: 0199 Loss: 0.61845 Time: 0.04047\n",
      "Iteration: 0200 Loss: 0.61816 Time: 0.04134\n",
      "Done! \n",
      "\n",
      "STEP 3/3 - MODEL EVALUATION \n",
      "\n",
      "Computing the final embedding vectors, for evaluation\n",
      "Done! \n",
      "\n",
      "Testing: link prediction\n",
      "Done! \n",
      "\n",
      "EXPERIMENTS ON MODEL 3 / 10 \n",
      "\n",
      "STEP 1/3 - PREPROCESSING STEPS \n",
      "\n",
      "Masking some edges from the training graph, for link prediction\n",
      "(validation set: 1.0 % of edges - test set: 10.0 % of edges)\n",
      "Done! \n",
      "\n",
      "Preprocessing node features\n",
      "Done! \n",
      "\n",
      "Running the Louvain algorithm for community detection\n",
      "as a preprocessing step for the encoder\n",
      "Done! Louvain has found 14 communities \n",
      "\n",
      "Setting up the model and the optimizer\n",
      "Done! \n",
      "\n",
      "Preprocessing on message passing matrices\n",
      "Done! \n",
      "\n",
      "Initializing TF session\n",
      "Done! \n",
      "\n",
      "STEP 2/3 - MODEL TRAINING \n",
      "\n",
      "Starting training\n",
      "Iteration: 0001 Loss: 1.70324 Time: 0.24851\n",
      "Iteration: 0002 Loss: 1.70321 Time: 0.04143\n",
      "Iteration: 0003 Loss: 1.70838 Time: 0.04201\n",
      "Iteration: 0004 Loss: 1.64204 Time: 0.04099\n",
      "Iteration: 0005 Loss: 1.61003 Time: 0.04178\n",
      "Iteration: 0006 Loss: 1.62801 Time: 0.04265\n",
      "Iteration: 0007 Loss: 1.58155 Time: 0.04499\n",
      "Iteration: 0008 Loss: 1.54146 Time: 0.04206\n",
      "Iteration: 0009 Loss: 1.53294 Time: 0.04132\n",
      "Iteration: 0010 Loss: 1.51241 Time: 0.04052\n",
      "Iteration: 0011 Loss: 1.50036 Time: 0.04086\n",
      "Iteration: 0012 Loss: 1.47489 Time: 0.04104\n",
      "Iteration: 0013 Loss: 1.44807 Time: 0.04401\n",
      "Iteration: 0014 Loss: 1.45306 Time: 0.04099\n",
      "Iteration: 0015 Loss: 1.42020 Time: 0.04147\n",
      "Iteration: 0016 Loss: 1.36882 Time: 0.04059\n",
      "Iteration: 0017 Loss: 1.37529 Time: 0.04128\n",
      "Iteration: 0018 Loss: 1.36834 Time: 0.04200\n",
      "Iteration: 0019 Loss: 1.35095 Time: 0.04096\n",
      "Iteration: 0020 Loss: 1.30320 Time: 0.04104\n",
      "Iteration: 0021 Loss: 1.31622 Time: 0.04312\n",
      "Iteration: 0022 Loss: 1.29798 Time: 0.03932\n",
      "Iteration: 0023 Loss: 1.29395 Time: 0.04037\n",
      "Iteration: 0024 Loss: 1.25774 Time: 0.04205\n",
      "Iteration: 0025 Loss: 1.23566 Time: 0.04205\n",
      "Iteration: 0026 Loss: 1.22863 Time: 0.04200\n",
      "Iteration: 0027 Loss: 1.21973 Time: 0.04057\n",
      "Iteration: 0028 Loss: 1.21395 Time: 0.04143\n",
      "Iteration: 0029 Loss: 1.18928 Time: 0.04168\n",
      "Iteration: 0030 Loss: 1.19347 Time: 0.04076\n",
      "Iteration: 0031 Loss: 1.16915 Time: 0.04100\n",
      "Iteration: 0032 Loss: 1.17062 Time: 0.04024\n",
      "Iteration: 0033 Loss: 1.15099 Time: 0.04145\n",
      "Iteration: 0034 Loss: 1.12694 Time: 0.04015\n",
      "Iteration: 0035 Loss: 1.14007 Time: 0.03978\n",
      "Iteration: 0036 Loss: 1.11264 Time: 0.03919\n",
      "Iteration: 0037 Loss: 1.10404 Time: 0.04030\n",
      "Iteration: 0038 Loss: 1.10064 Time: 0.04016\n",
      "Iteration: 0039 Loss: 1.09858 Time: 0.04105\n",
      "Iteration: 0040 Loss: 1.09374 Time: 0.03818\n",
      "Iteration: 0041 Loss: 1.07203 Time: 0.03733\n",
      "Iteration: 0042 Loss: 1.06267 Time: 0.03770\n",
      "Iteration: 0043 Loss: 1.03210 Time: 0.03917\n",
      "Iteration: 0044 Loss: 1.05156 Time: 0.04339\n",
      "Iteration: 0045 Loss: 1.03545 Time: 0.04219\n",
      "Iteration: 0046 Loss: 1.01096 Time: 0.04096\n",
      "Iteration: 0047 Loss: 1.02385 Time: 0.04523\n",
      "Iteration: 0048 Loss: 0.99226 Time: 0.04219\n",
      "Iteration: 0049 Loss: 0.99887 Time: 0.04062\n",
      "Iteration: 0050 Loss: 0.98434 Time: 0.04095\n",
      "Iteration: 0051 Loss: 0.97440 Time: 0.04338\n",
      "Iteration: 0052 Loss: 0.96259 Time: 0.03972\n",
      "Iteration: 0053 Loss: 0.96946 Time: 0.04128\n",
      "Iteration: 0054 Loss: 0.96128 Time: 0.03931\n",
      "Iteration: 0055 Loss: 0.95703 Time: 0.04135\n",
      "Iteration: 0056 Loss: 0.93224 Time: 0.04095\n",
      "Iteration: 0057 Loss: 0.93146 Time: 0.04053\n",
      "Iteration: 0058 Loss: 0.92649 Time: 0.04032\n",
      "Iteration: 0059 Loss: 0.91366 Time: 0.04225\n",
      "Iteration: 0060 Loss: 0.90038 Time: 0.04149\n",
      "Iteration: 0061 Loss: 0.90464 Time: 0.04136\n",
      "Iteration: 0062 Loss: 0.90831 Time: 0.04067\n",
      "Iteration: 0063 Loss: 0.90557 Time: 0.04093\n",
      "Iteration: 0064 Loss: 0.88630 Time: 0.04015\n",
      "Iteration: 0065 Loss: 0.87619 Time: 0.04058\n",
      "Iteration: 0066 Loss: 0.88256 Time: 0.04195\n",
      "Iteration: 0067 Loss: 0.87280 Time: 0.04030\n",
      "Iteration: 0068 Loss: 0.86951 Time: 0.04063\n",
      "Iteration: 0069 Loss: 0.86524 Time: 0.03978\n",
      "Iteration: 0070 Loss: 0.85414 Time: 0.04153\n",
      "Iteration: 0071 Loss: 0.86406 Time: 0.04105\n",
      "Iteration: 0072 Loss: 0.86022 Time: 0.04000\n",
      "Iteration: 0073 Loss: 0.84976 Time: 0.04167\n",
      "Iteration: 0074 Loss: 0.83926 Time: 0.04131\n",
      "Iteration: 0075 Loss: 0.85353 Time: 0.04192\n",
      "Iteration: 0076 Loss: 0.82637 Time: 0.04196\n",
      "Iteration: 0077 Loss: 0.83218 Time: 0.04116\n",
      "Iteration: 0078 Loss: 0.83459 Time: 0.04091\n",
      "Iteration: 0079 Loss: 0.83759 Time: 0.04200\n",
      "Iteration: 0080 Loss: 0.82532 Time: 0.04194\n",
      "Iteration: 0081 Loss: 0.81701 Time: 0.04167\n",
      "Iteration: 0082 Loss: 0.81510 Time: 0.04133\n",
      "Iteration: 0083 Loss: 0.81454 Time: 0.04128\n",
      "Iteration: 0084 Loss: 0.81384 Time: 0.04123\n",
      "Iteration: 0085 Loss: 0.79168 Time: 0.04053\n",
      "Iteration: 0086 Loss: 0.80393 Time: 0.03972\n",
      "Iteration: 0087 Loss: 0.79308 Time: 0.03920\n",
      "Iteration: 0088 Loss: 0.79350 Time: 0.03746\n",
      "Iteration: 0089 Loss: 0.79558 Time: 0.04003\n",
      "Iteration: 0090 Loss: 0.79400 Time: 0.04147\n",
      "Iteration: 0091 Loss: 0.78417 Time: 0.03860\n",
      "Iteration: 0092 Loss: 0.78142 Time: 0.03988\n",
      "Iteration: 0093 Loss: 0.77847 Time: 0.04025\n",
      "Iteration: 0094 Loss: 0.77127 Time: 0.04069\n",
      "Iteration: 0095 Loss: 0.77152 Time: 0.03976\n",
      "Iteration: 0096 Loss: 0.77385 Time: 0.04132\n",
      "Iteration: 0097 Loss: 0.77142 Time: 0.04061\n",
      "Iteration: 0098 Loss: 0.75527 Time: 0.04036\n",
      "Iteration: 0099 Loss: 0.76444 Time: 0.04138\n",
      "Iteration: 0100 Loss: 0.76285 Time: 0.04107\n",
      "Iteration: 0101 Loss: 0.75324 Time: 0.04297\n",
      "Iteration: 0102 Loss: 0.75248 Time: 0.04173\n",
      "Iteration: 0103 Loss: 0.74973 Time: 0.04008\n",
      "Iteration: 0104 Loss: 0.75002 Time: 0.04084\n",
      "Iteration: 0105 Loss: 0.75293 Time: 0.04200\n",
      "Iteration: 0106 Loss: 0.74777 Time: 0.04069\n",
      "Iteration: 0107 Loss: 0.74143 Time: 0.04097\n",
      "Iteration: 0108 Loss: 0.74048 Time: 0.04096\n",
      "Iteration: 0109 Loss: 0.72705 Time: 0.04160\n",
      "Iteration: 0110 Loss: 0.72693 Time: 0.04204\n",
      "Iteration: 0111 Loss: 0.72823 Time: 0.04284\n",
      "Iteration: 0112 Loss: 0.73117 Time: 0.04210\n",
      "Iteration: 0113 Loss: 0.72501 Time: 0.03947\n",
      "Iteration: 0114 Loss: 0.72496 Time: 0.04138\n",
      "Iteration: 0115 Loss: 0.71705 Time: 0.04057\n",
      "Iteration: 0116 Loss: 0.72284 Time: 0.04100\n",
      "Iteration: 0117 Loss: 0.71594 Time: 0.04040\n",
      "Iteration: 0118 Loss: 0.71070 Time: 0.04045\n",
      "Iteration: 0119 Loss: 0.71451 Time: 0.04051\n",
      "Iteration: 0120 Loss: 0.71035 Time: 0.04124\n",
      "Iteration: 0121 Loss: 0.70905 Time: 0.04237\n",
      "Iteration: 0122 Loss: 0.70594 Time: 0.04125\n",
      "Iteration: 0123 Loss: 0.70523 Time: 0.04020\n",
      "Iteration: 0124 Loss: 0.70161 Time: 0.04111\n",
      "Iteration: 0125 Loss: 0.70314 Time: 0.04074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0126 Loss: 0.69939 Time: 0.04191\n",
      "Iteration: 0127 Loss: 0.69384 Time: 0.03903\n",
      "Iteration: 0128 Loss: 0.69623 Time: 0.04094\n",
      "Iteration: 0129 Loss: 0.69071 Time: 0.04100\n",
      "Iteration: 0130 Loss: 0.69153 Time: 0.03901\n",
      "Iteration: 0131 Loss: 0.69593 Time: 0.04047\n",
      "Iteration: 0132 Loss: 0.69469 Time: 0.04052\n",
      "Iteration: 0133 Loss: 0.68730 Time: 0.04004\n",
      "Iteration: 0134 Loss: 0.68997 Time: 0.03815\n",
      "Iteration: 0135 Loss: 0.68059 Time: 0.03875\n",
      "Iteration: 0136 Loss: 0.68332 Time: 0.03914\n",
      "Iteration: 0137 Loss: 0.68652 Time: 0.03984\n",
      "Iteration: 0138 Loss: 0.67964 Time: 0.04002\n",
      "Iteration: 0139 Loss: 0.68079 Time: 0.03879\n",
      "Iteration: 0140 Loss: 0.67681 Time: 0.03974\n",
      "Iteration: 0141 Loss: 0.68184 Time: 0.04100\n",
      "Iteration: 0142 Loss: 0.67111 Time: 0.04247\n",
      "Iteration: 0143 Loss: 0.66962 Time: 0.04096\n",
      "Iteration: 0144 Loss: 0.66860 Time: 0.04071\n",
      "Iteration: 0145 Loss: 0.66732 Time: 0.04126\n",
      "Iteration: 0146 Loss: 0.66813 Time: 0.04091\n",
      "Iteration: 0147 Loss: 0.67253 Time: 0.04099\n",
      "Iteration: 0148 Loss: 0.66801 Time: 0.04098\n",
      "Iteration: 0149 Loss: 0.66484 Time: 0.04338\n",
      "Iteration: 0150 Loss: 0.65901 Time: 0.04077\n",
      "Iteration: 0151 Loss: 0.66073 Time: 0.04298\n",
      "Iteration: 0152 Loss: 0.66484 Time: 0.04290\n",
      "Iteration: 0153 Loss: 0.66084 Time: 0.04293\n",
      "Iteration: 0154 Loss: 0.65409 Time: 0.04100\n",
      "Iteration: 0155 Loss: 0.65962 Time: 0.04200\n",
      "Iteration: 0156 Loss: 0.65670 Time: 0.04305\n",
      "Iteration: 0157 Loss: 0.65403 Time: 0.04250\n",
      "Iteration: 0158 Loss: 0.65185 Time: 0.04098\n",
      "Iteration: 0159 Loss: 0.65933 Time: 0.04122\n",
      "Iteration: 0160 Loss: 0.65280 Time: 0.04301\n",
      "Iteration: 0161 Loss: 0.64266 Time: 0.04144\n",
      "Iteration: 0162 Loss: 0.65292 Time: 0.04094\n",
      "Iteration: 0163 Loss: 0.64807 Time: 0.04108\n",
      "Iteration: 0164 Loss: 0.65299 Time: 0.04098\n",
      "Iteration: 0165 Loss: 0.64531 Time: 0.04061\n",
      "Iteration: 0166 Loss: 0.64418 Time: 0.04232\n",
      "Iteration: 0167 Loss: 0.64884 Time: 0.04136\n",
      "Iteration: 0168 Loss: 0.64441 Time: 0.04206\n",
      "Iteration: 0169 Loss: 0.64532 Time: 0.04187\n",
      "Iteration: 0170 Loss: 0.64237 Time: 0.04082\n",
      "Iteration: 0171 Loss: 0.64281 Time: 0.04069\n",
      "Iteration: 0172 Loss: 0.63728 Time: 0.04193\n",
      "Iteration: 0173 Loss: 0.64668 Time: 0.04166\n",
      "Iteration: 0174 Loss: 0.64099 Time: 0.04200\n",
      "Iteration: 0175 Loss: 0.62988 Time: 0.04158\n",
      "Iteration: 0176 Loss: 0.63206 Time: 0.04099\n",
      "Iteration: 0177 Loss: 0.63683 Time: 0.04406\n",
      "Iteration: 0178 Loss: 0.63992 Time: 0.04145\n",
      "Iteration: 0179 Loss: 0.62642 Time: 0.04159\n",
      "Iteration: 0180 Loss: 0.63219 Time: 0.04088\n",
      "Iteration: 0181 Loss: 0.63020 Time: 0.04000\n",
      "Iteration: 0182 Loss: 0.63254 Time: 0.03997\n",
      "Iteration: 0183 Loss: 0.62726 Time: 0.03855\n",
      "Iteration: 0184 Loss: 0.62953 Time: 0.03976\n",
      "Iteration: 0185 Loss: 0.63467 Time: 0.03848\n",
      "Iteration: 0186 Loss: 0.62905 Time: 0.03838\n",
      "Iteration: 0187 Loss: 0.62818 Time: 0.04077\n",
      "Iteration: 0188 Loss: 0.62416 Time: 0.03772\n",
      "Iteration: 0189 Loss: 0.62805 Time: 0.04213\n",
      "Iteration: 0190 Loss: 0.62548 Time: 0.04098\n",
      "Iteration: 0191 Loss: 0.62190 Time: 0.04045\n",
      "Iteration: 0192 Loss: 0.62788 Time: 0.04284\n",
      "Iteration: 0193 Loss: 0.62328 Time: 0.04170\n",
      "Iteration: 0194 Loss: 0.62289 Time: 0.04127\n",
      "Iteration: 0195 Loss: 0.62423 Time: 0.04086\n",
      "Iteration: 0196 Loss: 0.62183 Time: 0.04156\n",
      "Iteration: 0197 Loss: 0.62436 Time: 0.04094\n",
      "Iteration: 0198 Loss: 0.61754 Time: 0.04150\n",
      "Iteration: 0199 Loss: 0.62029 Time: 0.04077\n",
      "Iteration: 0200 Loss: 0.62511 Time: 0.04221\n",
      "Done! \n",
      "\n",
      "STEP 3/3 - MODEL EVALUATION \n",
      "\n",
      "Computing the final embedding vectors, for evaluation\n",
      "Done! \n",
      "\n",
      "Testing: link prediction\n",
      "Done! \n",
      "\n",
      "EXPERIMENTS ON MODEL 4 / 10 \n",
      "\n",
      "STEP 1/3 - PREPROCESSING STEPS \n",
      "\n",
      "Masking some edges from the training graph, for link prediction\n",
      "(validation set: 1.0 % of edges - test set: 10.0 % of edges)\n",
      "Done! \n",
      "\n",
      "Preprocessing node features\n",
      "Done! \n",
      "\n",
      "Running the Louvain algorithm for community detection\n",
      "as a preprocessing step for the encoder\n",
      "Done! Louvain has found 19 communities \n",
      "\n",
      "Setting up the model and the optimizer\n",
      "Done! \n",
      "\n",
      "Preprocessing on message passing matrices\n",
      "Done! \n",
      "\n",
      "Initializing TF session\n",
      "Done! \n",
      "\n",
      "STEP 2/3 - MODEL TRAINING \n",
      "\n",
      "Starting training\n",
      "Iteration: 0001 Loss: 1.76008 Time: 0.25752\n",
      "Iteration: 0002 Loss: 1.75822 Time: 0.03977\n",
      "Iteration: 0003 Loss: 1.71503 Time: 0.03869\n",
      "Iteration: 0004 Loss: 1.63885 Time: 0.03875\n",
      "Iteration: 0005 Loss: 1.61424 Time: 0.03788\n",
      "Iteration: 0006 Loss: 1.61224 Time: 0.04031\n",
      "Iteration: 0007 Loss: 1.59874 Time: 0.04258\n",
      "Iteration: 0008 Loss: 1.57351 Time: 0.04159\n",
      "Iteration: 0009 Loss: 1.53316 Time: 0.04035\n",
      "Iteration: 0010 Loss: 1.48093 Time: 0.04093\n",
      "Iteration: 0011 Loss: 1.50033 Time: 0.04201\n",
      "Iteration: 0012 Loss: 1.43574 Time: 0.04234\n",
      "Iteration: 0013 Loss: 1.44091 Time: 0.04332\n",
      "Iteration: 0014 Loss: 1.42502 Time: 0.04030\n",
      "Iteration: 0015 Loss: 1.41475 Time: 0.04102\n",
      "Iteration: 0016 Loss: 1.39194 Time: 0.03955\n",
      "Iteration: 0017 Loss: 1.39656 Time: 0.04168\n",
      "Iteration: 0018 Loss: 1.36211 Time: 0.04400\n",
      "Iteration: 0019 Loss: 1.35839 Time: 0.04157\n",
      "Iteration: 0020 Loss: 1.32923 Time: 0.04160\n",
      "Iteration: 0021 Loss: 1.31668 Time: 0.04178\n",
      "Iteration: 0022 Loss: 1.29550 Time: 0.04152\n",
      "Iteration: 0023 Loss: 1.26884 Time: 0.04208\n",
      "Iteration: 0024 Loss: 1.27581 Time: 0.04066\n",
      "Iteration: 0025 Loss: 1.26290 Time: 0.04045\n",
      "Iteration: 0026 Loss: 1.26544 Time: 0.04199\n",
      "Iteration: 0027 Loss: 1.23956 Time: 0.04201\n",
      "Iteration: 0028 Loss: 1.21091 Time: 0.04241\n",
      "Iteration: 0029 Loss: 1.18474 Time: 0.04225\n",
      "Iteration: 0030 Loss: 1.19428 Time: 0.04133\n",
      "Iteration: 0031 Loss: 1.16520 Time: 0.04021\n",
      "Iteration: 0032 Loss: 1.16043 Time: 0.04040\n",
      "Iteration: 0033 Loss: 1.14616 Time: 0.04036\n",
      "Iteration: 0034 Loss: 1.14365 Time: 0.04258\n",
      "Iteration: 0035 Loss: 1.11974 Time: 0.04180\n",
      "Iteration: 0036 Loss: 1.12163 Time: 0.04202\n",
      "Iteration: 0037 Loss: 1.09336 Time: 0.04065\n",
      "Iteration: 0038 Loss: 1.10195 Time: 0.03954\n",
      "Iteration: 0039 Loss: 1.07383 Time: 0.04100\n",
      "Iteration: 0040 Loss: 1.06844 Time: 0.04141\n",
      "Iteration: 0041 Loss: 1.06460 Time: 0.04241\n",
      "Iteration: 0042 Loss: 1.05525 Time: 0.04062\n",
      "Iteration: 0043 Loss: 1.06032 Time: 0.04001\n",
      "Iteration: 0044 Loss: 1.02740 Time: 0.04247\n",
      "Iteration: 0045 Loss: 1.03423 Time: 0.04025\n",
      "Iteration: 0046 Loss: 1.00702 Time: 0.04302\n",
      "Iteration: 0047 Loss: 0.99230 Time: 0.04100\n",
      "Iteration: 0048 Loss: 0.98863 Time: 0.03957\n",
      "Iteration: 0049 Loss: 1.00831 Time: 0.03978\n",
      "Iteration: 0050 Loss: 0.99301 Time: 0.03974\n",
      "Iteration: 0051 Loss: 0.97488 Time: 0.04236\n",
      "Iteration: 0052 Loss: 0.95022 Time: 0.04002\n",
      "Iteration: 0053 Loss: 0.96055 Time: 0.03809\n",
      "Iteration: 0054 Loss: 0.95071 Time: 0.03920\n",
      "Iteration: 0055 Loss: 0.94439 Time: 0.04120\n",
      "Iteration: 0056 Loss: 0.93272 Time: 0.04114\n",
      "Iteration: 0057 Loss: 0.94667 Time: 0.04144\n",
      "Iteration: 0058 Loss: 0.93124 Time: 0.04113\n",
      "Iteration: 0059 Loss: 0.92099 Time: 0.04600\n",
      "Iteration: 0060 Loss: 0.90444 Time: 0.04103\n",
      "Iteration: 0061 Loss: 0.91724 Time: 0.04360\n",
      "Iteration: 0062 Loss: 0.90028 Time: 0.04300\n",
      "Iteration: 0063 Loss: 0.89591 Time: 0.04049\n",
      "Iteration: 0064 Loss: 0.89656 Time: 0.04111\n",
      "Iteration: 0065 Loss: 0.87943 Time: 0.04157\n",
      "Iteration: 0066 Loss: 0.88755 Time: 0.04072\n",
      "Iteration: 0067 Loss: 0.89150 Time: 0.04207\n",
      "Iteration: 0068 Loss: 0.88339 Time: 0.03904\n",
      "Iteration: 0069 Loss: 0.88227 Time: 0.04177\n",
      "Iteration: 0070 Loss: 0.85843 Time: 0.04042\n",
      "Iteration: 0071 Loss: 0.86287 Time: 0.04357\n",
      "Iteration: 0072 Loss: 0.86378 Time: 0.04093\n",
      "Iteration: 0073 Loss: 0.86426 Time: 0.04093\n",
      "Iteration: 0074 Loss: 0.85385 Time: 0.04137\n",
      "Iteration: 0075 Loss: 0.84469 Time: 0.04121\n",
      "Iteration: 0076 Loss: 0.83434 Time: 0.04220\n",
      "Iteration: 0077 Loss: 0.83416 Time: 0.04183\n",
      "Iteration: 0078 Loss: 0.83758 Time: 0.03993\n",
      "Iteration: 0079 Loss: 0.82261 Time: 0.04200\n",
      "Iteration: 0080 Loss: 0.82194 Time: 0.04057\n",
      "Iteration: 0081 Loss: 0.82306 Time: 0.04110\n",
      "Iteration: 0082 Loss: 0.81598 Time: 0.04154\n",
      "Iteration: 0083 Loss: 0.81496 Time: 0.04003\n",
      "Iteration: 0084 Loss: 0.81239 Time: 0.03997\n",
      "Iteration: 0085 Loss: 0.81246 Time: 0.03898\n",
      "Iteration: 0086 Loss: 0.80088 Time: 0.04325\n",
      "Iteration: 0087 Loss: 0.79086 Time: 0.04108\n",
      "Iteration: 0088 Loss: 0.79860 Time: 0.04099\n",
      "Iteration: 0089 Loss: 0.79252 Time: 0.04101\n",
      "Iteration: 0090 Loss: 0.78881 Time: 0.04197\n",
      "Iteration: 0091 Loss: 0.79522 Time: 0.04135\n",
      "Iteration: 0092 Loss: 0.78340 Time: 0.04101\n",
      "Iteration: 0093 Loss: 0.77131 Time: 0.04100\n",
      "Iteration: 0094 Loss: 0.78273 Time: 0.04157\n",
      "Iteration: 0095 Loss: 0.76400 Time: 0.04085\n",
      "Iteration: 0096 Loss: 0.76567 Time: 0.04401\n",
      "Iteration: 0097 Loss: 0.76634 Time: 0.04014\n",
      "Iteration: 0098 Loss: 0.77081 Time: 0.03985\n",
      "Iteration: 0099 Loss: 0.77130 Time: 0.03952\n",
      "Iteration: 0100 Loss: 0.76136 Time: 0.03989\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0101 Loss: 0.76130 Time: 0.04057\n",
      "Iteration: 0102 Loss: 0.75593 Time: 0.04147\n",
      "Iteration: 0103 Loss: 0.74466 Time: 0.04000\n",
      "Iteration: 0104 Loss: 0.74814 Time: 0.04157\n",
      "Iteration: 0105 Loss: 0.74935 Time: 0.04099\n",
      "Iteration: 0106 Loss: 0.74529 Time: 0.04073\n",
      "Iteration: 0107 Loss: 0.74660 Time: 0.04213\n",
      "Iteration: 0108 Loss: 0.73704 Time: 0.04032\n",
      "Iteration: 0109 Loss: 0.73271 Time: 0.04111\n",
      "Iteration: 0110 Loss: 0.73071 Time: 0.04033\n",
      "Iteration: 0111 Loss: 0.72177 Time: 0.04421\n",
      "Iteration: 0112 Loss: 0.72651 Time: 0.04082\n",
      "Iteration: 0113 Loss: 0.72229 Time: 0.03999\n",
      "Iteration: 0114 Loss: 0.72936 Time: 0.03995\n",
      "Iteration: 0115 Loss: 0.71903 Time: 0.04399\n",
      "Iteration: 0116 Loss: 0.71712 Time: 0.04060\n",
      "Iteration: 0117 Loss: 0.71188 Time: 0.04037\n",
      "Iteration: 0118 Loss: 0.71733 Time: 0.04044\n",
      "Iteration: 0119 Loss: 0.71171 Time: 0.04055\n",
      "Iteration: 0120 Loss: 0.71541 Time: 0.03929\n",
      "Iteration: 0121 Loss: 0.71135 Time: 0.04065\n",
      "Iteration: 0122 Loss: 0.70774 Time: 0.04199\n",
      "Iteration: 0123 Loss: 0.70966 Time: 0.04220\n",
      "Iteration: 0124 Loss: 0.70438 Time: 0.03942\n",
      "Iteration: 0125 Loss: 0.70212 Time: 0.04258\n",
      "Iteration: 0126 Loss: 0.69365 Time: 0.04200\n",
      "Iteration: 0127 Loss: 0.70272 Time: 0.04107\n",
      "Iteration: 0128 Loss: 0.69626 Time: 0.04216\n",
      "Iteration: 0129 Loss: 0.70140 Time: 0.04186\n",
      "Iteration: 0130 Loss: 0.69544 Time: 0.04075\n",
      "Iteration: 0131 Loss: 0.68493 Time: 0.04072\n",
      "Iteration: 0132 Loss: 0.69288 Time: 0.04051\n",
      "Iteration: 0133 Loss: 0.68462 Time: 0.04195\n",
      "Iteration: 0134 Loss: 0.69417 Time: 0.04201\n",
      "Iteration: 0135 Loss: 0.68285 Time: 0.04197\n",
      "Iteration: 0136 Loss: 0.68280 Time: 0.04215\n",
      "Iteration: 0137 Loss: 0.68826 Time: 0.04104\n",
      "Iteration: 0138 Loss: 0.68146 Time: 0.04184\n",
      "Iteration: 0139 Loss: 0.68111 Time: 0.04098\n",
      "Iteration: 0140 Loss: 0.67818 Time: 0.04067\n",
      "Iteration: 0141 Loss: 0.68220 Time: 0.04096\n",
      "Iteration: 0142 Loss: 0.67522 Time: 0.04235\n",
      "Iteration: 0143 Loss: 0.67192 Time: 0.04099\n",
      "Iteration: 0144 Loss: 0.66931 Time: 0.04201\n",
      "Iteration: 0145 Loss: 0.67803 Time: 0.03921\n",
      "Iteration: 0146 Loss: 0.66792 Time: 0.04001\n",
      "Iteration: 0147 Loss: 0.66655 Time: 0.04075\n",
      "Iteration: 0148 Loss: 0.66519 Time: 0.04239\n",
      "Iteration: 0149 Loss: 0.66094 Time: 0.03894\n",
      "Iteration: 0150 Loss: 0.66619 Time: 0.03899\n",
      "Iteration: 0151 Loss: 0.66559 Time: 0.03851\n",
      "Iteration: 0152 Loss: 0.65822 Time: 0.04042\n",
      "Iteration: 0153 Loss: 0.65721 Time: 0.04150\n",
      "Iteration: 0154 Loss: 0.66231 Time: 0.03998\n",
      "Iteration: 0155 Loss: 0.66084 Time: 0.03982\n",
      "Iteration: 0156 Loss: 0.66512 Time: 0.04024\n",
      "Iteration: 0157 Loss: 0.65433 Time: 0.04101\n",
      "Iteration: 0158 Loss: 0.65490 Time: 0.04161\n",
      "Iteration: 0159 Loss: 0.64972 Time: 0.04040\n",
      "Iteration: 0160 Loss: 0.65269 Time: 0.04418\n",
      "Iteration: 0161 Loss: 0.65259 Time: 0.04072\n",
      "Iteration: 0162 Loss: 0.65110 Time: 0.04128\n",
      "Iteration: 0163 Loss: 0.64905 Time: 0.04207\n",
      "Iteration: 0164 Loss: 0.65483 Time: 0.04118\n",
      "Iteration: 0165 Loss: 0.64767 Time: 0.04095\n",
      "Iteration: 0166 Loss: 0.64599 Time: 0.04100\n",
      "Iteration: 0167 Loss: 0.64565 Time: 0.04122\n",
      "Iteration: 0168 Loss: 0.64927 Time: 0.04111\n",
      "Iteration: 0169 Loss: 0.64248 Time: 0.03996\n",
      "Iteration: 0170 Loss: 0.64480 Time: 0.04094\n",
      "Iteration: 0171 Loss: 0.64418 Time: 0.04070\n",
      "Iteration: 0172 Loss: 0.64586 Time: 0.03997\n",
      "Iteration: 0173 Loss: 0.63997 Time: 0.04116\n",
      "Iteration: 0174 Loss: 0.64213 Time: 0.04476\n",
      "Iteration: 0175 Loss: 0.64122 Time: 0.03947\n",
      "Iteration: 0176 Loss: 0.64011 Time: 0.03940\n",
      "Iteration: 0177 Loss: 0.64081 Time: 0.04033\n",
      "Iteration: 0178 Loss: 0.63718 Time: 0.04267\n",
      "Iteration: 0179 Loss: 0.63535 Time: 0.04015\n",
      "Iteration: 0180 Loss: 0.63696 Time: 0.03974\n",
      "Iteration: 0181 Loss: 0.63631 Time: 0.04088\n",
      "Iteration: 0182 Loss: 0.63252 Time: 0.04047\n",
      "Iteration: 0183 Loss: 0.63398 Time: 0.04098\n",
      "Iteration: 0184 Loss: 0.62759 Time: 0.04394\n",
      "Iteration: 0185 Loss: 0.63424 Time: 0.04194\n",
      "Iteration: 0186 Loss: 0.63416 Time: 0.04150\n",
      "Iteration: 0187 Loss: 0.63034 Time: 0.04175\n",
      "Iteration: 0188 Loss: 0.62869 Time: 0.04074\n",
      "Iteration: 0189 Loss: 0.62857 Time: 0.04021\n",
      "Iteration: 0190 Loss: 0.62245 Time: 0.04169\n",
      "Iteration: 0191 Loss: 0.62012 Time: 0.04057\n",
      "Iteration: 0192 Loss: 0.62513 Time: 0.04138\n",
      "Iteration: 0193 Loss: 0.62450 Time: 0.03952\n",
      "Iteration: 0194 Loss: 0.62948 Time: 0.04007\n",
      "Iteration: 0195 Loss: 0.62140 Time: 0.03991\n",
      "Iteration: 0196 Loss: 0.62269 Time: 0.03905\n",
      "Iteration: 0197 Loss: 0.62248 Time: 0.03772\n",
      "Iteration: 0198 Loss: 0.62015 Time: 0.03962\n",
      "Iteration: 0199 Loss: 0.61951 Time: 0.04014\n",
      "Iteration: 0200 Loss: 0.62286 Time: 0.04192\n",
      "Done! \n",
      "\n",
      "STEP 3/3 - MODEL EVALUATION \n",
      "\n",
      "Computing the final embedding vectors, for evaluation\n",
      "Done! \n",
      "\n",
      "Testing: link prediction\n",
      "Done! \n",
      "\n",
      "EXPERIMENTS ON MODEL 5 / 10 \n",
      "\n",
      "STEP 1/3 - PREPROCESSING STEPS \n",
      "\n",
      "Masking some edges from the training graph, for link prediction\n",
      "(validation set: 1.0 % of edges - test set: 10.0 % of edges)\n",
      "Done! \n",
      "\n",
      "Preprocessing node features\n",
      "Done! \n",
      "\n",
      "Running the Louvain algorithm for community detection\n",
      "as a preprocessing step for the encoder\n",
      "Done! Louvain has found 12 communities \n",
      "\n",
      "Setting up the model and the optimizer\n",
      "Done! \n",
      "\n",
      "Preprocessing on message passing matrices\n",
      "Done! \n",
      "\n",
      "Initializing TF session\n",
      "Done! \n",
      "\n",
      "STEP 2/3 - MODEL TRAINING \n",
      "\n",
      "Starting training\n",
      "Iteration: 0001 Loss: 1.75005 Time: 0.25899\n",
      "Iteration: 0002 Loss: 1.73302 Time: 0.04150\n",
      "Iteration: 0003 Loss: 1.65459 Time: 0.04000\n",
      "Iteration: 0004 Loss: 1.62728 Time: 0.04157\n",
      "Iteration: 0005 Loss: 1.62278 Time: 0.04038\n",
      "Iteration: 0006 Loss: 1.56063 Time: 0.04266\n",
      "Iteration: 0007 Loss: 1.55736 Time: 0.03969\n",
      "Iteration: 0008 Loss: 1.51861 Time: 0.04071\n",
      "Iteration: 0009 Loss: 1.55167 Time: 0.03877\n",
      "Iteration: 0010 Loss: 1.50364 Time: 0.03768\n",
      "Iteration: 0011 Loss: 1.50328 Time: 0.03880\n",
      "Iteration: 0012 Loss: 1.45935 Time: 0.04131\n",
      "Iteration: 0013 Loss: 1.46257 Time: 0.03928\n",
      "Iteration: 0014 Loss: 1.44131 Time: 0.03782\n",
      "Iteration: 0015 Loss: 1.40438 Time: 0.03880\n",
      "Iteration: 0016 Loss: 1.36743 Time: 0.04556\n",
      "Iteration: 0017 Loss: 1.37470 Time: 0.04100\n",
      "Iteration: 0018 Loss: 1.37021 Time: 0.04122\n",
      "Iteration: 0019 Loss: 1.32315 Time: 0.04072\n",
      "Iteration: 0020 Loss: 1.34303 Time: 0.04061\n",
      "Iteration: 0021 Loss: 1.30184 Time: 0.04032\n",
      "Iteration: 0022 Loss: 1.31435 Time: 0.04199\n",
      "Iteration: 0023 Loss: 1.27766 Time: 0.04201\n",
      "Iteration: 0024 Loss: 1.27004 Time: 0.04099\n",
      "Iteration: 0025 Loss: 1.25916 Time: 0.04300\n",
      "Iteration: 0026 Loss: 1.24821 Time: 0.04302\n",
      "Iteration: 0027 Loss: 1.22937 Time: 0.04198\n",
      "Iteration: 0028 Loss: 1.20338 Time: 0.04164\n",
      "Iteration: 0029 Loss: 1.19761 Time: 0.04119\n",
      "Iteration: 0030 Loss: 1.19719 Time: 0.03952\n",
      "Iteration: 0031 Loss: 1.18699 Time: 0.04108\n",
      "Iteration: 0032 Loss: 1.16927 Time: 0.04300\n",
      "Iteration: 0033 Loss: 1.14189 Time: 0.04000\n",
      "Iteration: 0034 Loss: 1.13039 Time: 0.04199\n",
      "Iteration: 0035 Loss: 1.14497 Time: 0.04098\n",
      "Iteration: 0036 Loss: 1.11682 Time: 0.04048\n",
      "Iteration: 0037 Loss: 1.10471 Time: 0.04100\n",
      "Iteration: 0038 Loss: 1.10277 Time: 0.04300\n",
      "Iteration: 0039 Loss: 1.07488 Time: 0.04217\n",
      "Iteration: 0040 Loss: 1.07038 Time: 0.04183\n",
      "Iteration: 0041 Loss: 1.04895 Time: 0.04300\n",
      "Iteration: 0042 Loss: 1.04616 Time: 0.04229\n",
      "Iteration: 0043 Loss: 1.03719 Time: 0.04077\n",
      "Iteration: 0044 Loss: 1.03732 Time: 0.04100\n",
      "Iteration: 0045 Loss: 1.02270 Time: 0.04201\n",
      "Iteration: 0046 Loss: 1.01493 Time: 0.04126\n",
      "Iteration: 0047 Loss: 0.98890 Time: 0.04073\n",
      "Iteration: 0048 Loss: 1.00010 Time: 0.04400\n",
      "Iteration: 0049 Loss: 0.99838 Time: 0.04101\n",
      "Iteration: 0050 Loss: 0.97923 Time: 0.04069\n",
      "Iteration: 0051 Loss: 0.97811 Time: 0.04100\n",
      "Iteration: 0052 Loss: 0.96401 Time: 0.04016\n",
      "Iteration: 0053 Loss: 0.97342 Time: 0.04100\n",
      "Iteration: 0054 Loss: 0.96686 Time: 0.04128\n",
      "Iteration: 0055 Loss: 0.92810 Time: 0.04000\n",
      "Iteration: 0056 Loss: 0.94218 Time: 0.04000\n",
      "Iteration: 0057 Loss: 0.93080 Time: 0.03884\n",
      "Iteration: 0058 Loss: 0.91710 Time: 0.03830\n",
      "Iteration: 0059 Loss: 0.92710 Time: 0.04173\n",
      "Iteration: 0060 Loss: 0.91210 Time: 0.03923\n",
      "Iteration: 0061 Loss: 0.90912 Time: 0.04009\n",
      "Iteration: 0062 Loss: 0.89768 Time: 0.03915\n",
      "Iteration: 0063 Loss: 0.89625 Time: 0.03960\n",
      "Iteration: 0064 Loss: 0.89343 Time: 0.04139\n",
      "Iteration: 0065 Loss: 0.88172 Time: 0.03948\n",
      "Iteration: 0066 Loss: 0.88001 Time: 0.04065\n",
      "Iteration: 0067 Loss: 0.88855 Time: 0.04299\n",
      "Iteration: 0068 Loss: 0.86842 Time: 0.04200\n",
      "Iteration: 0069 Loss: 0.86968 Time: 0.04500\n",
      "Iteration: 0070 Loss: 0.87286 Time: 0.04028\n",
      "Iteration: 0071 Loss: 0.87157 Time: 0.04171\n",
      "Iteration: 0072 Loss: 0.85599 Time: 0.04200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0073 Loss: 0.84007 Time: 0.04136\n",
      "Iteration: 0074 Loss: 0.85227 Time: 0.04264\n",
      "Iteration: 0075 Loss: 0.84026 Time: 0.04008\n",
      "Iteration: 0076 Loss: 0.84373 Time: 0.04262\n",
      "Iteration: 0077 Loss: 0.83198 Time: 0.04110\n",
      "Iteration: 0078 Loss: 0.83248 Time: 0.04089\n",
      "Iteration: 0079 Loss: 0.83560 Time: 0.04000\n",
      "Iteration: 0080 Loss: 0.82202 Time: 0.04200\n",
      "Iteration: 0081 Loss: 0.82020 Time: 0.04201\n",
      "Iteration: 0082 Loss: 0.81593 Time: 0.04044\n",
      "Iteration: 0083 Loss: 0.81583 Time: 0.04052\n",
      "Iteration: 0084 Loss: 0.81261 Time: 0.04192\n",
      "Iteration: 0085 Loss: 0.81468 Time: 0.04100\n",
      "Iteration: 0086 Loss: 0.80934 Time: 0.04045\n",
      "Iteration: 0087 Loss: 0.80766 Time: 0.03998\n",
      "Iteration: 0088 Loss: 0.79764 Time: 0.04166\n",
      "Iteration: 0089 Loss: 0.79733 Time: 0.04146\n",
      "Iteration: 0090 Loss: 0.78448 Time: 0.04462\n",
      "Iteration: 0091 Loss: 0.78909 Time: 0.04730\n",
      "Iteration: 0092 Loss: 0.78028 Time: 0.04238\n",
      "Iteration: 0093 Loss: 0.77657 Time: 0.04300\n",
      "Iteration: 0094 Loss: 0.78006 Time: 0.04201\n",
      "Iteration: 0095 Loss: 0.76303 Time: 0.04112\n",
      "Iteration: 0096 Loss: 0.78159 Time: 0.04094\n",
      "Iteration: 0097 Loss: 0.76749 Time: 0.04120\n",
      "Iteration: 0098 Loss: 0.77089 Time: 0.04102\n",
      "Iteration: 0099 Loss: 0.75869 Time: 0.04216\n",
      "Iteration: 0100 Loss: 0.76800 Time: 0.04091\n",
      "Iteration: 0101 Loss: 0.74939 Time: 0.04060\n",
      "Iteration: 0102 Loss: 0.75878 Time: 0.03934\n",
      "Iteration: 0103 Loss: 0.75120 Time: 0.04086\n",
      "Iteration: 0104 Loss: 0.75244 Time: 0.04137\n",
      "Iteration: 0105 Loss: 0.74152 Time: 0.04031\n",
      "Iteration: 0106 Loss: 0.74561 Time: 0.03933\n",
      "Iteration: 0107 Loss: 0.73885 Time: 0.04008\n",
      "Iteration: 0108 Loss: 0.74301 Time: 0.03865\n",
      "Iteration: 0109 Loss: 0.74359 Time: 0.04216\n",
      "Iteration: 0110 Loss: 0.73548 Time: 0.04037\n",
      "Iteration: 0111 Loss: 0.73674 Time: 0.03851\n",
      "Iteration: 0112 Loss: 0.73431 Time: 0.04050\n",
      "Iteration: 0113 Loss: 0.72773 Time: 0.04275\n",
      "Iteration: 0114 Loss: 0.72609 Time: 0.04133\n",
      "Iteration: 0115 Loss: 0.72658 Time: 0.04173\n",
      "Iteration: 0116 Loss: 0.72599 Time: 0.04274\n",
      "Iteration: 0117 Loss: 0.72545 Time: 0.04205\n",
      "Iteration: 0118 Loss: 0.72102 Time: 0.04151\n",
      "Iteration: 0119 Loss: 0.72292 Time: 0.04116\n",
      "Iteration: 0120 Loss: 0.71127 Time: 0.04023\n",
      "Iteration: 0121 Loss: 0.70616 Time: 0.04194\n",
      "Iteration: 0122 Loss: 0.71386 Time: 0.04032\n",
      "Iteration: 0123 Loss: 0.70573 Time: 0.04293\n",
      "Iteration: 0124 Loss: 0.70501 Time: 0.04231\n",
      "Iteration: 0125 Loss: 0.70468 Time: 0.04069\n",
      "Iteration: 0126 Loss: 0.69676 Time: 0.04113\n",
      "Iteration: 0127 Loss: 0.70323 Time: 0.04057\n",
      "Iteration: 0128 Loss: 0.70256 Time: 0.04067\n",
      "Iteration: 0129 Loss: 0.69911 Time: 0.04015\n",
      "Iteration: 0130 Loss: 0.69552 Time: 0.04042\n",
      "Iteration: 0131 Loss: 0.69574 Time: 0.04157\n",
      "Iteration: 0132 Loss: 0.68996 Time: 0.04045\n",
      "Iteration: 0133 Loss: 0.68729 Time: 0.04072\n",
      "Iteration: 0134 Loss: 0.68382 Time: 0.04195\n",
      "Iteration: 0135 Loss: 0.67917 Time: 0.04142\n",
      "Iteration: 0136 Loss: 0.68402 Time: 0.03999\n",
      "Iteration: 0137 Loss: 0.68201 Time: 0.04209\n",
      "Iteration: 0138 Loss: 0.68466 Time: 0.04050\n",
      "Iteration: 0139 Loss: 0.67990 Time: 0.04164\n",
      "Iteration: 0140 Loss: 0.68211 Time: 0.04137\n",
      "Iteration: 0141 Loss: 0.68409 Time: 0.04282\n",
      "Iteration: 0142 Loss: 0.67250 Time: 0.04200\n",
      "Iteration: 0143 Loss: 0.67124 Time: 0.04111\n",
      "Iteration: 0144 Loss: 0.66829 Time: 0.04189\n",
      "Iteration: 0145 Loss: 0.66996 Time: 0.04156\n",
      "Iteration: 0146 Loss: 0.66480 Time: 0.03978\n",
      "Iteration: 0147 Loss: 0.66865 Time: 0.04253\n",
      "Iteration: 0148 Loss: 0.65984 Time: 0.03912\n",
      "Iteration: 0149 Loss: 0.66678 Time: 0.04335\n",
      "Iteration: 0150 Loss: 0.67022 Time: 0.04152\n",
      "Iteration: 0151 Loss: 0.66350 Time: 0.04044\n",
      "Iteration: 0152 Loss: 0.66752 Time: 0.04241\n",
      "Iteration: 0153 Loss: 0.66122 Time: 0.03900\n",
      "Iteration: 0154 Loss: 0.66117 Time: 0.03859\n",
      "Iteration: 0155 Loss: 0.65830 Time: 0.03928\n",
      "Iteration: 0156 Loss: 0.65187 Time: 0.03803\n",
      "Iteration: 0157 Loss: 0.65670 Time: 0.04203\n",
      "Iteration: 0158 Loss: 0.65918 Time: 0.03925\n",
      "Iteration: 0159 Loss: 0.65868 Time: 0.03855\n",
      "Iteration: 0160 Loss: 0.65485 Time: 0.03887\n",
      "Iteration: 0161 Loss: 0.65801 Time: 0.04028\n",
      "Iteration: 0162 Loss: 0.65049 Time: 0.04229\n",
      "Iteration: 0163 Loss: 0.65173 Time: 0.04101\n",
      "Iteration: 0164 Loss: 0.64290 Time: 0.04149\n",
      "Iteration: 0165 Loss: 0.64919 Time: 0.04156\n",
      "Iteration: 0166 Loss: 0.64703 Time: 0.04078\n",
      "Iteration: 0167 Loss: 0.65116 Time: 0.04008\n",
      "Iteration: 0168 Loss: 0.64479 Time: 0.04400\n",
      "Iteration: 0169 Loss: 0.64801 Time: 0.04115\n",
      "Iteration: 0170 Loss: 0.63699 Time: 0.04066\n",
      "Iteration: 0171 Loss: 0.63758 Time: 0.04051\n",
      "Iteration: 0172 Loss: 0.63961 Time: 0.04199\n",
      "Iteration: 0173 Loss: 0.63973 Time: 0.04006\n",
      "Iteration: 0174 Loss: 0.63650 Time: 0.04194\n",
      "Iteration: 0175 Loss: 0.63811 Time: 0.04127\n",
      "Iteration: 0176 Loss: 0.63840 Time: 0.04086\n",
      "Iteration: 0177 Loss: 0.63296 Time: 0.04100\n",
      "Iteration: 0178 Loss: 0.63341 Time: 0.04103\n",
      "Iteration: 0179 Loss: 0.63586 Time: 0.04115\n",
      "Iteration: 0180 Loss: 0.63316 Time: 0.04096\n",
      "Iteration: 0181 Loss: 0.63184 Time: 0.04036\n",
      "Iteration: 0182 Loss: 0.62813 Time: 0.04161\n",
      "Iteration: 0183 Loss: 0.63123 Time: 0.04173\n",
      "Iteration: 0184 Loss: 0.62737 Time: 0.04126\n",
      "Iteration: 0185 Loss: 0.62903 Time: 0.04164\n",
      "Iteration: 0186 Loss: 0.62064 Time: 0.04255\n",
      "Iteration: 0187 Loss: 0.62441 Time: 0.04067\n",
      "Iteration: 0188 Loss: 0.62745 Time: 0.03890\n",
      "Iteration: 0189 Loss: 0.63087 Time: 0.04200\n",
      "Iteration: 0190 Loss: 0.62897 Time: 0.04194\n",
      "Iteration: 0191 Loss: 0.61860 Time: 0.04204\n",
      "Iteration: 0192 Loss: 0.62165 Time: 0.04099\n",
      "Iteration: 0193 Loss: 0.61910 Time: 0.04128\n",
      "Iteration: 0194 Loss: 0.61861 Time: 0.04144\n",
      "Iteration: 0195 Loss: 0.62312 Time: 0.04054\n",
      "Iteration: 0196 Loss: 0.61876 Time: 0.04140\n",
      "Iteration: 0197 Loss: 0.62132 Time: 0.04215\n",
      "Iteration: 0198 Loss: 0.61567 Time: 0.04081\n",
      "Iteration: 0199 Loss: 0.62280 Time: 0.04176\n",
      "Iteration: 0200 Loss: 0.61765 Time: 0.04251\n",
      "Done! \n",
      "\n",
      "STEP 3/3 - MODEL EVALUATION \n",
      "\n",
      "Computing the final embedding vectors, for evaluation\n",
      "Done! \n",
      "\n",
      "Testing: link prediction\n",
      "Done! \n",
      "\n",
      "EXPERIMENTS ON MODEL 6 / 10 \n",
      "\n",
      "STEP 1/3 - PREPROCESSING STEPS \n",
      "\n",
      "Masking some edges from the training graph, for link prediction\n",
      "(validation set: 1.0 % of edges - test set: 10.0 % of edges)\n",
      "Done! \n",
      "\n",
      "Preprocessing node features\n",
      "Done! \n",
      "\n",
      "Running the Louvain algorithm for community detection\n",
      "as a preprocessing step for the encoder\n",
      "Done! Louvain has found 16 communities \n",
      "\n",
      "Setting up the model and the optimizer\n",
      "Done! \n",
      "\n",
      "Preprocessing on message passing matrices\n",
      "Done! \n",
      "\n",
      "Initializing TF session\n",
      "Done! \n",
      "\n",
      "STEP 2/3 - MODEL TRAINING \n",
      "\n",
      "Starting training\n",
      "Iteration: 0001 Loss: 1.70980 Time: 0.26891\n",
      "Iteration: 0002 Loss: 1.70133 Time: 0.04007\n",
      "Iteration: 0003 Loss: 1.64995 Time: 0.04315\n",
      "Iteration: 0004 Loss: 1.66712 Time: 0.04149\n",
      "Iteration: 0005 Loss: 1.61730 Time: 0.04130\n",
      "Iteration: 0006 Loss: 1.62061 Time: 0.04106\n",
      "Iteration: 0007 Loss: 1.54818 Time: 0.04242\n",
      "Iteration: 0008 Loss: 1.54648 Time: 0.04100\n",
      "Iteration: 0009 Loss: 1.52433 Time: 0.04400\n",
      "Iteration: 0010 Loss: 1.53919 Time: 0.04389\n",
      "Iteration: 0011 Loss: 1.50247 Time: 0.04208\n",
      "Iteration: 0012 Loss: 1.47034 Time: 0.04208\n",
      "Iteration: 0013 Loss: 1.42682 Time: 0.04097\n",
      "Iteration: 0014 Loss: 1.40657 Time: 0.04196\n",
      "Iteration: 0015 Loss: 1.40344 Time: 0.04121\n",
      "Iteration: 0016 Loss: 1.41895 Time: 0.04303\n",
      "Iteration: 0017 Loss: 1.37562 Time: 0.04087\n",
      "Iteration: 0018 Loss: 1.36402 Time: 0.04017\n",
      "Iteration: 0019 Loss: 1.36746 Time: 0.04001\n",
      "Iteration: 0020 Loss: 1.34805 Time: 0.03977\n",
      "Iteration: 0021 Loss: 1.32173 Time: 0.03859\n",
      "Iteration: 0022 Loss: 1.26713 Time: 0.04168\n",
      "Iteration: 0023 Loss: 1.28683 Time: 0.03911\n",
      "Iteration: 0024 Loss: 1.27246 Time: 0.03993\n",
      "Iteration: 0025 Loss: 1.25197 Time: 0.04208\n",
      "Iteration: 0026 Loss: 1.23333 Time: 0.04185\n",
      "Iteration: 0027 Loss: 1.20133 Time: 0.04401\n",
      "Iteration: 0028 Loss: 1.20228 Time: 0.04092\n",
      "Iteration: 0029 Loss: 1.19528 Time: 0.04232\n",
      "Iteration: 0030 Loss: 1.18492 Time: 0.04396\n",
      "Iteration: 0031 Loss: 1.18780 Time: 0.04196\n",
      "Iteration: 0032 Loss: 1.17254 Time: 0.04096\n",
      "Iteration: 0033 Loss: 1.14261 Time: 0.04116\n",
      "Iteration: 0034 Loss: 1.15929 Time: 0.04099\n",
      "Iteration: 0035 Loss: 1.11663 Time: 0.04299\n",
      "Iteration: 0036 Loss: 1.10878 Time: 0.04312\n",
      "Iteration: 0037 Loss: 1.10972 Time: 0.04200\n",
      "Iteration: 0038 Loss: 1.09793 Time: 0.04336\n",
      "Iteration: 0039 Loss: 1.07532 Time: 0.04199\n",
      "Iteration: 0040 Loss: 1.08561 Time: 0.04023\n",
      "Iteration: 0041 Loss: 1.05634 Time: 0.04203\n",
      "Iteration: 0042 Loss: 1.05509 Time: 0.04123\n",
      "Iteration: 0043 Loss: 1.06253 Time: 0.04105\n",
      "Iteration: 0044 Loss: 1.03435 Time: 0.04099\n",
      "Iteration: 0045 Loss: 1.02473 Time: 0.04267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0046 Loss: 1.01418 Time: 0.04307\n",
      "Iteration: 0047 Loss: 1.01260 Time: 0.04200\n",
      "Iteration: 0048 Loss: 0.99460 Time: 0.04300\n",
      "Iteration: 0049 Loss: 0.99037 Time: 0.04117\n",
      "Iteration: 0050 Loss: 0.99441 Time: 0.04280\n",
      "Iteration: 0051 Loss: 0.97593 Time: 0.04131\n",
      "Iteration: 0052 Loss: 0.96953 Time: 0.04199\n",
      "Iteration: 0053 Loss: 0.96865 Time: 0.04201\n",
      "Iteration: 0054 Loss: 0.95508 Time: 0.04403\n",
      "Iteration: 0055 Loss: 0.94990 Time: 0.04037\n",
      "Iteration: 0056 Loss: 0.94392 Time: 0.04290\n",
      "Iteration: 0057 Loss: 0.92995 Time: 0.04194\n",
      "Iteration: 0058 Loss: 0.93126 Time: 0.04102\n",
      "Iteration: 0059 Loss: 0.92147 Time: 0.04196\n",
      "Iteration: 0060 Loss: 0.92589 Time: 0.04204\n",
      "Iteration: 0061 Loss: 0.91576 Time: 0.04246\n",
      "Iteration: 0062 Loss: 0.89690 Time: 0.04291\n",
      "Iteration: 0063 Loss: 0.89707 Time: 0.04309\n",
      "Iteration: 0064 Loss: 0.88869 Time: 0.04116\n",
      "Iteration: 0065 Loss: 0.89456 Time: 0.03957\n",
      "Iteration: 0066 Loss: 0.87605 Time: 0.04035\n",
      "Iteration: 0067 Loss: 0.88732 Time: 0.04218\n",
      "Iteration: 0068 Loss: 0.87481 Time: 0.03988\n",
      "Iteration: 0069 Loss: 0.87801 Time: 0.04005\n",
      "Iteration: 0070 Loss: 0.86207 Time: 0.03965\n",
      "Iteration: 0071 Loss: 0.85918 Time: 0.03890\n",
      "Iteration: 0072 Loss: 0.85270 Time: 0.04127\n",
      "Iteration: 0073 Loss: 0.85846 Time: 0.04295\n",
      "Iteration: 0074 Loss: 0.84829 Time: 0.04321\n",
      "Iteration: 0075 Loss: 0.84948 Time: 0.04180\n",
      "Iteration: 0076 Loss: 0.84332 Time: 0.04046\n",
      "Iteration: 0077 Loss: 0.83055 Time: 0.04089\n",
      "Iteration: 0078 Loss: 0.83041 Time: 0.04754\n",
      "Iteration: 0079 Loss: 0.82978 Time: 0.04225\n",
      "Iteration: 0080 Loss: 0.82503 Time: 0.04377\n",
      "Iteration: 0081 Loss: 0.81840 Time: 0.04112\n",
      "Iteration: 0082 Loss: 0.80857 Time: 0.04399\n",
      "Iteration: 0083 Loss: 0.81929 Time: 0.04012\n",
      "Iteration: 0084 Loss: 0.81355 Time: 0.04497\n",
      "Iteration: 0085 Loss: 0.81230 Time: 0.04300\n",
      "Iteration: 0086 Loss: 0.80673 Time: 0.03996\n",
      "Iteration: 0087 Loss: 0.80262 Time: 0.04245\n",
      "Iteration: 0088 Loss: 0.78345 Time: 0.04093\n",
      "Iteration: 0089 Loss: 0.79124 Time: 0.04203\n",
      "Iteration: 0090 Loss: 0.78170 Time: 0.04099\n",
      "Iteration: 0091 Loss: 0.78312 Time: 0.04118\n",
      "Iteration: 0092 Loss: 0.77923 Time: 0.04100\n",
      "Iteration: 0093 Loss: 0.77644 Time: 0.04100\n",
      "Iteration: 0094 Loss: 0.77603 Time: 0.04100\n",
      "Iteration: 0095 Loss: 0.76981 Time: 0.04015\n",
      "Iteration: 0096 Loss: 0.76646 Time: 0.04056\n",
      "Iteration: 0097 Loss: 0.77345 Time: 0.04178\n",
      "Iteration: 0098 Loss: 0.76875 Time: 0.04199\n",
      "Iteration: 0099 Loss: 0.75660 Time: 0.04205\n",
      "Iteration: 0100 Loss: 0.75532 Time: 0.04095\n",
      "Iteration: 0101 Loss: 0.75860 Time: 0.04100\n",
      "Iteration: 0102 Loss: 0.74958 Time: 0.03995\n",
      "Iteration: 0103 Loss: 0.74768 Time: 0.04089\n",
      "Iteration: 0104 Loss: 0.74663 Time: 0.04101\n",
      "Iteration: 0105 Loss: 0.74964 Time: 0.04089\n",
      "Iteration: 0106 Loss: 0.74608 Time: 0.03984\n",
      "Iteration: 0107 Loss: 0.74016 Time: 0.04200\n",
      "Iteration: 0108 Loss: 0.74425 Time: 0.04198\n",
      "Iteration: 0109 Loss: 0.73997 Time: 0.04327\n",
      "Iteration: 0110 Loss: 0.73907 Time: 0.04069\n",
      "Iteration: 0111 Loss: 0.73098 Time: 0.04300\n",
      "Iteration: 0112 Loss: 0.73422 Time: 0.04099\n",
      "Iteration: 0113 Loss: 0.73055 Time: 0.04098\n",
      "Iteration: 0114 Loss: 0.72195 Time: 0.03849\n",
      "Iteration: 0115 Loss: 0.73066 Time: 0.03849\n",
      "Iteration: 0116 Loss: 0.72525 Time: 0.03886\n",
      "Iteration: 0117 Loss: 0.72746 Time: 0.03746\n",
      "Iteration: 0118 Loss: 0.71434 Time: 0.04052\n",
      "Iteration: 0119 Loss: 0.71548 Time: 0.04032\n",
      "Iteration: 0120 Loss: 0.71839 Time: 0.04131\n",
      "Iteration: 0121 Loss: 0.70594 Time: 0.04100\n",
      "Iteration: 0122 Loss: 0.70523 Time: 0.04201\n",
      "Iteration: 0123 Loss: 0.70460 Time: 0.04281\n",
      "Iteration: 0124 Loss: 0.70022 Time: 0.04056\n",
      "Iteration: 0125 Loss: 0.70007 Time: 0.03976\n",
      "Iteration: 0126 Loss: 0.70016 Time: 0.04119\n",
      "Iteration: 0127 Loss: 0.69906 Time: 0.04181\n",
      "Iteration: 0128 Loss: 0.69575 Time: 0.04231\n",
      "Iteration: 0129 Loss: 0.68744 Time: 0.04061\n",
      "Iteration: 0130 Loss: 0.69148 Time: 0.04300\n",
      "Iteration: 0131 Loss: 0.69219 Time: 0.04033\n",
      "Iteration: 0132 Loss: 0.68554 Time: 0.04201\n",
      "Iteration: 0133 Loss: 0.68626 Time: 0.04399\n",
      "Iteration: 0134 Loss: 0.68390 Time: 0.04224\n",
      "Iteration: 0135 Loss: 0.67888 Time: 0.04220\n",
      "Iteration: 0136 Loss: 0.67828 Time: 0.04199\n",
      "Iteration: 0137 Loss: 0.67758 Time: 0.04200\n",
      "Iteration: 0138 Loss: 0.68432 Time: 0.04208\n",
      "Iteration: 0139 Loss: 0.68129 Time: 0.04100\n",
      "Iteration: 0140 Loss: 0.66679 Time: 0.04196\n",
      "Iteration: 0141 Loss: 0.67459 Time: 0.04203\n",
      "Iteration: 0142 Loss: 0.66924 Time: 0.04207\n",
      "Iteration: 0143 Loss: 0.67269 Time: 0.04099\n",
      "Iteration: 0144 Loss: 0.67338 Time: 0.04000\n",
      "Iteration: 0145 Loss: 0.66615 Time: 0.04106\n",
      "Iteration: 0146 Loss: 0.66692 Time: 0.04099\n",
      "Iteration: 0147 Loss: 0.66754 Time: 0.04601\n",
      "Iteration: 0148 Loss: 0.66142 Time: 0.04099\n",
      "Iteration: 0149 Loss: 0.66794 Time: 0.04300\n",
      "Iteration: 0150 Loss: 0.66575 Time: 0.04204\n",
      "Iteration: 0151 Loss: 0.66208 Time: 0.04098\n",
      "Iteration: 0152 Loss: 0.66049 Time: 0.04025\n",
      "Iteration: 0153 Loss: 0.65731 Time: 0.04097\n",
      "Iteration: 0154 Loss: 0.65651 Time: 0.04104\n",
      "Iteration: 0155 Loss: 0.65794 Time: 0.04195\n",
      "Iteration: 0156 Loss: 0.65696 Time: 0.04200\n",
      "Iteration: 0157 Loss: 0.64995 Time: 0.04095\n",
      "Iteration: 0158 Loss: 0.65672 Time: 0.04206\n",
      "Iteration: 0159 Loss: 0.64882 Time: 0.04300\n",
      "Iteration: 0160 Loss: 0.65214 Time: 0.04199\n",
      "Iteration: 0161 Loss: 0.64879 Time: 0.03921\n",
      "Iteration: 0162 Loss: 0.65212 Time: 0.03994\n",
      "Iteration: 0163 Loss: 0.64988 Time: 0.03967\n",
      "Iteration: 0164 Loss: 0.64324 Time: 0.04287\n",
      "Iteration: 0165 Loss: 0.64901 Time: 0.04059\n",
      "Iteration: 0166 Loss: 0.64950 Time: 0.04130\n",
      "Iteration: 0167 Loss: 0.64468 Time: 0.03778\n",
      "Iteration: 0168 Loss: 0.64838 Time: 0.04004\n",
      "Iteration: 0169 Loss: 0.64356 Time: 0.04190\n",
      "Iteration: 0170 Loss: 0.63872 Time: 0.04198\n",
      "Iteration: 0171 Loss: 0.64224 Time: 0.03999\n",
      "Iteration: 0172 Loss: 0.64584 Time: 0.04163\n",
      "Iteration: 0173 Loss: 0.64486 Time: 0.04087\n",
      "Iteration: 0174 Loss: 0.63495 Time: 0.04242\n",
      "Iteration: 0175 Loss: 0.63675 Time: 0.04096\n",
      "Iteration: 0176 Loss: 0.63864 Time: 0.04102\n",
      "Iteration: 0177 Loss: 0.63419 Time: 0.04202\n",
      "Iteration: 0178 Loss: 0.63665 Time: 0.04502\n",
      "Iteration: 0179 Loss: 0.63409 Time: 0.04298\n",
      "Iteration: 0180 Loss: 0.63139 Time: 0.04106\n",
      "Iteration: 0181 Loss: 0.63300 Time: 0.04314\n",
      "Iteration: 0182 Loss: 0.63425 Time: 0.04040\n",
      "Iteration: 0183 Loss: 0.63060 Time: 0.04215\n",
      "Iteration: 0184 Loss: 0.62642 Time: 0.04403\n",
      "Iteration: 0185 Loss: 0.63193 Time: 0.04155\n",
      "Iteration: 0186 Loss: 0.63072 Time: 0.04185\n",
      "Iteration: 0187 Loss: 0.63156 Time: 0.04206\n",
      "Iteration: 0188 Loss: 0.62898 Time: 0.04203\n",
      "Iteration: 0189 Loss: 0.62525 Time: 0.04295\n",
      "Iteration: 0190 Loss: 0.62635 Time: 0.04263\n",
      "Iteration: 0191 Loss: 0.62389 Time: 0.04042\n",
      "Iteration: 0192 Loss: 0.62097 Time: 0.04088\n",
      "Iteration: 0193 Loss: 0.62438 Time: 0.04300\n",
      "Iteration: 0194 Loss: 0.61660 Time: 0.04154\n",
      "Iteration: 0195 Loss: 0.62449 Time: 0.04167\n",
      "Iteration: 0196 Loss: 0.62077 Time: 0.04208\n",
      "Iteration: 0197 Loss: 0.61632 Time: 0.04229\n",
      "Iteration: 0198 Loss: 0.62376 Time: 0.04151\n",
      "Iteration: 0199 Loss: 0.61628 Time: 0.04201\n",
      "Iteration: 0200 Loss: 0.61954 Time: 0.04235\n",
      "Done! \n",
      "\n",
      "STEP 3/3 - MODEL EVALUATION \n",
      "\n",
      "Computing the final embedding vectors, for evaluation\n",
      "Done! \n",
      "\n",
      "Testing: link prediction\n",
      "Done! \n",
      "\n",
      "EXPERIMENTS ON MODEL 7 / 10 \n",
      "\n",
      "STEP 1/3 - PREPROCESSING STEPS \n",
      "\n",
      "Masking some edges from the training graph, for link prediction\n",
      "(validation set: 1.0 % of edges - test set: 10.0 % of edges)\n",
      "Done! \n",
      "\n",
      "Preprocessing node features\n",
      "Done! \n",
      "\n",
      "Running the Louvain algorithm for community detection\n",
      "as a preprocessing step for the encoder\n",
      "Done! Louvain has found 19 communities \n",
      "\n",
      "Setting up the model and the optimizer\n",
      "Done! \n",
      "\n",
      "Preprocessing on message passing matrices\n",
      "Done! \n",
      "\n",
      "Initializing TF session\n",
      "Done! \n",
      "\n",
      "STEP 2/3 - MODEL TRAINING \n",
      "\n",
      "Starting training\n",
      "Iteration: 0001 Loss: 1.73522 Time: 0.28965\n",
      "Iteration: 0002 Loss: 1.70331 Time: 0.04110\n",
      "Iteration: 0003 Loss: 1.64174 Time: 0.04231\n",
      "Iteration: 0004 Loss: 1.65338 Time: 0.04013\n",
      "Iteration: 0005 Loss: 1.62564 Time: 0.04245\n",
      "Iteration: 0006 Loss: 1.63144 Time: 0.04205\n",
      "Iteration: 0007 Loss: 1.56387 Time: 0.04295\n",
      "Iteration: 0008 Loss: 1.57272 Time: 0.03985\n",
      "Iteration: 0009 Loss: 1.51773 Time: 0.04310\n",
      "Iteration: 0010 Loss: 1.52220 Time: 0.04290\n",
      "Iteration: 0011 Loss: 1.49364 Time: 0.04312\n",
      "Iteration: 0012 Loss: 1.46900 Time: 0.04081\n",
      "Iteration: 0013 Loss: 1.45359 Time: 0.04196\n",
      "Iteration: 0014 Loss: 1.43613 Time: 0.04200\n",
      "Iteration: 0015 Loss: 1.40468 Time: 0.04071\n",
      "Iteration: 0016 Loss: 1.40214 Time: 0.04304\n",
      "Iteration: 0017 Loss: 1.34386 Time: 0.04096\n",
      "Iteration: 0018 Loss: 1.33360 Time: 0.04204\n",
      "Iteration: 0019 Loss: 1.34966 Time: 0.04207\n",
      "Iteration: 0020 Loss: 1.32156 Time: 0.04097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0021 Loss: 1.32719 Time: 0.04195\n",
      "Iteration: 0022 Loss: 1.29338 Time: 0.04200\n",
      "Iteration: 0023 Loss: 1.29425 Time: 0.03958\n",
      "Iteration: 0024 Loss: 1.25737 Time: 0.04341\n",
      "Iteration: 0025 Loss: 1.24862 Time: 0.03876\n",
      "Iteration: 0026 Loss: 1.24725 Time: 0.03912\n",
      "Iteration: 0027 Loss: 1.22315 Time: 0.04110\n",
      "Iteration: 0028 Loss: 1.19627 Time: 0.03876\n",
      "Iteration: 0029 Loss: 1.18615 Time: 0.04073\n",
      "Iteration: 0030 Loss: 1.19115 Time: 0.04100\n",
      "Iteration: 0031 Loss: 1.16284 Time: 0.04104\n",
      "Iteration: 0032 Loss: 1.16411 Time: 0.04224\n",
      "Iteration: 0033 Loss: 1.15909 Time: 0.04175\n",
      "Iteration: 0034 Loss: 1.12673 Time: 0.04097\n",
      "Iteration: 0035 Loss: 1.11003 Time: 0.04300\n",
      "Iteration: 0036 Loss: 1.11545 Time: 0.04004\n",
      "Iteration: 0037 Loss: 1.08661 Time: 0.04299\n",
      "Iteration: 0038 Loss: 1.09212 Time: 0.04198\n",
      "Iteration: 0039 Loss: 1.07972 Time: 0.04295\n",
      "Iteration: 0040 Loss: 1.07123 Time: 0.04023\n",
      "Iteration: 0041 Loss: 1.03939 Time: 0.04101\n",
      "Iteration: 0042 Loss: 1.04581 Time: 0.04100\n",
      "Iteration: 0043 Loss: 1.04711 Time: 0.04499\n",
      "Iteration: 0044 Loss: 1.04216 Time: 0.04200\n",
      "Iteration: 0045 Loss: 1.01993 Time: 0.04142\n",
      "Iteration: 0046 Loss: 1.01033 Time: 0.04101\n",
      "Iteration: 0047 Loss: 1.01176 Time: 0.04279\n",
      "Iteration: 0048 Loss: 0.99650 Time: 0.04299\n",
      "Iteration: 0049 Loss: 0.99260 Time: 0.04314\n",
      "Iteration: 0050 Loss: 0.96998 Time: 0.04229\n",
      "Iteration: 0051 Loss: 0.97913 Time: 0.03900\n",
      "Iteration: 0052 Loss: 0.96530 Time: 0.04048\n",
      "Iteration: 0053 Loss: 0.95509 Time: 0.04007\n",
      "Iteration: 0054 Loss: 0.94813 Time: 0.04326\n",
      "Iteration: 0055 Loss: 0.94552 Time: 0.04042\n",
      "Iteration: 0056 Loss: 0.94582 Time: 0.04199\n",
      "Iteration: 0057 Loss: 0.92757 Time: 0.04000\n",
      "Iteration: 0058 Loss: 0.92801 Time: 0.04200\n",
      "Iteration: 0059 Loss: 0.91858 Time: 0.04198\n",
      "Iteration: 0060 Loss: 0.91361 Time: 0.04200\n",
      "Iteration: 0061 Loss: 0.91495 Time: 0.04100\n",
      "Iteration: 0062 Loss: 0.91188 Time: 0.04109\n",
      "Iteration: 0063 Loss: 0.89835 Time: 0.03990\n",
      "Iteration: 0064 Loss: 0.89287 Time: 0.04300\n",
      "Iteration: 0065 Loss: 0.89369 Time: 0.04060\n",
      "Iteration: 0066 Loss: 0.88540 Time: 0.04199\n",
      "Iteration: 0067 Loss: 0.87902 Time: 0.04099\n",
      "Iteration: 0068 Loss: 0.86685 Time: 0.04274\n",
      "Iteration: 0069 Loss: 0.87224 Time: 0.04029\n",
      "Iteration: 0070 Loss: 0.88051 Time: 0.04122\n",
      "Iteration: 0071 Loss: 0.87681 Time: 0.03997\n",
      "Iteration: 0072 Loss: 0.83933 Time: 0.03905\n",
      "Iteration: 0073 Loss: 0.84235 Time: 0.03976\n",
      "Iteration: 0074 Loss: 0.84681 Time: 0.04234\n",
      "Iteration: 0075 Loss: 0.84990 Time: 0.04176\n",
      "Iteration: 0076 Loss: 0.83237 Time: 0.03890\n",
      "Iteration: 0077 Loss: 0.83524 Time: 0.04123\n",
      "Iteration: 0078 Loss: 0.83365 Time: 0.03943\n",
      "Iteration: 0079 Loss: 0.82381 Time: 0.03996\n",
      "Iteration: 0080 Loss: 0.80811 Time: 0.04150\n",
      "Iteration: 0081 Loss: 0.82357 Time: 0.03996\n",
      "Iteration: 0082 Loss: 0.81315 Time: 0.04098\n",
      "Iteration: 0083 Loss: 0.80971 Time: 0.04103\n",
      "Iteration: 0084 Loss: 0.79646 Time: 0.04579\n",
      "Iteration: 0085 Loss: 0.80432 Time: 0.04289\n",
      "Iteration: 0086 Loss: 0.79508 Time: 0.04096\n",
      "Iteration: 0087 Loss: 0.81027 Time: 0.04100\n",
      "Iteration: 0088 Loss: 0.78617 Time: 0.04107\n",
      "Iteration: 0089 Loss: 0.79054 Time: 0.04195\n",
      "Iteration: 0090 Loss: 0.78965 Time: 0.04104\n",
      "Iteration: 0091 Loss: 0.79144 Time: 0.04236\n",
      "Iteration: 0092 Loss: 0.78748 Time: 0.03999\n",
      "Iteration: 0093 Loss: 0.77825 Time: 0.04307\n",
      "Iteration: 0094 Loss: 0.77003 Time: 0.04018\n",
      "Iteration: 0095 Loss: 0.76609 Time: 0.04284\n",
      "Iteration: 0096 Loss: 0.77136 Time: 0.04194\n",
      "Iteration: 0097 Loss: 0.76963 Time: 0.04199\n",
      "Iteration: 0098 Loss: 0.76089 Time: 0.04100\n",
      "Iteration: 0099 Loss: 0.76062 Time: 0.04501\n",
      "Iteration: 0100 Loss: 0.75995 Time: 0.04246\n",
      "Iteration: 0101 Loss: 0.76301 Time: 0.04041\n",
      "Iteration: 0102 Loss: 0.74781 Time: 0.04136\n",
      "Iteration: 0103 Loss: 0.74845 Time: 0.04138\n",
      "Iteration: 0104 Loss: 0.74772 Time: 0.04089\n",
      "Iteration: 0105 Loss: 0.74658 Time: 0.04238\n",
      "Iteration: 0106 Loss: 0.74422 Time: 0.04222\n",
      "Iteration: 0107 Loss: 0.73820 Time: 0.04374\n",
      "Iteration: 0108 Loss: 0.73976 Time: 0.04163\n",
      "Iteration: 0109 Loss: 0.72837 Time: 0.04067\n",
      "Iteration: 0110 Loss: 0.72544 Time: 0.04180\n",
      "Iteration: 0111 Loss: 0.72799 Time: 0.04094\n",
      "Iteration: 0112 Loss: 0.73067 Time: 0.04139\n",
      "Iteration: 0113 Loss: 0.72593 Time: 0.04015\n",
      "Iteration: 0114 Loss: 0.72262 Time: 0.04202\n",
      "Iteration: 0115 Loss: 0.71884 Time: 0.04112\n",
      "Iteration: 0116 Loss: 0.72140 Time: 0.04281\n",
      "Iteration: 0117 Loss: 0.72478 Time: 0.03892\n",
      "Iteration: 0118 Loss: 0.71120 Time: 0.04081\n",
      "Iteration: 0119 Loss: 0.70951 Time: 0.03852\n",
      "Iteration: 0120 Loss: 0.71583 Time: 0.03934\n",
      "Iteration: 0121 Loss: 0.70706 Time: 0.04431\n",
      "Iteration: 0122 Loss: 0.70702 Time: 0.03967\n",
      "Iteration: 0123 Loss: 0.70251 Time: 0.03821\n",
      "Iteration: 0124 Loss: 0.70363 Time: 0.04119\n",
      "Iteration: 0125 Loss: 0.70005 Time: 0.04279\n",
      "Iteration: 0126 Loss: 0.69340 Time: 0.04099\n",
      "Iteration: 0127 Loss: 0.69800 Time: 0.04172\n",
      "Iteration: 0128 Loss: 0.69591 Time: 0.03897\n",
      "Iteration: 0129 Loss: 0.69305 Time: 0.04292\n",
      "Iteration: 0130 Loss: 0.69278 Time: 0.04040\n",
      "Iteration: 0131 Loss: 0.69507 Time: 0.04299\n",
      "Iteration: 0132 Loss: 0.69050 Time: 0.04095\n",
      "Iteration: 0133 Loss: 0.68691 Time: 0.04233\n",
      "Iteration: 0134 Loss: 0.67910 Time: 0.04060\n",
      "Iteration: 0135 Loss: 0.69006 Time: 0.04254\n",
      "Iteration: 0136 Loss: 0.68209 Time: 0.04188\n",
      "Iteration: 0137 Loss: 0.67961 Time: 0.04129\n",
      "Iteration: 0138 Loss: 0.67919 Time: 0.04105\n",
      "Iteration: 0139 Loss: 0.67693 Time: 0.04095\n",
      "Iteration: 0140 Loss: 0.68779 Time: 0.04298\n",
      "Iteration: 0141 Loss: 0.67247 Time: 0.04200\n",
      "Iteration: 0142 Loss: 0.67449 Time: 0.04199\n",
      "Iteration: 0143 Loss: 0.67544 Time: 0.04094\n",
      "Iteration: 0144 Loss: 0.67446 Time: 0.04208\n",
      "Iteration: 0145 Loss: 0.67245 Time: 0.04196\n",
      "Iteration: 0146 Loss: 0.67114 Time: 0.04095\n",
      "Iteration: 0147 Loss: 0.67607 Time: 0.04202\n",
      "Iteration: 0148 Loss: 0.66663 Time: 0.04098\n",
      "Iteration: 0149 Loss: 0.66586 Time: 0.04500\n",
      "Iteration: 0150 Loss: 0.66673 Time: 0.04342\n",
      "Iteration: 0151 Loss: 0.66680 Time: 0.04099\n",
      "Iteration: 0152 Loss: 0.66225 Time: 0.04221\n",
      "Iteration: 0153 Loss: 0.65457 Time: 0.04193\n",
      "Iteration: 0154 Loss: 0.65901 Time: 0.04114\n",
      "Iteration: 0155 Loss: 0.65600 Time: 0.04295\n",
      "Iteration: 0156 Loss: 0.65786 Time: 0.04249\n",
      "Iteration: 0157 Loss: 0.65804 Time: 0.04211\n",
      "Iteration: 0158 Loss: 0.66188 Time: 0.04290\n",
      "Iteration: 0159 Loss: 0.65125 Time: 0.04299\n",
      "Iteration: 0160 Loss: 0.65400 Time: 0.04164\n",
      "Iteration: 0161 Loss: 0.65200 Time: 0.04222\n",
      "Iteration: 0162 Loss: 0.65276 Time: 0.04167\n",
      "Iteration: 0163 Loss: 0.64953 Time: 0.04144\n",
      "Iteration: 0164 Loss: 0.64987 Time: 0.04219\n",
      "Iteration: 0165 Loss: 0.65126 Time: 0.04105\n",
      "Iteration: 0166 Loss: 0.64508 Time: 0.04044\n",
      "Iteration: 0167 Loss: 0.64904 Time: 0.03803\n",
      "Iteration: 0168 Loss: 0.64589 Time: 0.03963\n",
      "Iteration: 0169 Loss: 0.64521 Time: 0.04008\n",
      "Iteration: 0170 Loss: 0.64157 Time: 0.03806\n",
      "Iteration: 0171 Loss: 0.64682 Time: 0.04192\n",
      "Iteration: 0172 Loss: 0.63666 Time: 0.04018\n",
      "Iteration: 0173 Loss: 0.63911 Time: 0.04205\n",
      "Iteration: 0174 Loss: 0.63990 Time: 0.04202\n",
      "Iteration: 0175 Loss: 0.64390 Time: 0.04395\n",
      "Iteration: 0176 Loss: 0.63652 Time: 0.04650\n",
      "Iteration: 0177 Loss: 0.63572 Time: 0.04135\n",
      "Iteration: 0178 Loss: 0.63844 Time: 0.04183\n",
      "Iteration: 0179 Loss: 0.63182 Time: 0.04093\n",
      "Iteration: 0180 Loss: 0.63738 Time: 0.04097\n",
      "Iteration: 0181 Loss: 0.62579 Time: 0.04096\n",
      "Iteration: 0182 Loss: 0.63343 Time: 0.04335\n",
      "Iteration: 0183 Loss: 0.63013 Time: 0.04028\n",
      "Iteration: 0184 Loss: 0.62704 Time: 0.04102\n",
      "Iteration: 0185 Loss: 0.62868 Time: 0.04100\n",
      "Iteration: 0186 Loss: 0.62406 Time: 0.04095\n",
      "Iteration: 0187 Loss: 0.62889 Time: 0.04200\n",
      "Iteration: 0188 Loss: 0.62694 Time: 0.04525\n",
      "Iteration: 0189 Loss: 0.62379 Time: 0.04175\n",
      "Iteration: 0190 Loss: 0.62656 Time: 0.04250\n",
      "Iteration: 0191 Loss: 0.62736 Time: 0.04168\n",
      "Iteration: 0192 Loss: 0.62549 Time: 0.04147\n",
      "Iteration: 0193 Loss: 0.61960 Time: 0.04299\n",
      "Iteration: 0194 Loss: 0.62575 Time: 0.03979\n",
      "Iteration: 0195 Loss: 0.62460 Time: 0.04096\n",
      "Iteration: 0196 Loss: 0.62282 Time: 0.04120\n",
      "Iteration: 0197 Loss: 0.61912 Time: 0.04138\n",
      "Iteration: 0198 Loss: 0.62474 Time: 0.04109\n",
      "Iteration: 0199 Loss: 0.62092 Time: 0.04296\n",
      "Iteration: 0200 Loss: 0.62377 Time: 0.04179\n",
      "Done! \n",
      "\n",
      "STEP 3/3 - MODEL EVALUATION \n",
      "\n",
      "Computing the final embedding vectors, for evaluation\n",
      "Done! \n",
      "\n",
      "Testing: link prediction\n",
      "Done! \n",
      "\n",
      "EXPERIMENTS ON MODEL 8 / 10 \n",
      "\n",
      "STEP 1/3 - PREPROCESSING STEPS \n",
      "\n",
      "Masking some edges from the training graph, for link prediction\n",
      "(validation set: 1.0 % of edges - test set: 10.0 % of edges)\n",
      "Done! \n",
      "\n",
      "Preprocessing node features\n",
      "Done! \n",
      "\n",
      "Running the Louvain algorithm for community detection\n",
      "as a preprocessing step for the encoder\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Louvain has found 21 communities \n",
      "\n",
      "Setting up the model and the optimizer\n",
      "Done! \n",
      "\n",
      "Preprocessing on message passing matrices\n",
      "Done! \n",
      "\n",
      "Initializing TF session\n",
      "Done! \n",
      "\n",
      "STEP 2/3 - MODEL TRAINING \n",
      "\n",
      "Starting training\n",
      "Iteration: 0001 Loss: 1.74930 Time: 0.28700\n",
      "Iteration: 0002 Loss: 1.72900 Time: 0.04300\n",
      "Iteration: 0003 Loss: 1.67894 Time: 0.04199\n",
      "Iteration: 0004 Loss: 1.63794 Time: 0.04251\n",
      "Iteration: 0005 Loss: 1.62056 Time: 0.04123\n",
      "Iteration: 0006 Loss: 1.66034 Time: 0.04280\n",
      "Iteration: 0007 Loss: 1.57189 Time: 0.04116\n",
      "Iteration: 0008 Loss: 1.55823 Time: 0.04032\n",
      "Iteration: 0009 Loss: 1.54182 Time: 0.04113\n",
      "Iteration: 0010 Loss: 1.50015 Time: 0.04111\n",
      "Iteration: 0011 Loss: 1.48068 Time: 0.04243\n",
      "Iteration: 0012 Loss: 1.48182 Time: 0.04341\n",
      "Iteration: 0013 Loss: 1.46157 Time: 0.04139\n",
      "Iteration: 0014 Loss: 1.42600 Time: 0.04197\n",
      "Iteration: 0015 Loss: 1.43370 Time: 0.04167\n",
      "Iteration: 0016 Loss: 1.39320 Time: 0.04015\n",
      "Iteration: 0017 Loss: 1.37437 Time: 0.04271\n",
      "Iteration: 0018 Loss: 1.35488 Time: 0.04192\n",
      "Iteration: 0019 Loss: 1.33359 Time: 0.04080\n",
      "Iteration: 0020 Loss: 1.31788 Time: 0.04089\n",
      "Iteration: 0021 Loss: 1.32119 Time: 0.04144\n",
      "Iteration: 0022 Loss: 1.28335 Time: 0.04148\n",
      "Iteration: 0023 Loss: 1.28500 Time: 0.04165\n",
      "Iteration: 0024 Loss: 1.28615 Time: 0.03965\n",
      "Iteration: 0025 Loss: 1.23833 Time: 0.03948\n",
      "Iteration: 0026 Loss: 1.23165 Time: 0.03933\n",
      "Iteration: 0027 Loss: 1.23834 Time: 0.04190\n",
      "Iteration: 0028 Loss: 1.22878 Time: 0.03964\n",
      "Iteration: 0029 Loss: 1.20025 Time: 0.03935\n",
      "Iteration: 0030 Loss: 1.17423 Time: 0.03906\n",
      "Iteration: 0031 Loss: 1.16391 Time: 0.03974\n",
      "Iteration: 0032 Loss: 1.16203 Time: 0.04235\n",
      "Iteration: 0033 Loss: 1.14646 Time: 0.03990\n",
      "Iteration: 0034 Loss: 1.13348 Time: 0.04258\n",
      "Iteration: 0035 Loss: 1.13607 Time: 0.04105\n",
      "Iteration: 0036 Loss: 1.14406 Time: 0.04191\n",
      "Iteration: 0037 Loss: 1.10339 Time: 0.04199\n",
      "Iteration: 0038 Loss: 1.09475 Time: 0.04100\n",
      "Iteration: 0039 Loss: 1.09301 Time: 0.04112\n",
      "Iteration: 0040 Loss: 1.09068 Time: 0.04428\n",
      "Iteration: 0041 Loss: 1.06926 Time: 0.04120\n",
      "Iteration: 0042 Loss: 1.04779 Time: 0.04281\n",
      "Iteration: 0043 Loss: 1.03857 Time: 0.04263\n",
      "Iteration: 0044 Loss: 1.02203 Time: 0.04238\n",
      "Iteration: 0045 Loss: 1.04114 Time: 0.04174\n",
      "Iteration: 0046 Loss: 1.00618 Time: 0.04304\n",
      "Iteration: 0047 Loss: 1.00574 Time: 0.04228\n",
      "Iteration: 0048 Loss: 1.00147 Time: 0.04387\n",
      "Iteration: 0049 Loss: 0.99560 Time: 0.04116\n",
      "Iteration: 0050 Loss: 0.97640 Time: 0.04229\n",
      "Iteration: 0051 Loss: 0.95700 Time: 0.04125\n",
      "Iteration: 0052 Loss: 0.97528 Time: 0.04200\n",
      "Iteration: 0053 Loss: 0.95142 Time: 0.04309\n",
      "Iteration: 0054 Loss: 0.94106 Time: 0.04120\n",
      "Iteration: 0055 Loss: 0.94072 Time: 0.04175\n",
      "Iteration: 0056 Loss: 0.94417 Time: 0.04101\n",
      "Iteration: 0057 Loss: 0.92032 Time: 0.04241\n",
      "Iteration: 0058 Loss: 0.93179 Time: 0.04154\n",
      "Iteration: 0059 Loss: 0.92903 Time: 0.04125\n",
      "Iteration: 0060 Loss: 0.91771 Time: 0.04082\n",
      "Iteration: 0061 Loss: 0.90873 Time: 0.04101\n",
      "Iteration: 0062 Loss: 0.90280 Time: 0.04496\n",
      "Iteration: 0063 Loss: 0.89902 Time: 0.04206\n",
      "Iteration: 0064 Loss: 0.89241 Time: 0.04320\n",
      "Iteration: 0065 Loss: 0.88420 Time: 0.04116\n",
      "Iteration: 0066 Loss: 0.88617 Time: 0.04230\n",
      "Iteration: 0067 Loss: 0.87465 Time: 0.04098\n",
      "Iteration: 0068 Loss: 0.87589 Time: 0.04134\n",
      "Iteration: 0069 Loss: 0.87322 Time: 0.04069\n",
      "Iteration: 0070 Loss: 0.87103 Time: 0.04173\n",
      "Iteration: 0071 Loss: 0.86386 Time: 0.04118\n",
      "Iteration: 0072 Loss: 0.84675 Time: 0.04287\n",
      "Iteration: 0073 Loss: 0.85073 Time: 0.03969\n",
      "Iteration: 0074 Loss: 0.83446 Time: 0.03887\n",
      "Iteration: 0075 Loss: 0.84767 Time: 0.03950\n",
      "Iteration: 0076 Loss: 0.82065 Time: 0.03933\n",
      "Iteration: 0077 Loss: 0.82907 Time: 0.04223\n",
      "Iteration: 0078 Loss: 0.82345 Time: 0.03974\n",
      "Iteration: 0079 Loss: 0.82060 Time: 0.04431\n",
      "Iteration: 0080 Loss: 0.82013 Time: 0.04265\n",
      "Iteration: 0081 Loss: 0.82088 Time: 0.04137\n",
      "Iteration: 0082 Loss: 0.81721 Time: 0.04082\n",
      "Iteration: 0083 Loss: 0.80426 Time: 0.04093\n",
      "Iteration: 0084 Loss: 0.81327 Time: 0.04287\n",
      "Iteration: 0085 Loss: 0.80708 Time: 0.04248\n",
      "Iteration: 0086 Loss: 0.79928 Time: 0.04028\n",
      "Iteration: 0087 Loss: 0.79445 Time: 0.04698\n",
      "Iteration: 0088 Loss: 0.79220 Time: 0.04237\n",
      "Iteration: 0089 Loss: 0.78720 Time: 0.04163\n",
      "Iteration: 0090 Loss: 0.78987 Time: 0.04260\n",
      "Iteration: 0091 Loss: 0.78417 Time: 0.04237\n",
      "Iteration: 0092 Loss: 0.77975 Time: 0.04134\n",
      "Iteration: 0093 Loss: 0.78736 Time: 0.04203\n",
      "Iteration: 0094 Loss: 0.76554 Time: 0.04129\n",
      "Iteration: 0095 Loss: 0.75774 Time: 0.04069\n",
      "Iteration: 0096 Loss: 0.76726 Time: 0.04340\n",
      "Iteration: 0097 Loss: 0.77776 Time: 0.04241\n",
      "Iteration: 0098 Loss: 0.76798 Time: 0.04058\n",
      "Iteration: 0099 Loss: 0.76757 Time: 0.04153\n",
      "Iteration: 0100 Loss: 0.76054 Time: 0.04142\n",
      "Iteration: 0101 Loss: 0.75743 Time: 0.04146\n",
      "Iteration: 0102 Loss: 0.75751 Time: 0.04176\n",
      "Iteration: 0103 Loss: 0.73518 Time: 0.04102\n",
      "Iteration: 0104 Loss: 0.75300 Time: 0.04070\n",
      "Iteration: 0105 Loss: 0.74717 Time: 0.04128\n",
      "Iteration: 0106 Loss: 0.73868 Time: 0.04095\n",
      "Iteration: 0107 Loss: 0.74551 Time: 0.04299\n",
      "Iteration: 0108 Loss: 0.74416 Time: 0.04118\n",
      "Iteration: 0109 Loss: 0.73562 Time: 0.04382\n",
      "Iteration: 0110 Loss: 0.73526 Time: 0.04161\n",
      "Iteration: 0111 Loss: 0.72891 Time: 0.04200\n",
      "Iteration: 0112 Loss: 0.71995 Time: 0.04477\n",
      "Iteration: 0113 Loss: 0.73144 Time: 0.04104\n",
      "Iteration: 0114 Loss: 0.72700 Time: 0.04331\n",
      "Iteration: 0115 Loss: 0.72382 Time: 0.04066\n",
      "Iteration: 0116 Loss: 0.72767 Time: 0.04470\n",
      "Iteration: 0117 Loss: 0.71196 Time: 0.03991\n",
      "Iteration: 0118 Loss: 0.72170 Time: 0.04210\n",
      "Iteration: 0119 Loss: 0.71454 Time: 0.04016\n",
      "Iteration: 0120 Loss: 0.70831 Time: 0.04074\n",
      "Iteration: 0121 Loss: 0.71457 Time: 0.03886\n",
      "Iteration: 0122 Loss: 0.71572 Time: 0.04118\n",
      "Iteration: 0123 Loss: 0.70102 Time: 0.03916\n",
      "Iteration: 0124 Loss: 0.70731 Time: 0.03977\n",
      "Iteration: 0125 Loss: 0.70662 Time: 0.03958\n",
      "Iteration: 0126 Loss: 0.70520 Time: 0.04004\n",
      "Iteration: 0127 Loss: 0.70268 Time: 0.04177\n",
      "Iteration: 0128 Loss: 0.69189 Time: 0.04228\n",
      "Iteration: 0129 Loss: 0.69311 Time: 0.03944\n",
      "Iteration: 0130 Loss: 0.68766 Time: 0.04267\n",
      "Iteration: 0131 Loss: 0.69134 Time: 0.04100\n",
      "Iteration: 0132 Loss: 0.68459 Time: 0.04146\n",
      "Iteration: 0133 Loss: 0.68838 Time: 0.04150\n",
      "Iteration: 0134 Loss: 0.68554 Time: 0.04241\n",
      "Iteration: 0135 Loss: 0.68720 Time: 0.04100\n",
      "Iteration: 0136 Loss: 0.68509 Time: 0.04162\n",
      "Iteration: 0137 Loss: 0.68556 Time: 0.04324\n",
      "Iteration: 0138 Loss: 0.67822 Time: 0.04248\n",
      "Iteration: 0139 Loss: 0.67536 Time: 0.04199\n",
      "Iteration: 0140 Loss: 0.67218 Time: 0.04100\n",
      "Iteration: 0141 Loss: 0.67403 Time: 0.04157\n",
      "Iteration: 0142 Loss: 0.66920 Time: 0.04144\n",
      "Iteration: 0143 Loss: 0.67072 Time: 0.04109\n",
      "Iteration: 0144 Loss: 0.66926 Time: 0.04111\n",
      "Iteration: 0145 Loss: 0.66430 Time: 0.04300\n",
      "Iteration: 0146 Loss: 0.66833 Time: 0.04070\n",
      "Iteration: 0147 Loss: 0.66539 Time: 0.04151\n",
      "Iteration: 0148 Loss: 0.66839 Time: 0.04176\n",
      "Iteration: 0149 Loss: 0.65867 Time: 0.04204\n",
      "Iteration: 0150 Loss: 0.67097 Time: 0.04115\n",
      "Iteration: 0151 Loss: 0.66126 Time: 0.04123\n",
      "Iteration: 0152 Loss: 0.66093 Time: 0.04132\n",
      "Iteration: 0153 Loss: 0.66275 Time: 0.04191\n",
      "Iteration: 0154 Loss: 0.66166 Time: 0.04166\n",
      "Iteration: 0155 Loss: 0.65543 Time: 0.04112\n",
      "Iteration: 0156 Loss: 0.65902 Time: 0.04125\n",
      "Iteration: 0157 Loss: 0.64992 Time: 0.04127\n",
      "Iteration: 0158 Loss: 0.66188 Time: 0.04146\n",
      "Iteration: 0159 Loss: 0.65345 Time: 0.04286\n",
      "Iteration: 0160 Loss: 0.65092 Time: 0.04025\n",
      "Iteration: 0161 Loss: 0.65046 Time: 0.04203\n",
      "Iteration: 0162 Loss: 0.64367 Time: 0.04225\n",
      "Iteration: 0163 Loss: 0.64925 Time: 0.04387\n",
      "Iteration: 0164 Loss: 0.64734 Time: 0.04246\n",
      "Iteration: 0165 Loss: 0.65378 Time: 0.04150\n",
      "Iteration: 0166 Loss: 0.64179 Time: 0.04170\n",
      "Iteration: 0167 Loss: 0.64781 Time: 0.04001\n",
      "Iteration: 0168 Loss: 0.64149 Time: 0.03954\n",
      "Iteration: 0169 Loss: 0.64152 Time: 0.04200\n",
      "Iteration: 0170 Loss: 0.63967 Time: 0.04009\n",
      "Iteration: 0171 Loss: 0.64064 Time: 0.04082\n",
      "Iteration: 0172 Loss: 0.64475 Time: 0.03955\n",
      "Iteration: 0173 Loss: 0.64158 Time: 0.04119\n",
      "Iteration: 0174 Loss: 0.63941 Time: 0.04043\n",
      "Iteration: 0175 Loss: 0.64109 Time: 0.04221\n",
      "Iteration: 0176 Loss: 0.63916 Time: 0.04181\n",
      "Iteration: 0177 Loss: 0.63535 Time: 0.04110\n",
      "Iteration: 0178 Loss: 0.63066 Time: 0.04210\n",
      "Iteration: 0179 Loss: 0.63616 Time: 0.04124\n",
      "Iteration: 0180 Loss: 0.62730 Time: 0.04111\n",
      "Iteration: 0181 Loss: 0.62675 Time: 0.04027\n",
      "Iteration: 0182 Loss: 0.63543 Time: 0.04207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0183 Loss: 0.63006 Time: 0.04195\n",
      "Iteration: 0184 Loss: 0.62828 Time: 0.04200\n",
      "Iteration: 0185 Loss: 0.62358 Time: 0.04200\n",
      "Iteration: 0186 Loss: 0.63213 Time: 0.04196\n",
      "Iteration: 0187 Loss: 0.63031 Time: 0.04405\n",
      "Iteration: 0188 Loss: 0.62862 Time: 0.04200\n",
      "Iteration: 0189 Loss: 0.63071 Time: 0.04299\n",
      "Iteration: 0190 Loss: 0.62598 Time: 0.04005\n",
      "Iteration: 0191 Loss: 0.63090 Time: 0.04195\n",
      "Iteration: 0192 Loss: 0.62625 Time: 0.03998\n",
      "Iteration: 0193 Loss: 0.62078 Time: 0.04301\n",
      "Iteration: 0194 Loss: 0.62556 Time: 0.04019\n",
      "Iteration: 0195 Loss: 0.62597 Time: 0.04181\n",
      "Iteration: 0196 Loss: 0.62170 Time: 0.04099\n",
      "Iteration: 0197 Loss: 0.62101 Time: 0.04301\n",
      "Iteration: 0198 Loss: 0.62070 Time: 0.04100\n",
      "Iteration: 0199 Loss: 0.62226 Time: 0.04249\n",
      "Iteration: 0200 Loss: 0.61425 Time: 0.04151\n",
      "Done! \n",
      "\n",
      "STEP 3/3 - MODEL EVALUATION \n",
      "\n",
      "Computing the final embedding vectors, for evaluation\n",
      "Done! \n",
      "\n",
      "Testing: link prediction\n",
      "Done! \n",
      "\n",
      "EXPERIMENTS ON MODEL 9 / 10 \n",
      "\n",
      "STEP 1/3 - PREPROCESSING STEPS \n",
      "\n",
      "Masking some edges from the training graph, for link prediction\n",
      "(validation set: 1.0 % of edges - test set: 10.0 % of edges)\n",
      "Done! \n",
      "\n",
      "Preprocessing node features\n",
      "Done! \n",
      "\n",
      "Running the Louvain algorithm for community detection\n",
      "as a preprocessing step for the encoder\n",
      "Done! Louvain has found 20 communities \n",
      "\n",
      "Setting up the model and the optimizer\n",
      "Done! \n",
      "\n",
      "Preprocessing on message passing matrices\n",
      "Done! \n",
      "\n",
      "Initializing TF session\n",
      "Done! \n",
      "\n",
      "STEP 2/3 - MODEL TRAINING \n",
      "\n",
      "Starting training\n",
      "Iteration: 0001 Loss: 1.76567 Time: 0.29805\n",
      "Iteration: 0002 Loss: 1.67597 Time: 0.04293\n",
      "Iteration: 0003 Loss: 1.67480 Time: 0.04301\n",
      "Iteration: 0004 Loss: 1.64462 Time: 0.04142\n",
      "Iteration: 0005 Loss: 1.62877 Time: 0.04138\n",
      "Iteration: 0006 Loss: 1.59053 Time: 0.04328\n",
      "Iteration: 0007 Loss: 1.58288 Time: 0.04368\n",
      "Iteration: 0008 Loss: 1.56697 Time: 0.04112\n",
      "Iteration: 0009 Loss: 1.52767 Time: 0.04210\n",
      "Iteration: 0010 Loss: 1.49048 Time: 0.04196\n",
      "Iteration: 0011 Loss: 1.48625 Time: 0.04303\n",
      "Iteration: 0012 Loss: 1.44977 Time: 0.04209\n",
      "Iteration: 0013 Loss: 1.44460 Time: 0.04224\n",
      "Iteration: 0014 Loss: 1.43330 Time: 0.04269\n",
      "Iteration: 0015 Loss: 1.42350 Time: 0.04049\n",
      "Iteration: 0016 Loss: 1.40658 Time: 0.04128\n",
      "Iteration: 0017 Loss: 1.37361 Time: 0.04017\n",
      "Iteration: 0018 Loss: 1.37273 Time: 0.04276\n",
      "Iteration: 0019 Loss: 1.33472 Time: 0.04122\n",
      "Iteration: 0020 Loss: 1.30456 Time: 0.04205\n",
      "Iteration: 0021 Loss: 1.30370 Time: 0.04071\n",
      "Iteration: 0022 Loss: 1.30355 Time: 0.04462\n",
      "Iteration: 0023 Loss: 1.29221 Time: 0.04039\n",
      "Iteration: 0024 Loss: 1.27085 Time: 0.04154\n",
      "Iteration: 0025 Loss: 1.26341 Time: 0.04049\n",
      "Iteration: 0026 Loss: 1.24894 Time: 0.04397\n",
      "Iteration: 0027 Loss: 1.21515 Time: 0.04096\n",
      "Iteration: 0028 Loss: 1.20581 Time: 0.04015\n",
      "Iteration: 0029 Loss: 1.18936 Time: 0.03971\n",
      "Iteration: 0030 Loss: 1.18247 Time: 0.03857\n",
      "Iteration: 0031 Loss: 1.14666 Time: 0.03950\n",
      "Iteration: 0032 Loss: 1.15032 Time: 0.04181\n",
      "Iteration: 0033 Loss: 1.13281 Time: 0.04045\n",
      "Iteration: 0034 Loss: 1.13327 Time: 0.03976\n",
      "Iteration: 0035 Loss: 1.11143 Time: 0.04258\n",
      "Iteration: 0036 Loss: 1.11495 Time: 0.04295\n",
      "Iteration: 0037 Loss: 1.10382 Time: 0.04186\n",
      "Iteration: 0038 Loss: 1.09724 Time: 0.04200\n",
      "Iteration: 0039 Loss: 1.09225 Time: 0.04120\n",
      "Iteration: 0040 Loss: 1.06085 Time: 0.04135\n",
      "Iteration: 0041 Loss: 1.07152 Time: 0.04251\n",
      "Iteration: 0042 Loss: 1.05714 Time: 0.03996\n",
      "Iteration: 0043 Loss: 1.03348 Time: 0.04227\n",
      "Iteration: 0044 Loss: 1.04212 Time: 0.04148\n",
      "Iteration: 0045 Loss: 1.03186 Time: 0.04139\n",
      "Iteration: 0046 Loss: 1.01957 Time: 0.04152\n",
      "Iteration: 0047 Loss: 1.01302 Time: 0.04312\n",
      "Iteration: 0048 Loss: 0.99632 Time: 0.03988\n",
      "Iteration: 0049 Loss: 0.97757 Time: 0.04201\n",
      "Iteration: 0050 Loss: 0.98537 Time: 0.04157\n",
      "Iteration: 0051 Loss: 0.96368 Time: 0.04195\n",
      "Iteration: 0052 Loss: 0.97453 Time: 0.04211\n",
      "Iteration: 0053 Loss: 0.95798 Time: 0.04135\n",
      "Iteration: 0054 Loss: 0.94181 Time: 0.04208\n",
      "Iteration: 0055 Loss: 0.93188 Time: 0.04165\n",
      "Iteration: 0056 Loss: 0.92823 Time: 0.04084\n",
      "Iteration: 0057 Loss: 0.95015 Time: 0.04297\n",
      "Iteration: 0058 Loss: 0.92566 Time: 0.04116\n",
      "Iteration: 0059 Loss: 0.92033 Time: 0.04306\n",
      "Iteration: 0060 Loss: 0.89599 Time: 0.04187\n",
      "Iteration: 0061 Loss: 0.89814 Time: 0.04143\n",
      "Iteration: 0062 Loss: 0.89881 Time: 0.04366\n",
      "Iteration: 0063 Loss: 0.89934 Time: 0.04128\n",
      "Iteration: 0064 Loss: 0.89185 Time: 0.04190\n",
      "Iteration: 0065 Loss: 0.87764 Time: 0.04122\n",
      "Iteration: 0066 Loss: 0.88677 Time: 0.04282\n",
      "Iteration: 0067 Loss: 0.87564 Time: 0.04125\n",
      "Iteration: 0068 Loss: 0.87455 Time: 0.04025\n",
      "Iteration: 0069 Loss: 0.87384 Time: 0.04052\n",
      "Iteration: 0070 Loss: 0.86382 Time: 0.04194\n",
      "Iteration: 0071 Loss: 0.85362 Time: 0.04017\n",
      "Iteration: 0072 Loss: 0.84329 Time: 0.04378\n",
      "Iteration: 0073 Loss: 0.83688 Time: 0.04032\n",
      "Iteration: 0074 Loss: 0.83938 Time: 0.04021\n",
      "Iteration: 0075 Loss: 0.83638 Time: 0.04010\n",
      "Iteration: 0076 Loss: 0.83392 Time: 0.04157\n",
      "Iteration: 0077 Loss: 0.82728 Time: 0.04243\n",
      "Iteration: 0078 Loss: 0.81523 Time: 0.04084\n",
      "Iteration: 0079 Loss: 0.82204 Time: 0.04032\n",
      "Iteration: 0080 Loss: 0.81074 Time: 0.03969\n",
      "Iteration: 0081 Loss: 0.81909 Time: 0.03999\n",
      "Iteration: 0082 Loss: 0.80062 Time: 0.04057\n",
      "Iteration: 0083 Loss: 0.80225 Time: 0.04178\n",
      "Iteration: 0084 Loss: 0.81097 Time: 0.04045\n",
      "Iteration: 0085 Loss: 0.79992 Time: 0.04171\n",
      "Iteration: 0086 Loss: 0.79667 Time: 0.04086\n",
      "Iteration: 0087 Loss: 0.79474 Time: 0.04291\n",
      "Iteration: 0088 Loss: 0.79385 Time: 0.04140\n",
      "Iteration: 0089 Loss: 0.77979 Time: 0.04158\n",
      "Iteration: 0090 Loss: 0.78031 Time: 0.04070\n",
      "Iteration: 0091 Loss: 0.78583 Time: 0.04304\n",
      "Iteration: 0092 Loss: 0.77568 Time: 0.04604\n",
      "Iteration: 0093 Loss: 0.78178 Time: 0.04046\n",
      "Iteration: 0094 Loss: 0.76589 Time: 0.04201\n",
      "Iteration: 0095 Loss: 0.77487 Time: 0.04194\n",
      "Iteration: 0096 Loss: 0.77162 Time: 0.04220\n",
      "Iteration: 0097 Loss: 0.76450 Time: 0.04117\n",
      "Iteration: 0098 Loss: 0.76663 Time: 0.04167\n",
      "Iteration: 0099 Loss: 0.75639 Time: 0.04208\n",
      "Iteration: 0100 Loss: 0.75785 Time: 0.04284\n",
      "Iteration: 0101 Loss: 0.74718 Time: 0.04297\n",
      "Iteration: 0102 Loss: 0.74760 Time: 0.04217\n",
      "Iteration: 0103 Loss: 0.74594 Time: 0.04184\n",
      "Iteration: 0104 Loss: 0.74432 Time: 0.04202\n",
      "Iteration: 0105 Loss: 0.74178 Time: 0.04201\n",
      "Iteration: 0106 Loss: 0.73348 Time: 0.04070\n",
      "Iteration: 0107 Loss: 0.73487 Time: 0.04206\n",
      "Iteration: 0108 Loss: 0.73618 Time: 0.04036\n",
      "Iteration: 0109 Loss: 0.72379 Time: 0.04249\n",
      "Iteration: 0110 Loss: 0.72533 Time: 0.04100\n",
      "Iteration: 0111 Loss: 0.72285 Time: 0.04278\n",
      "Iteration: 0112 Loss: 0.72157 Time: 0.04268\n",
      "Iteration: 0113 Loss: 0.71387 Time: 0.04122\n",
      "Iteration: 0114 Loss: 0.71631 Time: 0.04110\n",
      "Iteration: 0115 Loss: 0.71708 Time: 0.04268\n",
      "Iteration: 0116 Loss: 0.71079 Time: 0.04207\n",
      "Iteration: 0117 Loss: 0.71447 Time: 0.04242\n",
      "Iteration: 0118 Loss: 0.71856 Time: 0.03997\n",
      "Iteration: 0119 Loss: 0.71025 Time: 0.04244\n",
      "Iteration: 0120 Loss: 0.71293 Time: 0.04166\n",
      "Iteration: 0121 Loss: 0.70845 Time: 0.04093\n",
      "Iteration: 0122 Loss: 0.70112 Time: 0.04192\n",
      "Iteration: 0123 Loss: 0.70540 Time: 0.04200\n",
      "Iteration: 0124 Loss: 0.70283 Time: 0.03975\n",
      "Iteration: 0125 Loss: 0.69991 Time: 0.03905\n",
      "Iteration: 0126 Loss: 0.70279 Time: 0.04287\n",
      "Iteration: 0127 Loss: 0.69262 Time: 0.03926\n",
      "Iteration: 0128 Loss: 0.69378 Time: 0.04308\n",
      "Iteration: 0129 Loss: 0.69269 Time: 0.03896\n",
      "Iteration: 0130 Loss: 0.68713 Time: 0.04118\n",
      "Iteration: 0131 Loss: 0.68944 Time: 0.04085\n",
      "Iteration: 0132 Loss: 0.68848 Time: 0.04305\n",
      "Iteration: 0133 Loss: 0.68489 Time: 0.04237\n",
      "Iteration: 0134 Loss: 0.68271 Time: 0.04220\n",
      "Iteration: 0135 Loss: 0.68870 Time: 0.04270\n",
      "Iteration: 0136 Loss: 0.67237 Time: 0.04150\n",
      "Iteration: 0137 Loss: 0.68487 Time: 0.04170\n",
      "Iteration: 0138 Loss: 0.68096 Time: 0.04177\n",
      "Iteration: 0139 Loss: 0.67613 Time: 0.04172\n",
      "Iteration: 0140 Loss: 0.68113 Time: 0.04342\n",
      "Iteration: 0141 Loss: 0.68206 Time: 0.04201\n",
      "Iteration: 0142 Loss: 0.67914 Time: 0.03932\n",
      "Iteration: 0143 Loss: 0.67013 Time: 0.04268\n",
      "Iteration: 0144 Loss: 0.67019 Time: 0.04210\n",
      "Iteration: 0145 Loss: 0.67269 Time: 0.04290\n",
      "Iteration: 0146 Loss: 0.66626 Time: 0.04157\n",
      "Iteration: 0147 Loss: 0.67396 Time: 0.04114\n",
      "Iteration: 0148 Loss: 0.66494 Time: 0.04071\n",
      "Iteration: 0149 Loss: 0.66104 Time: 0.04295\n",
      "Iteration: 0150 Loss: 0.66657 Time: 0.04019\n",
      "Iteration: 0151 Loss: 0.66140 Time: 0.04408\n",
      "Iteration: 0152 Loss: 0.65966 Time: 0.04101\n",
      "Iteration: 0153 Loss: 0.65614 Time: 0.04111\n",
      "Iteration: 0154 Loss: 0.66107 Time: 0.04099\n",
      "Iteration: 0155 Loss: 0.65523 Time: 0.04195\n",
      "Iteration: 0156 Loss: 0.65612 Time: 0.04008\n",
      "Iteration: 0157 Loss: 0.65887 Time: 0.04031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0158 Loss: 0.65989 Time: 0.04153\n",
      "Iteration: 0159 Loss: 0.65305 Time: 0.04194\n",
      "Iteration: 0160 Loss: 0.65605 Time: 0.04143\n",
      "Iteration: 0161 Loss: 0.65628 Time: 0.04236\n",
      "Iteration: 0162 Loss: 0.64949 Time: 0.04099\n",
      "Iteration: 0163 Loss: 0.64812 Time: 0.04115\n",
      "Iteration: 0164 Loss: 0.64940 Time: 0.04099\n",
      "Iteration: 0165 Loss: 0.64351 Time: 0.04100\n",
      "Iteration: 0166 Loss: 0.64409 Time: 0.03997\n",
      "Iteration: 0167 Loss: 0.64423 Time: 0.04158\n",
      "Iteration: 0168 Loss: 0.64119 Time: 0.04086\n",
      "Iteration: 0169 Loss: 0.64267 Time: 0.04198\n",
      "Iteration: 0170 Loss: 0.63939 Time: 0.03980\n",
      "Iteration: 0171 Loss: 0.64103 Time: 0.04106\n",
      "Iteration: 0172 Loss: 0.63635 Time: 0.03928\n",
      "Iteration: 0173 Loss: 0.63974 Time: 0.04010\n",
      "Iteration: 0174 Loss: 0.64118 Time: 0.04120\n",
      "Iteration: 0175 Loss: 0.63233 Time: 0.03981\n",
      "Iteration: 0176 Loss: 0.63995 Time: 0.03942\n",
      "Iteration: 0177 Loss: 0.62810 Time: 0.04036\n",
      "Iteration: 0178 Loss: 0.63858 Time: 0.04036\n",
      "Iteration: 0179 Loss: 0.63453 Time: 0.04105\n",
      "Iteration: 0180 Loss: 0.63401 Time: 0.04187\n",
      "Iteration: 0181 Loss: 0.63581 Time: 0.04304\n",
      "Iteration: 0182 Loss: 0.63605 Time: 0.04332\n",
      "Iteration: 0183 Loss: 0.62860 Time: 0.04045\n",
      "Iteration: 0184 Loss: 0.62576 Time: 0.04138\n",
      "Iteration: 0185 Loss: 0.62657 Time: 0.04156\n",
      "Iteration: 0186 Loss: 0.62408 Time: 0.04124\n",
      "Iteration: 0187 Loss: 0.62959 Time: 0.04214\n",
      "Iteration: 0188 Loss: 0.62599 Time: 0.04116\n",
      "Iteration: 0189 Loss: 0.62987 Time: 0.04282\n",
      "Iteration: 0190 Loss: 0.62274 Time: 0.04092\n",
      "Iteration: 0191 Loss: 0.62564 Time: 0.04076\n",
      "Iteration: 0192 Loss: 0.62483 Time: 0.04072\n",
      "Iteration: 0193 Loss: 0.62146 Time: 0.04114\n",
      "Iteration: 0194 Loss: 0.62092 Time: 0.04297\n",
      "Iteration: 0195 Loss: 0.62442 Time: 0.04126\n",
      "Iteration: 0196 Loss: 0.62469 Time: 0.04116\n",
      "Iteration: 0197 Loss: 0.62106 Time: 0.04314\n",
      "Iteration: 0198 Loss: 0.62024 Time: 0.04189\n",
      "Iteration: 0199 Loss: 0.62101 Time: 0.04215\n",
      "Iteration: 0200 Loss: 0.62227 Time: 0.04078\n",
      "Done! \n",
      "\n",
      "STEP 3/3 - MODEL EVALUATION \n",
      "\n",
      "Computing the final embedding vectors, for evaluation\n",
      "Done! \n",
      "\n",
      "Testing: link prediction\n",
      "Done! \n",
      "\n",
      "EXPERIMENTS ON MODEL 10 / 10 \n",
      "\n",
      "STEP 1/3 - PREPROCESSING STEPS \n",
      "\n",
      "Masking some edges from the training graph, for link prediction\n",
      "(validation set: 1.0 % of edges - test set: 10.0 % of edges)\n",
      "Done! \n",
      "\n",
      "Preprocessing node features\n",
      "Done! \n",
      "\n",
      "Running the Louvain algorithm for community detection\n",
      "as a preprocessing step for the encoder\n",
      "Done! Louvain has found 15 communities \n",
      "\n",
      "Setting up the model and the optimizer\n",
      "Done! \n",
      "\n",
      "Preprocessing on message passing matrices\n",
      "Done! \n",
      "\n",
      "Initializing TF session\n",
      "Done! \n",
      "\n",
      "STEP 2/3 - MODEL TRAINING \n",
      "\n",
      "Starting training\n",
      "Iteration: 0001 Loss: 1.73918 Time: 0.30262\n",
      "Iteration: 0002 Loss: 1.69086 Time: 0.04348\n",
      "Iteration: 0003 Loss: 1.64452 Time: 0.04203\n",
      "Iteration: 0004 Loss: 1.63492 Time: 0.04155\n",
      "Iteration: 0005 Loss: 1.59666 Time: 0.04188\n",
      "Iteration: 0006 Loss: 1.59759 Time: 0.04200\n",
      "Iteration: 0007 Loss: 1.56024 Time: 0.04313\n",
      "Iteration: 0008 Loss: 1.54458 Time: 0.04428\n",
      "Iteration: 0009 Loss: 1.54125 Time: 0.04258\n",
      "Iteration: 0010 Loss: 1.51498 Time: 0.04300\n",
      "Iteration: 0011 Loss: 1.48483 Time: 0.04205\n",
      "Iteration: 0012 Loss: 1.45916 Time: 0.04498\n",
      "Iteration: 0013 Loss: 1.42524 Time: 0.04313\n",
      "Iteration: 0014 Loss: 1.42969 Time: 0.04145\n",
      "Iteration: 0015 Loss: 1.40273 Time: 0.04055\n",
      "Iteration: 0016 Loss: 1.37609 Time: 0.04396\n",
      "Iteration: 0017 Loss: 1.38480 Time: 0.04364\n",
      "Iteration: 0018 Loss: 1.34910 Time: 0.04196\n",
      "Iteration: 0019 Loss: 1.36018 Time: 0.04099\n",
      "Iteration: 0020 Loss: 1.31759 Time: 0.04199\n",
      "Iteration: 0021 Loss: 1.31406 Time: 0.04223\n",
      "Iteration: 0022 Loss: 1.28832 Time: 0.04288\n",
      "Iteration: 0023 Loss: 1.26516 Time: 0.04277\n",
      "Iteration: 0024 Loss: 1.24483 Time: 0.04000\n",
      "Iteration: 0025 Loss: 1.25926 Time: 0.04265\n",
      "Iteration: 0026 Loss: 1.23924 Time: 0.04444\n",
      "Iteration: 0027 Loss: 1.21699 Time: 0.04167\n",
      "Iteration: 0028 Loss: 1.22340 Time: 0.04234\n",
      "Iteration: 0029 Loss: 1.20390 Time: 0.04007\n",
      "Iteration: 0030 Loss: 1.16685 Time: 0.03873\n",
      "Iteration: 0031 Loss: 1.17210 Time: 0.04003\n",
      "Iteration: 0032 Loss: 1.15049 Time: 0.04038\n",
      "Iteration: 0033 Loss: 1.14537 Time: 0.04028\n",
      "Iteration: 0034 Loss: 1.13786 Time: 0.03897\n",
      "Iteration: 0035 Loss: 1.11483 Time: 0.03957\n",
      "Iteration: 0036 Loss: 1.11115 Time: 0.04159\n",
      "Iteration: 0037 Loss: 1.10694 Time: 0.04345\n",
      "Iteration: 0038 Loss: 1.09694 Time: 0.04102\n",
      "Iteration: 0039 Loss: 1.07327 Time: 0.04099\n",
      "Iteration: 0040 Loss: 1.06035 Time: 0.04127\n",
      "Iteration: 0041 Loss: 1.03953 Time: 0.04173\n",
      "Iteration: 0042 Loss: 1.05010 Time: 0.04199\n",
      "Iteration: 0043 Loss: 1.04254 Time: 0.04300\n",
      "Iteration: 0044 Loss: 1.02967 Time: 0.04134\n",
      "Iteration: 0045 Loss: 1.02761 Time: 0.04267\n",
      "Iteration: 0046 Loss: 1.00519 Time: 0.04262\n",
      "Iteration: 0047 Loss: 0.99774 Time: 0.04300\n",
      "Iteration: 0048 Loss: 0.98021 Time: 0.04199\n",
      "Iteration: 0049 Loss: 0.97661 Time: 0.04228\n",
      "Iteration: 0050 Loss: 0.96844 Time: 0.04126\n",
      "Iteration: 0051 Loss: 0.96664 Time: 0.04116\n",
      "Iteration: 0052 Loss: 0.95986 Time: 0.04224\n",
      "Iteration: 0053 Loss: 0.95966 Time: 0.04305\n",
      "Iteration: 0054 Loss: 0.94813 Time: 0.04099\n",
      "Iteration: 0055 Loss: 0.93575 Time: 0.04084\n",
      "Iteration: 0056 Loss: 0.92186 Time: 0.04177\n",
      "Iteration: 0057 Loss: 0.92647 Time: 0.04242\n",
      "Iteration: 0058 Loss: 0.92297 Time: 0.04208\n",
      "Iteration: 0059 Loss: 0.90562 Time: 0.04122\n",
      "Iteration: 0060 Loss: 0.90575 Time: 0.04362\n",
      "Iteration: 0061 Loss: 0.90586 Time: 0.04326\n",
      "Iteration: 0062 Loss: 0.90358 Time: 0.04203\n",
      "Iteration: 0063 Loss: 0.89686 Time: 0.04174\n",
      "Iteration: 0064 Loss: 0.90106 Time: 0.04199\n",
      "Iteration: 0065 Loss: 0.87746 Time: 0.04117\n",
      "Iteration: 0066 Loss: 0.89235 Time: 0.04109\n",
      "Iteration: 0067 Loss: 0.89055 Time: 0.04202\n",
      "Iteration: 0068 Loss: 0.86687 Time: 0.04301\n",
      "Iteration: 0069 Loss: 0.86089 Time: 0.04299\n",
      "Iteration: 0070 Loss: 0.85524 Time: 0.04152\n",
      "Iteration: 0071 Loss: 0.86756 Time: 0.04348\n",
      "Iteration: 0072 Loss: 0.84956 Time: 0.04099\n",
      "Iteration: 0073 Loss: 0.85749 Time: 0.04240\n",
      "Iteration: 0074 Loss: 0.84249 Time: 0.04323\n",
      "Iteration: 0075 Loss: 0.84397 Time: 0.04160\n",
      "Iteration: 0076 Loss: 0.83354 Time: 0.04034\n",
      "Iteration: 0077 Loss: 0.83096 Time: 0.04075\n",
      "Iteration: 0078 Loss: 0.82158 Time: 0.03892\n",
      "Iteration: 0079 Loss: 0.82313 Time: 0.04187\n",
      "Iteration: 0080 Loss: 0.81795 Time: 0.03879\n",
      "Iteration: 0081 Loss: 0.81668 Time: 0.03891\n",
      "Iteration: 0082 Loss: 0.82177 Time: 0.03881\n",
      "Iteration: 0083 Loss: 0.81415 Time: 0.03917\n",
      "Iteration: 0084 Loss: 0.80062 Time: 0.04185\n",
      "Iteration: 0085 Loss: 0.80384 Time: 0.04200\n",
      "Iteration: 0086 Loss: 0.80426 Time: 0.04300\n",
      "Iteration: 0087 Loss: 0.79718 Time: 0.04301\n",
      "Iteration: 0088 Loss: 0.78597 Time: 0.04099\n",
      "Iteration: 0089 Loss: 0.79505 Time: 0.04326\n",
      "Iteration: 0090 Loss: 0.79169 Time: 0.04360\n",
      "Iteration: 0091 Loss: 0.77782 Time: 0.04075\n",
      "Iteration: 0092 Loss: 0.77579 Time: 0.04259\n",
      "Iteration: 0093 Loss: 0.77483 Time: 0.04100\n",
      "Iteration: 0094 Loss: 0.77638 Time: 0.04312\n",
      "Iteration: 0095 Loss: 0.77186 Time: 0.04214\n",
      "Iteration: 0096 Loss: 0.76351 Time: 0.04100\n",
      "Iteration: 0097 Loss: 0.76950 Time: 0.04083\n",
      "Iteration: 0098 Loss: 0.76298 Time: 0.04124\n",
      "Iteration: 0099 Loss: 0.76160 Time: 0.04077\n",
      "Iteration: 0100 Loss: 0.75444 Time: 0.04207\n",
      "Iteration: 0101 Loss: 0.75873 Time: 0.04024\n",
      "Iteration: 0102 Loss: 0.74927 Time: 0.04173\n",
      "Iteration: 0103 Loss: 0.74242 Time: 0.04122\n",
      "Iteration: 0104 Loss: 0.74732 Time: 0.04226\n",
      "Iteration: 0105 Loss: 0.74764 Time: 0.04101\n",
      "Iteration: 0106 Loss: 0.73543 Time: 0.04326\n",
      "Iteration: 0107 Loss: 0.74850 Time: 0.04094\n",
      "Iteration: 0108 Loss: 0.73635 Time: 0.04277\n",
      "Iteration: 0109 Loss: 0.72751 Time: 0.04203\n",
      "Iteration: 0110 Loss: 0.73483 Time: 0.04200\n",
      "Iteration: 0111 Loss: 0.73574 Time: 0.04300\n",
      "Iteration: 0112 Loss: 0.72586 Time: 0.04253\n",
      "Iteration: 0113 Loss: 0.71896 Time: 0.04179\n",
      "Iteration: 0114 Loss: 0.72651 Time: 0.04027\n",
      "Iteration: 0115 Loss: 0.72197 Time: 0.04094\n",
      "Iteration: 0116 Loss: 0.72015 Time: 0.04143\n",
      "Iteration: 0117 Loss: 0.71930 Time: 0.04099\n",
      "Iteration: 0118 Loss: 0.70708 Time: 0.04200\n",
      "Iteration: 0119 Loss: 0.71749 Time: 0.04099\n",
      "Iteration: 0120 Loss: 0.70874 Time: 0.04169\n",
      "Iteration: 0121 Loss: 0.71174 Time: 0.04158\n",
      "Iteration: 0122 Loss: 0.70665 Time: 0.04158\n",
      "Iteration: 0123 Loss: 0.70186 Time: 0.04299\n",
      "Iteration: 0124 Loss: 0.70357 Time: 0.04105\n",
      "Iteration: 0125 Loss: 0.69896 Time: 0.04089\n",
      "Iteration: 0126 Loss: 0.69507 Time: 0.03991\n",
      "Iteration: 0127 Loss: 0.69424 Time: 0.04048\n",
      "Iteration: 0128 Loss: 0.70439 Time: 0.03959\n",
      "Iteration: 0129 Loss: 0.69623 Time: 0.04086\n",
      "Iteration: 0130 Loss: 0.69681 Time: 0.04054\n",
      "Iteration: 0131 Loss: 0.69485 Time: 0.03967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0132 Loss: 0.69595 Time: 0.04107\n",
      "Iteration: 0133 Loss: 0.68939 Time: 0.04184\n",
      "Iteration: 0134 Loss: 0.68698 Time: 0.04109\n",
      "Iteration: 0135 Loss: 0.69047 Time: 0.04156\n",
      "Iteration: 0136 Loss: 0.68463 Time: 0.04302\n",
      "Iteration: 0137 Loss: 0.67737 Time: 0.04212\n",
      "Iteration: 0138 Loss: 0.68337 Time: 0.04199\n",
      "Iteration: 0139 Loss: 0.67796 Time: 0.04300\n",
      "Iteration: 0140 Loss: 0.67841 Time: 0.04140\n",
      "Iteration: 0141 Loss: 0.67891 Time: 0.04000\n",
      "Iteration: 0142 Loss: 0.68029 Time: 0.04154\n",
      "Iteration: 0143 Loss: 0.67126 Time: 0.04000\n",
      "Iteration: 0144 Loss: 0.67238 Time: 0.04339\n",
      "Iteration: 0145 Loss: 0.67379 Time: 0.04080\n",
      "Iteration: 0146 Loss: 0.66731 Time: 0.04147\n",
      "Iteration: 0147 Loss: 0.66535 Time: 0.04099\n",
      "Iteration: 0148 Loss: 0.66722 Time: 0.04300\n",
      "Iteration: 0149 Loss: 0.66678 Time: 0.04100\n",
      "Iteration: 0150 Loss: 0.66742 Time: 0.04300\n",
      "Iteration: 0151 Loss: 0.66318 Time: 0.04108\n",
      "Iteration: 0152 Loss: 0.65922 Time: 0.04129\n",
      "Iteration: 0153 Loss: 0.65879 Time: 0.04183\n",
      "Iteration: 0154 Loss: 0.65956 Time: 0.04224\n",
      "Iteration: 0155 Loss: 0.65542 Time: 0.04108\n",
      "Iteration: 0156 Loss: 0.65703 Time: 0.04143\n",
      "Iteration: 0157 Loss: 0.65718 Time: 0.03998\n",
      "Iteration: 0158 Loss: 0.65765 Time: 0.04112\n",
      "Iteration: 0159 Loss: 0.65187 Time: 0.04452\n",
      "Iteration: 0160 Loss: 0.65891 Time: 0.04100\n",
      "Iteration: 0161 Loss: 0.65247 Time: 0.04215\n",
      "Iteration: 0162 Loss: 0.64924 Time: 0.04289\n",
      "Iteration: 0163 Loss: 0.65046 Time: 0.04120\n",
      "Iteration: 0164 Loss: 0.65013 Time: 0.04335\n",
      "Iteration: 0165 Loss: 0.64994 Time: 0.04107\n",
      "Iteration: 0166 Loss: 0.65157 Time: 0.04157\n",
      "Iteration: 0167 Loss: 0.64533 Time: 0.04154\n",
      "Iteration: 0168 Loss: 0.64355 Time: 0.04296\n",
      "Iteration: 0169 Loss: 0.64389 Time: 0.04196\n",
      "Iteration: 0170 Loss: 0.64097 Time: 0.04204\n",
      "Iteration: 0171 Loss: 0.64545 Time: 0.04300\n",
      "Iteration: 0172 Loss: 0.64310 Time: 0.04128\n",
      "Iteration: 0173 Loss: 0.64446 Time: 0.04031\n",
      "Iteration: 0174 Loss: 0.64030 Time: 0.04214\n",
      "Iteration: 0175 Loss: 0.63908 Time: 0.03964\n",
      "Iteration: 0176 Loss: 0.63757 Time: 0.04001\n",
      "Iteration: 0177 Loss: 0.64215 Time: 0.03872\n",
      "Iteration: 0178 Loss: 0.63936 Time: 0.04063\n",
      "Iteration: 0179 Loss: 0.64039 Time: 0.03999\n",
      "Iteration: 0180 Loss: 0.63508 Time: 0.04157\n",
      "Iteration: 0181 Loss: 0.63241 Time: 0.04100\n",
      "Iteration: 0182 Loss: 0.62987 Time: 0.04146\n",
      "Iteration: 0183 Loss: 0.62726 Time: 0.04099\n",
      "Iteration: 0184 Loss: 0.63272 Time: 0.04201\n",
      "Iteration: 0185 Loss: 0.63491 Time: 0.04042\n",
      "Iteration: 0186 Loss: 0.62800 Time: 0.04499\n",
      "Iteration: 0187 Loss: 0.63482 Time: 0.04130\n",
      "Iteration: 0188 Loss: 0.63443 Time: 0.04275\n",
      "Iteration: 0189 Loss: 0.62638 Time: 0.04107\n",
      "Iteration: 0190 Loss: 0.63309 Time: 0.04099\n",
      "Iteration: 0191 Loss: 0.62578 Time: 0.04101\n",
      "Iteration: 0192 Loss: 0.62382 Time: 0.04329\n",
      "Iteration: 0193 Loss: 0.62247 Time: 0.03999\n",
      "Iteration: 0194 Loss: 0.61843 Time: 0.04127\n",
      "Iteration: 0195 Loss: 0.62489 Time: 0.04128\n",
      "Iteration: 0196 Loss: 0.62700 Time: 0.04172\n",
      "Iteration: 0197 Loss: 0.61932 Time: 0.04199\n",
      "Iteration: 0198 Loss: 0.62099 Time: 0.04120\n",
      "Iteration: 0199 Loss: 0.61627 Time: 0.04300\n",
      "Iteration: 0200 Loss: 0.62108 Time: 0.04148\n",
      "Done! \n",
      "\n",
      "STEP 3/3 - MODEL EVALUATION \n",
      "\n",
      "Computing the final embedding vectors, for evaluation\n",
      "Done! \n",
      "\n",
      "Testing: link prediction\n",
      "Done! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We repeat the entire training+test process FLAGS.nb_run times\n",
    "for i in range(FLAGS.nb_run):\n",
    "    if FLAGS.verbose:\n",
    "        print(\"EXPERIMENTS ON MODEL\", i + 1, \"/\", FLAGS.nb_run, \"\\n\")\n",
    "        print(\"STEP 1/3 - PREPROCESSING STEPS \\n\")\n",
    "\n",
    "\n",
    "    # Edge masking for Link Prediction:\n",
    "    if FLAGS.task == 'task_2':\n",
    "        # Compute Train/Validation/Test sets\n",
    "        if FLAGS.verbose:\n",
    "            print(\"Masking some edges from the training graph, for link prediction\")\n",
    "            print(\"(validation set:\", FLAGS.prop_val, \"% of edges - test set:\",\n",
    "                  FLAGS.prop_test, \"% of edges)\")\n",
    "        #adj, val_edges, val_edges_false, test_edges, test_edges_false = mask_test_edges(adj_init, FLAGS.prop_test, FLAGS.prop_val)\n",
    "        adj, test_edges, test_edges_false = mask_test_edges(adj_init, FLAGS.prop_test)\n",
    "        if FLAGS.verbose:\n",
    "            print(\"Done! \\n\")\n",
    "    else:\n",
    "        adj = adj_init\n",
    "\n",
    "    # Compute the number of nodes\n",
    "    num_nodes = adj.shape[0]\n",
    "\n",
    "    # Preprocessing on node features\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Preprocessing node features\")\n",
    "    if FLAGS.features:\n",
    "        features = features_init\n",
    "    else:\n",
    "        # If features are not used, replace feature matrix by identity matrix\n",
    "        features = sp.identity(num_nodes)\n",
    "    features = sparse_to_tuple(features)\n",
    "    num_features = features[2][1]\n",
    "    features_nonzero = features[1].shape[0]\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Done! \\n\")\n",
    "\n",
    "\n",
    "    # Community detection using Louvain, as a preprocessing step\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Running the Louvain algorithm for community detection\")\n",
    "        print(\"as a preprocessing step for the encoder\")\n",
    "    # Get binary community matrix (adj_louvain_init[i,j] = 1 if nodes i and j are\n",
    "    # in the same community) as well as number of communities found by Louvain\n",
    "    adj_louvain_init, nb_communities_louvain, partition = louvain_clustering(adj, FLAGS.s_reg)\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Done! Louvain has found\", nb_communities_louvain, \"communities \\n\")\n",
    "\n",
    "    # FastGAE: node sampling for stochastic subgraph decoding - used when facing scalability issues\n",
    "    if FLAGS.fastgae:\n",
    "        if FLAGS.verbose:\n",
    "            print(\"Preprocessing operations related to the FastGAE method\")\n",
    "        # Node-level p_i degree-based, core-based or uniform distribution\n",
    "        node_distribution = get_distribution(FLAGS.measure, FLAGS.alpha, adj)\n",
    "        # Node sampling for initializations\n",
    "        sampled_nodes, adj_label, adj_sampled_sparse = node_sampling(adj, node_distribution, FLAGS.nb_node_samples, FLAGS.replace)\n",
    "        if FLAGS.verbose:\n",
    "            print(\"Done! \\n\")\n",
    "    else:\n",
    "        sampled_nodes = np.array(range(FLAGS.nb_node_samples))\n",
    "\n",
    "\n",
    "    # Placeholders\n",
    "    if FLAGS.verbose:\n",
    "        print('Setting up the model and the optimizer')\n",
    "    placeholders = {\n",
    "        'features': tf.sparse_placeholder(tf.float32),\n",
    "        'adj': tf.sparse_placeholder(tf.float32),\n",
    "        'adj_layer2': tf.sparse_placeholder(tf.float32), # Only used for 2-layer GCN encoders\n",
    "        'degree_matrix': tf.sparse_placeholder(tf.float32),\n",
    "        'adj_orig': tf.sparse_placeholder(tf.float32),\n",
    "        'dropout': tf.placeholder_with_default(0., shape = ()),\n",
    "        'sampled_nodes': tf.placeholder_with_default(sampled_nodes, shape = [FLAGS.nb_node_samples])\n",
    "    }\n",
    "\n",
    "\n",
    "    # Create model\n",
    "    if FLAGS.model == 'linear_ae':\n",
    "        # Linear Graph Autoencoder\n",
    "        model = LinearModelAE(placeholders, num_features, features_nonzero)\n",
    "    elif FLAGS.model == 'linear_vae':\n",
    "        # Linear Graph Variational Autoencoder\n",
    "        model = LinearModelVAE(placeholders, num_features, num_nodes, features_nonzero)\n",
    "    elif FLAGS.model == 'gcn_ae':\n",
    "        # 2-layer GCN Graph Autoencoder\n",
    "        model = GCNModelAE(placeholders, num_features, features_nonzero)\n",
    "    elif FLAGS.model == 'gcn_vae':\n",
    "        # 2-layer GCN Graph Variational Autoencoder\n",
    "        model = GCNModelVAE(placeholders, num_features, num_nodes, features_nonzero)\n",
    "    else:\n",
    "        raise ValueError('Undefined model!')\n",
    "\n",
    "\n",
    "    # Optimizer\n",
    "    if FLAGS.fastgae:\n",
    "        num_sampled = adj_sampled_sparse.shape[0]\n",
    "        sum_sampled = adj_sampled_sparse.sum()\n",
    "        pos_weight = float(num_sampled * num_sampled - sum_sampled) / sum_sampled\n",
    "        norm = num_sampled * num_sampled / float((num_sampled * num_sampled - sum_sampled) * 2)\n",
    "    else:\n",
    "        pos_weight = float(num_nodes * num_nodes - adj.sum()) / adj.sum()\n",
    "        norm = num_nodes * num_nodes / float((num_nodes * num_nodes - adj.sum()) * 2)\n",
    "\n",
    "    if FLAGS.model in ('gcn_ae', 'linear_ae'):\n",
    "        opt = OptimizerAE(preds = model.reconstructions,\n",
    "                          labels = tf.reshape(tf.sparse_tensor_to_dense(placeholders['adj_orig'], validate_indices = False), [-1]),\n",
    "                          degree_matrix = tf.reshape(tf.sparse_tensor_to_dense(placeholders['degree_matrix'], validate_indices = False), [-1]),\n",
    "                          num_nodes = num_nodes,\n",
    "                          pos_weight = pos_weight,\n",
    "                          norm = norm,\n",
    "                          clusters_distance = model.clusters)\n",
    "    else:\n",
    "        opt = OptimizerVAE(preds = model.reconstructions,\n",
    "                           labels = tf.reshape(tf.sparse_tensor_to_dense(placeholders['adj_orig'], validate_indices = False), [-1]),\n",
    "                           degree_matrix = tf.reshape(tf.sparse_tensor_to_dense(placeholders['degree_matrix'], validate_indices = False), [-1]),\n",
    "                           model = model,\n",
    "                           num_nodes = num_nodes,\n",
    "                           pos_weight = pos_weight,\n",
    "                           norm = norm,\n",
    "                           clusters_distance = model.clusters)\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Done! \\n\")\n",
    "\n",
    "\n",
    "    # Symmetrically normalized \"message passing\" matrices\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Preprocessing on message passing matrices\")\n",
    "    adj_norm = preprocess_graph(adj + FLAGS.lamb*adj_louvain_init)\n",
    "    adj_norm_layer2 = preprocess_graph(adj)\n",
    "    if not FLAGS.fastgae:\n",
    "        adj_label = sparse_to_tuple(adj + sp.eye(num_nodes))\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Done! \\n\")\n",
    "\n",
    "\n",
    "    # Degree matrices\n",
    "    deg_matrix, deg_matrix_init = preprocess_degree(adj, FLAGS.simple)\n",
    "\n",
    "\n",
    "    # Initialize TF session\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Initializing TF session\")\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Done! \\n\")\n",
    "\n",
    "\n",
    "    # Model training\n",
    "    if FLAGS.verbose:\n",
    "        print(\"STEP 2/3 - MODEL TRAINING \\n\")\n",
    "        print(\"Starting training\")\n",
    "\n",
    "    for iter in range(FLAGS.iterations):\n",
    "\n",
    "        # Flag to compute running time for each iteration\n",
    "        t = time.time()\n",
    "\n",
    "        # Construct feed dictionary\n",
    "        feed_dict = construct_feed_dict(adj_norm, adj_norm_layer2, adj_label, features, deg_matrix, placeholders)\n",
    "        if FLAGS.fastgae:\n",
    "            # Update sampled subgraph and sampled Louvain matrix\n",
    "            feed_dict.update({placeholders['sampled_nodes']: sampled_nodes})\n",
    "            # New node sampling\n",
    "            sampled_nodes, adj_label, adj_label_sparse, = node_sampling(adj, node_distribution, FLAGS.nb_node_samples, FLAGS.replace)\n",
    "\n",
    "        # Weights update\n",
    "        outs = sess.run([opt.opt_op, opt.cost, opt.cost_adj, opt.cost_mod],\n",
    "                        feed_dict = feed_dict)\n",
    "\n",
    "        # Compute average loss\n",
    "        avg_cost = outs[1]\n",
    "        if FLAGS.verbose:\n",
    "            # Display information on the iteration\n",
    "            print(\"Iteration:\", '%04d' % (iter + 1), \"Loss:\", \"{:.5f}\".format(avg_cost),\n",
    "                  \"Time:\", \"{:.5f}\".format(time.time() - t))\n",
    "\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Done! \\n\")\n",
    "\n",
    "    # Compute embedding vectors, for evaluation\n",
    "    if FLAGS.verbose:\n",
    "        print(\"STEP 3/3 - MODEL EVALUATION \\n\")\n",
    "        print(\"Computing the final embedding vectors, for evaluation\")\n",
    "    emb = sess.run(model.z_mean, feed_dict = feed_dict)\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Done! \\n\")\n",
    "\n",
    "    # Test model: link prediction (classification edges/non-edges)\n",
    "\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Testing: link prediction\")\n",
    "    # Get ROC and AP scores\n",
    "    roc_score, ap_score = link_prediction(test_edges, test_edges_false, emb)\n",
    "    mean_roc.append(roc_score)\n",
    "    mean_ap.append(ap_score)\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Done! \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bef29d76",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-28T09:13:10.100579Z",
     "start_time": "2022-10-28T09:13:10.085537Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL RESULTS \n",
      "\n",
      "Recall: the selected task was \"Task 2\", i.e., joint community detection and link prediction, on email\n",
      "All scores reported below are computed over the 10 run(s) \n",
      "\n",
      "Link prediction:\n",
      "\n",
      "Mean AUC score:  0.9065097183817225\n",
      "Std of AUC scores:  0.0034874404057760535 \n",
      "\n",
      "Mean AP score:  0.9141709467315475\n",
      "Std of AP scores:  0.0035821363900837115 \n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Report final results\n",
    "print(\"FINAL RESULTS \\n\")\n",
    "\n",
    "print('Recall: the selected task was \"Task 2\", i.e., joint community detection and link prediction, on', FLAGS.dataset)\n",
    "\n",
    "print(\"All scores reported below are computed over the\", FLAGS.nb_run, \"run(s) \\n\")\n",
    "\n",
    "print(\"Link prediction:\\n\")\n",
    "print(\"Mean AUC score: \", np.mean(mean_roc))\n",
    "print(\"Std of AUC scores: \", np.std(mean_roc), \"\\n\")\n",
    "\n",
    "print(\"Mean AP score: \", np.mean(mean_ap))\n",
    "print(\"Std of AP scores: \", np.std(mean_ap), \"\\n \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553e6eb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf1.15",
   "language": "python",
   "name": "tf1.15"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
