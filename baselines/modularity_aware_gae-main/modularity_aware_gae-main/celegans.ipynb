{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67dbcca0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-28T08:53:09.901878Z",
     "start_time": "2022-10-28T08:53:07.763846Z"
    }
   },
   "outputs": [],
   "source": [
    "from modularity_aware_gae.evaluation import community_detection, link_prediction\n",
    "from modularity_aware_gae.input_data import load_data, load_labels\n",
    "from modularity_aware_gae.louvain import louvain_clustering\n",
    "from modularity_aware_gae.model import  GCNModelAE, GCNModelVAE, LinearModelAE, LinearModelVAE\n",
    "from modularity_aware_gae.optimizer import OptimizerAE, OptimizerVAE\n",
    "from modularity_aware_gae.preprocessing import *\n",
    "from modularity_aware_gae.sampling import get_distribution, node_sampling\n",
    "import numpy as np\n",
    "import os\n",
    "import scipy.sparse as sp\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel')#添加的，不报错\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14ffd620",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-28T08:53:18.136550Z",
     "start_time": "2022-10-28T08:53:18.116368Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " \n",
      " \n",
      "[MODULARITY-AWARE GRAPH AUTOENCODERS]\n",
      " \n",
      " \n",
      " \n",
      "\n",
      "EXPERIMENTAL SETTING \n",
      "\n",
      "- Graph dataset: celegans\n",
      "- Mode name: linear_vae\n",
      "- Number of models to train: 10\n",
      "- Number of training iterations for each model: 200\n",
      "- Learning rate: 0.01\n",
      "- Dropout rate: 0.0\n",
      "- Use of node features in the input layer: False\n",
      "- Dimension of the output layer: 16\n",
      "- lambda: 0.0\n",
      "- beta: 0.0\n",
      "- gamma: 1.0\n",
      "- s: 2\n",
      "- FastGAE: no \n",
      "\n",
      "Final embedding vectors will be evaluated on:\n",
      "- Task 2, i.e., joint community detection and link prediction\n",
      "\n",
      " \n",
      " \n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "# Parameters related to the input data\n",
    "flags.DEFINE_string('dataset', 'celegans', 'Graph dataset, among: cora, citeseer, wiki, celegans, email, polbooks, texas, wisconsin')\n",
    "\n",
    "flags.DEFINE_boolean('features', False, 'Whether to include node features')\n",
    "\n",
    "\n",
    "# Parameters related to the Modularity-Aware GAE/VGAE model to train\n",
    "\n",
    "# 1/3 - General parameters associated with GAE/VGAE\n",
    "flags.DEFINE_string('model', 'linear_vae', 'Model to train, among: gcn_ae, gcn_vae, \\\n",
    "                                            linear_ae, linear_vae')\n",
    "flags.DEFINE_float('dropout', 0., 'Dropout rate')\n",
    "flags.DEFINE_integer('iterations', 200, 'Number of iterations in training')\n",
    "flags.DEFINE_float('learning_rate', 0.01, 'Initial learning rate (Adam)')\n",
    "flags.DEFINE_integer('hidden', 32, 'Dimension of the GCN hidden layer')\n",
    "flags.DEFINE_integer('dimension', 16, 'Dimension of the output layer, i.e., \\\n",
    "                                       dimension of the embedding space')\n",
    "\n",
    "# 2/3 - Additional parameters, specific to Modularity-Aware models\n",
    "flags.DEFINE_float('beta', 0.0, 'Beta hyperparameter of Mod.-Aware models')\n",
    "flags.DEFINE_float('lamb', 0.0, 'Lambda hyperparameter of Mod.-Aware models')\n",
    "flags.DEFINE_float('gamma', 1.0, 'Gamma hyperparameter of Mod.-Aware models')\n",
    "flags.DEFINE_integer('s_reg', 2, 's hyperparameter of Mod.-Aware models')\n",
    "\n",
    "# 3/3 - Additional parameters, aiming to improve scalability\n",
    "flags.DEFINE_boolean('fastgae', False, 'Whether to use the FastGAE framework')\n",
    "flags.DEFINE_integer('nb_node_samples', 1000, 'In FastGAE, number of nodes to \\\n",
    "                                               sample at each iteration, i.e., \\\n",
    "                                               size of decoded subgraph')\n",
    "flags.DEFINE_string('measure', 'degree', 'In FastGAE, node importance measure used \\\n",
    "                                          for sampling: degree, core, or uniform')\n",
    "flags.DEFINE_float('alpha', 1.0, 'alpha hyperparameter associated with the FastGAE sampling')\n",
    "flags.DEFINE_boolean('replace', False, 'Sample nodes with or without replacement')\n",
    "flags.DEFINE_boolean('simple', True, 'Use simpler (and faster) modularity in optimizers')\n",
    "\n",
    "\n",
    "# Parameters related to the experimental setup\n",
    "flags.DEFINE_string('task', 'task_2', 'task_1: pure community detection \\\n",
    "                                       task_2: joint link prediction and \\\n",
    "                                               community detection')\n",
    "flags.DEFINE_integer('nb_run', 10, 'Number of model run + test')\n",
    "flags.DEFINE_float('prop_val', 1., 'Proportion of edges in validation set \\\n",
    "                                    for the link prediction task')\n",
    "flags.DEFINE_float('prop_test', 10., 'Proportion of edges in test set \\\n",
    "                                      for the link prediction task')\n",
    "flags.DEFINE_boolean('validation', False, 'Whether to compute validation \\\n",
    "                                           results at each iteration, for \\\n",
    "                                           the link prediction task')\n",
    "flags.DEFINE_boolean('verbose', True, 'Whether to print all comments details')\n",
    "\n",
    "\n",
    "# Introductory message\n",
    "if FLAGS.verbose:\n",
    "    introductory_message()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ce16ae4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-28T08:53:18.993637Z",
     "start_time": "2022-10-28T08:53:18.977647Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADING DATA\n",
      "\n",
      "Loading the celegans graph\n",
      "- Number of nodes: 297\n",
      "- Use of node features: False\n",
      "Done! \n",
      " \n",
      " \n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize lists to collect final results\n",
    "mean_ami = []\n",
    "mean_ari = []\n",
    "\n",
    "mean_roc = []\n",
    "mean_ap = []\n",
    "\n",
    "# Load data\n",
    "if FLAGS.verbose:\n",
    "    print(\"LOADING DATA\\n\")\n",
    "    print(\"Loading the\", FLAGS.dataset, \"graph\")\n",
    "adj_init, features_init = load_data(FLAGS.dataset)\n",
    "#labels = load_labels(adj_init.shape[0])\n",
    "if FLAGS.verbose:\n",
    "    print(\"- Number of nodes:\", adj_init.shape[0])\n",
    "    #print(\"- Number of communities:\", len(np.unique(labels)))\n",
    "    print(\"- Use of node features:\", FLAGS.features)\n",
    "    print(\"Done! \\n \\n \\n \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd19b933",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-28T08:53:42.842778Z",
     "start_time": "2022-10-28T08:53:21.074103Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENTS ON MODEL 1 / 10 \n",
      "\n",
      "STEP 1/3 - PREPROCESSING STEPS \n",
      "\n",
      "Masking some edges from the training graph, for link prediction\n",
      "(validation set: 1.0 % of edges - test set: 10.0 % of edges)\n",
      "Done! \n",
      "\n",
      "Preprocessing node features\n",
      "Done! \n",
      "\n",
      "Running the Louvain algorithm for community detection\n",
      "as a preprocessing step for the encoder\n",
      "Done! Louvain has found 6 communities \n",
      "\n",
      "Setting up the model and the optimizer\n",
      "Done! \n",
      "\n",
      "Preprocessing on message passing matrices\n",
      "Done! \n",
      "\n",
      "Initializing TF session\n",
      "Done! \n",
      "\n",
      "STEP 2/3 - MODEL TRAINING \n",
      "\n",
      "Starting training\n",
      "Iteration: 0001 Loss: 1.69279 Time: 0.10698\n",
      "Iteration: 0002 Loss: 1.63475 Time: 0.00798\n",
      "Iteration: 0003 Loss: 1.64568 Time: 0.00701\n",
      "Iteration: 0004 Loss: 1.68335 Time: 0.00700\n",
      "Iteration: 0005 Loss: 1.64407 Time: 0.00602\n",
      "Iteration: 0006 Loss: 1.65096 Time: 0.00598\n",
      "Iteration: 0007 Loss: 1.59208 Time: 0.00602\n",
      "Iteration: 0008 Loss: 1.54860 Time: 0.00596\n",
      "Iteration: 0009 Loss: 1.55702 Time: 0.00764\n",
      "Iteration: 0010 Loss: 1.49890 Time: 0.00678\n",
      "Iteration: 0011 Loss: 1.45423 Time: 0.00609\n",
      "Iteration: 0012 Loss: 1.44946 Time: 0.00518\n",
      "Iteration: 0013 Loss: 1.45003 Time: 0.00594\n",
      "Iteration: 0014 Loss: 1.46754 Time: 0.00638\n",
      "Iteration: 0015 Loss: 1.47294 Time: 0.00542\n",
      "Iteration: 0016 Loss: 1.38637 Time: 0.00567\n",
      "Iteration: 0017 Loss: 1.36151 Time: 0.00587\n",
      "Iteration: 0018 Loss: 1.39312 Time: 0.00640\n",
      "Iteration: 0019 Loss: 1.34768 Time: 0.00566\n",
      "Iteration: 0020 Loss: 1.35990 Time: 0.00558\n",
      "Iteration: 0021 Loss: 1.33762 Time: 0.00619\n",
      "Iteration: 0022 Loss: 1.32539 Time: 0.00543\n",
      "Iteration: 0023 Loss: 1.30280 Time: 0.00594\n",
      "Iteration: 0024 Loss: 1.25310 Time: 0.00570\n",
      "Iteration: 0025 Loss: 1.24282 Time: 0.00592\n",
      "Iteration: 0026 Loss: 1.25504 Time: 0.00500\n",
      "Iteration: 0027 Loss: 1.24843 Time: 0.00596\n",
      "Iteration: 0028 Loss: 1.22232 Time: 0.00533\n",
      "Iteration: 0029 Loss: 1.23735 Time: 0.00497\n",
      "Iteration: 0030 Loss: 1.21302 Time: 0.00618\n",
      "Iteration: 0031 Loss: 1.18623 Time: 0.00633\n",
      "Iteration: 0032 Loss: 1.19127 Time: 0.00702\n",
      "Iteration: 0033 Loss: 1.17279 Time: 0.00602\n",
      "Iteration: 0034 Loss: 1.15660 Time: 0.00597\n",
      "Iteration: 0035 Loss: 1.12523 Time: 0.00693\n",
      "Iteration: 0036 Loss: 1.14166 Time: 0.00698\n",
      "Iteration: 0037 Loss: 1.16698 Time: 0.00699\n",
      "Iteration: 0038 Loss: 1.10216 Time: 0.00641\n",
      "Iteration: 0039 Loss: 1.10598 Time: 0.00602\n",
      "Iteration: 0040 Loss: 1.09014 Time: 0.00560\n",
      "Iteration: 0041 Loss: 1.12377 Time: 0.00597\n",
      "Iteration: 0042 Loss: 1.08566 Time: 0.00700\n",
      "Iteration: 0043 Loss: 1.07448 Time: 0.00553\n",
      "Iteration: 0044 Loss: 1.07661 Time: 0.00652\n",
      "Iteration: 0045 Loss: 1.09563 Time: 0.00694\n",
      "Iteration: 0046 Loss: 1.06859 Time: 0.00682\n",
      "Iteration: 0047 Loss: 1.04175 Time: 0.00650\n",
      "Iteration: 0048 Loss: 1.02228 Time: 0.00550\n",
      "Iteration: 0049 Loss: 1.04692 Time: 0.00607\n",
      "Iteration: 0050 Loss: 1.04941 Time: 0.00604\n",
      "Iteration: 0051 Loss: 1.01292 Time: 0.00543\n",
      "Iteration: 0052 Loss: 1.00173 Time: 0.00592\n",
      "Iteration: 0053 Loss: 0.97039 Time: 0.00595\n",
      "Iteration: 0054 Loss: 1.03213 Time: 0.00533\n",
      "Iteration: 0055 Loss: 0.98424 Time: 0.00670\n",
      "Iteration: 0056 Loss: 0.99920 Time: 0.00597\n",
      "Iteration: 0057 Loss: 0.99112 Time: 0.00509\n",
      "Iteration: 0058 Loss: 0.97804 Time: 0.00697\n",
      "Iteration: 0059 Loss: 0.93995 Time: 0.00599\n",
      "Iteration: 0060 Loss: 0.96470 Time: 0.00503\n",
      "Iteration: 0061 Loss: 0.96389 Time: 0.00735\n",
      "Iteration: 0062 Loss: 0.95307 Time: 0.00505\n",
      "Iteration: 0063 Loss: 0.95142 Time: 0.00497\n",
      "Iteration: 0064 Loss: 0.94421 Time: 0.00598\n",
      "Iteration: 0065 Loss: 0.95288 Time: 0.00654\n",
      "Iteration: 0066 Loss: 0.91589 Time: 0.00632\n",
      "Iteration: 0067 Loss: 0.94819 Time: 0.00626\n",
      "Iteration: 0068 Loss: 0.94691 Time: 0.00605\n",
      "Iteration: 0069 Loss: 0.92139 Time: 0.00592\n",
      "Iteration: 0070 Loss: 0.92210 Time: 0.00598\n",
      "Iteration: 0071 Loss: 0.92608 Time: 0.00703\n",
      "Iteration: 0072 Loss: 0.91098 Time: 0.00701\n",
      "Iteration: 0073 Loss: 0.90505 Time: 0.00699\n",
      "Iteration: 0074 Loss: 0.90666 Time: 0.00801\n",
      "Iteration: 0075 Loss: 0.89701 Time: 0.00747\n",
      "Iteration: 0076 Loss: 0.90478 Time: 0.00540\n",
      "Iteration: 0077 Loss: 0.89035 Time: 0.00468\n",
      "Iteration: 0078 Loss: 0.87881 Time: 0.00500\n",
      "Iteration: 0079 Loss: 0.90702 Time: 0.00595\n",
      "Iteration: 0080 Loss: 0.86666 Time: 0.00518\n",
      "Iteration: 0081 Loss: 0.87793 Time: 0.00609\n",
      "Iteration: 0082 Loss: 0.84987 Time: 0.00636\n",
      "Iteration: 0083 Loss: 0.86742 Time: 0.00497\n",
      "Iteration: 0084 Loss: 0.86800 Time: 0.00493\n",
      "Iteration: 0085 Loss: 0.84470 Time: 0.00497\n",
      "Iteration: 0086 Loss: 0.85036 Time: 0.00601\n",
      "Iteration: 0087 Loss: 0.84564 Time: 0.00631\n",
      "Iteration: 0088 Loss: 0.83148 Time: 0.00565\n",
      "Iteration: 0089 Loss: 0.85520 Time: 0.00636\n",
      "Iteration: 0090 Loss: 0.81465 Time: 0.00605\n",
      "Iteration: 0091 Loss: 0.84395 Time: 0.00596\n",
      "Iteration: 0092 Loss: 0.82329 Time: 0.00615\n",
      "Iteration: 0093 Loss: 0.85060 Time: 0.00601\n",
      "Iteration: 0094 Loss: 0.82508 Time: 0.00560\n",
      "Iteration: 0095 Loss: 0.81919 Time: 0.00599\n",
      "Iteration: 0096 Loss: 0.82734 Time: 0.00559\n",
      "Iteration: 0097 Loss: 0.82176 Time: 0.00556\n",
      "Iteration: 0098 Loss: 0.81881 Time: 0.00785\n",
      "Iteration: 0099 Loss: 0.81234 Time: 0.00593\n",
      "Iteration: 0100 Loss: 0.81452 Time: 0.00547\n",
      "Iteration: 0101 Loss: 0.79074 Time: 0.00549\n",
      "Iteration: 0102 Loss: 0.79742 Time: 0.00637\n",
      "Iteration: 0103 Loss: 0.79926 Time: 0.00552\n",
      "Iteration: 0104 Loss: 0.80692 Time: 0.00501\n",
      "Iteration: 0105 Loss: 0.79536 Time: 0.00561\n",
      "Iteration: 0106 Loss: 0.79902 Time: 0.00633\n",
      "Iteration: 0107 Loss: 0.78021 Time: 0.00505\n",
      "Iteration: 0108 Loss: 0.76070 Time: 0.00510\n",
      "Iteration: 0109 Loss: 0.78381 Time: 0.00495\n",
      "Iteration: 0110 Loss: 0.76438 Time: 0.00502\n",
      "Iteration: 0111 Loss: 0.78458 Time: 0.00594\n",
      "Iteration: 0112 Loss: 0.77551 Time: 0.00606\n",
      "Iteration: 0113 Loss: 0.76344 Time: 0.00596\n",
      "Iteration: 0114 Loss: 0.77585 Time: 0.00599\n",
      "Iteration: 0115 Loss: 0.76066 Time: 0.00599\n",
      "Iteration: 0116 Loss: 0.76834 Time: 0.00552\n",
      "Iteration: 0117 Loss: 0.74777 Time: 0.00652\n",
      "Iteration: 0118 Loss: 0.75996 Time: 0.00673\n",
      "Iteration: 0119 Loss: 0.75787 Time: 0.00395\n",
      "Iteration: 0120 Loss: 0.76698 Time: 0.00596\n",
      "Iteration: 0121 Loss: 0.74997 Time: 0.00599\n",
      "Iteration: 0122 Loss: 0.75382 Time: 0.00604\n",
      "Iteration: 0123 Loss: 0.73541 Time: 0.00543\n",
      "Iteration: 0124 Loss: 0.73609 Time: 0.00605\n",
      "Iteration: 0125 Loss: 0.74389 Time: 0.00595\n",
      "Iteration: 0126 Loss: 0.74983 Time: 0.00605\n",
      "Iteration: 0127 Loss: 0.72833 Time: 0.00640\n",
      "Iteration: 0128 Loss: 0.74862 Time: 0.00593\n",
      "Iteration: 0129 Loss: 0.74236 Time: 0.00614\n",
      "Iteration: 0130 Loss: 0.72892 Time: 0.00548\n",
      "Iteration: 0131 Loss: 0.72943 Time: 0.00603\n",
      "Iteration: 0132 Loss: 0.73610 Time: 0.00502\n",
      "Iteration: 0133 Loss: 0.73085 Time: 0.00603\n",
      "Iteration: 0134 Loss: 0.72375 Time: 0.00620\n",
      "Iteration: 0135 Loss: 0.72547 Time: 0.00672\n",
      "Iteration: 0136 Loss: 0.72395 Time: 0.00632\n",
      "Iteration: 0137 Loss: 0.72455 Time: 0.00497\n",
      "Iteration: 0138 Loss: 0.73448 Time: 0.00605\n",
      "Iteration: 0139 Loss: 0.72948 Time: 0.00596\n",
      "Iteration: 0140 Loss: 0.71265 Time: 0.00602\n",
      "Iteration: 0141 Loss: 0.71737 Time: 0.00597\n",
      "Iteration: 0142 Loss: 0.70883 Time: 0.00533\n",
      "Iteration: 0143 Loss: 0.71260 Time: 0.00671\n",
      "Iteration: 0144 Loss: 0.70063 Time: 0.00604\n",
      "Iteration: 0145 Loss: 0.71472 Time: 0.00623\n",
      "Iteration: 0146 Loss: 0.71206 Time: 0.00505\n",
      "Iteration: 0147 Loss: 0.71532 Time: 0.00790\n",
      "Iteration: 0148 Loss: 0.69918 Time: 0.00568\n",
      "Iteration: 0149 Loss: 0.70974 Time: 0.00500\n",
      "Iteration: 0150 Loss: 0.69784 Time: 0.00598\n",
      "Iteration: 0151 Loss: 0.71238 Time: 0.00495\n",
      "Iteration: 0152 Loss: 0.69046 Time: 0.00667\n",
      "Iteration: 0153 Loss: 0.69087 Time: 0.00639\n",
      "Iteration: 0154 Loss: 0.71570 Time: 0.00595\n",
      "Iteration: 0155 Loss: 0.71220 Time: 0.00605\n",
      "Iteration: 0156 Loss: 0.68703 Time: 0.00698\n",
      "Iteration: 0157 Loss: 0.69673 Time: 0.00501\n",
      "Iteration: 0158 Loss: 0.68718 Time: 0.00595\n",
      "Iteration: 0159 Loss: 0.69550 Time: 0.00548\n",
      "Iteration: 0160 Loss: 0.68414 Time: 0.00602\n",
      "Iteration: 0161 Loss: 0.69633 Time: 0.00536\n",
      "Iteration: 0162 Loss: 0.69425 Time: 0.00567\n",
      "Iteration: 0163 Loss: 0.68178 Time: 0.00666\n",
      "Iteration: 0164 Loss: 0.68512 Time: 0.00546\n",
      "Iteration: 0165 Loss: 0.68167 Time: 0.00655\n",
      "Iteration: 0166 Loss: 0.68714 Time: 0.00560\n",
      "Iteration: 0167 Loss: 0.67816 Time: 0.00498\n",
      "Iteration: 0168 Loss: 0.69171 Time: 0.00599\n",
      "Iteration: 0169 Loss: 0.67388 Time: 0.00697\n",
      "Iteration: 0170 Loss: 0.67259 Time: 0.00604\n",
      "Iteration: 0171 Loss: 0.66324 Time: 0.00613\n",
      "Iteration: 0172 Loss: 0.68197 Time: 0.00582\n",
      "Iteration: 0173 Loss: 0.66672 Time: 0.00602\n",
      "Iteration: 0174 Loss: 0.66917 Time: 0.00602\n",
      "Iteration: 0175 Loss: 0.67872 Time: 0.00522\n",
      "Iteration: 0176 Loss: 0.67449 Time: 0.00584\n",
      "Iteration: 0177 Loss: 0.67148 Time: 0.00505\n",
      "Iteration: 0178 Loss: 0.66188 Time: 0.00595\n",
      "Iteration: 0179 Loss: 0.66150 Time: 0.00695\n",
      "Iteration: 0180 Loss: 0.66877 Time: 0.00608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0181 Loss: 0.66212 Time: 0.00592\n",
      "Iteration: 0182 Loss: 0.67430 Time: 0.00660\n",
      "Iteration: 0183 Loss: 0.67469 Time: 0.00548\n",
      "Iteration: 0184 Loss: 0.67004 Time: 0.00538\n",
      "Iteration: 0185 Loss: 0.67636 Time: 0.00661\n",
      "Iteration: 0186 Loss: 0.66593 Time: 0.00505\n",
      "Iteration: 0187 Loss: 0.65492 Time: 0.00560\n",
      "Iteration: 0188 Loss: 0.65893 Time: 0.00609\n",
      "Iteration: 0189 Loss: 0.65589 Time: 0.00571\n",
      "Iteration: 0190 Loss: 0.65625 Time: 0.00634\n",
      "Iteration: 0191 Loss: 0.66073 Time: 0.00500\n",
      "Iteration: 0192 Loss: 0.66616 Time: 0.00604\n",
      "Iteration: 0193 Loss: 0.65335 Time: 0.00497\n",
      "Iteration: 0194 Loss: 0.65130 Time: 0.00592\n",
      "Iteration: 0195 Loss: 0.65868 Time: 0.00596\n",
      "Iteration: 0196 Loss: 0.66045 Time: 0.00542\n",
      "Iteration: 0197 Loss: 0.65039 Time: 0.00608\n",
      "Iteration: 0198 Loss: 0.64519 Time: 0.00601\n",
      "Iteration: 0199 Loss: 0.65418 Time: 0.00388\n",
      "Iteration: 0200 Loss: 0.64544 Time: 0.00696\n",
      "Done! \n",
      "\n",
      "STEP 3/3 - MODEL EVALUATION \n",
      "\n",
      "Computing the final embedding vectors, for evaluation\n",
      "Done! \n",
      "\n",
      "Testing: link prediction\n",
      "Done! \n",
      "\n",
      "EXPERIMENTS ON MODEL 2 / 10 \n",
      "\n",
      "STEP 1/3 - PREPROCESSING STEPS \n",
      "\n",
      "Masking some edges from the training graph, for link prediction\n",
      "(validation set: 1.0 % of edges - test set: 10.0 % of edges)\n",
      "Done! \n",
      "\n",
      "Preprocessing node features\n",
      "Done! \n",
      "\n",
      "Running the Louvain algorithm for community detection\n",
      "as a preprocessing step for the encoder\n",
      "Done! Louvain has found 8 communities \n",
      "\n",
      "Setting up the model and the optimizer\n",
      "Done! \n",
      "\n",
      "Preprocessing on message passing matrices\n",
      "Done! \n",
      "\n",
      "Initializing TF session\n",
      "Done! \n",
      "\n",
      "STEP 2/3 - MODEL TRAINING \n",
      "\n",
      "Starting training\n",
      "Iteration: 0001 Loss: 1.77983 Time: 0.09210\n",
      "Iteration: 0002 Loss: 1.72709 Time: 0.00584\n",
      "Iteration: 0003 Loss: 1.65251 Time: 0.00687\n",
      "Iteration: 0004 Loss: 1.62025 Time: 0.00651\n",
      "Iteration: 0005 Loss: 1.65478 Time: 0.00744\n",
      "Iteration: 0006 Loss: 1.63083 Time: 0.00599\n",
      "Iteration: 0007 Loss: 1.55734 Time: 0.00600\n",
      "Iteration: 0008 Loss: 1.58378 Time: 0.00800\n",
      "Iteration: 0009 Loss: 1.54551 Time: 0.00603\n",
      "Iteration: 0010 Loss: 1.56188 Time: 0.00501\n",
      "Iteration: 0011 Loss: 1.48057 Time: 0.00796\n",
      "Iteration: 0012 Loss: 1.51607 Time: 0.00699\n",
      "Iteration: 0013 Loss: 1.43694 Time: 0.00669\n",
      "Iteration: 0014 Loss: 1.41501 Time: 0.00501\n",
      "Iteration: 0015 Loss: 1.41910 Time: 0.00600\n",
      "Iteration: 0016 Loss: 1.37535 Time: 0.00599\n",
      "Iteration: 0017 Loss: 1.40116 Time: 0.00497\n",
      "Iteration: 0018 Loss: 1.36210 Time: 0.00602\n",
      "Iteration: 0019 Loss: 1.34694 Time: 0.00753\n",
      "Iteration: 0020 Loss: 1.36183 Time: 0.00559\n",
      "Iteration: 0021 Loss: 1.35015 Time: 0.00540\n",
      "Iteration: 0022 Loss: 1.31169 Time: 0.00594\n",
      "Iteration: 0023 Loss: 1.30084 Time: 0.00601\n",
      "Iteration: 0024 Loss: 1.28713 Time: 0.00562\n",
      "Iteration: 0025 Loss: 1.27091 Time: 0.00508\n",
      "Iteration: 0026 Loss: 1.21078 Time: 0.00600\n",
      "Iteration: 0027 Loss: 1.18924 Time: 0.00560\n",
      "Iteration: 0028 Loss: 1.20481 Time: 0.00537\n",
      "Iteration: 0029 Loss: 1.20208 Time: 0.00664\n",
      "Iteration: 0030 Loss: 1.20490 Time: 0.00601\n",
      "Iteration: 0031 Loss: 1.17322 Time: 0.00602\n",
      "Iteration: 0032 Loss: 1.18338 Time: 0.00858\n",
      "Iteration: 0033 Loss: 1.22096 Time: 0.00641\n",
      "Iteration: 0034 Loss: 1.17553 Time: 0.00542\n",
      "Iteration: 0035 Loss: 1.12790 Time: 0.00561\n",
      "Iteration: 0036 Loss: 1.12861 Time: 0.00593\n",
      "Iteration: 0037 Loss: 1.15074 Time: 0.00573\n",
      "Iteration: 0038 Loss: 1.11579 Time: 0.00608\n",
      "Iteration: 0039 Loss: 1.08906 Time: 0.00595\n",
      "Iteration: 0040 Loss: 1.09170 Time: 0.00751\n",
      "Iteration: 0041 Loss: 1.10577 Time: 0.00551\n",
      "Iteration: 0042 Loss: 1.11485 Time: 0.00794\n",
      "Iteration: 0043 Loss: 1.09264 Time: 0.00606\n",
      "Iteration: 0044 Loss: 1.06317 Time: 0.00500\n",
      "Iteration: 0045 Loss: 1.03918 Time: 0.00698\n",
      "Iteration: 0046 Loss: 1.07676 Time: 0.00524\n",
      "Iteration: 0047 Loss: 1.06354 Time: 0.00539\n",
      "Iteration: 0048 Loss: 1.07621 Time: 0.00556\n",
      "Iteration: 0049 Loss: 1.01895 Time: 0.00800\n",
      "Iteration: 0050 Loss: 1.00431 Time: 0.00700\n",
      "Iteration: 0051 Loss: 1.03721 Time: 0.00504\n",
      "Iteration: 0052 Loss: 1.02345 Time: 0.00600\n",
      "Iteration: 0053 Loss: 1.00625 Time: 0.00533\n",
      "Iteration: 0054 Loss: 1.00260 Time: 0.00567\n",
      "Iteration: 0055 Loss: 1.01795 Time: 0.00507\n",
      "Iteration: 0056 Loss: 0.99188 Time: 0.00599\n",
      "Iteration: 0057 Loss: 0.98196 Time: 0.00600\n",
      "Iteration: 0058 Loss: 0.97859 Time: 0.00498\n",
      "Iteration: 0059 Loss: 0.96116 Time: 0.00533\n",
      "Iteration: 0060 Loss: 0.98055 Time: 0.00671\n",
      "Iteration: 0061 Loss: 0.95892 Time: 0.00633\n",
      "Iteration: 0062 Loss: 0.92589 Time: 0.00496\n",
      "Iteration: 0063 Loss: 0.94054 Time: 0.00600\n",
      "Iteration: 0064 Loss: 0.97302 Time: 0.00620\n",
      "Iteration: 0065 Loss: 0.94959 Time: 0.00628\n",
      "Iteration: 0066 Loss: 0.92457 Time: 0.00561\n",
      "Iteration: 0067 Loss: 0.93424 Time: 0.00605\n",
      "Iteration: 0068 Loss: 0.93577 Time: 0.00691\n",
      "Iteration: 0069 Loss: 0.92448 Time: 0.00704\n",
      "Iteration: 0070 Loss: 0.94043 Time: 0.00635\n",
      "Iteration: 0071 Loss: 0.92382 Time: 0.00698\n",
      "Iteration: 0072 Loss: 0.90894 Time: 0.00667\n",
      "Iteration: 0073 Loss: 0.93300 Time: 0.00688\n",
      "Iteration: 0074 Loss: 0.91188 Time: 0.00699\n",
      "Iteration: 0075 Loss: 0.89440 Time: 0.00600\n",
      "Iteration: 0076 Loss: 0.87977 Time: 0.00653\n",
      "Iteration: 0077 Loss: 0.88555 Time: 0.00512\n",
      "Iteration: 0078 Loss: 0.88374 Time: 0.00691\n",
      "Iteration: 0079 Loss: 0.89607 Time: 0.00694\n",
      "Iteration: 0080 Loss: 0.85967 Time: 0.00687\n",
      "Iteration: 0081 Loss: 0.89288 Time: 0.00599\n",
      "Iteration: 0082 Loss: 0.86975 Time: 0.00621\n",
      "Iteration: 0083 Loss: 0.86497 Time: 0.00582\n",
      "Iteration: 0084 Loss: 0.85786 Time: 0.00752\n",
      "Iteration: 0085 Loss: 0.84491 Time: 0.00501\n",
      "Iteration: 0086 Loss: 0.83596 Time: 0.00594\n",
      "Iteration: 0087 Loss: 0.84436 Time: 0.00662\n",
      "Iteration: 0088 Loss: 0.84044 Time: 0.00528\n",
      "Iteration: 0089 Loss: 0.84264 Time: 0.00672\n",
      "Iteration: 0090 Loss: 0.83571 Time: 0.00638\n",
      "Iteration: 0091 Loss: 0.83207 Time: 0.00596\n",
      "Iteration: 0092 Loss: 0.83227 Time: 0.00500\n",
      "Iteration: 0093 Loss: 0.83575 Time: 0.00653\n",
      "Iteration: 0094 Loss: 0.83014 Time: 0.00792\n",
      "Iteration: 0095 Loss: 0.82297 Time: 0.00917\n",
      "Iteration: 0096 Loss: 0.80915 Time: 0.00652\n",
      "Iteration: 0097 Loss: 0.81082 Time: 0.00553\n",
      "Iteration: 0098 Loss: 0.80472 Time: 0.00601\n",
      "Iteration: 0099 Loss: 0.80800 Time: 0.00598\n",
      "Iteration: 0100 Loss: 0.80452 Time: 0.00697\n",
      "Iteration: 0101 Loss: 0.80540 Time: 0.00660\n",
      "Iteration: 0102 Loss: 0.80264 Time: 0.00645\n",
      "Iteration: 0103 Loss: 0.79832 Time: 0.00735\n",
      "Iteration: 0104 Loss: 0.78495 Time: 0.00665\n",
      "Iteration: 0105 Loss: 0.80817 Time: 0.00796\n",
      "Iteration: 0106 Loss: 0.79662 Time: 0.00704\n",
      "Iteration: 0107 Loss: 0.78712 Time: 0.00695\n",
      "Iteration: 0108 Loss: 0.78516 Time: 0.00595\n",
      "Iteration: 0109 Loss: 0.76853 Time: 0.00698\n",
      "Iteration: 0110 Loss: 0.76935 Time: 0.00606\n",
      "Iteration: 0111 Loss: 0.78103 Time: 0.00604\n",
      "Iteration: 0112 Loss: 0.76034 Time: 0.00599\n",
      "Iteration: 0113 Loss: 0.75674 Time: 0.00693\n",
      "Iteration: 0114 Loss: 0.75255 Time: 0.00511\n",
      "Iteration: 0115 Loss: 0.76066 Time: 0.00604\n",
      "Iteration: 0116 Loss: 0.75810 Time: 0.00650\n",
      "Iteration: 0117 Loss: 0.74818 Time: 0.00546\n",
      "Iteration: 0118 Loss: 0.75016 Time: 0.00603\n",
      "Iteration: 0119 Loss: 0.76547 Time: 0.00602\n",
      "Iteration: 0120 Loss: 0.74271 Time: 0.00524\n",
      "Iteration: 0121 Loss: 0.74878 Time: 0.00622\n",
      "Iteration: 0122 Loss: 0.74966 Time: 0.00553\n",
      "Iteration: 0123 Loss: 0.74234 Time: 0.00503\n",
      "Iteration: 0124 Loss: 0.73994 Time: 0.00699\n",
      "Iteration: 0125 Loss: 0.73154 Time: 0.00661\n",
      "Iteration: 0126 Loss: 0.73434 Time: 0.00596\n",
      "Iteration: 0127 Loss: 0.73377 Time: 0.00749\n",
      "Iteration: 0128 Loss: 0.72723 Time: 0.00576\n",
      "Iteration: 0129 Loss: 0.72723 Time: 0.00778\n",
      "Iteration: 0130 Loss: 0.72351 Time: 0.00636\n",
      "Iteration: 0131 Loss: 0.72859 Time: 0.00524\n",
      "Iteration: 0132 Loss: 0.72306 Time: 0.00699\n",
      "Iteration: 0133 Loss: 0.72170 Time: 0.00598\n",
      "Iteration: 0134 Loss: 0.72565 Time: 0.00727\n",
      "Iteration: 0135 Loss: 0.72595 Time: 0.00497\n",
      "Iteration: 0136 Loss: 0.72528 Time: 0.00599\n",
      "Iteration: 0137 Loss: 0.71680 Time: 0.00517\n",
      "Iteration: 0138 Loss: 0.72259 Time: 0.00621\n",
      "Iteration: 0139 Loss: 0.71990 Time: 0.00662\n",
      "Iteration: 0140 Loss: 0.71794 Time: 0.00544\n",
      "Iteration: 0141 Loss: 0.72573 Time: 0.00491\n",
      "Iteration: 0142 Loss: 0.71178 Time: 0.00647\n",
      "Iteration: 0143 Loss: 0.71129 Time: 0.00497\n",
      "Iteration: 0144 Loss: 0.70527 Time: 0.00496\n",
      "Iteration: 0145 Loss: 0.70434 Time: 0.00636\n",
      "Iteration: 0146 Loss: 0.70532 Time: 0.00570\n",
      "Iteration: 0147 Loss: 0.71035 Time: 0.00596\n",
      "Iteration: 0148 Loss: 0.70558 Time: 0.00533\n",
      "Iteration: 0149 Loss: 0.69684 Time: 0.00530\n",
      "Iteration: 0150 Loss: 0.69395 Time: 0.00566\n",
      "Iteration: 0151 Loss: 0.68841 Time: 0.00604\n",
      "Iteration: 0152 Loss: 0.69021 Time: 0.00556\n",
      "Iteration: 0153 Loss: 0.67777 Time: 0.00501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0154 Loss: 0.69189 Time: 0.00655\n",
      "Iteration: 0155 Loss: 0.69014 Time: 0.00594\n",
      "Iteration: 0156 Loss: 0.69831 Time: 0.00704\n",
      "Iteration: 0157 Loss: 0.68835 Time: 0.00649\n",
      "Iteration: 0158 Loss: 0.69011 Time: 0.00496\n",
      "Iteration: 0159 Loss: 0.69778 Time: 0.00501\n",
      "Iteration: 0160 Loss: 0.68547 Time: 0.00561\n",
      "Iteration: 0161 Loss: 0.67819 Time: 0.00594\n",
      "Iteration: 0162 Loss: 0.69174 Time: 0.00531\n",
      "Iteration: 0163 Loss: 0.68517 Time: 0.00571\n",
      "Iteration: 0164 Loss: 0.68628 Time: 0.00598\n",
      "Iteration: 0165 Loss: 0.66981 Time: 0.00599\n",
      "Iteration: 0166 Loss: 0.67856 Time: 0.00546\n",
      "Iteration: 0167 Loss: 0.67616 Time: 0.00554\n",
      "Iteration: 0168 Loss: 0.67418 Time: 0.00562\n",
      "Iteration: 0169 Loss: 0.68462 Time: 0.00538\n",
      "Iteration: 0170 Loss: 0.67660 Time: 0.00581\n",
      "Iteration: 0171 Loss: 0.66608 Time: 0.00683\n",
      "Iteration: 0172 Loss: 0.66874 Time: 0.00497\n",
      "Iteration: 0173 Loss: 0.67101 Time: 0.00630\n",
      "Iteration: 0174 Loss: 0.67149 Time: 0.00535\n",
      "Iteration: 0175 Loss: 0.66881 Time: 0.00493\n",
      "Iteration: 0176 Loss: 0.67147 Time: 0.00604\n",
      "Iteration: 0177 Loss: 0.66387 Time: 0.00546\n",
      "Iteration: 0178 Loss: 0.67659 Time: 0.00495\n",
      "Iteration: 0179 Loss: 0.66586 Time: 0.00404\n",
      "Iteration: 0180 Loss: 0.66797 Time: 0.00629\n",
      "Iteration: 0181 Loss: 0.65933 Time: 0.00568\n",
      "Iteration: 0182 Loss: 0.66495 Time: 0.00503\n",
      "Iteration: 0183 Loss: 0.66926 Time: 0.00541\n",
      "Iteration: 0184 Loss: 0.64896 Time: 0.00500\n",
      "Iteration: 0185 Loss: 0.66684 Time: 0.00650\n",
      "Iteration: 0186 Loss: 0.65930 Time: 0.00592\n",
      "Iteration: 0187 Loss: 0.66458 Time: 0.00623\n",
      "Iteration: 0188 Loss: 0.67152 Time: 0.00577\n",
      "Iteration: 0189 Loss: 0.66132 Time: 0.00500\n",
      "Iteration: 0190 Loss: 0.66746 Time: 0.00634\n",
      "Iteration: 0191 Loss: 0.65722 Time: 0.00482\n",
      "Iteration: 0192 Loss: 0.65148 Time: 0.00506\n",
      "Iteration: 0193 Loss: 0.65365 Time: 0.00598\n",
      "Iteration: 0194 Loss: 0.64810 Time: 0.00495\n",
      "Iteration: 0195 Loss: 0.64338 Time: 0.00630\n",
      "Iteration: 0196 Loss: 0.65944 Time: 0.00521\n",
      "Iteration: 0197 Loss: 0.65227 Time: 0.00581\n",
      "Iteration: 0198 Loss: 0.64875 Time: 0.00700\n",
      "Iteration: 0199 Loss: 0.65114 Time: 0.00544\n",
      "Iteration: 0200 Loss: 0.63946 Time: 0.00554\n",
      "Done! \n",
      "\n",
      "STEP 3/3 - MODEL EVALUATION \n",
      "\n",
      "Computing the final embedding vectors, for evaluation\n",
      "Done! \n",
      "\n",
      "Testing: link prediction\n",
      "Done! \n",
      "\n",
      "EXPERIMENTS ON MODEL 3 / 10 \n",
      "\n",
      "STEP 1/3 - PREPROCESSING STEPS \n",
      "\n",
      "Masking some edges from the training graph, for link prediction\n",
      "(validation set: 1.0 % of edges - test set: 10.0 % of edges)\n",
      "Done! \n",
      "\n",
      "Preprocessing node features\n",
      "Done! \n",
      "\n",
      "Running the Louvain algorithm for community detection\n",
      "as a preprocessing step for the encoder\n",
      "Done! Louvain has found 9 communities \n",
      "\n",
      "Setting up the model and the optimizer\n",
      "Done! \n",
      "\n",
      "Preprocessing on message passing matrices\n",
      "Done! \n",
      "\n",
      "Initializing TF session\n",
      "Done! \n",
      "\n",
      "STEP 2/3 - MODEL TRAINING \n",
      "\n",
      "Starting training\n",
      "Iteration: 0001 Loss: 1.76861 Time: 0.10609\n",
      "Iteration: 0002 Loss: 1.67364 Time: 0.00660\n",
      "Iteration: 0003 Loss: 1.64133 Time: 0.00540\n",
      "Iteration: 0004 Loss: 1.63013 Time: 0.00595\n",
      "Iteration: 0005 Loss: 1.63069 Time: 0.00652\n",
      "Iteration: 0006 Loss: 1.55602 Time: 0.00566\n",
      "Iteration: 0007 Loss: 1.54930 Time: 0.00699\n",
      "Iteration: 0008 Loss: 1.55332 Time: 0.00501\n",
      "Iteration: 0009 Loss: 1.47600 Time: 0.00597\n",
      "Iteration: 0010 Loss: 1.48279 Time: 0.00641\n",
      "Iteration: 0011 Loss: 1.54490 Time: 0.00590\n",
      "Iteration: 0012 Loss: 1.40372 Time: 0.00739\n",
      "Iteration: 0013 Loss: 1.44882 Time: 0.00603\n",
      "Iteration: 0014 Loss: 1.45393 Time: 0.00594\n",
      "Iteration: 0015 Loss: 1.43095 Time: 0.00602\n",
      "Iteration: 0016 Loss: 1.33929 Time: 0.00586\n",
      "Iteration: 0017 Loss: 1.39309 Time: 0.00630\n",
      "Iteration: 0018 Loss: 1.36365 Time: 0.00547\n",
      "Iteration: 0019 Loss: 1.38622 Time: 0.00552\n",
      "Iteration: 0020 Loss: 1.34142 Time: 0.00596\n",
      "Iteration: 0021 Loss: 1.31764 Time: 0.00496\n",
      "Iteration: 0022 Loss: 1.28452 Time: 0.00488\n",
      "Iteration: 0023 Loss: 1.29913 Time: 0.00595\n",
      "Iteration: 0024 Loss: 1.26515 Time: 0.00535\n",
      "Iteration: 0025 Loss: 1.28098 Time: 0.00650\n",
      "Iteration: 0026 Loss: 1.27449 Time: 0.00596\n",
      "Iteration: 0027 Loss: 1.22677 Time: 0.00601\n",
      "Iteration: 0028 Loss: 1.27349 Time: 0.00500\n",
      "Iteration: 0029 Loss: 1.21300 Time: 0.00598\n",
      "Iteration: 0030 Loss: 1.22494 Time: 0.00594\n",
      "Iteration: 0031 Loss: 1.21282 Time: 0.00637\n",
      "Iteration: 0032 Loss: 1.18886 Time: 0.00763\n",
      "Iteration: 0033 Loss: 1.24756 Time: 0.00740\n",
      "Iteration: 0034 Loss: 1.16470 Time: 0.00566\n",
      "Iteration: 0035 Loss: 1.17893 Time: 0.00597\n",
      "Iteration: 0036 Loss: 1.17383 Time: 0.00603\n",
      "Iteration: 0037 Loss: 1.13631 Time: 0.00694\n",
      "Iteration: 0038 Loss: 1.13401 Time: 0.00604\n",
      "Iteration: 0039 Loss: 1.11586 Time: 0.00700\n",
      "Iteration: 0040 Loss: 1.10156 Time: 0.00528\n",
      "Iteration: 0041 Loss: 1.09454 Time: 0.00702\n",
      "Iteration: 0042 Loss: 1.09199 Time: 0.00560\n",
      "Iteration: 0043 Loss: 1.05087 Time: 0.00697\n",
      "Iteration: 0044 Loss: 1.04956 Time: 0.00499\n",
      "Iteration: 0045 Loss: 1.06285 Time: 0.00500\n",
      "Iteration: 0046 Loss: 1.04191 Time: 0.00600\n",
      "Iteration: 0047 Loss: 1.05494 Time: 0.00547\n",
      "Iteration: 0048 Loss: 1.05885 Time: 0.00497\n",
      "Iteration: 0049 Loss: 1.06027 Time: 0.00705\n",
      "Iteration: 0050 Loss: 1.04408 Time: 0.00597\n",
      "Iteration: 0051 Loss: 1.00915 Time: 0.00604\n",
      "Iteration: 0052 Loss: 1.02263 Time: 0.00539\n",
      "Iteration: 0053 Loss: 1.02221 Time: 0.00592\n",
      "Iteration: 0054 Loss: 1.01733 Time: 0.00744\n",
      "Iteration: 0055 Loss: 0.97905 Time: 0.00565\n",
      "Iteration: 0056 Loss: 0.99363 Time: 0.00602\n",
      "Iteration: 0057 Loss: 0.96422 Time: 0.00599\n",
      "Iteration: 0058 Loss: 0.97912 Time: 0.00619\n",
      "Iteration: 0059 Loss: 0.99458 Time: 0.00581\n",
      "Iteration: 0060 Loss: 0.97189 Time: 0.00563\n",
      "Iteration: 0061 Loss: 0.94090 Time: 0.00595\n",
      "Iteration: 0062 Loss: 0.97273 Time: 0.00823\n",
      "Iteration: 0063 Loss: 0.97362 Time: 0.00400\n",
      "Iteration: 0064 Loss: 0.97149 Time: 0.00541\n",
      "Iteration: 0065 Loss: 0.94448 Time: 0.00663\n",
      "Iteration: 0066 Loss: 0.93844 Time: 0.00596\n",
      "Iteration: 0067 Loss: 0.92236 Time: 0.00701\n",
      "Iteration: 0068 Loss: 0.91948 Time: 0.00599\n",
      "Iteration: 0069 Loss: 0.92211 Time: 0.00601\n",
      "Iteration: 0070 Loss: 0.92930 Time: 0.00708\n",
      "Iteration: 0071 Loss: 0.94320 Time: 0.00647\n",
      "Iteration: 0072 Loss: 0.94233 Time: 0.00645\n",
      "Iteration: 0073 Loss: 0.91214 Time: 0.00799\n",
      "Iteration: 0074 Loss: 0.91399 Time: 0.00593\n",
      "Iteration: 0075 Loss: 0.89781 Time: 0.00577\n",
      "Iteration: 0076 Loss: 0.88533 Time: 0.00550\n",
      "Iteration: 0077 Loss: 0.88082 Time: 0.00500\n",
      "Iteration: 0078 Loss: 0.88500 Time: 0.00501\n",
      "Iteration: 0079 Loss: 0.88605 Time: 0.00700\n",
      "Iteration: 0080 Loss: 0.89148 Time: 0.00598\n",
      "Iteration: 0081 Loss: 0.88731 Time: 0.00572\n",
      "Iteration: 0082 Loss: 0.88410 Time: 0.00600\n",
      "Iteration: 0083 Loss: 0.87817 Time: 0.00700\n",
      "Iteration: 0084 Loss: 0.86242 Time: 0.00501\n",
      "Iteration: 0085 Loss: 0.85883 Time: 0.00600\n",
      "Iteration: 0086 Loss: 0.85575 Time: 0.00600\n",
      "Iteration: 0087 Loss: 0.87460 Time: 0.00509\n",
      "Iteration: 0088 Loss: 0.84931 Time: 0.00644\n",
      "Iteration: 0089 Loss: 0.85850 Time: 0.00603\n",
      "Iteration: 0090 Loss: 0.84967 Time: 0.00698\n",
      "Iteration: 0091 Loss: 0.84834 Time: 0.00645\n",
      "Iteration: 0092 Loss: 0.85241 Time: 0.00500\n",
      "Iteration: 0093 Loss: 0.83739 Time: 0.00600\n",
      "Iteration: 0094 Loss: 0.81816 Time: 0.00631\n",
      "Iteration: 0095 Loss: 0.83409 Time: 0.00568\n",
      "Iteration: 0096 Loss: 0.84661 Time: 0.00703\n",
      "Iteration: 0097 Loss: 0.82796 Time: 0.00500\n",
      "Iteration: 0098 Loss: 0.79120 Time: 0.00495\n",
      "Iteration: 0099 Loss: 0.80947 Time: 0.00622\n",
      "Iteration: 0100 Loss: 0.80406 Time: 0.00582\n",
      "Iteration: 0101 Loss: 0.80173 Time: 0.00696\n",
      "Iteration: 0102 Loss: 0.79432 Time: 0.00501\n",
      "Iteration: 0103 Loss: 0.78984 Time: 0.00501\n",
      "Iteration: 0104 Loss: 0.79822 Time: 0.00600\n",
      "Iteration: 0105 Loss: 0.79019 Time: 0.00533\n",
      "Iteration: 0106 Loss: 0.80322 Time: 0.00611\n",
      "Iteration: 0107 Loss: 0.79615 Time: 0.00655\n",
      "Iteration: 0108 Loss: 0.78480 Time: 0.00500\n",
      "Iteration: 0109 Loss: 0.78242 Time: 0.00600\n",
      "Iteration: 0110 Loss: 0.75666 Time: 0.00505\n",
      "Iteration: 0111 Loss: 0.77806 Time: 0.00500\n",
      "Iteration: 0112 Loss: 0.76306 Time: 0.00599\n",
      "Iteration: 0113 Loss: 0.77329 Time: 0.00600\n",
      "Iteration: 0114 Loss: 0.77738 Time: 0.00606\n",
      "Iteration: 0115 Loss: 0.76455 Time: 0.00611\n",
      "Iteration: 0116 Loss: 0.76220 Time: 0.00645\n",
      "Iteration: 0117 Loss: 0.75799 Time: 0.00495\n",
      "Iteration: 0118 Loss: 0.76906 Time: 0.00600\n",
      "Iteration: 0119 Loss: 0.75355 Time: 0.00623\n",
      "Iteration: 0120 Loss: 0.75627 Time: 0.00637\n",
      "Iteration: 0121 Loss: 0.75477 Time: 0.00699\n",
      "Iteration: 0122 Loss: 0.75676 Time: 0.00500\n",
      "Iteration: 0123 Loss: 0.74496 Time: 0.00596\n",
      "Iteration: 0124 Loss: 0.74451 Time: 0.00535\n",
      "Iteration: 0125 Loss: 0.73278 Time: 0.00469\n",
      "Iteration: 0126 Loss: 0.71946 Time: 0.00600\n",
      "Iteration: 0127 Loss: 0.72563 Time: 0.00500\n",
      "Iteration: 0128 Loss: 0.72622 Time: 0.00700\n",
      "Iteration: 0129 Loss: 0.72901 Time: 0.00600\n",
      "Iteration: 0130 Loss: 0.73683 Time: 0.00504\n",
      "Iteration: 0131 Loss: 0.73301 Time: 0.00596\n",
      "Iteration: 0132 Loss: 0.72682 Time: 0.00634\n",
      "Iteration: 0133 Loss: 0.72787 Time: 0.00766\n",
      "Iteration: 0134 Loss: 0.72539 Time: 0.00701\n",
      "Iteration: 0135 Loss: 0.70182 Time: 0.00600\n",
      "Iteration: 0136 Loss: 0.72169 Time: 0.00605\n",
      "Iteration: 0137 Loss: 0.71163 Time: 0.00601\n",
      "Iteration: 0138 Loss: 0.72506 Time: 0.00603\n",
      "Iteration: 0139 Loss: 0.70484 Time: 0.00700\n",
      "Iteration: 0140 Loss: 0.71222 Time: 0.00531\n",
      "Iteration: 0141 Loss: 0.71703 Time: 0.00689\n",
      "Iteration: 0142 Loss: 0.70871 Time: 0.00603\n",
      "Iteration: 0143 Loss: 0.71309 Time: 0.00541\n",
      "Iteration: 0144 Loss: 0.69980 Time: 0.00658\n",
      "Iteration: 0145 Loss: 0.71410 Time: 0.00559\n",
      "Iteration: 0146 Loss: 0.69974 Time: 0.00780\n",
      "Iteration: 0147 Loss: 0.69592 Time: 0.00598\n",
      "Iteration: 0148 Loss: 0.71126 Time: 0.00603\n",
      "Iteration: 0149 Loss: 0.71501 Time: 0.00598\n",
      "Iteration: 0150 Loss: 0.70435 Time: 0.00499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0151 Loss: 0.70418 Time: 0.00706\n",
      "Iteration: 0152 Loss: 0.68745 Time: 0.00822\n",
      "Iteration: 0153 Loss: 0.70856 Time: 0.00511\n",
      "Iteration: 0154 Loss: 0.69962 Time: 0.00689\n",
      "Iteration: 0155 Loss: 0.69579 Time: 0.00626\n",
      "Iteration: 0156 Loss: 0.69192 Time: 0.00474\n",
      "Iteration: 0157 Loss: 0.68149 Time: 0.00632\n",
      "Iteration: 0158 Loss: 0.69188 Time: 0.00578\n",
      "Iteration: 0159 Loss: 0.67933 Time: 0.00800\n",
      "Iteration: 0160 Loss: 0.68392 Time: 0.00701\n",
      "Iteration: 0161 Loss: 0.68253 Time: 0.00600\n",
      "Iteration: 0162 Loss: 0.68822 Time: 0.00800\n",
      "Iteration: 0163 Loss: 0.67862 Time: 0.00544\n",
      "Iteration: 0164 Loss: 0.67990 Time: 0.00599\n",
      "Iteration: 0165 Loss: 0.67412 Time: 0.00654\n",
      "Iteration: 0166 Loss: 0.67460 Time: 0.00517\n",
      "Iteration: 0167 Loss: 0.67885 Time: 0.00630\n",
      "Iteration: 0168 Loss: 0.67519 Time: 0.00655\n",
      "Iteration: 0169 Loss: 0.68106 Time: 0.00605\n",
      "Iteration: 0170 Loss: 0.66839 Time: 0.00588\n",
      "Iteration: 0171 Loss: 0.67125 Time: 0.00534\n",
      "Iteration: 0172 Loss: 0.68057 Time: 0.00566\n",
      "Iteration: 0173 Loss: 0.66984 Time: 0.00713\n",
      "Iteration: 0174 Loss: 0.66524 Time: 0.00491\n",
      "Iteration: 0175 Loss: 0.67228 Time: 0.00697\n",
      "Iteration: 0176 Loss: 0.67169 Time: 0.00638\n",
      "Iteration: 0177 Loss: 0.66952 Time: 0.00501\n",
      "Iteration: 0178 Loss: 0.66125 Time: 0.00637\n",
      "Iteration: 0179 Loss: 0.66275 Time: 0.00538\n",
      "Iteration: 0180 Loss: 0.67256 Time: 0.00500\n",
      "Iteration: 0181 Loss: 0.66293 Time: 0.00731\n",
      "Iteration: 0182 Loss: 0.66378 Time: 0.00525\n",
      "Iteration: 0183 Loss: 0.65448 Time: 0.00500\n",
      "Iteration: 0184 Loss: 0.66048 Time: 0.00714\n",
      "Iteration: 0185 Loss: 0.66787 Time: 0.00586\n",
      "Iteration: 0186 Loss: 0.66962 Time: 0.00699\n",
      "Iteration: 0187 Loss: 0.66419 Time: 0.00525\n",
      "Iteration: 0188 Loss: 0.66204 Time: 0.00494\n",
      "Iteration: 0189 Loss: 0.66634 Time: 0.00700\n",
      "Iteration: 0190 Loss: 0.66437 Time: 0.00547\n",
      "Iteration: 0191 Loss: 0.65878 Time: 0.00591\n",
      "Iteration: 0192 Loss: 0.65867 Time: 0.00699\n",
      "Iteration: 0193 Loss: 0.64590 Time: 0.00557\n",
      "Iteration: 0194 Loss: 0.64996 Time: 0.00600\n",
      "Iteration: 0195 Loss: 0.65243 Time: 0.00633\n",
      "Iteration: 0196 Loss: 0.64962 Time: 0.00496\n",
      "Iteration: 0197 Loss: 0.65586 Time: 0.00652\n",
      "Iteration: 0198 Loss: 0.65207 Time: 0.00500\n",
      "Iteration: 0199 Loss: 0.65225 Time: 0.00606\n",
      "Iteration: 0200 Loss: 0.64739 Time: 0.00801\n",
      "Done! \n",
      "\n",
      "STEP 3/3 - MODEL EVALUATION \n",
      "\n",
      "Computing the final embedding vectors, for evaluation\n",
      "Done! \n",
      "\n",
      "Testing: link prediction\n",
      "Done! \n",
      "\n",
      "EXPERIMENTS ON MODEL 4 / 10 \n",
      "\n",
      "STEP 1/3 - PREPROCESSING STEPS \n",
      "\n",
      "Masking some edges from the training graph, for link prediction\n",
      "(validation set: 1.0 % of edges - test set: 10.0 % of edges)\n",
      "Done! \n",
      "\n",
      "Preprocessing node features\n",
      "Done! \n",
      "\n",
      "Running the Louvain algorithm for community detection\n",
      "as a preprocessing step for the encoder\n",
      "Done! Louvain has found 7 communities \n",
      "\n",
      "Setting up the model and the optimizer\n",
      "Done! \n",
      "\n",
      "Preprocessing on message passing matrices\n",
      "Done! \n",
      "\n",
      "Initializing TF session\n",
      "Done! \n",
      "\n",
      "STEP 2/3 - MODEL TRAINING \n",
      "\n",
      "Starting training\n",
      "Iteration: 0001 Loss: 1.71768 Time: 0.12339\n",
      "Iteration: 0002 Loss: 1.72458 Time: 0.00616\n",
      "Iteration: 0003 Loss: 1.69335 Time: 0.00496\n",
      "Iteration: 0004 Loss: 1.68730 Time: 0.00670\n",
      "Iteration: 0005 Loss: 1.61486 Time: 0.00598\n",
      "Iteration: 0006 Loss: 1.58701 Time: 0.00602\n",
      "Iteration: 0007 Loss: 1.61714 Time: 0.00805\n",
      "Iteration: 0008 Loss: 1.54476 Time: 0.00596\n",
      "Iteration: 0009 Loss: 1.56036 Time: 0.00503\n",
      "Iteration: 0010 Loss: 1.49373 Time: 0.00630\n",
      "Iteration: 0011 Loss: 1.47190 Time: 0.00626\n",
      "Iteration: 0012 Loss: 1.44266 Time: 0.00500\n",
      "Iteration: 0013 Loss: 1.48657 Time: 0.00594\n",
      "Iteration: 0014 Loss: 1.39673 Time: 0.00605\n",
      "Iteration: 0015 Loss: 1.43651 Time: 0.00553\n",
      "Iteration: 0016 Loss: 1.41191 Time: 0.00597\n",
      "Iteration: 0017 Loss: 1.42164 Time: 0.00556\n",
      "Iteration: 0018 Loss: 1.33039 Time: 0.00595\n",
      "Iteration: 0019 Loss: 1.39282 Time: 0.00552\n",
      "Iteration: 0020 Loss: 1.29543 Time: 0.00803\n",
      "Iteration: 0021 Loss: 1.28367 Time: 0.00497\n",
      "Iteration: 0022 Loss: 1.31315 Time: 0.00701\n",
      "Iteration: 0023 Loss: 1.31000 Time: 0.00537\n",
      "Iteration: 0024 Loss: 1.30703 Time: 0.00584\n",
      "Iteration: 0025 Loss: 1.25490 Time: 0.00673\n",
      "Iteration: 0026 Loss: 1.25823 Time: 0.00636\n",
      "Iteration: 0027 Loss: 1.28603 Time: 0.00661\n",
      "Iteration: 0028 Loss: 1.21415 Time: 0.00596\n",
      "Iteration: 0029 Loss: 1.20053 Time: 0.00504\n",
      "Iteration: 0030 Loss: 1.22670 Time: 0.00655\n",
      "Iteration: 0031 Loss: 1.18829 Time: 0.00554\n",
      "Iteration: 0032 Loss: 1.18140 Time: 0.00646\n",
      "Iteration: 0033 Loss: 1.14725 Time: 0.00644\n",
      "Iteration: 0034 Loss: 1.16399 Time: 0.00604\n",
      "Iteration: 0035 Loss: 1.15091 Time: 0.00601\n",
      "Iteration: 0036 Loss: 1.12519 Time: 0.00604\n",
      "Iteration: 0037 Loss: 1.13371 Time: 0.00499\n",
      "Iteration: 0038 Loss: 1.13267 Time: 0.00602\n",
      "Iteration: 0039 Loss: 1.10964 Time: 0.00496\n",
      "Iteration: 0040 Loss: 1.07570 Time: 0.00601\n",
      "Iteration: 0041 Loss: 1.09246 Time: 0.00524\n",
      "Iteration: 0042 Loss: 1.08600 Time: 0.00478\n",
      "Iteration: 0043 Loss: 1.08819 Time: 0.00542\n",
      "Iteration: 0044 Loss: 1.05674 Time: 0.00557\n",
      "Iteration: 0045 Loss: 1.10373 Time: 0.00524\n",
      "Iteration: 0046 Loss: 1.06805 Time: 0.00576\n",
      "Iteration: 0047 Loss: 1.06073 Time: 0.00604\n",
      "Iteration: 0048 Loss: 1.06767 Time: 0.00551\n",
      "Iteration: 0049 Loss: 1.04803 Time: 0.00745\n",
      "Iteration: 0050 Loss: 1.02646 Time: 0.00603\n",
      "Iteration: 0051 Loss: 1.03791 Time: 0.00536\n",
      "Iteration: 0052 Loss: 1.02778 Time: 0.00665\n",
      "Iteration: 0053 Loss: 1.01703 Time: 0.00563\n",
      "Iteration: 0054 Loss: 1.02510 Time: 0.00603\n",
      "Iteration: 0055 Loss: 1.01712 Time: 0.00597\n",
      "Iteration: 0056 Loss: 1.03267 Time: 0.00558\n",
      "Iteration: 0057 Loss: 1.01219 Time: 0.00702\n",
      "Iteration: 0058 Loss: 0.99070 Time: 0.00639\n",
      "Iteration: 0059 Loss: 0.99563 Time: 0.00587\n",
      "Iteration: 0060 Loss: 1.00299 Time: 0.00538\n",
      "Iteration: 0061 Loss: 0.97454 Time: 0.00682\n",
      "Iteration: 0062 Loss: 0.96825 Time: 0.00688\n",
      "Iteration: 0063 Loss: 0.98739 Time: 0.00502\n",
      "Iteration: 0064 Loss: 0.95504 Time: 0.00496\n",
      "Iteration: 0065 Loss: 0.95025 Time: 0.00708\n",
      "Iteration: 0066 Loss: 0.94872 Time: 0.00497\n",
      "Iteration: 0067 Loss: 0.94402 Time: 0.00699\n",
      "Iteration: 0068 Loss: 0.94284 Time: 0.00700\n",
      "Iteration: 0069 Loss: 0.93797 Time: 0.00500\n",
      "Iteration: 0070 Loss: 0.93973 Time: 0.00589\n",
      "Iteration: 0071 Loss: 0.90664 Time: 0.00503\n",
      "Iteration: 0072 Loss: 0.93168 Time: 0.00600\n",
      "Iteration: 0073 Loss: 0.92094 Time: 0.00726\n",
      "Iteration: 0074 Loss: 0.90076 Time: 0.00631\n",
      "Iteration: 0075 Loss: 0.91632 Time: 0.00639\n",
      "Iteration: 0076 Loss: 0.90894 Time: 0.00515\n",
      "Iteration: 0077 Loss: 0.90075 Time: 0.00679\n",
      "Iteration: 0078 Loss: 0.90733 Time: 0.00621\n",
      "Iteration: 0079 Loss: 0.89238 Time: 0.00670\n",
      "Iteration: 0080 Loss: 0.89034 Time: 0.00700\n",
      "Iteration: 0081 Loss: 0.88662 Time: 0.00557\n",
      "Iteration: 0082 Loss: 0.89571 Time: 0.00550\n",
      "Iteration: 0083 Loss: 0.89913 Time: 0.00693\n",
      "Iteration: 0084 Loss: 0.87083 Time: 0.00500\n",
      "Iteration: 0085 Loss: 0.84771 Time: 0.00599\n",
      "Iteration: 0086 Loss: 0.89164 Time: 0.00700\n",
      "Iteration: 0087 Loss: 0.85804 Time: 0.00501\n",
      "Iteration: 0088 Loss: 0.84878 Time: 0.00605\n",
      "Iteration: 0089 Loss: 0.85934 Time: 0.00608\n",
      "Iteration: 0090 Loss: 0.85248 Time: 0.00589\n",
      "Iteration: 0091 Loss: 0.84834 Time: 0.00605\n",
      "Iteration: 0092 Loss: 0.85439 Time: 0.00637\n",
      "Iteration: 0093 Loss: 0.85820 Time: 0.00535\n",
      "Iteration: 0094 Loss: 0.83234 Time: 0.00564\n",
      "Iteration: 0095 Loss: 0.82489 Time: 0.00492\n",
      "Iteration: 0096 Loss: 0.81907 Time: 0.00601\n",
      "Iteration: 0097 Loss: 0.82992 Time: 0.00505\n",
      "Iteration: 0098 Loss: 0.82055 Time: 0.00501\n",
      "Iteration: 0099 Loss: 0.81672 Time: 0.00630\n",
      "Iteration: 0100 Loss: 0.78957 Time: 0.00500\n",
      "Iteration: 0101 Loss: 0.82998 Time: 0.00594\n",
      "Iteration: 0102 Loss: 0.82447 Time: 0.00800\n",
      "Iteration: 0103 Loss: 0.81032 Time: 0.00549\n",
      "Iteration: 0104 Loss: 0.82231 Time: 0.00595\n",
      "Iteration: 0105 Loss: 0.78991 Time: 0.00604\n",
      "Iteration: 0106 Loss: 0.78790 Time: 0.00638\n",
      "Iteration: 0107 Loss: 0.78828 Time: 0.00641\n",
      "Iteration: 0108 Loss: 0.79294 Time: 0.00687\n",
      "Iteration: 0109 Loss: 0.76019 Time: 0.00590\n",
      "Iteration: 0110 Loss: 0.79025 Time: 0.00596\n",
      "Iteration: 0111 Loss: 0.75846 Time: 0.00548\n",
      "Iteration: 0112 Loss: 0.77255 Time: 0.00732\n",
      "Iteration: 0113 Loss: 0.78287 Time: 0.00530\n",
      "Iteration: 0114 Loss: 0.76516 Time: 0.00564\n",
      "Iteration: 0115 Loss: 0.75272 Time: 0.00700\n",
      "Iteration: 0116 Loss: 0.74867 Time: 0.00600\n",
      "Iteration: 0117 Loss: 0.74853 Time: 0.00605\n",
      "Iteration: 0118 Loss: 0.74288 Time: 0.00495\n",
      "Iteration: 0119 Loss: 0.76269 Time: 0.00657\n",
      "Iteration: 0120 Loss: 0.75462 Time: 0.00499\n",
      "Iteration: 0121 Loss: 0.75361 Time: 0.00499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0122 Loss: 0.73827 Time: 0.00595\n",
      "Iteration: 0123 Loss: 0.73205 Time: 0.00690\n",
      "Iteration: 0124 Loss: 0.73817 Time: 0.00503\n",
      "Iteration: 0125 Loss: 0.73291 Time: 0.00504\n",
      "Iteration: 0126 Loss: 0.74415 Time: 0.00601\n",
      "Iteration: 0127 Loss: 0.74098 Time: 0.00642\n",
      "Iteration: 0128 Loss: 0.74693 Time: 0.00599\n",
      "Iteration: 0129 Loss: 0.73150 Time: 0.00500\n",
      "Iteration: 0130 Loss: 0.72410 Time: 0.00589\n",
      "Iteration: 0131 Loss: 0.73005 Time: 0.00593\n",
      "Iteration: 0132 Loss: 0.72417 Time: 0.00523\n",
      "Iteration: 0133 Loss: 0.71848 Time: 0.00481\n",
      "Iteration: 0134 Loss: 0.72144 Time: 0.00600\n",
      "Iteration: 0135 Loss: 0.71250 Time: 0.00397\n",
      "Iteration: 0136 Loss: 0.70355 Time: 0.00500\n",
      "Iteration: 0137 Loss: 0.70379 Time: 0.00611\n",
      "Iteration: 0138 Loss: 0.71395 Time: 0.00547\n",
      "Iteration: 0139 Loss: 0.70917 Time: 0.00653\n",
      "Iteration: 0140 Loss: 0.70760 Time: 0.00601\n",
      "Iteration: 0141 Loss: 0.71688 Time: 0.00599\n",
      "Iteration: 0142 Loss: 0.70727 Time: 0.00601\n",
      "Iteration: 0143 Loss: 0.69108 Time: 0.00501\n",
      "Iteration: 0144 Loss: 0.70778 Time: 0.00505\n",
      "Iteration: 0145 Loss: 0.71101 Time: 0.00657\n",
      "Iteration: 0146 Loss: 0.69579 Time: 0.00604\n",
      "Iteration: 0147 Loss: 0.69020 Time: 0.00595\n",
      "Iteration: 0148 Loss: 0.70230 Time: 0.00604\n",
      "Iteration: 0149 Loss: 0.69766 Time: 0.00500\n",
      "Iteration: 0150 Loss: 0.71209 Time: 0.00602\n",
      "Iteration: 0151 Loss: 0.70464 Time: 0.00795\n",
      "Iteration: 0152 Loss: 0.70197 Time: 0.00500\n",
      "Iteration: 0153 Loss: 0.68142 Time: 0.00700\n",
      "Iteration: 0154 Loss: 0.69487 Time: 0.00604\n",
      "Iteration: 0155 Loss: 0.67930 Time: 0.00599\n",
      "Iteration: 0156 Loss: 0.69481 Time: 0.00597\n",
      "Iteration: 0157 Loss: 0.67894 Time: 0.00600\n",
      "Iteration: 0158 Loss: 0.68662 Time: 0.00605\n",
      "Iteration: 0159 Loss: 0.67560 Time: 0.00501\n",
      "Iteration: 0160 Loss: 0.69015 Time: 0.00537\n",
      "Iteration: 0161 Loss: 0.69728 Time: 0.00499\n",
      "Iteration: 0162 Loss: 0.69035 Time: 0.00643\n",
      "Iteration: 0163 Loss: 0.68648 Time: 0.00651\n",
      "Iteration: 0164 Loss: 0.68449 Time: 0.00600\n",
      "Iteration: 0165 Loss: 0.68423 Time: 0.00600\n",
      "Iteration: 0166 Loss: 0.67876 Time: 0.00502\n",
      "Iteration: 0167 Loss: 0.68105 Time: 0.00552\n",
      "Iteration: 0168 Loss: 0.68463 Time: 0.00501\n",
      "Iteration: 0169 Loss: 0.67536 Time: 0.00604\n",
      "Iteration: 0170 Loss: 0.67135 Time: 0.00597\n",
      "Iteration: 0171 Loss: 0.68529 Time: 0.00800\n",
      "Iteration: 0172 Loss: 0.68302 Time: 0.00611\n",
      "Iteration: 0173 Loss: 0.66570 Time: 0.00635\n",
      "Iteration: 0174 Loss: 0.67298 Time: 0.00601\n",
      "Iteration: 0175 Loss: 0.65690 Time: 0.00655\n",
      "Iteration: 0176 Loss: 0.67094 Time: 0.00582\n",
      "Iteration: 0177 Loss: 0.66226 Time: 0.00700\n",
      "Iteration: 0178 Loss: 0.67063 Time: 0.00500\n",
      "Iteration: 0179 Loss: 0.66113 Time: 0.00643\n",
      "Iteration: 0180 Loss: 0.67678 Time: 0.00596\n",
      "Iteration: 0181 Loss: 0.66379 Time: 0.00600\n",
      "Iteration: 0182 Loss: 0.66336 Time: 0.00800\n",
      "Iteration: 0183 Loss: 0.66565 Time: 0.00800\n",
      "Iteration: 0184 Loss: 0.66681 Time: 0.00852\n",
      "Iteration: 0185 Loss: 0.65747 Time: 0.00622\n",
      "Iteration: 0186 Loss: 0.66405 Time: 0.00777\n",
      "Iteration: 0187 Loss: 0.65868 Time: 0.00600\n",
      "Iteration: 0188 Loss: 0.64941 Time: 0.00599\n",
      "Iteration: 0189 Loss: 0.66214 Time: 0.00700\n",
      "Iteration: 0190 Loss: 0.65900 Time: 0.00627\n",
      "Iteration: 0191 Loss: 0.65979 Time: 0.00608\n",
      "Iteration: 0192 Loss: 0.64330 Time: 0.00590\n",
      "Iteration: 0193 Loss: 0.64942 Time: 0.00577\n",
      "Iteration: 0194 Loss: 0.64282 Time: 0.00618\n",
      "Iteration: 0195 Loss: 0.65554 Time: 0.00592\n",
      "Iteration: 0196 Loss: 0.64076 Time: 0.00590\n",
      "Iteration: 0197 Loss: 0.65009 Time: 0.00600\n",
      "Iteration: 0198 Loss: 0.65795 Time: 0.00656\n",
      "Iteration: 0199 Loss: 0.66199 Time: 0.00499\n",
      "Iteration: 0200 Loss: 0.64783 Time: 0.00500\n",
      "Done! \n",
      "\n",
      "STEP 3/3 - MODEL EVALUATION \n",
      "\n",
      "Computing the final embedding vectors, for evaluation\n",
      "Done! \n",
      "\n",
      "Testing: link prediction\n",
      "Done! \n",
      "\n",
      "EXPERIMENTS ON MODEL 5 / 10 \n",
      "\n",
      "STEP 1/3 - PREPROCESSING STEPS \n",
      "\n",
      "Masking some edges from the training graph, for link prediction\n",
      "(validation set: 1.0 % of edges - test set: 10.0 % of edges)\n",
      "Done! \n",
      "\n",
      "Preprocessing node features\n",
      "Done! \n",
      "\n",
      "Running the Louvain algorithm for community detection\n",
      "as a preprocessing step for the encoder\n",
      "Done! Louvain has found 7 communities \n",
      "\n",
      "Setting up the model and the optimizer\n",
      "Done! \n",
      "\n",
      "Preprocessing on message passing matrices\n",
      "Done! \n",
      "\n",
      "Initializing TF session\n",
      "Done! \n",
      "\n",
      "STEP 2/3 - MODEL TRAINING \n",
      "\n",
      "Starting training\n",
      "Iteration: 0001 Loss: 1.70634 Time: 0.12299\n",
      "Iteration: 0002 Loss: 1.66449 Time: 0.00683\n",
      "Iteration: 0003 Loss: 1.68059 Time: 0.00601\n",
      "Iteration: 0004 Loss: 1.71568 Time: 0.00605\n",
      "Iteration: 0005 Loss: 1.59649 Time: 0.00828\n",
      "Iteration: 0006 Loss: 1.56336 Time: 0.00550\n",
      "Iteration: 0007 Loss: 1.59830 Time: 0.00650\n",
      "Iteration: 0008 Loss: 1.58775 Time: 0.00601\n",
      "Iteration: 0009 Loss: 1.57217 Time: 0.00674\n",
      "Iteration: 0010 Loss: 1.48353 Time: 0.00686\n",
      "Iteration: 0011 Loss: 1.47306 Time: 0.00633\n",
      "Iteration: 0012 Loss: 1.44695 Time: 0.00694\n",
      "Iteration: 0013 Loss: 1.40776 Time: 0.00699\n",
      "Iteration: 0014 Loss: 1.45285 Time: 0.00800\n",
      "Iteration: 0015 Loss: 1.44480 Time: 0.00701\n",
      "Iteration: 0016 Loss: 1.45512 Time: 0.00504\n",
      "Iteration: 0017 Loss: 1.35652 Time: 0.00695\n",
      "Iteration: 0018 Loss: 1.35800 Time: 0.00900\n",
      "Iteration: 0019 Loss: 1.37651 Time: 0.00695\n",
      "Iteration: 0020 Loss: 1.34948 Time: 0.00705\n",
      "Iteration: 0021 Loss: 1.32162 Time: 0.00601\n",
      "Iteration: 0022 Loss: 1.34601 Time: 0.00700\n",
      "Iteration: 0023 Loss: 1.31454 Time: 0.00636\n",
      "Iteration: 0024 Loss: 1.26776 Time: 0.00714\n",
      "Iteration: 0025 Loss: 1.30141 Time: 0.00691\n",
      "Iteration: 0026 Loss: 1.31524 Time: 0.00598\n",
      "Iteration: 0027 Loss: 1.20454 Time: 0.00658\n",
      "Iteration: 0028 Loss: 1.21185 Time: 0.00696\n",
      "Iteration: 0029 Loss: 1.22632 Time: 0.00806\n",
      "Iteration: 0030 Loss: 1.25191 Time: 0.00648\n",
      "Iteration: 0031 Loss: 1.16821 Time: 0.00793\n",
      "Iteration: 0032 Loss: 1.19639 Time: 0.00647\n",
      "Iteration: 0033 Loss: 1.15809 Time: 0.00753\n",
      "Iteration: 0034 Loss: 1.15217 Time: 0.00630\n",
      "Iteration: 0035 Loss: 1.12777 Time: 0.00694\n",
      "Iteration: 0036 Loss: 1.15227 Time: 0.00600\n",
      "Iteration: 0037 Loss: 1.13908 Time: 0.00661\n",
      "Iteration: 0038 Loss: 1.12567 Time: 0.00740\n",
      "Iteration: 0039 Loss: 1.10584 Time: 0.00604\n",
      "Iteration: 0040 Loss: 1.07252 Time: 0.00696\n",
      "Iteration: 0041 Loss: 1.08819 Time: 0.00699\n",
      "Iteration: 0042 Loss: 1.08806 Time: 0.00799\n",
      "Iteration: 0043 Loss: 1.07217 Time: 0.00796\n",
      "Iteration: 0044 Loss: 1.05186 Time: 0.00700\n",
      "Iteration: 0045 Loss: 1.07072 Time: 0.00799\n",
      "Iteration: 0046 Loss: 1.07388 Time: 0.00706\n",
      "Iteration: 0047 Loss: 1.08601 Time: 0.00740\n",
      "Iteration: 0048 Loss: 1.04567 Time: 0.00651\n",
      "Iteration: 0049 Loss: 1.06235 Time: 0.00593\n",
      "Iteration: 0050 Loss: 1.04093 Time: 0.00606\n",
      "Iteration: 0051 Loss: 1.01511 Time: 0.00594\n",
      "Iteration: 0052 Loss: 1.02296 Time: 0.00637\n",
      "Iteration: 0053 Loss: 1.01816 Time: 0.00728\n",
      "Iteration: 0054 Loss: 1.03141 Time: 0.00796\n",
      "Iteration: 0055 Loss: 1.01232 Time: 0.00586\n",
      "Iteration: 0056 Loss: 1.00382 Time: 0.00638\n",
      "Iteration: 0057 Loss: 1.01272 Time: 0.00634\n",
      "Iteration: 0058 Loss: 0.98856 Time: 0.00595\n",
      "Iteration: 0059 Loss: 0.99046 Time: 0.00747\n",
      "Iteration: 0060 Loss: 0.97977 Time: 0.00634\n",
      "Iteration: 0061 Loss: 0.98620 Time: 0.00661\n",
      "Iteration: 0062 Loss: 0.95857 Time: 0.00599\n",
      "Iteration: 0063 Loss: 0.96305 Time: 0.00503\n",
      "Iteration: 0064 Loss: 0.93509 Time: 0.00797\n",
      "Iteration: 0065 Loss: 0.94757 Time: 0.00501\n",
      "Iteration: 0066 Loss: 0.95308 Time: 0.00700\n",
      "Iteration: 0067 Loss: 0.96151 Time: 0.00647\n",
      "Iteration: 0068 Loss: 0.95504 Time: 0.00606\n",
      "Iteration: 0069 Loss: 0.95308 Time: 0.00831\n",
      "Iteration: 0070 Loss: 0.92710 Time: 0.00702\n",
      "Iteration: 0071 Loss: 0.94680 Time: 0.00663\n",
      "Iteration: 0072 Loss: 0.94104 Time: 0.00731\n",
      "Iteration: 0073 Loss: 0.92943 Time: 0.00667\n",
      "Iteration: 0074 Loss: 0.93159 Time: 0.00639\n",
      "Iteration: 0075 Loss: 0.91400 Time: 0.00533\n",
      "Iteration: 0076 Loss: 0.92326 Time: 0.00662\n",
      "Iteration: 0077 Loss: 0.91675 Time: 0.00600\n",
      "Iteration: 0078 Loss: 0.91973 Time: 0.00702\n",
      "Iteration: 0079 Loss: 0.91238 Time: 0.00599\n",
      "Iteration: 0080 Loss: 0.89859 Time: 0.00600\n",
      "Iteration: 0081 Loss: 0.88749 Time: 0.00645\n",
      "Iteration: 0082 Loss: 0.89332 Time: 0.00655\n",
      "Iteration: 0083 Loss: 0.91125 Time: 0.00650\n",
      "Iteration: 0084 Loss: 0.91235 Time: 0.00713\n",
      "Iteration: 0085 Loss: 0.87187 Time: 0.00596\n",
      "Iteration: 0086 Loss: 0.87914 Time: 0.00671\n",
      "Iteration: 0087 Loss: 0.90827 Time: 0.00700\n",
      "Iteration: 0088 Loss: 0.88350 Time: 0.00700\n",
      "Iteration: 0089 Loss: 0.85194 Time: 0.00501\n",
      "Iteration: 0090 Loss: 0.88558 Time: 0.00597\n",
      "Iteration: 0091 Loss: 0.85599 Time: 0.00702\n",
      "Iteration: 0092 Loss: 0.86870 Time: 0.00599\n",
      "Iteration: 0093 Loss: 0.85641 Time: 0.00595\n",
      "Iteration: 0094 Loss: 0.85629 Time: 0.00504\n",
      "Iteration: 0095 Loss: 0.87044 Time: 0.00603\n",
      "Iteration: 0096 Loss: 0.83485 Time: 0.00597\n",
      "Iteration: 0097 Loss: 0.85437 Time: 0.00562\n",
      "Iteration: 0098 Loss: 0.84192 Time: 0.00500\n",
      "Iteration: 0099 Loss: 0.84578 Time: 0.00694\n",
      "Iteration: 0100 Loss: 0.83329 Time: 0.00542\n",
      "Iteration: 0101 Loss: 0.83709 Time: 0.00596\n",
      "Iteration: 0102 Loss: 0.84006 Time: 0.00632\n",
      "Iteration: 0103 Loss: 0.82810 Time: 0.00689\n",
      "Iteration: 0104 Loss: 0.81598 Time: 0.00582\n",
      "Iteration: 0105 Loss: 0.82155 Time: 0.00796\n",
      "Iteration: 0106 Loss: 0.82411 Time: 0.00702\n",
      "Iteration: 0107 Loss: 0.80924 Time: 0.00602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0108 Loss: 0.80411 Time: 0.00622\n",
      "Iteration: 0109 Loss: 0.80226 Time: 0.00678\n",
      "Iteration: 0110 Loss: 0.80739 Time: 0.00601\n",
      "Iteration: 0111 Loss: 0.79953 Time: 0.00795\n",
      "Iteration: 0112 Loss: 0.79004 Time: 0.00704\n",
      "Iteration: 0113 Loss: 0.79360 Time: 0.00697\n",
      "Iteration: 0114 Loss: 0.76188 Time: 0.00735\n",
      "Iteration: 0115 Loss: 0.78448 Time: 0.00669\n",
      "Iteration: 0116 Loss: 0.78115 Time: 0.00754\n",
      "Iteration: 0117 Loss: 0.78237 Time: 0.00669\n",
      "Iteration: 0118 Loss: 0.77694 Time: 0.00600\n",
      "Iteration: 0119 Loss: 0.76904 Time: 0.00599\n",
      "Iteration: 0120 Loss: 0.75657 Time: 0.00600\n",
      "Iteration: 0121 Loss: 0.78189 Time: 0.00699\n",
      "Iteration: 0122 Loss: 0.74467 Time: 0.00665\n",
      "Iteration: 0123 Loss: 0.76600 Time: 0.00699\n",
      "Iteration: 0124 Loss: 0.77278 Time: 0.00701\n",
      "Iteration: 0125 Loss: 0.76940 Time: 0.00699\n",
      "Iteration: 0126 Loss: 0.75794 Time: 0.00800\n",
      "Iteration: 0127 Loss: 0.75912 Time: 0.00701\n",
      "Iteration: 0128 Loss: 0.73673 Time: 0.00800\n",
      "Iteration: 0129 Loss: 0.74302 Time: 0.00600\n",
      "Iteration: 0130 Loss: 0.73419 Time: 0.00700\n",
      "Iteration: 0131 Loss: 0.73948 Time: 0.00599\n",
      "Iteration: 0132 Loss: 0.73588 Time: 0.00602\n",
      "Iteration: 0133 Loss: 0.74100 Time: 0.00671\n",
      "Iteration: 0134 Loss: 0.74414 Time: 0.00601\n",
      "Iteration: 0135 Loss: 0.71371 Time: 0.00700\n",
      "Iteration: 0136 Loss: 0.71770 Time: 0.00699\n",
      "Iteration: 0137 Loss: 0.74114 Time: 0.00700\n",
      "Iteration: 0138 Loss: 0.73873 Time: 0.00853\n",
      "Iteration: 0139 Loss: 0.72746 Time: 0.00747\n",
      "Iteration: 0140 Loss: 0.71624 Time: 0.00773\n",
      "Iteration: 0141 Loss: 0.72216 Time: 0.00644\n",
      "Iteration: 0142 Loss: 0.71470 Time: 0.00603\n",
      "Iteration: 0143 Loss: 0.70577 Time: 0.00638\n",
      "Iteration: 0144 Loss: 0.72409 Time: 0.00595\n",
      "Iteration: 0145 Loss: 0.71941 Time: 0.00699\n",
      "Iteration: 0146 Loss: 0.71353 Time: 0.00500\n",
      "Iteration: 0147 Loss: 0.71577 Time: 0.00763\n",
      "Iteration: 0148 Loss: 0.71448 Time: 0.00632\n",
      "Iteration: 0149 Loss: 0.69999 Time: 0.00800\n",
      "Iteration: 0150 Loss: 0.71297 Time: 0.00606\n",
      "Iteration: 0151 Loss: 0.71038 Time: 0.00595\n",
      "Iteration: 0152 Loss: 0.69807 Time: 0.00732\n",
      "Iteration: 0153 Loss: 0.69569 Time: 0.00630\n",
      "Iteration: 0154 Loss: 0.71099 Time: 0.00663\n",
      "Iteration: 0155 Loss: 0.70993 Time: 0.00701\n",
      "Iteration: 0156 Loss: 0.69959 Time: 0.00699\n",
      "Iteration: 0157 Loss: 0.69091 Time: 0.00633\n",
      "Iteration: 0158 Loss: 0.69577 Time: 0.00599\n",
      "Iteration: 0159 Loss: 0.70413 Time: 0.00629\n",
      "Iteration: 0160 Loss: 0.69051 Time: 0.00599\n",
      "Iteration: 0161 Loss: 0.69242 Time: 0.00578\n",
      "Iteration: 0162 Loss: 0.68745 Time: 0.00602\n",
      "Iteration: 0163 Loss: 0.69450 Time: 0.00597\n",
      "Iteration: 0164 Loss: 0.67945 Time: 0.00599\n",
      "Iteration: 0165 Loss: 0.68788 Time: 0.00701\n",
      "Iteration: 0166 Loss: 0.67932 Time: 0.00500\n",
      "Iteration: 0167 Loss: 0.69101 Time: 0.00716\n",
      "Iteration: 0168 Loss: 0.68534 Time: 0.00581\n",
      "Iteration: 0169 Loss: 0.67793 Time: 0.00602\n",
      "Iteration: 0170 Loss: 0.68755 Time: 0.00701\n",
      "Iteration: 0171 Loss: 0.68124 Time: 0.00599\n",
      "Iteration: 0172 Loss: 0.68655 Time: 0.00702\n",
      "Iteration: 0173 Loss: 0.67311 Time: 0.00804\n",
      "Iteration: 0174 Loss: 0.68472 Time: 0.00794\n",
      "Iteration: 0175 Loss: 0.67486 Time: 0.00600\n",
      "Iteration: 0176 Loss: 0.69085 Time: 0.00600\n",
      "Iteration: 0177 Loss: 0.67388 Time: 0.00670\n",
      "Iteration: 0178 Loss: 0.67442 Time: 0.00600\n",
      "Iteration: 0179 Loss: 0.66914 Time: 0.00600\n",
      "Iteration: 0180 Loss: 0.67080 Time: 0.00600\n",
      "Iteration: 0181 Loss: 0.67493 Time: 0.00550\n",
      "Iteration: 0182 Loss: 0.67195 Time: 0.00599\n",
      "Iteration: 0183 Loss: 0.66391 Time: 0.00623\n",
      "Iteration: 0184 Loss: 0.66956 Time: 0.00777\n",
      "Iteration: 0185 Loss: 0.66532 Time: 0.00600\n",
      "Iteration: 0186 Loss: 0.67218 Time: 0.00499\n",
      "Iteration: 0187 Loss: 0.67071 Time: 0.00618\n",
      "Iteration: 0188 Loss: 0.67354 Time: 0.00582\n",
      "Iteration: 0189 Loss: 0.66754 Time: 0.00600\n",
      "Iteration: 0190 Loss: 0.66193 Time: 0.00656\n",
      "Iteration: 0191 Loss: 0.66843 Time: 0.00634\n",
      "Iteration: 0192 Loss: 0.66412 Time: 0.00694\n",
      "Iteration: 0193 Loss: 0.66114 Time: 0.00604\n",
      "Iteration: 0194 Loss: 0.65181 Time: 0.00500\n",
      "Iteration: 0195 Loss: 0.66727 Time: 0.00652\n",
      "Iteration: 0196 Loss: 0.65241 Time: 0.00549\n",
      "Iteration: 0197 Loss: 0.65448 Time: 0.00529\n",
      "Iteration: 0198 Loss: 0.66259 Time: 0.00666\n",
      "Iteration: 0199 Loss: 0.65908 Time: 0.00600\n",
      "Iteration: 0200 Loss: 0.65377 Time: 0.00700\n",
      "Done! \n",
      "\n",
      "STEP 3/3 - MODEL EVALUATION \n",
      "\n",
      "Computing the final embedding vectors, for evaluation\n",
      "Done! \n",
      "\n",
      "Testing: link prediction\n",
      "Done! \n",
      "\n",
      "EXPERIMENTS ON MODEL 6 / 10 \n",
      "\n",
      "STEP 1/3 - PREPROCESSING STEPS \n",
      "\n",
      "Masking some edges from the training graph, for link prediction\n",
      "(validation set: 1.0 % of edges - test set: 10.0 % of edges)\n",
      "Done! \n",
      "\n",
      "Preprocessing node features\n",
      "Done! \n",
      "\n",
      "Running the Louvain algorithm for community detection\n",
      "as a preprocessing step for the encoder\n",
      "Done! Louvain has found 7 communities \n",
      "\n",
      "Setting up the model and the optimizer\n",
      "Done! \n",
      "\n",
      "Preprocessing on message passing matrices\n",
      "Done! \n",
      "\n",
      "Initializing TF session\n",
      "Done! \n",
      "\n",
      "STEP 2/3 - MODEL TRAINING \n",
      "\n",
      "Starting training\n",
      "Iteration: 0001 Loss: 1.80542 Time: 0.13005\n",
      "Iteration: 0002 Loss: 1.70919 Time: 0.00793\n",
      "Iteration: 0003 Loss: 1.65583 Time: 0.00597\n",
      "Iteration: 0004 Loss: 1.67460 Time: 0.00805\n",
      "Iteration: 0005 Loss: 1.61871 Time: 0.00784\n",
      "Iteration: 0006 Loss: 1.53685 Time: 0.00704\n",
      "Iteration: 0007 Loss: 1.58591 Time: 0.00600\n",
      "Iteration: 0008 Loss: 1.60133 Time: 0.00800\n",
      "Iteration: 0009 Loss: 1.60795 Time: 0.00598\n",
      "Iteration: 0010 Loss: 1.53246 Time: 0.00692\n",
      "Iteration: 0011 Loss: 1.57205 Time: 0.00706\n",
      "Iteration: 0012 Loss: 1.45012 Time: 0.00794\n",
      "Iteration: 0013 Loss: 1.43488 Time: 0.00719\n",
      "Iteration: 0014 Loss: 1.40046 Time: 0.00600\n",
      "Iteration: 0015 Loss: 1.40485 Time: 0.00699\n",
      "Iteration: 0016 Loss: 1.41961 Time: 0.00554\n",
      "Iteration: 0017 Loss: 1.36473 Time: 0.00600\n",
      "Iteration: 0018 Loss: 1.37847 Time: 0.00501\n",
      "Iteration: 0019 Loss: 1.34260 Time: 0.00605\n",
      "Iteration: 0020 Loss: 1.35377 Time: 0.00699\n",
      "Iteration: 0021 Loss: 1.35327 Time: 0.00700\n",
      "Iteration: 0022 Loss: 1.29915 Time: 0.00608\n",
      "Iteration: 0023 Loss: 1.29800 Time: 0.00644\n",
      "Iteration: 0024 Loss: 1.24764 Time: 0.00553\n",
      "Iteration: 0025 Loss: 1.26123 Time: 0.00544\n",
      "Iteration: 0026 Loss: 1.21591 Time: 0.00545\n",
      "Iteration: 0027 Loss: 1.22682 Time: 0.00599\n",
      "Iteration: 0028 Loss: 1.19533 Time: 0.00660\n",
      "Iteration: 0029 Loss: 1.21891 Time: 0.00697\n",
      "Iteration: 0030 Loss: 1.20446 Time: 0.00603\n",
      "Iteration: 0031 Loss: 1.21462 Time: 0.00797\n",
      "Iteration: 0032 Loss: 1.17796 Time: 0.00606\n",
      "Iteration: 0033 Loss: 1.17144 Time: 0.00599\n",
      "Iteration: 0034 Loss: 1.16047 Time: 0.00660\n",
      "Iteration: 0035 Loss: 1.16266 Time: 0.00647\n",
      "Iteration: 0036 Loss: 1.10615 Time: 0.00617\n",
      "Iteration: 0037 Loss: 1.12655 Time: 0.00679\n",
      "Iteration: 0038 Loss: 1.10327 Time: 0.00602\n",
      "Iteration: 0039 Loss: 1.08275 Time: 0.00555\n",
      "Iteration: 0040 Loss: 1.09514 Time: 0.00601\n",
      "Iteration: 0041 Loss: 1.07164 Time: 0.00694\n",
      "Iteration: 0042 Loss: 1.07828 Time: 0.00707\n",
      "Iteration: 0043 Loss: 1.07942 Time: 0.00820\n",
      "Iteration: 0044 Loss: 1.06831 Time: 0.00674\n",
      "Iteration: 0045 Loss: 1.06967 Time: 0.00801\n",
      "Iteration: 0046 Loss: 1.06191 Time: 0.00698\n",
      "Iteration: 0047 Loss: 1.06632 Time: 0.00701\n",
      "Iteration: 0048 Loss: 1.05488 Time: 0.00632\n",
      "Iteration: 0049 Loss: 1.05524 Time: 0.00597\n",
      "Iteration: 0050 Loss: 1.03538 Time: 0.00597\n",
      "Iteration: 0051 Loss: 1.00542 Time: 0.00600\n",
      "Iteration: 0052 Loss: 1.03128 Time: 0.00699\n",
      "Iteration: 0053 Loss: 1.00881 Time: 0.00505\n",
      "Iteration: 0054 Loss: 1.00860 Time: 0.00694\n",
      "Iteration: 0055 Loss: 1.01167 Time: 0.00716\n",
      "Iteration: 0056 Loss: 1.02269 Time: 0.00537\n",
      "Iteration: 0057 Loss: 0.98490 Time: 0.00647\n",
      "Iteration: 0058 Loss: 0.99289 Time: 0.00699\n",
      "Iteration: 0059 Loss: 0.96664 Time: 0.00500\n",
      "Iteration: 0060 Loss: 0.99592 Time: 0.00699\n",
      "Iteration: 0061 Loss: 0.96903 Time: 0.00500\n",
      "Iteration: 0062 Loss: 0.97915 Time: 0.00600\n",
      "Iteration: 0063 Loss: 0.97169 Time: 0.00600\n",
      "Iteration: 0064 Loss: 0.94650 Time: 0.00500\n",
      "Iteration: 0065 Loss: 0.96859 Time: 0.00630\n",
      "Iteration: 0066 Loss: 0.94492 Time: 0.00667\n",
      "Iteration: 0067 Loss: 0.95028 Time: 0.00700\n",
      "Iteration: 0068 Loss: 0.95344 Time: 0.00632\n",
      "Iteration: 0069 Loss: 0.91935 Time: 0.00552\n",
      "Iteration: 0070 Loss: 0.91503 Time: 0.00600\n",
      "Iteration: 0071 Loss: 0.92996 Time: 0.00600\n",
      "Iteration: 0072 Loss: 0.91086 Time: 0.00503\n",
      "Iteration: 0073 Loss: 0.93352 Time: 0.00697\n",
      "Iteration: 0074 Loss: 0.91645 Time: 0.00556\n",
      "Iteration: 0075 Loss: 0.91687 Time: 0.00529\n",
      "Iteration: 0076 Loss: 0.91313 Time: 0.00672\n",
      "Iteration: 0077 Loss: 0.88348 Time: 0.00526\n",
      "Iteration: 0078 Loss: 0.90215 Time: 0.00601\n",
      "Iteration: 0079 Loss: 0.90718 Time: 0.00547\n",
      "Iteration: 0080 Loss: 0.88880 Time: 0.00601\n",
      "Iteration: 0081 Loss: 0.87818 Time: 0.00556\n",
      "Iteration: 0082 Loss: 0.88542 Time: 0.00594\n",
      "Iteration: 0083 Loss: 0.86729 Time: 0.00601\n",
      "Iteration: 0084 Loss: 0.88227 Time: 0.00643\n",
      "Iteration: 0085 Loss: 0.88232 Time: 0.00558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0086 Loss: 0.85687 Time: 0.00600\n",
      "Iteration: 0087 Loss: 0.86047 Time: 0.00731\n",
      "Iteration: 0088 Loss: 0.86634 Time: 0.00601\n",
      "Iteration: 0089 Loss: 0.83593 Time: 0.00594\n",
      "Iteration: 0090 Loss: 0.82651 Time: 0.00653\n",
      "Iteration: 0091 Loss: 0.85623 Time: 0.00601\n",
      "Iteration: 0092 Loss: 0.85883 Time: 0.00620\n",
      "Iteration: 0093 Loss: 0.84946 Time: 0.00596\n",
      "Iteration: 0094 Loss: 0.84506 Time: 0.00573\n",
      "Iteration: 0095 Loss: 0.82891 Time: 0.00526\n",
      "Iteration: 0096 Loss: 0.83953 Time: 0.00475\n",
      "Iteration: 0097 Loss: 0.82132 Time: 0.00650\n",
      "Iteration: 0098 Loss: 0.80659 Time: 0.00551\n",
      "Iteration: 0099 Loss: 0.82778 Time: 0.00599\n",
      "Iteration: 0100 Loss: 0.79940 Time: 0.00745\n",
      "Iteration: 0101 Loss: 0.83005 Time: 0.00565\n",
      "Iteration: 0102 Loss: 0.81344 Time: 0.00631\n",
      "Iteration: 0103 Loss: 0.78086 Time: 0.00630\n",
      "Iteration: 0104 Loss: 0.80381 Time: 0.00534\n",
      "Iteration: 0105 Loss: 0.78342 Time: 0.00603\n",
      "Iteration: 0106 Loss: 0.80306 Time: 0.00545\n",
      "Iteration: 0107 Loss: 0.79618 Time: 0.00583\n",
      "Iteration: 0108 Loss: 0.77284 Time: 0.00499\n",
      "Iteration: 0109 Loss: 0.79075 Time: 0.00726\n",
      "Iteration: 0110 Loss: 0.77028 Time: 0.00600\n",
      "Iteration: 0111 Loss: 0.79237 Time: 0.00693\n",
      "Iteration: 0112 Loss: 0.74944 Time: 0.00601\n",
      "Iteration: 0113 Loss: 0.75468 Time: 0.00600\n",
      "Iteration: 0114 Loss: 0.78584 Time: 0.00604\n",
      "Iteration: 0115 Loss: 0.76781 Time: 0.00703\n",
      "Iteration: 0116 Loss: 0.76200 Time: 0.00616\n",
      "Iteration: 0117 Loss: 0.75123 Time: 0.00677\n",
      "Iteration: 0118 Loss: 0.75155 Time: 0.00600\n",
      "Iteration: 0119 Loss: 0.76353 Time: 0.00618\n",
      "Iteration: 0120 Loss: 0.74352 Time: 0.00506\n",
      "Iteration: 0121 Loss: 0.75870 Time: 0.00606\n",
      "Iteration: 0122 Loss: 0.76138 Time: 0.00592\n",
      "Iteration: 0123 Loss: 0.73811 Time: 0.00597\n",
      "Iteration: 0124 Loss: 0.76289 Time: 0.00507\n",
      "Iteration: 0125 Loss: 0.74352 Time: 0.00595\n",
      "Iteration: 0126 Loss: 0.73680 Time: 0.00635\n",
      "Iteration: 0127 Loss: 0.73453 Time: 0.00589\n",
      "Iteration: 0128 Loss: 0.74238 Time: 0.00679\n",
      "Iteration: 0129 Loss: 0.72825 Time: 0.00644\n",
      "Iteration: 0130 Loss: 0.74110 Time: 0.00554\n",
      "Iteration: 0131 Loss: 0.73029 Time: 0.00662\n",
      "Iteration: 0132 Loss: 0.73109 Time: 0.00617\n",
      "Iteration: 0133 Loss: 0.72011 Time: 0.00809\n",
      "Iteration: 0134 Loss: 0.73456 Time: 0.00607\n",
      "Iteration: 0135 Loss: 0.70779 Time: 0.00581\n",
      "Iteration: 0136 Loss: 0.71934 Time: 0.00663\n",
      "Iteration: 0137 Loss: 0.72009 Time: 0.00577\n",
      "Iteration: 0138 Loss: 0.72017 Time: 0.00619\n",
      "Iteration: 0139 Loss: 0.70956 Time: 0.00658\n",
      "Iteration: 0140 Loss: 0.71791 Time: 0.00505\n",
      "Iteration: 0141 Loss: 0.72241 Time: 0.00594\n",
      "Iteration: 0142 Loss: 0.71886 Time: 0.00641\n",
      "Iteration: 0143 Loss: 0.70478 Time: 0.00494\n",
      "Iteration: 0144 Loss: 0.71506 Time: 0.00800\n",
      "Iteration: 0145 Loss: 0.69258 Time: 0.00616\n",
      "Iteration: 0146 Loss: 0.71170 Time: 0.00681\n",
      "Iteration: 0147 Loss: 0.71699 Time: 0.00600\n",
      "Iteration: 0148 Loss: 0.70231 Time: 0.00568\n",
      "Iteration: 0149 Loss: 0.71414 Time: 0.00676\n",
      "Iteration: 0150 Loss: 0.70592 Time: 0.00500\n",
      "Iteration: 0151 Loss: 0.69551 Time: 0.00598\n",
      "Iteration: 0152 Loss: 0.70961 Time: 0.00591\n",
      "Iteration: 0153 Loss: 0.69592 Time: 0.00665\n",
      "Iteration: 0154 Loss: 0.69307 Time: 0.00594\n",
      "Iteration: 0155 Loss: 0.70223 Time: 0.00601\n",
      "Iteration: 0156 Loss: 0.68328 Time: 0.00601\n",
      "Iteration: 0157 Loss: 0.70924 Time: 0.00600\n",
      "Iteration: 0158 Loss: 0.69149 Time: 0.00552\n",
      "Iteration: 0159 Loss: 0.67920 Time: 0.00697\n",
      "Iteration: 0160 Loss: 0.67915 Time: 0.00541\n",
      "Iteration: 0161 Loss: 0.69638 Time: 0.00538\n",
      "Iteration: 0162 Loss: 0.67776 Time: 0.00782\n",
      "Iteration: 0163 Loss: 0.69036 Time: 0.00683\n",
      "Iteration: 0164 Loss: 0.68781 Time: 0.00599\n",
      "Iteration: 0165 Loss: 0.68933 Time: 0.00600\n",
      "Iteration: 0166 Loss: 0.68410 Time: 0.00498\n",
      "Iteration: 0167 Loss: 0.68510 Time: 0.00700\n",
      "Iteration: 0168 Loss: 0.68658 Time: 0.00600\n",
      "Iteration: 0169 Loss: 0.67860 Time: 0.00742\n",
      "Iteration: 0170 Loss: 0.67183 Time: 0.00597\n",
      "Iteration: 0171 Loss: 0.68286 Time: 0.00516\n",
      "Iteration: 0172 Loss: 0.67219 Time: 0.00577\n",
      "Iteration: 0173 Loss: 0.67034 Time: 0.00595\n",
      "Iteration: 0174 Loss: 0.67157 Time: 0.00604\n",
      "Iteration: 0175 Loss: 0.68156 Time: 0.00810\n",
      "Iteration: 0176 Loss: 0.67197 Time: 0.00495\n",
      "Iteration: 0177 Loss: 0.67915 Time: 0.00600\n",
      "Iteration: 0178 Loss: 0.66526 Time: 0.00600\n",
      "Iteration: 0179 Loss: 0.66764 Time: 0.00500\n",
      "Iteration: 0180 Loss: 0.66002 Time: 0.00739\n",
      "Iteration: 0181 Loss: 0.66722 Time: 0.00569\n",
      "Iteration: 0182 Loss: 0.65958 Time: 0.00593\n",
      "Iteration: 0183 Loss: 0.66126 Time: 0.00599\n",
      "Iteration: 0184 Loss: 0.65804 Time: 0.00500\n",
      "Iteration: 0185 Loss: 0.66097 Time: 0.00535\n",
      "Iteration: 0186 Loss: 0.67158 Time: 0.00504\n",
      "Iteration: 0187 Loss: 0.67028 Time: 0.00603\n",
      "Iteration: 0188 Loss: 0.66841 Time: 0.00593\n",
      "Iteration: 0189 Loss: 0.67510 Time: 0.00472\n",
      "Iteration: 0190 Loss: 0.66586 Time: 0.00699\n",
      "Iteration: 0191 Loss: 0.66292 Time: 0.00558\n",
      "Iteration: 0192 Loss: 0.66282 Time: 0.00600\n",
      "Iteration: 0193 Loss: 0.66162 Time: 0.00607\n",
      "Iteration: 0194 Loss: 0.65264 Time: 0.00542\n",
      "Iteration: 0195 Loss: 0.66466 Time: 0.00525\n",
      "Iteration: 0196 Loss: 0.65497 Time: 0.00500\n",
      "Iteration: 0197 Loss: 0.65523 Time: 0.00610\n",
      "Iteration: 0198 Loss: 0.64753 Time: 0.00700\n",
      "Iteration: 0199 Loss: 0.64009 Time: 0.00501\n",
      "Iteration: 0200 Loss: 0.65242 Time: 0.00500\n",
      "Done! \n",
      "\n",
      "STEP 3/3 - MODEL EVALUATION \n",
      "\n",
      "Computing the final embedding vectors, for evaluation\n",
      "Done! \n",
      "\n",
      "Testing: link prediction\n",
      "Done! \n",
      "\n",
      "EXPERIMENTS ON MODEL 7 / 10 \n",
      "\n",
      "STEP 1/3 - PREPROCESSING STEPS \n",
      "\n",
      "Masking some edges from the training graph, for link prediction\n",
      "(validation set: 1.0 % of edges - test set: 10.0 % of edges)\n",
      "Done! \n",
      "\n",
      "Preprocessing node features\n",
      "Done! \n",
      "\n",
      "Running the Louvain algorithm for community detection\n",
      "as a preprocessing step for the encoder\n",
      "Done! Louvain has found 7 communities \n",
      "\n",
      "Setting up the model and the optimizer\n",
      "Done! \n",
      "\n",
      "Preprocessing on message passing matrices\n",
      "Done! \n",
      "\n",
      "Initializing TF session\n",
      "Done! \n",
      "\n",
      "STEP 2/3 - MODEL TRAINING \n",
      "\n",
      "Starting training\n",
      "Iteration: 0001 Loss: 1.70666 Time: 0.14112\n",
      "Iteration: 0002 Loss: 1.66576 Time: 0.00701\n",
      "Iteration: 0003 Loss: 1.66446 Time: 0.00599\n",
      "Iteration: 0004 Loss: 1.62661 Time: 0.00501\n",
      "Iteration: 0005 Loss: 1.59749 Time: 0.00699\n",
      "Iteration: 0006 Loss: 1.63937 Time: 0.00599\n",
      "Iteration: 0007 Loss: 1.56491 Time: 0.00599\n",
      "Iteration: 0008 Loss: 1.51383 Time: 0.00546\n",
      "Iteration: 0009 Loss: 1.48011 Time: 0.00500\n",
      "Iteration: 0010 Loss: 1.49434 Time: 0.00768\n",
      "Iteration: 0011 Loss: 1.47020 Time: 0.00501\n",
      "Iteration: 0012 Loss: 1.45138 Time: 0.00701\n",
      "Iteration: 0013 Loss: 1.42797 Time: 0.00660\n",
      "Iteration: 0014 Loss: 1.42537 Time: 0.00597\n",
      "Iteration: 0015 Loss: 1.41518 Time: 0.00598\n",
      "Iteration: 0016 Loss: 1.37700 Time: 0.00601\n",
      "Iteration: 0017 Loss: 1.35961 Time: 0.00650\n",
      "Iteration: 0018 Loss: 1.37239 Time: 0.00750\n",
      "Iteration: 0019 Loss: 1.33156 Time: 0.00600\n",
      "Iteration: 0020 Loss: 1.34620 Time: 0.00746\n",
      "Iteration: 0021 Loss: 1.28252 Time: 0.00595\n",
      "Iteration: 0022 Loss: 1.27279 Time: 0.00600\n",
      "Iteration: 0023 Loss: 1.29096 Time: 0.00621\n",
      "Iteration: 0024 Loss: 1.26829 Time: 0.00501\n",
      "Iteration: 0025 Loss: 1.27395 Time: 0.00700\n",
      "Iteration: 0026 Loss: 1.27661 Time: 0.00627\n",
      "Iteration: 0027 Loss: 1.20744 Time: 0.00608\n",
      "Iteration: 0028 Loss: 1.22173 Time: 0.00766\n",
      "Iteration: 0029 Loss: 1.17573 Time: 0.00699\n",
      "Iteration: 0030 Loss: 1.23427 Time: 0.00700\n",
      "Iteration: 0031 Loss: 1.20725 Time: 0.00601\n",
      "Iteration: 0032 Loss: 1.21962 Time: 0.00614\n",
      "Iteration: 0033 Loss: 1.16090 Time: 0.00647\n",
      "Iteration: 0034 Loss: 1.12886 Time: 0.00600\n",
      "Iteration: 0035 Loss: 1.10037 Time: 0.00603\n",
      "Iteration: 0036 Loss: 1.11211 Time: 0.00602\n",
      "Iteration: 0037 Loss: 1.12783 Time: 0.00594\n",
      "Iteration: 0038 Loss: 1.16867 Time: 0.00706\n",
      "Iteration: 0039 Loss: 1.10556 Time: 0.00818\n",
      "Iteration: 0040 Loss: 1.09052 Time: 0.00581\n",
      "Iteration: 0041 Loss: 1.08402 Time: 0.00600\n",
      "Iteration: 0042 Loss: 1.06091 Time: 0.00515\n",
      "Iteration: 0043 Loss: 1.09817 Time: 0.00800\n",
      "Iteration: 0044 Loss: 1.06819 Time: 0.00594\n",
      "Iteration: 0045 Loss: 1.06451 Time: 0.00591\n",
      "Iteration: 0046 Loss: 1.06385 Time: 0.00600\n",
      "Iteration: 0047 Loss: 1.05848 Time: 0.00399\n",
      "Iteration: 0048 Loss: 1.03740 Time: 0.00638\n",
      "Iteration: 0049 Loss: 1.02279 Time: 0.00695\n",
      "Iteration: 0050 Loss: 1.02440 Time: 0.00593\n",
      "Iteration: 0051 Loss: 1.04306 Time: 0.00600\n",
      "Iteration: 0052 Loss: 1.01386 Time: 0.00560\n",
      "Iteration: 0053 Loss: 1.03268 Time: 0.00592\n",
      "Iteration: 0054 Loss: 1.01056 Time: 0.00644\n",
      "Iteration: 0055 Loss: 0.97433 Time: 0.00555\n",
      "Iteration: 0056 Loss: 0.98713 Time: 0.00723\n",
      "Iteration: 0057 Loss: 0.98705 Time: 0.00507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0058 Loss: 0.97912 Time: 0.00593\n",
      "Iteration: 0059 Loss: 0.94925 Time: 0.00600\n",
      "Iteration: 0060 Loss: 0.95373 Time: 0.00601\n",
      "Iteration: 0061 Loss: 0.94117 Time: 0.00599\n",
      "Iteration: 0062 Loss: 0.94053 Time: 0.00600\n",
      "Iteration: 0063 Loss: 0.95653 Time: 0.00555\n",
      "Iteration: 0064 Loss: 0.94930 Time: 0.00664\n",
      "Iteration: 0065 Loss: 0.93188 Time: 0.00584\n",
      "Iteration: 0066 Loss: 0.94249 Time: 0.00598\n",
      "Iteration: 0067 Loss: 0.92834 Time: 0.00608\n",
      "Iteration: 0068 Loss: 0.91061 Time: 0.00624\n",
      "Iteration: 0069 Loss: 0.93254 Time: 0.00569\n",
      "Iteration: 0070 Loss: 0.93046 Time: 0.00599\n",
      "Iteration: 0071 Loss: 0.91567 Time: 0.00558\n",
      "Iteration: 0072 Loss: 0.91153 Time: 0.00601\n",
      "Iteration: 0073 Loss: 0.91794 Time: 0.00631\n",
      "Iteration: 0074 Loss: 0.91132 Time: 0.00564\n",
      "Iteration: 0075 Loss: 0.89671 Time: 0.00610\n",
      "Iteration: 0076 Loss: 0.89224 Time: 0.00491\n",
      "Iteration: 0077 Loss: 0.89410 Time: 0.00600\n",
      "Iteration: 0078 Loss: 0.87614 Time: 0.00520\n",
      "Iteration: 0079 Loss: 0.86864 Time: 0.00580\n",
      "Iteration: 0080 Loss: 0.86873 Time: 0.00599\n",
      "Iteration: 0081 Loss: 0.84851 Time: 0.00499\n",
      "Iteration: 0082 Loss: 0.86518 Time: 0.00611\n",
      "Iteration: 0083 Loss: 0.85718 Time: 0.00561\n",
      "Iteration: 0084 Loss: 0.86108 Time: 0.00562\n",
      "Iteration: 0085 Loss: 0.85493 Time: 0.00582\n",
      "Iteration: 0086 Loss: 0.85948 Time: 0.00538\n",
      "Iteration: 0087 Loss: 0.83627 Time: 0.00608\n",
      "Iteration: 0088 Loss: 0.85030 Time: 0.00655\n",
      "Iteration: 0089 Loss: 0.84457 Time: 0.00600\n",
      "Iteration: 0090 Loss: 0.82072 Time: 0.00700\n",
      "Iteration: 0091 Loss: 0.82771 Time: 0.00700\n",
      "Iteration: 0092 Loss: 0.82929 Time: 0.00622\n",
      "Iteration: 0093 Loss: 0.81380 Time: 0.00678\n",
      "Iteration: 0094 Loss: 0.80973 Time: 0.00600\n",
      "Iteration: 0095 Loss: 0.80633 Time: 0.00543\n",
      "Iteration: 0096 Loss: 0.82298 Time: 0.00523\n",
      "Iteration: 0097 Loss: 0.81621 Time: 0.00597\n",
      "Iteration: 0098 Loss: 0.80788 Time: 0.00681\n",
      "Iteration: 0099 Loss: 0.80795 Time: 0.00599\n",
      "Iteration: 0100 Loss: 0.78215 Time: 0.00642\n",
      "Iteration: 0101 Loss: 0.78357 Time: 0.00658\n",
      "Iteration: 0102 Loss: 0.79402 Time: 0.00511\n",
      "Iteration: 0103 Loss: 0.79158 Time: 0.00587\n",
      "Iteration: 0104 Loss: 0.77986 Time: 0.00600\n",
      "Iteration: 0105 Loss: 0.79972 Time: 0.00600\n",
      "Iteration: 0106 Loss: 0.79199 Time: 0.00900\n",
      "Iteration: 0107 Loss: 0.78112 Time: 0.00706\n",
      "Iteration: 0108 Loss: 0.77113 Time: 0.00694\n",
      "Iteration: 0109 Loss: 0.77083 Time: 0.00645\n",
      "Iteration: 0110 Loss: 0.75080 Time: 0.00555\n",
      "Iteration: 0111 Loss: 0.77253 Time: 0.00617\n",
      "Iteration: 0112 Loss: 0.76792 Time: 0.00618\n",
      "Iteration: 0113 Loss: 0.77450 Time: 0.00644\n",
      "Iteration: 0114 Loss: 0.76602 Time: 0.00522\n",
      "Iteration: 0115 Loss: 0.74732 Time: 0.00600\n",
      "Iteration: 0116 Loss: 0.75842 Time: 0.00500\n",
      "Iteration: 0117 Loss: 0.75593 Time: 0.00506\n",
      "Iteration: 0118 Loss: 0.75272 Time: 0.00530\n",
      "Iteration: 0119 Loss: 0.74659 Time: 0.00669\n",
      "Iteration: 0120 Loss: 0.73558 Time: 0.00701\n",
      "Iteration: 0121 Loss: 0.74456 Time: 0.00599\n",
      "Iteration: 0122 Loss: 0.74344 Time: 0.00558\n",
      "Iteration: 0123 Loss: 0.75903 Time: 0.00542\n",
      "Iteration: 0124 Loss: 0.74608 Time: 0.00636\n",
      "Iteration: 0125 Loss: 0.73606 Time: 0.00625\n",
      "Iteration: 0126 Loss: 0.73122 Time: 0.00574\n",
      "Iteration: 0127 Loss: 0.73701 Time: 0.00600\n",
      "Iteration: 0128 Loss: 0.74026 Time: 0.00660\n",
      "Iteration: 0129 Loss: 0.71635 Time: 0.00537\n",
      "Iteration: 0130 Loss: 0.71228 Time: 0.00601\n",
      "Iteration: 0131 Loss: 0.73004 Time: 0.00651\n",
      "Iteration: 0132 Loss: 0.71655 Time: 0.00582\n",
      "Iteration: 0133 Loss: 0.72322 Time: 0.00666\n",
      "Iteration: 0134 Loss: 0.71309 Time: 0.00600\n",
      "Iteration: 0135 Loss: 0.72436 Time: 0.00601\n",
      "Iteration: 0136 Loss: 0.72348 Time: 0.00599\n",
      "Iteration: 0137 Loss: 0.70410 Time: 0.00700\n",
      "Iteration: 0138 Loss: 0.73479 Time: 0.00629\n",
      "Iteration: 0139 Loss: 0.70985 Time: 0.00578\n",
      "Iteration: 0140 Loss: 0.72843 Time: 0.00618\n",
      "Iteration: 0141 Loss: 0.70401 Time: 0.00624\n",
      "Iteration: 0142 Loss: 0.71511 Time: 0.00581\n",
      "Iteration: 0143 Loss: 0.68662 Time: 0.00694\n",
      "Iteration: 0144 Loss: 0.70139 Time: 0.00601\n",
      "Iteration: 0145 Loss: 0.70221 Time: 0.00543\n",
      "Iteration: 0146 Loss: 0.70014 Time: 0.00494\n",
      "Iteration: 0147 Loss: 0.69519 Time: 0.00499\n",
      "Iteration: 0148 Loss: 0.69209 Time: 0.00665\n",
      "Iteration: 0149 Loss: 0.69222 Time: 0.00697\n",
      "Iteration: 0150 Loss: 0.69643 Time: 0.00499\n",
      "Iteration: 0151 Loss: 0.68879 Time: 0.00800\n",
      "Iteration: 0152 Loss: 0.70563 Time: 0.00638\n",
      "Iteration: 0153 Loss: 0.68539 Time: 0.00562\n",
      "Iteration: 0154 Loss: 0.69438 Time: 0.00618\n",
      "Iteration: 0155 Loss: 0.67947 Time: 0.00554\n",
      "Iteration: 0156 Loss: 0.68521 Time: 0.00597\n",
      "Iteration: 0157 Loss: 0.67327 Time: 0.00646\n",
      "Iteration: 0158 Loss: 0.68418 Time: 0.00553\n",
      "Iteration: 0159 Loss: 0.68744 Time: 0.00652\n",
      "Iteration: 0160 Loss: 0.68172 Time: 0.00649\n",
      "Iteration: 0161 Loss: 0.68095 Time: 0.00596\n",
      "Iteration: 0162 Loss: 0.68024 Time: 0.00600\n",
      "Iteration: 0163 Loss: 0.68614 Time: 0.00500\n",
      "Iteration: 0164 Loss: 0.67537 Time: 0.00599\n",
      "Iteration: 0165 Loss: 0.67591 Time: 0.00601\n",
      "Iteration: 0166 Loss: 0.67272 Time: 0.00628\n",
      "Iteration: 0167 Loss: 0.66029 Time: 0.00672\n",
      "Iteration: 0168 Loss: 0.66729 Time: 0.00501\n",
      "Iteration: 0169 Loss: 0.66882 Time: 0.00599\n",
      "Iteration: 0170 Loss: 0.67164 Time: 0.00701\n",
      "Iteration: 0171 Loss: 0.67186 Time: 0.00673\n",
      "Iteration: 0172 Loss: 0.68124 Time: 0.00592\n",
      "Iteration: 0173 Loss: 0.66457 Time: 0.00495\n",
      "Iteration: 0174 Loss: 0.66557 Time: 0.00602\n",
      "Iteration: 0175 Loss: 0.66452 Time: 0.00653\n",
      "Iteration: 0176 Loss: 0.66804 Time: 0.00581\n",
      "Iteration: 0177 Loss: 0.66189 Time: 0.00600\n",
      "Iteration: 0178 Loss: 0.65052 Time: 0.00636\n",
      "Iteration: 0179 Loss: 0.65462 Time: 0.00600\n",
      "Iteration: 0180 Loss: 0.65781 Time: 0.00600\n",
      "Iteration: 0181 Loss: 0.67140 Time: 0.00799\n",
      "Iteration: 0182 Loss: 0.66685 Time: 0.00504\n",
      "Iteration: 0183 Loss: 0.66943 Time: 0.00635\n",
      "Iteration: 0184 Loss: 0.66300 Time: 0.00506\n",
      "Iteration: 0185 Loss: 0.65363 Time: 0.00695\n",
      "Iteration: 0186 Loss: 0.66529 Time: 0.00600\n",
      "Iteration: 0187 Loss: 0.65701 Time: 0.00530\n",
      "Iteration: 0188 Loss: 0.65741 Time: 0.00605\n",
      "Iteration: 0189 Loss: 0.64525 Time: 0.00610\n",
      "Iteration: 0190 Loss: 0.64489 Time: 0.00599\n",
      "Iteration: 0191 Loss: 0.64942 Time: 0.00700\n",
      "Iteration: 0192 Loss: 0.66391 Time: 0.00700\n",
      "Iteration: 0193 Loss: 0.64638 Time: 0.00570\n",
      "Iteration: 0194 Loss: 0.63994 Time: 0.00501\n",
      "Iteration: 0195 Loss: 0.65229 Time: 0.00526\n",
      "Iteration: 0196 Loss: 0.64639 Time: 0.00575\n",
      "Iteration: 0197 Loss: 0.64987 Time: 0.00500\n",
      "Iteration: 0198 Loss: 0.65316 Time: 0.00535\n",
      "Iteration: 0199 Loss: 0.64167 Time: 0.00622\n",
      "Iteration: 0200 Loss: 0.64415 Time: 0.00599\n",
      "Done! \n",
      "\n",
      "STEP 3/3 - MODEL EVALUATION \n",
      "\n",
      "Computing the final embedding vectors, for evaluation\n",
      "Done! \n",
      "\n",
      "Testing: link prediction\n",
      "Done! \n",
      "\n",
      "EXPERIMENTS ON MODEL 8 / 10 \n",
      "\n",
      "STEP 1/3 - PREPROCESSING STEPS \n",
      "\n",
      "Masking some edges from the training graph, for link prediction\n",
      "(validation set: 1.0 % of edges - test set: 10.0 % of edges)\n",
      "Done! \n",
      "\n",
      "Preprocessing node features\n",
      "Done! \n",
      "\n",
      "Running the Louvain algorithm for community detection\n",
      "as a preprocessing step for the encoder\n",
      "Done! Louvain has found 8 communities \n",
      "\n",
      "Setting up the model and the optimizer\n",
      "Done! \n",
      "\n",
      "Preprocessing on message passing matrices\n",
      "Done! \n",
      "\n",
      "Initializing TF session\n",
      "Done! \n",
      "\n",
      "STEP 2/3 - MODEL TRAINING \n",
      "\n",
      "Starting training\n",
      "Iteration: 0001 Loss: 1.77976 Time: 0.15700\n",
      "Iteration: 0002 Loss: 1.65679 Time: 0.00700\n",
      "Iteration: 0003 Loss: 1.72340 Time: 0.00701\n",
      "Iteration: 0004 Loss: 1.64095 Time: 0.00600\n",
      "Iteration: 0005 Loss: 1.66990 Time: 0.00599\n",
      "Iteration: 0006 Loss: 1.61645 Time: 0.00600\n",
      "Iteration: 0007 Loss: 1.58962 Time: 0.00900\n",
      "Iteration: 0008 Loss: 1.57522 Time: 0.00599\n",
      "Iteration: 0009 Loss: 1.55468 Time: 0.00601\n",
      "Iteration: 0010 Loss: 1.51809 Time: 0.00718\n",
      "Iteration: 0011 Loss: 1.51325 Time: 0.00604\n",
      "Iteration: 0012 Loss: 1.49362 Time: 0.00599\n",
      "Iteration: 0013 Loss: 1.43980 Time: 0.00499\n",
      "Iteration: 0014 Loss: 1.43213 Time: 0.00703\n",
      "Iteration: 0015 Loss: 1.38584 Time: 0.00600\n",
      "Iteration: 0016 Loss: 1.34517 Time: 0.00501\n",
      "Iteration: 0017 Loss: 1.35929 Time: 0.00699\n",
      "Iteration: 0018 Loss: 1.39329 Time: 0.00700\n",
      "Iteration: 0019 Loss: 1.30662 Time: 0.00600\n",
      "Iteration: 0020 Loss: 1.29528 Time: 0.00635\n",
      "Iteration: 0021 Loss: 1.35042 Time: 0.00599\n",
      "Iteration: 0022 Loss: 1.27231 Time: 0.00597\n",
      "Iteration: 0023 Loss: 1.30195 Time: 0.00709\n",
      "Iteration: 0024 Loss: 1.29505 Time: 0.00591\n",
      "Iteration: 0025 Loss: 1.21677 Time: 0.00600\n",
      "Iteration: 0026 Loss: 1.23258 Time: 0.00639\n",
      "Iteration: 0027 Loss: 1.24974 Time: 0.00593\n",
      "Iteration: 0028 Loss: 1.25196 Time: 0.00672\n",
      "Iteration: 0029 Loss: 1.25521 Time: 0.00500\n",
      "Iteration: 0030 Loss: 1.22432 Time: 0.00506\n",
      "Iteration: 0031 Loss: 1.20758 Time: 0.00694\n",
      "Iteration: 0032 Loss: 1.19488 Time: 0.00626\n",
      "Iteration: 0033 Loss: 1.17034 Time: 0.00498\n",
      "Iteration: 0034 Loss: 1.18807 Time: 0.00501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0035 Loss: 1.15725 Time: 0.00699\n",
      "Iteration: 0036 Loss: 1.14616 Time: 0.00705\n",
      "Iteration: 0037 Loss: 1.14095 Time: 0.00696\n",
      "Iteration: 0038 Loss: 1.09630 Time: 0.00595\n",
      "Iteration: 0039 Loss: 1.14526 Time: 0.00613\n",
      "Iteration: 0040 Loss: 1.08820 Time: 0.00547\n",
      "Iteration: 0041 Loss: 1.11487 Time: 0.00679\n",
      "Iteration: 0042 Loss: 1.08900 Time: 0.00619\n",
      "Iteration: 0043 Loss: 1.04229 Time: 0.00500\n",
      "Iteration: 0044 Loss: 1.10107 Time: 0.00600\n",
      "Iteration: 0045 Loss: 1.04805 Time: 0.00605\n",
      "Iteration: 0046 Loss: 1.07622 Time: 0.00595\n",
      "Iteration: 0047 Loss: 1.06323 Time: 0.00662\n",
      "Iteration: 0048 Loss: 1.04604 Time: 0.00639\n",
      "Iteration: 0049 Loss: 1.03346 Time: 0.00506\n",
      "Iteration: 0050 Loss: 1.04997 Time: 0.00616\n",
      "Iteration: 0051 Loss: 1.03402 Time: 0.00578\n",
      "Iteration: 0052 Loss: 1.04222 Time: 0.00642\n",
      "Iteration: 0053 Loss: 1.02331 Time: 0.00590\n",
      "Iteration: 0054 Loss: 0.99554 Time: 0.00703\n",
      "Iteration: 0055 Loss: 1.04001 Time: 0.00750\n",
      "Iteration: 0056 Loss: 0.99865 Time: 0.00501\n",
      "Iteration: 0057 Loss: 1.00062 Time: 0.00692\n",
      "Iteration: 0058 Loss: 0.97137 Time: 0.00600\n",
      "Iteration: 0059 Loss: 1.01996 Time: 0.00800\n",
      "Iteration: 0060 Loss: 0.95719 Time: 0.00552\n",
      "Iteration: 0061 Loss: 0.97059 Time: 0.00500\n",
      "Iteration: 0062 Loss: 0.97654 Time: 0.00608\n",
      "Iteration: 0063 Loss: 0.96842 Time: 0.00538\n",
      "Iteration: 0064 Loss: 0.97712 Time: 0.00455\n",
      "Iteration: 0065 Loss: 0.93699 Time: 0.00626\n",
      "Iteration: 0066 Loss: 0.94046 Time: 0.00574\n",
      "Iteration: 0067 Loss: 0.93837 Time: 0.00600\n",
      "Iteration: 0068 Loss: 0.91242 Time: 0.00747\n",
      "Iteration: 0069 Loss: 0.92341 Time: 0.00557\n",
      "Iteration: 0070 Loss: 0.93809 Time: 0.00716\n",
      "Iteration: 0071 Loss: 0.91633 Time: 0.00573\n",
      "Iteration: 0072 Loss: 0.91698 Time: 0.00703\n",
      "Iteration: 0073 Loss: 0.92127 Time: 0.00636\n",
      "Iteration: 0074 Loss: 0.91355 Time: 0.00508\n",
      "Iteration: 0075 Loss: 0.91229 Time: 0.00832\n",
      "Iteration: 0076 Loss: 0.90457 Time: 0.00559\n",
      "Iteration: 0077 Loss: 0.89837 Time: 0.00700\n",
      "Iteration: 0078 Loss: 0.90247 Time: 0.00652\n",
      "Iteration: 0079 Loss: 0.88619 Time: 0.00602\n",
      "Iteration: 0080 Loss: 0.90530 Time: 0.01004\n",
      "Iteration: 0081 Loss: 0.90522 Time: 0.00700\n",
      "Iteration: 0082 Loss: 0.86730 Time: 0.00700\n",
      "Iteration: 0083 Loss: 0.85686 Time: 0.00529\n",
      "Iteration: 0084 Loss: 0.89872 Time: 0.00667\n",
      "Iteration: 0085 Loss: 0.88472 Time: 0.00789\n",
      "Iteration: 0086 Loss: 0.84519 Time: 0.00594\n",
      "Iteration: 0087 Loss: 0.84611 Time: 0.00800\n",
      "Iteration: 0088 Loss: 0.85001 Time: 0.00524\n",
      "Iteration: 0089 Loss: 0.84841 Time: 0.00578\n",
      "Iteration: 0090 Loss: 0.85538 Time: 0.00798\n",
      "Iteration: 0091 Loss: 0.84129 Time: 0.00800\n",
      "Iteration: 0092 Loss: 0.84377 Time: 0.00801\n",
      "Iteration: 0093 Loss: 0.83626 Time: 0.00616\n",
      "Iteration: 0094 Loss: 0.85209 Time: 0.00710\n",
      "Iteration: 0095 Loss: 0.84931 Time: 0.00574\n",
      "Iteration: 0096 Loss: 0.84297 Time: 0.00699\n",
      "Iteration: 0097 Loss: 0.82528 Time: 0.00700\n",
      "Iteration: 0098 Loss: 0.82440 Time: 0.00600\n",
      "Iteration: 0099 Loss: 0.81980 Time: 0.00754\n",
      "Iteration: 0100 Loss: 0.81176 Time: 0.00549\n",
      "Iteration: 0101 Loss: 0.81817 Time: 0.00596\n",
      "Iteration: 0102 Loss: 0.82056 Time: 0.00601\n",
      "Iteration: 0103 Loss: 0.79977 Time: 0.00636\n",
      "Iteration: 0104 Loss: 0.78730 Time: 0.00630\n",
      "Iteration: 0105 Loss: 0.78921 Time: 0.00559\n",
      "Iteration: 0106 Loss: 0.78007 Time: 0.00641\n",
      "Iteration: 0107 Loss: 0.80265 Time: 0.00700\n",
      "Iteration: 0108 Loss: 0.77620 Time: 0.00600\n",
      "Iteration: 0109 Loss: 0.77975 Time: 0.00600\n",
      "Iteration: 0110 Loss: 0.78335 Time: 0.00623\n",
      "Iteration: 0111 Loss: 0.80247 Time: 0.00582\n",
      "Iteration: 0112 Loss: 0.78996 Time: 0.00604\n",
      "Iteration: 0113 Loss: 0.76105 Time: 0.00642\n",
      "Iteration: 0114 Loss: 0.74574 Time: 0.00654\n",
      "Iteration: 0115 Loss: 0.76008 Time: 0.00701\n",
      "Iteration: 0116 Loss: 0.77605 Time: 0.00599\n",
      "Iteration: 0117 Loss: 0.77943 Time: 0.00616\n",
      "Iteration: 0118 Loss: 0.74922 Time: 0.00590\n",
      "Iteration: 0119 Loss: 0.74076 Time: 0.00589\n",
      "Iteration: 0120 Loss: 0.75920 Time: 0.00597\n",
      "Iteration: 0121 Loss: 0.74847 Time: 0.00642\n",
      "Iteration: 0122 Loss: 0.75288 Time: 0.00636\n",
      "Iteration: 0123 Loss: 0.76503 Time: 0.00700\n",
      "Iteration: 0124 Loss: 0.74758 Time: 0.00601\n",
      "Iteration: 0125 Loss: 0.74555 Time: 0.00602\n",
      "Iteration: 0126 Loss: 0.73673 Time: 0.00696\n",
      "Iteration: 0127 Loss: 0.73498 Time: 0.00612\n",
      "Iteration: 0128 Loss: 0.74031 Time: 0.00594\n",
      "Iteration: 0129 Loss: 0.75276 Time: 0.00594\n",
      "Iteration: 0130 Loss: 0.72208 Time: 0.00612\n",
      "Iteration: 0131 Loss: 0.72490 Time: 0.00648\n",
      "Iteration: 0132 Loss: 0.73161 Time: 0.00701\n",
      "Iteration: 0133 Loss: 0.72257 Time: 0.00617\n",
      "Iteration: 0134 Loss: 0.72643 Time: 0.00583\n",
      "Iteration: 0135 Loss: 0.71231 Time: 0.00727\n",
      "Iteration: 0136 Loss: 0.72295 Time: 0.00673\n",
      "Iteration: 0137 Loss: 0.71367 Time: 0.00623\n",
      "Iteration: 0138 Loss: 0.72147 Time: 0.00600\n",
      "Iteration: 0139 Loss: 0.71472 Time: 0.00601\n",
      "Iteration: 0140 Loss: 0.70326 Time: 0.00717\n",
      "Iteration: 0141 Loss: 0.71548 Time: 0.00583\n",
      "Iteration: 0142 Loss: 0.69997 Time: 0.00600\n",
      "Iteration: 0143 Loss: 0.70845 Time: 0.00600\n",
      "Iteration: 0144 Loss: 0.70901 Time: 0.00613\n",
      "Iteration: 0145 Loss: 0.70755 Time: 0.00702\n",
      "Iteration: 0146 Loss: 0.70926 Time: 0.00698\n",
      "Iteration: 0147 Loss: 0.69317 Time: 0.00699\n",
      "Iteration: 0148 Loss: 0.69995 Time: 0.00700\n",
      "Iteration: 0149 Loss: 0.69680 Time: 0.00711\n",
      "Iteration: 0150 Loss: 0.70207 Time: 0.00701\n",
      "Iteration: 0151 Loss: 0.70846 Time: 0.00800\n",
      "Iteration: 0152 Loss: 0.70052 Time: 0.00704\n",
      "Iteration: 0153 Loss: 0.69541 Time: 0.00699\n",
      "Iteration: 0154 Loss: 0.68814 Time: 0.00800\n",
      "Iteration: 0155 Loss: 0.69144 Time: 0.00605\n",
      "Iteration: 0156 Loss: 0.69844 Time: 0.00796\n",
      "Iteration: 0157 Loss: 0.68608 Time: 0.00600\n",
      "Iteration: 0158 Loss: 0.69062 Time: 0.00701\n",
      "Iteration: 0159 Loss: 0.69715 Time: 0.00599\n",
      "Iteration: 0160 Loss: 0.67911 Time: 0.00629\n",
      "Iteration: 0161 Loss: 0.69786 Time: 0.00590\n",
      "Iteration: 0162 Loss: 0.69686 Time: 0.00601\n",
      "Iteration: 0163 Loss: 0.68334 Time: 0.00600\n",
      "Iteration: 0164 Loss: 0.69007 Time: 0.00699\n",
      "Iteration: 0165 Loss: 0.67329 Time: 0.00600\n",
      "Iteration: 0166 Loss: 0.66529 Time: 0.00701\n",
      "Iteration: 0167 Loss: 0.68141 Time: 0.00699\n",
      "Iteration: 0168 Loss: 0.67964 Time: 0.00701\n",
      "Iteration: 0169 Loss: 0.67648 Time: 0.00600\n",
      "Iteration: 0170 Loss: 0.67350 Time: 0.00700\n",
      "Iteration: 0171 Loss: 0.68649 Time: 0.00615\n",
      "Iteration: 0172 Loss: 0.67530 Time: 0.00685\n",
      "Iteration: 0173 Loss: 0.67175 Time: 0.00702\n",
      "Iteration: 0174 Loss: 0.67471 Time: 0.00600\n",
      "Iteration: 0175 Loss: 0.66415 Time: 0.00604\n",
      "Iteration: 0176 Loss: 0.66836 Time: 0.00796\n",
      "Iteration: 0177 Loss: 0.67325 Time: 0.00800\n",
      "Iteration: 0178 Loss: 0.66706 Time: 0.00704\n",
      "Iteration: 0179 Loss: 0.65261 Time: 0.00600\n",
      "Iteration: 0180 Loss: 0.65447 Time: 0.00800\n",
      "Iteration: 0181 Loss: 0.66287 Time: 0.00600\n",
      "Iteration: 0182 Loss: 0.67283 Time: 0.00600\n",
      "Iteration: 0183 Loss: 0.67413 Time: 0.00599\n",
      "Iteration: 0184 Loss: 0.66659 Time: 0.00602\n",
      "Iteration: 0185 Loss: 0.66610 Time: 0.00699\n",
      "Iteration: 0186 Loss: 0.66563 Time: 0.00700\n",
      "Iteration: 0187 Loss: 0.66316 Time: 0.00755\n",
      "Iteration: 0188 Loss: 0.66397 Time: 0.00700\n",
      "Iteration: 0189 Loss: 0.66927 Time: 0.00701\n",
      "Iteration: 0190 Loss: 0.66072 Time: 0.00642\n",
      "Iteration: 0191 Loss: 0.65761 Time: 0.00657\n",
      "Iteration: 0192 Loss: 0.65338 Time: 0.00799\n",
      "Iteration: 0193 Loss: 0.65341 Time: 0.00601\n",
      "Iteration: 0194 Loss: 0.66411 Time: 0.00799\n",
      "Iteration: 0195 Loss: 0.66509 Time: 0.00630\n",
      "Iteration: 0196 Loss: 0.65347 Time: 0.00671\n",
      "Iteration: 0197 Loss: 0.65322 Time: 0.00699\n",
      "Iteration: 0198 Loss: 0.65761 Time: 0.00601\n",
      "Iteration: 0199 Loss: 0.65037 Time: 0.00699\n",
      "Iteration: 0200 Loss: 0.65803 Time: 0.00618\n",
      "Done! \n",
      "\n",
      "STEP 3/3 - MODEL EVALUATION \n",
      "\n",
      "Computing the final embedding vectors, for evaluation\n",
      "Done! \n",
      "\n",
      "Testing: link prediction\n",
      "Done! \n",
      "\n",
      "EXPERIMENTS ON MODEL 9 / 10 \n",
      "\n",
      "STEP 1/3 - PREPROCESSING STEPS \n",
      "\n",
      "Masking some edges from the training graph, for link prediction\n",
      "(validation set: 1.0 % of edges - test set: 10.0 % of edges)\n",
      "Done! \n",
      "\n",
      "Preprocessing node features\n",
      "Done! \n",
      "\n",
      "Running the Louvain algorithm for community detection\n",
      "as a preprocessing step for the encoder\n",
      "Done! Louvain has found 5 communities \n",
      "\n",
      "Setting up the model and the optimizer\n",
      "Done! \n",
      "\n",
      "Preprocessing on message passing matrices\n",
      "Done! \n",
      "\n",
      "Initializing TF session\n",
      "Done! \n",
      "\n",
      "STEP 2/3 - MODEL TRAINING \n",
      "\n",
      "Starting training\n",
      "Iteration: 0001 Loss: 1.68898 Time: 0.20611\n",
      "Iteration: 0002 Loss: 1.71977 Time: 0.00699\n",
      "Iteration: 0003 Loss: 1.68129 Time: 0.00700\n",
      "Iteration: 0004 Loss: 1.63279 Time: 0.00700\n",
      "Iteration: 0005 Loss: 1.61239 Time: 0.00700\n",
      "Iteration: 0006 Loss: 1.59346 Time: 0.00701\n",
      "Iteration: 0007 Loss: 1.51487 Time: 0.00599\n",
      "Iteration: 0008 Loss: 1.54599 Time: 0.00700\n",
      "Iteration: 0009 Loss: 1.60390 Time: 0.00601\n",
      "Iteration: 0010 Loss: 1.49487 Time: 0.00699\n",
      "Iteration: 0011 Loss: 1.50731 Time: 0.01000\n",
      "Iteration: 0012 Loss: 1.46278 Time: 0.00900\n",
      "Iteration: 0013 Loss: 1.46674 Time: 0.00900\n",
      "Iteration: 0014 Loss: 1.46022 Time: 0.00800\n",
      "Iteration: 0015 Loss: 1.41347 Time: 0.00700\n",
      "Iteration: 0016 Loss: 1.40801 Time: 0.00700\n",
      "Iteration: 0017 Loss: 1.39637 Time: 0.00699\n",
      "Iteration: 0018 Loss: 1.36487 Time: 0.00800\n",
      "Iteration: 0019 Loss: 1.38960 Time: 0.00840\n",
      "Iteration: 0020 Loss: 1.32992 Time: 0.00861\n",
      "Iteration: 0021 Loss: 1.32946 Time: 0.00800\n",
      "Iteration: 0022 Loss: 1.28460 Time: 0.00800\n",
      "Iteration: 0023 Loss: 1.29515 Time: 0.00700\n",
      "Iteration: 0024 Loss: 1.27790 Time: 0.00700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0025 Loss: 1.25711 Time: 0.00800\n",
      "Iteration: 0026 Loss: 1.24995 Time: 0.00599\n",
      "Iteration: 0027 Loss: 1.22569 Time: 0.00700\n",
      "Iteration: 0028 Loss: 1.22359 Time: 0.00800\n",
      "Iteration: 0029 Loss: 1.24255 Time: 0.00701\n",
      "Iteration: 0030 Loss: 1.19367 Time: 0.00699\n",
      "Iteration: 0031 Loss: 1.21768 Time: 0.00700\n",
      "Iteration: 0032 Loss: 1.15309 Time: 0.00710\n",
      "Iteration: 0033 Loss: 1.15250 Time: 0.00637\n",
      "Iteration: 0034 Loss: 1.19188 Time: 0.00764\n",
      "Iteration: 0035 Loss: 1.11184 Time: 0.00553\n",
      "Iteration: 0036 Loss: 1.14076 Time: 0.00748\n",
      "Iteration: 0037 Loss: 1.12922 Time: 0.00601\n",
      "Iteration: 0038 Loss: 1.14884 Time: 0.00599\n",
      "Iteration: 0039 Loss: 1.13160 Time: 0.00720\n",
      "Iteration: 0040 Loss: 1.09505 Time: 0.00581\n",
      "Iteration: 0041 Loss: 1.09388 Time: 0.00699\n",
      "Iteration: 0042 Loss: 1.09638 Time: 0.00700\n",
      "Iteration: 0043 Loss: 1.11032 Time: 0.00617\n",
      "Iteration: 0044 Loss: 1.07253 Time: 0.00783\n",
      "Iteration: 0045 Loss: 1.06246 Time: 0.00601\n",
      "Iteration: 0046 Loss: 1.06042 Time: 0.00800\n",
      "Iteration: 0047 Loss: 1.04472 Time: 0.00902\n",
      "Iteration: 0048 Loss: 1.03288 Time: 0.00798\n",
      "Iteration: 0049 Loss: 1.02668 Time: 0.00694\n",
      "Iteration: 0050 Loss: 1.04116 Time: 0.00711\n",
      "Iteration: 0051 Loss: 1.03095 Time: 0.00731\n",
      "Iteration: 0052 Loss: 1.00830 Time: 0.00658\n",
      "Iteration: 0053 Loss: 1.01385 Time: 0.00700\n",
      "Iteration: 0054 Loss: 0.98715 Time: 0.00700\n",
      "Iteration: 0055 Loss: 0.99377 Time: 0.00700\n",
      "Iteration: 0056 Loss: 1.00324 Time: 0.00700\n",
      "Iteration: 0057 Loss: 0.99195 Time: 0.00701\n",
      "Iteration: 0058 Loss: 0.96781 Time: 0.00622\n",
      "Iteration: 0059 Loss: 0.97129 Time: 0.00622\n",
      "Iteration: 0060 Loss: 0.99108 Time: 0.00562\n",
      "Iteration: 0061 Loss: 0.95275 Time: 0.00594\n",
      "Iteration: 0062 Loss: 0.94318 Time: 0.00653\n",
      "Iteration: 0063 Loss: 0.96315 Time: 0.00600\n",
      "Iteration: 0064 Loss: 0.93577 Time: 0.00597\n",
      "Iteration: 0065 Loss: 0.94848 Time: 0.00601\n",
      "Iteration: 0066 Loss: 0.92167 Time: 0.00600\n",
      "Iteration: 0067 Loss: 0.92709 Time: 0.00591\n",
      "Iteration: 0068 Loss: 0.93566 Time: 0.00822\n",
      "Iteration: 0069 Loss: 0.93868 Time: 0.00627\n",
      "Iteration: 0070 Loss: 0.91744 Time: 0.00600\n",
      "Iteration: 0071 Loss: 0.92873 Time: 0.00607\n",
      "Iteration: 0072 Loss: 0.91042 Time: 0.00593\n",
      "Iteration: 0073 Loss: 0.89577 Time: 0.00700\n",
      "Iteration: 0074 Loss: 0.90780 Time: 0.00708\n",
      "Iteration: 0075 Loss: 0.90132 Time: 0.00674\n",
      "Iteration: 0076 Loss: 0.88114 Time: 0.00600\n",
      "Iteration: 0077 Loss: 0.88206 Time: 0.00763\n",
      "Iteration: 0078 Loss: 0.88308 Time: 0.00720\n",
      "Iteration: 0079 Loss: 0.87229 Time: 0.00600\n",
      "Iteration: 0080 Loss: 0.88092 Time: 0.00600\n",
      "Iteration: 0081 Loss: 0.88261 Time: 0.00631\n",
      "Iteration: 0082 Loss: 0.88351 Time: 0.00612\n",
      "Iteration: 0083 Loss: 0.85611 Time: 0.00594\n",
      "Iteration: 0084 Loss: 0.89228 Time: 0.00593\n",
      "Iteration: 0085 Loss: 0.84518 Time: 0.00600\n",
      "Iteration: 0086 Loss: 0.85317 Time: 0.00635\n",
      "Iteration: 0087 Loss: 0.83900 Time: 0.00613\n",
      "Iteration: 0088 Loss: 0.82528 Time: 0.00586\n",
      "Iteration: 0089 Loss: 0.83220 Time: 0.00666\n",
      "Iteration: 0090 Loss: 0.83236 Time: 0.00600\n",
      "Iteration: 0091 Loss: 0.83031 Time: 0.00600\n",
      "Iteration: 0092 Loss: 0.83137 Time: 0.00636\n",
      "Iteration: 0093 Loss: 0.82946 Time: 0.00585\n",
      "Iteration: 0094 Loss: 0.81904 Time: 0.00668\n",
      "Iteration: 0095 Loss: 0.82113 Time: 0.00684\n",
      "Iteration: 0096 Loss: 0.80396 Time: 0.00678\n",
      "Iteration: 0097 Loss: 0.79531 Time: 0.00550\n",
      "Iteration: 0098 Loss: 0.79683 Time: 0.00653\n",
      "Iteration: 0099 Loss: 0.79648 Time: 0.00602\n",
      "Iteration: 0100 Loss: 0.81729 Time: 0.00599\n",
      "Iteration: 0101 Loss: 0.80888 Time: 0.00695\n",
      "Iteration: 0102 Loss: 0.82164 Time: 0.00600\n",
      "Iteration: 0103 Loss: 0.80088 Time: 0.00600\n",
      "Iteration: 0104 Loss: 0.78095 Time: 0.00648\n",
      "Iteration: 0105 Loss: 0.78131 Time: 0.00588\n",
      "Iteration: 0106 Loss: 0.77939 Time: 0.00494\n",
      "Iteration: 0107 Loss: 0.77095 Time: 0.00500\n",
      "Iteration: 0108 Loss: 0.77527 Time: 0.00616\n",
      "Iteration: 0109 Loss: 0.78615 Time: 0.00600\n",
      "Iteration: 0110 Loss: 0.76381 Time: 0.00598\n",
      "Iteration: 0111 Loss: 0.77300 Time: 0.00719\n",
      "Iteration: 0112 Loss: 0.77150 Time: 0.00613\n",
      "Iteration: 0113 Loss: 0.76076 Time: 0.00586\n",
      "Iteration: 0114 Loss: 0.77279 Time: 0.00700\n",
      "Iteration: 0115 Loss: 0.76708 Time: 0.00702\n",
      "Iteration: 0116 Loss: 0.75469 Time: 0.00727\n",
      "Iteration: 0117 Loss: 0.74967 Time: 0.00571\n",
      "Iteration: 0118 Loss: 0.75005 Time: 0.00700\n",
      "Iteration: 0119 Loss: 0.74286 Time: 0.00751\n",
      "Iteration: 0120 Loss: 0.74531 Time: 0.00649\n",
      "Iteration: 0121 Loss: 0.75642 Time: 0.00601\n",
      "Iteration: 0122 Loss: 0.74403 Time: 0.00629\n",
      "Iteration: 0123 Loss: 0.74583 Time: 0.00672\n",
      "Iteration: 0124 Loss: 0.74817 Time: 0.00722\n",
      "Iteration: 0125 Loss: 0.73970 Time: 0.00577\n",
      "Iteration: 0126 Loss: 0.73513 Time: 0.00700\n",
      "Iteration: 0127 Loss: 0.73083 Time: 0.00750\n",
      "Iteration: 0128 Loss: 0.73490 Time: 0.00723\n",
      "Iteration: 0129 Loss: 0.74053 Time: 0.00677\n",
      "Iteration: 0130 Loss: 0.72122 Time: 0.00700\n",
      "Iteration: 0131 Loss: 0.72566 Time: 0.00600\n",
      "Iteration: 0132 Loss: 0.72066 Time: 0.00601\n",
      "Iteration: 0133 Loss: 0.72206 Time: 0.00700\n",
      "Iteration: 0134 Loss: 0.72096 Time: 0.00599\n",
      "Iteration: 0135 Loss: 0.71760 Time: 0.00599\n",
      "Iteration: 0136 Loss: 0.71501 Time: 0.00700\n",
      "Iteration: 0137 Loss: 0.72383 Time: 0.00699\n",
      "Iteration: 0138 Loss: 0.73030 Time: 0.00700\n",
      "Iteration: 0139 Loss: 0.72499 Time: 0.00600\n",
      "Iteration: 0140 Loss: 0.71009 Time: 0.00700\n",
      "Iteration: 0141 Loss: 0.71621 Time: 0.00801\n",
      "Iteration: 0142 Loss: 0.70818 Time: 0.00700\n",
      "Iteration: 0143 Loss: 0.70224 Time: 0.00699\n",
      "Iteration: 0144 Loss: 0.70354 Time: 0.00700\n",
      "Iteration: 0145 Loss: 0.71645 Time: 0.00700\n",
      "Iteration: 0146 Loss: 0.70601 Time: 0.00600\n",
      "Iteration: 0147 Loss: 0.70070 Time: 0.00718\n",
      "Iteration: 0148 Loss: 0.72098 Time: 0.00600\n",
      "Iteration: 0149 Loss: 0.70422 Time: 0.00600\n",
      "Iteration: 0150 Loss: 0.71403 Time: 0.00700\n",
      "Iteration: 0151 Loss: 0.70462 Time: 0.00600\n",
      "Iteration: 0152 Loss: 0.69545 Time: 0.00700\n",
      "Iteration: 0153 Loss: 0.70468 Time: 0.00600\n",
      "Iteration: 0154 Loss: 0.69149 Time: 0.00601\n",
      "Iteration: 0155 Loss: 0.69332 Time: 0.00699\n",
      "Iteration: 0156 Loss: 0.70672 Time: 0.00601\n",
      "Iteration: 0157 Loss: 0.68472 Time: 0.00734\n",
      "Iteration: 0158 Loss: 0.68462 Time: 0.00606\n",
      "Iteration: 0159 Loss: 0.70379 Time: 0.00894\n",
      "Iteration: 0160 Loss: 0.68706 Time: 0.00801\n",
      "Iteration: 0161 Loss: 0.68125 Time: 0.00999\n",
      "Iteration: 0162 Loss: 0.69065 Time: 0.00800\n",
      "Iteration: 0163 Loss: 0.68365 Time: 0.00700\n",
      "Iteration: 0164 Loss: 0.68231 Time: 0.00700\n",
      "Iteration: 0165 Loss: 0.69096 Time: 0.00700\n",
      "Iteration: 0166 Loss: 0.67613 Time: 0.00700\n",
      "Iteration: 0167 Loss: 0.68776 Time: 0.00721\n",
      "Iteration: 0168 Loss: 0.66838 Time: 0.00564\n",
      "Iteration: 0169 Loss: 0.68939 Time: 0.00500\n",
      "Iteration: 0170 Loss: 0.68960 Time: 0.00700\n",
      "Iteration: 0171 Loss: 0.67027 Time: 0.00700\n",
      "Iteration: 0172 Loss: 0.68682 Time: 0.00601\n",
      "Iteration: 0173 Loss: 0.67199 Time: 0.00699\n",
      "Iteration: 0174 Loss: 0.66354 Time: 0.00700\n",
      "Iteration: 0175 Loss: 0.67920 Time: 0.00638\n",
      "Iteration: 0176 Loss: 0.68002 Time: 0.00568\n",
      "Iteration: 0177 Loss: 0.67939 Time: 0.00694\n",
      "Iteration: 0178 Loss: 0.66970 Time: 0.00600\n",
      "Iteration: 0179 Loss: 0.65927 Time: 0.00618\n",
      "Iteration: 0180 Loss: 0.67257 Time: 0.00782\n",
      "Iteration: 0181 Loss: 0.67395 Time: 0.00700\n",
      "Iteration: 0182 Loss: 0.67042 Time: 0.00801\n",
      "Iteration: 0183 Loss: 0.65332 Time: 0.00600\n",
      "Iteration: 0184 Loss: 0.66283 Time: 0.00699\n",
      "Iteration: 0185 Loss: 0.67172 Time: 0.00717\n",
      "Iteration: 0186 Loss: 0.66053 Time: 0.00615\n",
      "Iteration: 0187 Loss: 0.66129 Time: 0.00669\n",
      "Iteration: 0188 Loss: 0.66630 Time: 0.00739\n",
      "Iteration: 0189 Loss: 0.67411 Time: 0.00696\n",
      "Iteration: 0190 Loss: 0.65805 Time: 0.00800\n",
      "Iteration: 0191 Loss: 0.66819 Time: 0.00800\n",
      "Iteration: 0192 Loss: 0.65992 Time: 0.00701\n",
      "Iteration: 0193 Loss: 0.65782 Time: 0.00699\n",
      "Iteration: 0194 Loss: 0.67099 Time: 0.00599\n",
      "Iteration: 0195 Loss: 0.66334 Time: 0.00701\n",
      "Iteration: 0196 Loss: 0.65665 Time: 0.00699\n",
      "Iteration: 0197 Loss: 0.65424 Time: 0.00601\n",
      "Iteration: 0198 Loss: 0.66355 Time: 0.00701\n",
      "Iteration: 0199 Loss: 0.66731 Time: 0.00700\n",
      "Iteration: 0200 Loss: 0.64568 Time: 0.00600\n",
      "Done! \n",
      "\n",
      "STEP 3/3 - MODEL EVALUATION \n",
      "\n",
      "Computing the final embedding vectors, for evaluation\n",
      "Done! \n",
      "\n",
      "Testing: link prediction\n",
      "Done! \n",
      "\n",
      "EXPERIMENTS ON MODEL 10 / 10 \n",
      "\n",
      "STEP 1/3 - PREPROCESSING STEPS \n",
      "\n",
      "Masking some edges from the training graph, for link prediction\n",
      "(validation set: 1.0 % of edges - test set: 10.0 % of edges)\n",
      "Done! \n",
      "\n",
      "Preprocessing node features\n",
      "Done! \n",
      "\n",
      "Running the Louvain algorithm for community detection\n",
      "as a preprocessing step for the encoder\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Louvain has found 6 communities \n",
      "\n",
      "Setting up the model and the optimizer\n",
      "Done! \n",
      "\n",
      "Preprocessing on message passing matrices\n",
      "Done! \n",
      "\n",
      "Initializing TF session\n",
      "Done! \n",
      "\n",
      "STEP 2/3 - MODEL TRAINING \n",
      "\n",
      "Starting training\n",
      "Iteration: 0001 Loss: 1.65759 Time: 0.19001\n",
      "Iteration: 0002 Loss: 1.70978 Time: 0.00801\n",
      "Iteration: 0003 Loss: 1.69929 Time: 0.00600\n",
      "Iteration: 0004 Loss: 1.66408 Time: 0.00700\n",
      "Iteration: 0005 Loss: 1.54505 Time: 0.00600\n",
      "Iteration: 0006 Loss: 1.60531 Time: 0.00800\n",
      "Iteration: 0007 Loss: 1.55414 Time: 0.00599\n",
      "Iteration: 0008 Loss: 1.59092 Time: 0.00701\n",
      "Iteration: 0009 Loss: 1.51445 Time: 0.00600\n",
      "Iteration: 0010 Loss: 1.58453 Time: 0.00600\n",
      "Iteration: 0011 Loss: 1.48316 Time: 0.00700\n",
      "Iteration: 0012 Loss: 1.51367 Time: 0.00794\n",
      "Iteration: 0013 Loss: 1.40420 Time: 0.00700\n",
      "Iteration: 0014 Loss: 1.43935 Time: 0.00795\n",
      "Iteration: 0015 Loss: 1.38609 Time: 0.00801\n",
      "Iteration: 0016 Loss: 1.34947 Time: 0.00824\n",
      "Iteration: 0017 Loss: 1.41395 Time: 0.00800\n",
      "Iteration: 0018 Loss: 1.36446 Time: 0.00700\n",
      "Iteration: 0019 Loss: 1.32863 Time: 0.00700\n",
      "Iteration: 0020 Loss: 1.31592 Time: 0.00700\n",
      "Iteration: 0021 Loss: 1.30126 Time: 0.00800\n",
      "Iteration: 0022 Loss: 1.34598 Time: 0.00800\n",
      "Iteration: 0023 Loss: 1.30636 Time: 0.00800\n",
      "Iteration: 0024 Loss: 1.28846 Time: 0.00636\n",
      "Iteration: 0025 Loss: 1.23631 Time: 0.00736\n",
      "Iteration: 0026 Loss: 1.26212 Time: 0.00648\n",
      "Iteration: 0027 Loss: 1.22214 Time: 0.00653\n",
      "Iteration: 0028 Loss: 1.19800 Time: 0.00598\n",
      "Iteration: 0029 Loss: 1.19051 Time: 0.00700\n",
      "Iteration: 0030 Loss: 1.19451 Time: 0.00901\n",
      "Iteration: 0031 Loss: 1.18727 Time: 0.00700\n",
      "Iteration: 0032 Loss: 1.16717 Time: 0.00700\n",
      "Iteration: 0033 Loss: 1.17965 Time: 0.00700\n",
      "Iteration: 0034 Loss: 1.13884 Time: 0.00599\n",
      "Iteration: 0035 Loss: 1.18003 Time: 0.00744\n",
      "Iteration: 0036 Loss: 1.13939 Time: 0.00756\n",
      "Iteration: 0037 Loss: 1.12039 Time: 0.00800\n",
      "Iteration: 0038 Loss: 1.10849 Time: 0.00700\n",
      "Iteration: 0039 Loss: 1.07640 Time: 0.00700\n",
      "Iteration: 0040 Loss: 1.10575 Time: 0.00700\n",
      "Iteration: 0041 Loss: 1.08386 Time: 0.00611\n",
      "Iteration: 0042 Loss: 1.08209 Time: 0.00610\n",
      "Iteration: 0043 Loss: 1.06200 Time: 0.00690\n",
      "Iteration: 0044 Loss: 1.07958 Time: 0.00729\n",
      "Iteration: 0045 Loss: 1.08332 Time: 0.00700\n",
      "Iteration: 0046 Loss: 1.06792 Time: 0.00706\n",
      "Iteration: 0047 Loss: 1.07172 Time: 0.00692\n",
      "Iteration: 0048 Loss: 1.05198 Time: 0.00800\n",
      "Iteration: 0049 Loss: 1.01868 Time: 0.00600\n",
      "Iteration: 0050 Loss: 1.05266 Time: 0.00901\n",
      "Iteration: 0051 Loss: 1.00431 Time: 0.00828\n",
      "Iteration: 0052 Loss: 1.01607 Time: 0.00672\n",
      "Iteration: 0053 Loss: 1.02268 Time: 0.00710\n",
      "Iteration: 0054 Loss: 1.02608 Time: 0.00669\n",
      "Iteration: 0055 Loss: 1.01195 Time: 0.00596\n",
      "Iteration: 0056 Loss: 0.99843 Time: 0.00600\n",
      "Iteration: 0057 Loss: 0.98823 Time: 0.00601\n",
      "Iteration: 0058 Loss: 0.98919 Time: 0.00666\n",
      "Iteration: 0059 Loss: 0.96234 Time: 0.00549\n",
      "Iteration: 0060 Loss: 0.97208 Time: 0.00668\n",
      "Iteration: 0061 Loss: 0.97541 Time: 0.00595\n",
      "Iteration: 0062 Loss: 0.96120 Time: 0.00801\n",
      "Iteration: 0063 Loss: 0.91795 Time: 0.00628\n",
      "Iteration: 0064 Loss: 0.94325 Time: 0.00557\n",
      "Iteration: 0065 Loss: 0.92098 Time: 0.00595\n",
      "Iteration: 0066 Loss: 0.94240 Time: 0.00600\n",
      "Iteration: 0067 Loss: 0.93201 Time: 0.00604\n",
      "Iteration: 0068 Loss: 0.94623 Time: 0.00896\n",
      "Iteration: 0069 Loss: 0.91379 Time: 0.00806\n",
      "Iteration: 0070 Loss: 0.90817 Time: 0.00771\n",
      "Iteration: 0071 Loss: 0.92465 Time: 0.00532\n",
      "Iteration: 0072 Loss: 0.88887 Time: 0.00661\n",
      "Iteration: 0073 Loss: 0.90515 Time: 0.00637\n",
      "Iteration: 0074 Loss: 0.90738 Time: 0.00700\n",
      "Iteration: 0075 Loss: 0.86883 Time: 0.00601\n",
      "Iteration: 0076 Loss: 0.89696 Time: 0.00604\n",
      "Iteration: 0077 Loss: 0.88616 Time: 0.00601\n",
      "Iteration: 0078 Loss: 0.88908 Time: 0.00599\n",
      "Iteration: 0079 Loss: 0.86963 Time: 0.00601\n",
      "Iteration: 0080 Loss: 0.85600 Time: 0.00799\n",
      "Iteration: 0081 Loss: 0.86384 Time: 0.00705\n",
      "Iteration: 0082 Loss: 0.86054 Time: 0.00595\n",
      "Iteration: 0083 Loss: 0.86210 Time: 0.00705\n",
      "Iteration: 0084 Loss: 0.87139 Time: 0.00695\n",
      "Iteration: 0085 Loss: 0.84182 Time: 0.00641\n",
      "Iteration: 0086 Loss: 0.87660 Time: 0.00564\n",
      "Iteration: 0087 Loss: 0.82089 Time: 0.00800\n",
      "Iteration: 0088 Loss: 0.84590 Time: 0.00795\n",
      "Iteration: 0089 Loss: 0.83475 Time: 0.00700\n",
      "Iteration: 0090 Loss: 0.84251 Time: 0.00500\n",
      "Iteration: 0091 Loss: 0.81393 Time: 0.00508\n",
      "Iteration: 0092 Loss: 0.84621 Time: 0.00624\n",
      "Iteration: 0093 Loss: 0.81230 Time: 0.00570\n",
      "Iteration: 0094 Loss: 0.82522 Time: 0.00600\n",
      "Iteration: 0095 Loss: 0.79818 Time: 0.00699\n",
      "Iteration: 0096 Loss: 0.81751 Time: 0.00551\n",
      "Iteration: 0097 Loss: 0.79736 Time: 0.00595\n",
      "Iteration: 0098 Loss: 0.81072 Time: 0.00636\n",
      "Iteration: 0099 Loss: 0.80481 Time: 0.00500\n",
      "Iteration: 0100 Loss: 0.80261 Time: 0.00600\n",
      "Iteration: 0101 Loss: 0.78340 Time: 0.00606\n",
      "Iteration: 0102 Loss: 0.81088 Time: 0.00698\n",
      "Iteration: 0103 Loss: 0.80154 Time: 0.00697\n",
      "Iteration: 0104 Loss: 0.78742 Time: 0.00500\n",
      "Iteration: 0105 Loss: 0.78732 Time: 0.00554\n",
      "Iteration: 0106 Loss: 0.78075 Time: 0.00538\n",
      "Iteration: 0107 Loss: 0.77113 Time: 0.00684\n",
      "Iteration: 0108 Loss: 0.78611 Time: 0.00704\n",
      "Iteration: 0109 Loss: 0.76490 Time: 0.00828\n",
      "Iteration: 0110 Loss: 0.78557 Time: 0.00568\n",
      "Iteration: 0111 Loss: 0.77263 Time: 0.00695\n",
      "Iteration: 0112 Loss: 0.75247 Time: 0.00700\n",
      "Iteration: 0113 Loss: 0.76341 Time: 0.00603\n",
      "Iteration: 0114 Loss: 0.77058 Time: 0.00696\n",
      "Iteration: 0115 Loss: 0.76417 Time: 0.00745\n",
      "Iteration: 0116 Loss: 0.76272 Time: 0.00599\n",
      "Iteration: 0117 Loss: 0.74204 Time: 0.00695\n",
      "Iteration: 0118 Loss: 0.76476 Time: 0.00604\n",
      "Iteration: 0119 Loss: 0.75588 Time: 0.00629\n",
      "Iteration: 0120 Loss: 0.74652 Time: 0.00616\n",
      "Iteration: 0121 Loss: 0.73981 Time: 0.00500\n",
      "Iteration: 0122 Loss: 0.73927 Time: 0.00503\n",
      "Iteration: 0123 Loss: 0.73968 Time: 0.00713\n",
      "Iteration: 0124 Loss: 0.73709 Time: 0.00560\n",
      "Iteration: 0125 Loss: 0.72203 Time: 0.00641\n",
      "Iteration: 0126 Loss: 0.73890 Time: 0.00603\n",
      "Iteration: 0127 Loss: 0.75557 Time: 0.00594\n",
      "Iteration: 0128 Loss: 0.73085 Time: 0.00647\n",
      "Iteration: 0129 Loss: 0.72866 Time: 0.00628\n",
      "Iteration: 0130 Loss: 0.72262 Time: 0.00500\n",
      "Iteration: 0131 Loss: 0.74073 Time: 0.00505\n",
      "Iteration: 0132 Loss: 0.73728 Time: 0.00579\n",
      "Iteration: 0133 Loss: 0.72613 Time: 0.00648\n",
      "Iteration: 0134 Loss: 0.72601 Time: 0.00497\n",
      "Iteration: 0135 Loss: 0.73488 Time: 0.00600\n",
      "Iteration: 0136 Loss: 0.71293 Time: 0.00605\n",
      "Iteration: 0137 Loss: 0.72042 Time: 0.00594\n",
      "Iteration: 0138 Loss: 0.72027 Time: 0.00596\n",
      "Iteration: 0139 Loss: 0.72783 Time: 0.00650\n",
      "Iteration: 0140 Loss: 0.72076 Time: 0.00681\n",
      "Iteration: 0141 Loss: 0.70113 Time: 0.00504\n",
      "Iteration: 0142 Loss: 0.70641 Time: 0.00534\n",
      "Iteration: 0143 Loss: 0.70729 Time: 0.00663\n",
      "Iteration: 0144 Loss: 0.70409 Time: 0.00605\n",
      "Iteration: 0145 Loss: 0.71667 Time: 0.00695\n",
      "Iteration: 0146 Loss: 0.71584 Time: 0.00653\n",
      "Iteration: 0147 Loss: 0.71142 Time: 0.00601\n",
      "Iteration: 0148 Loss: 0.69910 Time: 0.00695\n",
      "Iteration: 0149 Loss: 0.70128 Time: 0.00600\n",
      "Iteration: 0150 Loss: 0.70601 Time: 0.00702\n",
      "Iteration: 0151 Loss: 0.70163 Time: 0.00657\n",
      "Iteration: 0152 Loss: 0.69346 Time: 0.00731\n",
      "Iteration: 0153 Loss: 0.69505 Time: 0.00597\n",
      "Iteration: 0154 Loss: 0.69011 Time: 0.00705\n",
      "Iteration: 0155 Loss: 0.69095 Time: 0.00696\n",
      "Iteration: 0156 Loss: 0.69201 Time: 0.00599\n",
      "Iteration: 0157 Loss: 0.69731 Time: 0.00505\n",
      "Iteration: 0158 Loss: 0.69038 Time: 0.00694\n",
      "Iteration: 0159 Loss: 0.68703 Time: 0.00602\n",
      "Iteration: 0160 Loss: 0.67991 Time: 0.00602\n",
      "Iteration: 0161 Loss: 0.68107 Time: 0.00607\n",
      "Iteration: 0162 Loss: 0.67467 Time: 0.00494\n",
      "Iteration: 0163 Loss: 0.68377 Time: 0.00694\n",
      "Iteration: 0164 Loss: 0.68506 Time: 0.00504\n",
      "Iteration: 0165 Loss: 0.67635 Time: 0.00701\n",
      "Iteration: 0166 Loss: 0.69198 Time: 0.00698\n",
      "Iteration: 0167 Loss: 0.68815 Time: 0.00614\n",
      "Iteration: 0168 Loss: 0.67747 Time: 0.00683\n",
      "Iteration: 0169 Loss: 0.68784 Time: 0.00659\n",
      "Iteration: 0170 Loss: 0.68114 Time: 0.00594\n",
      "Iteration: 0171 Loss: 0.66060 Time: 0.00630\n",
      "Iteration: 0172 Loss: 0.67427 Time: 0.00505\n",
      "Iteration: 0173 Loss: 0.67543 Time: 0.00501\n",
      "Iteration: 0174 Loss: 0.68038 Time: 0.00640\n",
      "Iteration: 0175 Loss: 0.67701 Time: 0.00596\n",
      "Iteration: 0176 Loss: 0.66633 Time: 0.00700\n",
      "Iteration: 0177 Loss: 0.65928 Time: 0.00601\n",
      "Iteration: 0178 Loss: 0.66302 Time: 0.00712\n",
      "Iteration: 0179 Loss: 0.65462 Time: 0.00588\n",
      "Iteration: 0180 Loss: 0.67597 Time: 0.00503\n",
      "Iteration: 0181 Loss: 0.66549 Time: 0.00697\n",
      "Iteration: 0182 Loss: 0.65780 Time: 0.00600\n",
      "Iteration: 0183 Loss: 0.67062 Time: 0.00621\n",
      "Iteration: 0184 Loss: 0.66121 Time: 0.00684\n",
      "Iteration: 0185 Loss: 0.64943 Time: 0.00655\n",
      "Iteration: 0186 Loss: 0.67171 Time: 0.00640\n",
      "Iteration: 0187 Loss: 0.65510 Time: 0.00597\n",
      "Iteration: 0188 Loss: 0.65966 Time: 0.00497\n",
      "Iteration: 0189 Loss: 0.65556 Time: 0.00797\n",
      "Iteration: 0190 Loss: 0.65164 Time: 0.00606\n",
      "Iteration: 0191 Loss: 0.65739 Time: 0.00501\n",
      "Iteration: 0192 Loss: 0.65934 Time: 0.00643\n",
      "Iteration: 0193 Loss: 0.65475 Time: 0.00505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0194 Loss: 0.64910 Time: 0.00696\n",
      "Iteration: 0195 Loss: 0.65790 Time: 0.00701\n",
      "Iteration: 0196 Loss: 0.65913 Time: 0.00603\n",
      "Iteration: 0197 Loss: 0.66074 Time: 0.00641\n",
      "Iteration: 0198 Loss: 0.65196 Time: 0.00653\n",
      "Iteration: 0199 Loss: 0.64725 Time: 0.00656\n",
      "Iteration: 0200 Loss: 0.65863 Time: 0.00595\n",
      "Done! \n",
      "\n",
      "STEP 3/3 - MODEL EVALUATION \n",
      "\n",
      "Computing the final embedding vectors, for evaluation\n",
      "Done! \n",
      "\n",
      "Testing: link prediction\n",
      "Done! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We repeat the entire training+test process FLAGS.nb_run times\n",
    "for i in range(FLAGS.nb_run):\n",
    "    if FLAGS.verbose:\n",
    "        print(\"EXPERIMENTS ON MODEL\", i + 1, \"/\", FLAGS.nb_run, \"\\n\")\n",
    "        print(\"STEP 1/3 - PREPROCESSING STEPS \\n\")\n",
    "\n",
    "\n",
    "    # Edge masking for Link Prediction:\n",
    "    if FLAGS.task == 'task_2':\n",
    "        # Compute Train/Validation/Test sets\n",
    "        if FLAGS.verbose:\n",
    "            print(\"Masking some edges from the training graph, for link prediction\")\n",
    "            print(\"(validation set:\", FLAGS.prop_val, \"% of edges - test set:\",\n",
    "                  FLAGS.prop_test, \"% of edges)\")\n",
    "        #adj, val_edges, val_edges_false, test_edges, test_edges_false = mask_test_edges(adj_init, FLAGS.prop_test, FLAGS.prop_val)\n",
    "        adj, test_edges, test_edges_false = mask_test_edges(adj_init, FLAGS.prop_test)\n",
    "        if FLAGS.verbose:\n",
    "            print(\"Done! \\n\")\n",
    "    else:\n",
    "        adj = adj_init\n",
    "\n",
    "    # Compute the number of nodes\n",
    "    num_nodes = adj.shape[0]\n",
    "\n",
    "    # Preprocessing on node features\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Preprocessing node features\")\n",
    "    if FLAGS.features:\n",
    "        features = features_init\n",
    "    else:\n",
    "        # If features are not used, replace feature matrix by identity matrix\n",
    "        features = sp.identity(num_nodes)\n",
    "    features = sparse_to_tuple(features)\n",
    "    num_features = features[2][1]\n",
    "    features_nonzero = features[1].shape[0]\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Done! \\n\")\n",
    "\n",
    "\n",
    "    # Community detection using Louvain, as a preprocessing step\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Running the Louvain algorithm for community detection\")\n",
    "        print(\"as a preprocessing step for the encoder\")\n",
    "    # Get binary community matrix (adj_louvain_init[i,j] = 1 if nodes i and j are\n",
    "    # in the same community) as well as number of communities found by Louvain\n",
    "    adj_louvain_init, nb_communities_louvain, partition = louvain_clustering(adj, FLAGS.s_reg)\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Done! Louvain has found\", nb_communities_louvain, \"communities \\n\")\n",
    "\n",
    "    # FastGAE: node sampling for stochastic subgraph decoding - used when facing scalability issues\n",
    "    if FLAGS.fastgae:\n",
    "        if FLAGS.verbose:\n",
    "            print(\"Preprocessing operations related to the FastGAE method\")\n",
    "        # Node-level p_i degree-based, core-based or uniform distribution\n",
    "        node_distribution = get_distribution(FLAGS.measure, FLAGS.alpha, adj)\n",
    "        # Node sampling for initializations\n",
    "        sampled_nodes, adj_label, adj_sampled_sparse = node_sampling(adj, node_distribution, FLAGS.nb_node_samples, FLAGS.replace)\n",
    "        if FLAGS.verbose:\n",
    "            print(\"Done! \\n\")\n",
    "    else:\n",
    "        sampled_nodes = np.array(range(FLAGS.nb_node_samples))\n",
    "\n",
    "\n",
    "    # Placeholders\n",
    "    if FLAGS.verbose:\n",
    "        print('Setting up the model and the optimizer')\n",
    "    placeholders = {\n",
    "        'features': tf.sparse_placeholder(tf.float32),\n",
    "        'adj': tf.sparse_placeholder(tf.float32),\n",
    "        'adj_layer2': tf.sparse_placeholder(tf.float32), # Only used for 2-layer GCN encoders\n",
    "        'degree_matrix': tf.sparse_placeholder(tf.float32),\n",
    "        'adj_orig': tf.sparse_placeholder(tf.float32),\n",
    "        'dropout': tf.placeholder_with_default(0., shape = ()),\n",
    "        'sampled_nodes': tf.placeholder_with_default(sampled_nodes, shape = [FLAGS.nb_node_samples])\n",
    "    }\n",
    "\n",
    "\n",
    "    # Create model\n",
    "    if FLAGS.model == 'linear_ae':\n",
    "        # Linear Graph Autoencoder\n",
    "        model = LinearModelAE(placeholders, num_features, features_nonzero)\n",
    "    elif FLAGS.model == 'linear_vae':\n",
    "        # Linear Graph Variational Autoencoder\n",
    "        model = LinearModelVAE(placeholders, num_features, num_nodes, features_nonzero)\n",
    "    elif FLAGS.model == 'gcn_ae':\n",
    "        # 2-layer GCN Graph Autoencoder\n",
    "        model = GCNModelAE(placeholders, num_features, features_nonzero)\n",
    "    elif FLAGS.model == 'gcn_vae':\n",
    "        # 2-layer GCN Graph Variational Autoencoder\n",
    "        model = GCNModelVAE(placeholders, num_features, num_nodes, features_nonzero)\n",
    "    else:\n",
    "        raise ValueError('Undefined model!')\n",
    "\n",
    "\n",
    "    # Optimizer\n",
    "    if FLAGS.fastgae:\n",
    "        num_sampled = adj_sampled_sparse.shape[0]\n",
    "        sum_sampled = adj_sampled_sparse.sum()\n",
    "        pos_weight = float(num_sampled * num_sampled - sum_sampled) / sum_sampled\n",
    "        norm = num_sampled * num_sampled / float((num_sampled * num_sampled - sum_sampled) * 2)\n",
    "    else:\n",
    "        pos_weight = float(num_nodes * num_nodes - adj.sum()) / adj.sum()\n",
    "        norm = num_nodes * num_nodes / float((num_nodes * num_nodes - adj.sum()) * 2)\n",
    "\n",
    "    if FLAGS.model in ('gcn_ae', 'linear_ae'):\n",
    "        opt = OptimizerAE(preds = model.reconstructions,\n",
    "                          labels = tf.reshape(tf.sparse_tensor_to_dense(placeholders['adj_orig'], validate_indices = False), [-1]),\n",
    "                          degree_matrix = tf.reshape(tf.sparse_tensor_to_dense(placeholders['degree_matrix'], validate_indices = False), [-1]),\n",
    "                          num_nodes = num_nodes,\n",
    "                          pos_weight = pos_weight,\n",
    "                          norm = norm,\n",
    "                          clusters_distance = model.clusters)\n",
    "    else:\n",
    "        opt = OptimizerVAE(preds = model.reconstructions,\n",
    "                           labels = tf.reshape(tf.sparse_tensor_to_dense(placeholders['adj_orig'], validate_indices = False), [-1]),\n",
    "                           degree_matrix = tf.reshape(tf.sparse_tensor_to_dense(placeholders['degree_matrix'], validate_indices = False), [-1]),\n",
    "                           model = model,\n",
    "                           num_nodes = num_nodes,\n",
    "                           pos_weight = pos_weight,\n",
    "                           norm = norm,\n",
    "                           clusters_distance = model.clusters)\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Done! \\n\")\n",
    "\n",
    "\n",
    "    # Symmetrically normalized \"message passing\" matrices\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Preprocessing on message passing matrices\")\n",
    "    adj_norm = preprocess_graph(adj + FLAGS.lamb*adj_louvain_init)\n",
    "    adj_norm_layer2 = preprocess_graph(adj)\n",
    "    if not FLAGS.fastgae:\n",
    "        adj_label = sparse_to_tuple(adj + sp.eye(num_nodes))\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Done! \\n\")\n",
    "\n",
    "\n",
    "    # Degree matrices\n",
    "    deg_matrix, deg_matrix_init = preprocess_degree(adj, FLAGS.simple)\n",
    "\n",
    "\n",
    "    # Initialize TF session\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Initializing TF session\")\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Done! \\n\")\n",
    "\n",
    "\n",
    "    # Model training\n",
    "    if FLAGS.verbose:\n",
    "        print(\"STEP 2/3 - MODEL TRAINING \\n\")\n",
    "        print(\"Starting training\")\n",
    "\n",
    "    for iter in range(FLAGS.iterations):\n",
    "\n",
    "        # Flag to compute running time for each iteration\n",
    "        t = time.time()\n",
    "\n",
    "        # Construct feed dictionary\n",
    "        feed_dict = construct_feed_dict(adj_norm, adj_norm_layer2, adj_label, features, deg_matrix, placeholders)\n",
    "        if FLAGS.fastgae:\n",
    "            # Update sampled subgraph and sampled Louvain matrix\n",
    "            feed_dict.update({placeholders['sampled_nodes']: sampled_nodes})\n",
    "            # New node sampling\n",
    "            sampled_nodes, adj_label, adj_label_sparse, = node_sampling(adj, node_distribution, FLAGS.nb_node_samples, FLAGS.replace)\n",
    "\n",
    "        # Weights update\n",
    "        outs = sess.run([opt.opt_op, opt.cost, opt.cost_adj, opt.cost_mod],\n",
    "                        feed_dict = feed_dict)\n",
    "\n",
    "        # Compute average loss\n",
    "        avg_cost = outs[1]\n",
    "        if FLAGS.verbose:\n",
    "            # Display information on the iteration\n",
    "            print(\"Iteration:\", '%04d' % (iter + 1), \"Loss:\", \"{:.5f}\".format(avg_cost),\n",
    "                  \"Time:\", \"{:.5f}\".format(time.time() - t))\n",
    "\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Done! \\n\")\n",
    "\n",
    "    # Compute embedding vectors, for evaluation\n",
    "    if FLAGS.verbose:\n",
    "        print(\"STEP 3/3 - MODEL EVALUATION \\n\")\n",
    "        print(\"Computing the final embedding vectors, for evaluation\")\n",
    "    emb = sess.run(model.z_mean, feed_dict = feed_dict)\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Done! \\n\")\n",
    "\n",
    "    # Test model: link prediction (classification edges/non-edges)\n",
    "\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Testing: link prediction\")\n",
    "    # Get ROC and AP scores\n",
    "    roc_score, ap_score = link_prediction(test_edges, test_edges_false, emb)\n",
    "    mean_roc.append(roc_score)\n",
    "    mean_ap.append(ap_score)\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Done! \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bef29d76",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-28T08:54:25.066899Z",
     "start_time": "2022-10-28T08:54:25.059217Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL RESULTS \n",
      "\n",
      "Recall: the selected task was \"Task 2\", i.e., joint community detection and link prediction, on celegans\n",
      "All scores reported below are computed over the 10 run(s) \n",
      "\n",
      "Link prediction:\n",
      "\n",
      "Mean AUC score:  0.8470477770984365\n",
      "Std of AUC scores:  0.016257558618347212 \n",
      "\n",
      "Mean AP score:  0.8339304731182088\n",
      "Std of AP scores:  0.01873121168411811 \n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Report final results\n",
    "print(\"FINAL RESULTS \\n\")\n",
    "\n",
    "print('Recall: the selected task was \"Task 2\", i.e., joint community detection and link prediction, on', FLAGS.dataset)\n",
    "\n",
    "print(\"All scores reported below are computed over the\", FLAGS.nb_run, \"run(s) \\n\")\n",
    "\n",
    "print(\"Link prediction:\\n\")\n",
    "print(\"Mean AUC score: \", np.mean(mean_roc))\n",
    "print(\"Std of AUC scores: \", np.std(mean_roc), \"\\n\")\n",
    "\n",
    "print(\"Mean AP score: \", np.mean(mean_ap))\n",
    "print(\"Std of AP scores: \", np.std(mean_ap), \"\\n \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f273fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf1.15",
   "language": "python",
   "name": "tf1.15"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
