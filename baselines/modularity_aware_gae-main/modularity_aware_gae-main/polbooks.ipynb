{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67dbcca0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-28T09:08:39.547261Z",
     "start_time": "2022-10-28T09:08:37.384443Z"
    }
   },
   "outputs": [],
   "source": [
    "from modularity_aware_gae.evaluation import community_detection, link_prediction\n",
    "from modularity_aware_gae.input_data import load_data, load_labels\n",
    "from modularity_aware_gae.louvain import louvain_clustering\n",
    "from modularity_aware_gae.model import  GCNModelAE, GCNModelVAE, LinearModelAE, LinearModelVAE\n",
    "from modularity_aware_gae.optimizer import OptimizerAE, OptimizerVAE\n",
    "from modularity_aware_gae.preprocessing import *\n",
    "from modularity_aware_gae.sampling import get_distribution, node_sampling\n",
    "import numpy as np\n",
    "import os\n",
    "import scipy.sparse as sp\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel')#添加的，不报错\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14ffd620",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-28T09:08:39.595553Z",
     "start_time": "2022-10-28T09:08:39.581249Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " \n",
      " \n",
      "[MODULARITY-AWARE GRAPH AUTOENCODERS]\n",
      " \n",
      " \n",
      " \n",
      "\n",
      "EXPERIMENTAL SETTING \n",
      "\n",
      "- Graph dataset: polbooks\n",
      "- Mode name: linear_vae\n",
      "- Number of models to train: 10\n",
      "- Number of training iterations for each model: 200\n",
      "- Learning rate: 0.01\n",
      "- Dropout rate: 0.0\n",
      "- Use of node features in the input layer: False\n",
      "- Dimension of the output layer: 16\n",
      "- lambda: 0.0\n",
      "- beta: 0.0\n",
      "- gamma: 1.0\n",
      "- s: 2\n",
      "- FastGAE: no \n",
      "\n",
      "Final embedding vectors will be evaluated on:\n",
      "- Task 2, i.e., joint community detection and link prediction\n",
      "\n",
      " \n",
      " \n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "# Parameters related to the input data\n",
    "flags.DEFINE_string('dataset', 'polbooks', 'Graph dataset, among: cora, citeseer, wiki, celegans, email, polbooks, texas, wisconsin')\n",
    "\n",
    "flags.DEFINE_boolean('features', False, 'Whether to include node features')\n",
    "\n",
    "\n",
    "# Parameters related to the Modularity-Aware GAE/VGAE model to train\n",
    "\n",
    "# 1/3 - General parameters associated with GAE/VGAE\n",
    "flags.DEFINE_string('model', 'linear_vae', 'Model to train, among: gcn_ae, gcn_vae, \\\n",
    "                                            linear_ae, linear_vae')\n",
    "flags.DEFINE_float('dropout', 0., 'Dropout rate')\n",
    "flags.DEFINE_integer('iterations', 200, 'Number of iterations in training')\n",
    "flags.DEFINE_float('learning_rate', 0.01, 'Initial learning rate (Adam)')\n",
    "flags.DEFINE_integer('hidden', 32, 'Dimension of the GCN hidden layer')\n",
    "flags.DEFINE_integer('dimension', 16, 'Dimension of the output layer, i.e., \\\n",
    "                                       dimension of the embedding space')\n",
    "\n",
    "# 2/3 - Additional parameters, specific to Modularity-Aware models\n",
    "flags.DEFINE_float('beta', 0.0, 'Beta hyperparameter of Mod.-Aware models')\n",
    "flags.DEFINE_float('lamb', 0.0, 'Lambda hyperparameter of Mod.-Aware models')\n",
    "flags.DEFINE_float('gamma', 1.0, 'Gamma hyperparameter of Mod.-Aware models')\n",
    "flags.DEFINE_integer('s_reg', 2, 's hyperparameter of Mod.-Aware models')\n",
    "\n",
    "# 3/3 - Additional parameters, aiming to improve scalability\n",
    "flags.DEFINE_boolean('fastgae', False, 'Whether to use the FastGAE framework')\n",
    "flags.DEFINE_integer('nb_node_samples', 1000, 'In FastGAE, number of nodes to \\\n",
    "                                               sample at each iteration, i.e., \\\n",
    "                                               size of decoded subgraph')\n",
    "flags.DEFINE_string('measure', 'degree', 'In FastGAE, node importance measure used \\\n",
    "                                          for sampling: degree, core, or uniform')\n",
    "flags.DEFINE_float('alpha', 1.0, 'alpha hyperparameter associated with the FastGAE sampling')\n",
    "flags.DEFINE_boolean('replace', False, 'Sample nodes with or without replacement')\n",
    "flags.DEFINE_boolean('simple', True, 'Use simpler (and faster) modularity in optimizers')\n",
    "\n",
    "\n",
    "# Parameters related to the experimental setup\n",
    "flags.DEFINE_string('task', 'task_2', 'task_1: pure community detection \\\n",
    "                                       task_2: joint link prediction and \\\n",
    "                                               community detection')\n",
    "flags.DEFINE_integer('nb_run', 10, 'Number of model run + test')\n",
    "flags.DEFINE_float('prop_val', 1., 'Proportion of edges in validation set \\\n",
    "                                    for the link prediction task')\n",
    "flags.DEFINE_float('prop_test', 10., 'Proportion of edges in test set \\\n",
    "                                      for the link prediction task')\n",
    "flags.DEFINE_boolean('validation', False, 'Whether to compute validation \\\n",
    "                                           results at each iteration, for \\\n",
    "                                           the link prediction task')\n",
    "flags.DEFINE_boolean('verbose', True, 'Whether to print all comments details')\n",
    "\n",
    "\n",
    "# Introductory message\n",
    "if FLAGS.verbose:\n",
    "    introductory_message()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ce16ae4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-28T09:08:39.643742Z",
     "start_time": "2022-10-28T09:08:39.629582Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADING DATA\n",
      "\n",
      "Loading the polbooks graph\n",
      "- Number of nodes: 105\n",
      "- Use of node features: False\n",
      "Done! \n",
      " \n",
      " \n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize lists to collect final results\n",
    "mean_ami = []\n",
    "mean_ari = []\n",
    "\n",
    "mean_roc = []\n",
    "mean_ap = []\n",
    "\n",
    "# Load data\n",
    "if FLAGS.verbose:\n",
    "    print(\"LOADING DATA\\n\")\n",
    "    print(\"Loading the\", FLAGS.dataset, \"graph\")\n",
    "adj_init, features_init = load_data(FLAGS.dataset)\n",
    "#labels = load_labels(adj_init.shape[0])\n",
    "if FLAGS.verbose:\n",
    "    print(\"- Number of nodes:\", adj_init.shape[0])\n",
    "    #print(\"- Number of communities:\", len(np.unique(labels)))\n",
    "    print(\"- Use of node features:\", FLAGS.features)\n",
    "    print(\"Done! \\n \\n \\n \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd19b933",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-28T09:08:52.365320Z",
     "start_time": "2022-10-28T09:08:39.677515Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENTS ON MODEL 1 / 10 \n",
      "\n",
      "STEP 1/3 - PREPROCESSING STEPS \n",
      "\n",
      "Masking some edges from the training graph, for link prediction\n",
      "(validation set: 1.0 % of edges - test set: 10.0 % of edges)\n",
      "Done! \n",
      "\n",
      "Preprocessing node features\n",
      "Done! \n",
      "\n",
      "Running the Louvain algorithm for community detection\n",
      "as a preprocessing step for the encoder\n",
      "Done! Louvain has found 5 communities \n",
      "\n",
      "Setting up the model and the optimizer\n",
      "Done! \n",
      "\n",
      "Preprocessing on message passing matrices\n",
      "Done! \n",
      "\n",
      "Initializing TF session\n",
      "Done! \n",
      "\n",
      "STEP 2/3 - MODEL TRAINING \n",
      "\n",
      "Starting training\n",
      "Iteration: 0001 Loss: 1.71352 Time: 0.15002\n",
      "Iteration: 0002 Loss: 1.73280 Time: 0.00298\n",
      "Iteration: 0003 Loss: 1.61126 Time: 0.00300\n",
      "Iteration: 0004 Loss: 1.72878 Time: 0.00400\n",
      "Iteration: 0005 Loss: 1.53318 Time: 0.00202\n",
      "Iteration: 0006 Loss: 1.62980 Time: 0.00200\n",
      "Iteration: 0007 Loss: 1.45259 Time: 0.00202\n",
      "Iteration: 0008 Loss: 1.63907 Time: 0.00299\n",
      "Iteration: 0009 Loss: 1.55149 Time: 0.00301\n",
      "Iteration: 0010 Loss: 1.51593 Time: 0.00298\n",
      "Iteration: 0011 Loss: 1.61382 Time: 0.00300\n",
      "Iteration: 0012 Loss: 1.49981 Time: 0.00307\n",
      "Iteration: 0013 Loss: 1.58420 Time: 0.00294\n",
      "Iteration: 0014 Loss: 1.44933 Time: 0.00301\n",
      "Iteration: 0015 Loss: 1.43213 Time: 0.00499\n",
      "Iteration: 0016 Loss: 1.35792 Time: 0.00200\n",
      "Iteration: 0017 Loss: 1.46353 Time: 0.00193\n",
      "Iteration: 0018 Loss: 1.41427 Time: 0.00307\n",
      "Iteration: 0019 Loss: 1.42502 Time: 0.00293\n",
      "Iteration: 0020 Loss: 1.34904 Time: 0.00300\n",
      "Iteration: 0021 Loss: 1.35156 Time: 0.00302\n",
      "Iteration: 0022 Loss: 1.42167 Time: 0.00200\n",
      "Iteration: 0023 Loss: 1.35956 Time: 0.00405\n",
      "Iteration: 0024 Loss: 1.33215 Time: 0.00295\n",
      "Iteration: 0025 Loss: 1.26986 Time: 0.00200\n",
      "Iteration: 0026 Loss: 1.23546 Time: 0.00401\n",
      "Iteration: 0027 Loss: 1.30333 Time: 0.00200\n",
      "Iteration: 0028 Loss: 1.16247 Time: 0.00200\n",
      "Iteration: 0029 Loss: 1.17268 Time: 0.00300\n",
      "Iteration: 0030 Loss: 1.23174 Time: 0.00200\n",
      "Iteration: 0031 Loss: 1.23126 Time: 0.00193\n",
      "Iteration: 0032 Loss: 1.16188 Time: 0.00306\n",
      "Iteration: 0033 Loss: 1.18467 Time: 0.00193\n",
      "Iteration: 0034 Loss: 1.17939 Time: 0.00200\n",
      "Iteration: 0035 Loss: 1.23488 Time: 0.00304\n",
      "Iteration: 0036 Loss: 1.10466 Time: 0.00196\n",
      "Iteration: 0037 Loss: 1.12399 Time: 0.00194\n",
      "Iteration: 0038 Loss: 1.06156 Time: 0.00301\n",
      "Iteration: 0039 Loss: 1.10639 Time: 0.00202\n",
      "Iteration: 0040 Loss: 1.17598 Time: 0.00293\n",
      "Iteration: 0041 Loss: 1.11385 Time: 0.00200\n",
      "Iteration: 0042 Loss: 1.02669 Time: 0.00300\n",
      "Iteration: 0043 Loss: 1.04492 Time: 0.00209\n",
      "Iteration: 0044 Loss: 1.11417 Time: 0.00188\n",
      "Iteration: 0045 Loss: 1.08578 Time: 0.00300\n",
      "Iteration: 0046 Loss: 1.06325 Time: 0.00306\n",
      "Iteration: 0047 Loss: 1.07841 Time: 0.00294\n",
      "Iteration: 0048 Loss: 1.11083 Time: 0.00400\n",
      "Iteration: 0049 Loss: 1.01875 Time: 0.00300\n",
      "Iteration: 0050 Loss: 0.92979 Time: 0.00307\n",
      "Iteration: 0051 Loss: 1.00325 Time: 0.00194\n",
      "Iteration: 0052 Loss: 0.95243 Time: 0.00309\n",
      "Iteration: 0053 Loss: 1.02376 Time: 0.00291\n",
      "Iteration: 0054 Loss: 1.00569 Time: 0.00298\n",
      "Iteration: 0055 Loss: 0.90381 Time: 0.00200\n",
      "Iteration: 0056 Loss: 1.00711 Time: 0.00300\n",
      "Iteration: 0057 Loss: 0.95597 Time: 0.00301\n",
      "Iteration: 0058 Loss: 0.93220 Time: 0.00200\n",
      "Iteration: 0059 Loss: 0.95092 Time: 0.00195\n",
      "Iteration: 0060 Loss: 0.93183 Time: 0.00199\n",
      "Iteration: 0061 Loss: 0.90127 Time: 0.00300\n",
      "Iteration: 0062 Loss: 0.87580 Time: 0.00195\n",
      "Iteration: 0063 Loss: 0.87939 Time: 0.00300\n",
      "Iteration: 0064 Loss: 0.91720 Time: 0.00302\n",
      "Iteration: 0065 Loss: 0.93070 Time: 0.00300\n",
      "Iteration: 0066 Loss: 0.90365 Time: 0.00306\n",
      "Iteration: 0067 Loss: 0.94718 Time: 0.00192\n",
      "Iteration: 0068 Loss: 0.84364 Time: 0.00300\n",
      "Iteration: 0069 Loss: 0.89484 Time: 0.00194\n",
      "Iteration: 0070 Loss: 0.83668 Time: 0.00300\n",
      "Iteration: 0071 Loss: 0.88345 Time: 0.00305\n",
      "Iteration: 0072 Loss: 0.85346 Time: 0.00302\n",
      "Iteration: 0073 Loss: 0.82995 Time: 0.00193\n",
      "Iteration: 0074 Loss: 0.83538 Time: 0.00300\n",
      "Iteration: 0075 Loss: 0.84847 Time: 0.00189\n",
      "Iteration: 0076 Loss: 0.78050 Time: 0.00306\n",
      "Iteration: 0077 Loss: 0.81496 Time: 0.00300\n",
      "Iteration: 0078 Loss: 0.79370 Time: 0.00206\n",
      "Iteration: 0079 Loss: 0.83592 Time: 0.00288\n",
      "Iteration: 0080 Loss: 0.78394 Time: 0.00201\n",
      "Iteration: 0081 Loss: 0.81609 Time: 0.00206\n",
      "Iteration: 0082 Loss: 0.80421 Time: 0.00195\n",
      "Iteration: 0083 Loss: 0.77135 Time: 0.00300\n",
      "Iteration: 0084 Loss: 0.76959 Time: 0.00200\n",
      "Iteration: 0085 Loss: 0.79763 Time: 0.00201\n",
      "Iteration: 0086 Loss: 0.72732 Time: 0.00200\n",
      "Iteration: 0087 Loss: 0.80831 Time: 0.00301\n",
      "Iteration: 0088 Loss: 0.75568 Time: 0.00200\n",
      "Iteration: 0089 Loss: 0.74069 Time: 0.00299\n",
      "Iteration: 0090 Loss: 0.75378 Time: 0.00201\n",
      "Iteration: 0091 Loss: 0.78434 Time: 0.00200\n",
      "Iteration: 0092 Loss: 0.74589 Time: 0.00307\n",
      "Iteration: 0093 Loss: 0.80055 Time: 0.00293\n",
      "Iteration: 0094 Loss: 0.74576 Time: 0.00200\n",
      "Iteration: 0095 Loss: 0.72666 Time: 0.00308\n",
      "Iteration: 0096 Loss: 0.73450 Time: 0.00293\n",
      "Iteration: 0097 Loss: 0.70680 Time: 0.00309\n",
      "Iteration: 0098 Loss: 0.73036 Time: 0.00199\n",
      "Iteration: 0099 Loss: 0.73979 Time: 0.00295\n",
      "Iteration: 0100 Loss: 0.73189 Time: 0.00300\n",
      "Iteration: 0101 Loss: 0.72999 Time: 0.00300\n",
      "Iteration: 0102 Loss: 0.74621 Time: 0.00106\n",
      "Iteration: 0103 Loss: 0.78531 Time: 0.00199\n",
      "Iteration: 0104 Loss: 0.72028 Time: 0.00200\n",
      "Iteration: 0105 Loss: 0.72049 Time: 0.00200\n",
      "Iteration: 0106 Loss: 0.71600 Time: 0.00101\n",
      "Iteration: 0107 Loss: 0.73185 Time: 0.00204\n",
      "Iteration: 0108 Loss: 0.72351 Time: 0.00197\n",
      "Iteration: 0109 Loss: 0.73169 Time: 0.00300\n",
      "Iteration: 0110 Loss: 0.75084 Time: 0.00199\n",
      "Iteration: 0111 Loss: 0.72108 Time: 0.00307\n",
      "Iteration: 0112 Loss: 0.70123 Time: 0.00193\n",
      "Iteration: 0113 Loss: 0.72205 Time: 0.00200\n",
      "Iteration: 0114 Loss: 0.72511 Time: 0.00301\n",
      "Iteration: 0115 Loss: 0.73868 Time: 0.00199\n",
      "Iteration: 0116 Loss: 0.73745 Time: 0.00200\n",
      "Iteration: 0117 Loss: 0.68335 Time: 0.00300\n",
      "Iteration: 0118 Loss: 0.71532 Time: 0.00200\n",
      "Iteration: 0119 Loss: 0.71552 Time: 0.00100\n",
      "Iteration: 0120 Loss: 0.69255 Time: 0.00207\n",
      "Iteration: 0121 Loss: 0.73005 Time: 0.00193\n",
      "Iteration: 0122 Loss: 0.66823 Time: 0.00204\n",
      "Iteration: 0123 Loss: 0.71605 Time: 0.00303\n",
      "Iteration: 0124 Loss: 0.70764 Time: 0.00394\n",
      "Iteration: 0125 Loss: 0.69355 Time: 0.00204\n",
      "Iteration: 0126 Loss: 0.69199 Time: 0.00196\n",
      "Iteration: 0127 Loss: 0.68012 Time: 0.00306\n",
      "Iteration: 0128 Loss: 0.71978 Time: 0.00203\n",
      "Iteration: 0129 Loss: 0.69603 Time: 0.00309\n",
      "Iteration: 0130 Loss: 0.71068 Time: 0.00191\n",
      "Iteration: 0131 Loss: 0.71382 Time: 0.00315\n",
      "Iteration: 0132 Loss: 0.70678 Time: 0.00188\n",
      "Iteration: 0133 Loss: 0.69704 Time: 0.00299\n",
      "Iteration: 0134 Loss: 0.68699 Time: 0.00308\n",
      "Iteration: 0135 Loss: 0.68362 Time: 0.00292\n",
      "Iteration: 0136 Loss: 0.70664 Time: 0.00400\n",
      "Iteration: 0137 Loss: 0.70214 Time: 0.00300\n",
      "Iteration: 0138 Loss: 0.69689 Time: 0.00300\n",
      "Iteration: 0139 Loss: 0.67202 Time: 0.00305\n",
      "Iteration: 0140 Loss: 0.66744 Time: 0.00197\n",
      "Iteration: 0141 Loss: 0.68728 Time: 0.00199\n",
      "Iteration: 0142 Loss: 0.69691 Time: 0.00098\n",
      "Iteration: 0143 Loss: 0.68276 Time: 0.00207\n",
      "Iteration: 0144 Loss: 0.67822 Time: 0.00198\n",
      "Iteration: 0145 Loss: 0.68015 Time: 0.00195\n",
      "Iteration: 0146 Loss: 0.66268 Time: 0.00302\n",
      "Iteration: 0147 Loss: 0.70627 Time: 0.00203\n",
      "Iteration: 0148 Loss: 0.67869 Time: 0.00200\n",
      "Iteration: 0149 Loss: 0.66877 Time: 0.00199\n",
      "Iteration: 0150 Loss: 0.66071 Time: 0.00300\n",
      "Iteration: 0151 Loss: 0.66956 Time: 0.00101\n",
      "Iteration: 0152 Loss: 0.65728 Time: 0.00205\n",
      "Iteration: 0153 Loss: 0.65814 Time: 0.00195\n",
      "Iteration: 0154 Loss: 0.63688 Time: 0.00301\n",
      "Iteration: 0155 Loss: 0.67257 Time: 0.00206\n",
      "Iteration: 0156 Loss: 0.64628 Time: 0.00296\n",
      "Iteration: 0157 Loss: 0.64060 Time: 0.00317\n",
      "Iteration: 0158 Loss: 0.67892 Time: 0.00123\n",
      "Iteration: 0159 Loss: 0.66837 Time: 0.00199\n",
      "Iteration: 0160 Loss: 0.68031 Time: 0.00297\n",
      "Iteration: 0161 Loss: 0.65835 Time: 0.00402\n",
      "Iteration: 0162 Loss: 0.68368 Time: 0.00199\n",
      "Iteration: 0163 Loss: 0.67842 Time: 0.00203\n",
      "Iteration: 0164 Loss: 0.64468 Time: 0.00197\n",
      "Iteration: 0165 Loss: 0.64988 Time: 0.00406\n",
      "Iteration: 0166 Loss: 0.66646 Time: 0.00295\n",
      "Iteration: 0167 Loss: 0.66861 Time: 0.00200\n",
      "Iteration: 0168 Loss: 0.65407 Time: 0.00197\n",
      "Iteration: 0169 Loss: 0.63971 Time: 0.00395\n",
      "Iteration: 0170 Loss: 0.65854 Time: 0.00210\n",
      "Iteration: 0171 Loss: 0.65857 Time: 0.00303\n",
      "Iteration: 0172 Loss: 0.66475 Time: 0.00198\n",
      "Iteration: 0173 Loss: 0.64461 Time: 0.00195\n",
      "Iteration: 0174 Loss: 0.64720 Time: 0.00201\n",
      "Iteration: 0175 Loss: 0.66420 Time: 0.00300\n",
      "Iteration: 0176 Loss: 0.66740 Time: 0.00304\n",
      "Iteration: 0177 Loss: 0.65537 Time: 0.00191\n",
      "Iteration: 0178 Loss: 0.65190 Time: 0.00200\n",
      "Iteration: 0179 Loss: 0.65942 Time: 0.00206\n",
      "Iteration: 0180 Loss: 0.65088 Time: 0.00194\n",
      "Iteration: 0181 Loss: 0.64359 Time: 0.00197\n",
      "Iteration: 0182 Loss: 0.65102 Time: 0.00198\n",
      "Iteration: 0183 Loss: 0.65186 Time: 0.00317\n",
      "Iteration: 0184 Loss: 0.64486 Time: 0.00127\n",
      "Iteration: 0185 Loss: 0.61665 Time: 0.00175\n",
      "Iteration: 0186 Loss: 0.65020 Time: 0.00305\n",
      "Iteration: 0187 Loss: 0.64574 Time: 0.00199\n",
      "Iteration: 0188 Loss: 0.64216 Time: 0.00298\n",
      "Iteration: 0189 Loss: 0.61850 Time: 0.00307\n",
      "Iteration: 0190 Loss: 0.63336 Time: 0.00200\n",
      "Iteration: 0191 Loss: 0.66766 Time: 0.00193\n",
      "Iteration: 0192 Loss: 0.65667 Time: 0.00201\n",
      "Iteration: 0193 Loss: 0.62940 Time: 0.00198\n",
      "Iteration: 0194 Loss: 0.64479 Time: 0.00304\n",
      "Iteration: 0195 Loss: 0.64267 Time: 0.00191\n",
      "Iteration: 0196 Loss: 0.63299 Time: 0.00312\n",
      "Iteration: 0197 Loss: 0.64409 Time: 0.00193\n",
      "Iteration: 0198 Loss: 0.64614 Time: 0.00197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0199 Loss: 0.64398 Time: 0.00300\n",
      "Iteration: 0200 Loss: 0.64079 Time: 0.00198\n",
      "Done! \n",
      "\n",
      "STEP 3/3 - MODEL EVALUATION \n",
      "\n",
      "Computing the final embedding vectors, for evaluation\n",
      "Done! \n",
      "\n",
      "Testing: link prediction\n",
      "Done! \n",
      "\n",
      "EXPERIMENTS ON MODEL 2 / 10 \n",
      "\n",
      "STEP 1/3 - PREPROCESSING STEPS \n",
      "\n",
      "Masking some edges from the training graph, for link prediction\n",
      "(validation set: 1.0 % of edges - test set: 10.0 % of edges)\n",
      "Done! \n",
      "\n",
      "Preprocessing node features\n",
      "Done! \n",
      "\n",
      "Running the Louvain algorithm for community detection\n",
      "as a preprocessing step for the encoder\n",
      "Done! Louvain has found 5 communities \n",
      "\n",
      "Setting up the model and the optimizer\n",
      "Done! \n",
      "\n",
      "Preprocessing on message passing matrices\n",
      "Done! \n",
      "\n",
      "Initializing TF session\n",
      "Done! \n",
      "\n",
      "STEP 2/3 - MODEL TRAINING \n",
      "\n",
      "Starting training\n",
      "Iteration: 0001 Loss: 1.66949 Time: 0.07974\n",
      "Iteration: 0002 Loss: 1.57607 Time: 0.00402\n",
      "Iteration: 0003 Loss: 1.61420 Time: 0.00298\n",
      "Iteration: 0004 Loss: 1.65514 Time: 0.00301\n",
      "Iteration: 0005 Loss: 1.66427 Time: 0.00300\n",
      "Iteration: 0006 Loss: 1.62601 Time: 0.00299\n",
      "Iteration: 0007 Loss: 1.58812 Time: 0.00201\n",
      "Iteration: 0008 Loss: 1.60316 Time: 0.00300\n",
      "Iteration: 0009 Loss: 1.48036 Time: 0.00302\n",
      "Iteration: 0010 Loss: 1.42261 Time: 0.00399\n",
      "Iteration: 0011 Loss: 1.51252 Time: 0.00199\n",
      "Iteration: 0012 Loss: 1.65849 Time: 0.00401\n",
      "Iteration: 0013 Loss: 1.59892 Time: 0.00199\n",
      "Iteration: 0014 Loss: 1.41613 Time: 0.00300\n",
      "Iteration: 0015 Loss: 1.39217 Time: 0.00300\n",
      "Iteration: 0016 Loss: 1.39979 Time: 0.00330\n",
      "Iteration: 0017 Loss: 1.40173 Time: 0.00200\n",
      "Iteration: 0018 Loss: 1.38030 Time: 0.00302\n",
      "Iteration: 0019 Loss: 1.35131 Time: 0.00399\n",
      "Iteration: 0020 Loss: 1.41181 Time: 0.00500\n",
      "Iteration: 0021 Loss: 1.32113 Time: 0.00300\n",
      "Iteration: 0022 Loss: 1.34949 Time: 0.00301\n",
      "Iteration: 0023 Loss: 1.32474 Time: 0.00198\n",
      "Iteration: 0024 Loss: 1.30435 Time: 0.00401\n",
      "Iteration: 0025 Loss: 1.22886 Time: 0.00400\n",
      "Iteration: 0026 Loss: 1.35497 Time: 0.00300\n",
      "Iteration: 0027 Loss: 1.28545 Time: 0.00312\n",
      "Iteration: 0028 Loss: 1.22868 Time: 0.00287\n",
      "Iteration: 0029 Loss: 1.18190 Time: 0.00199\n",
      "Iteration: 0030 Loss: 1.25889 Time: 0.00301\n",
      "Iteration: 0031 Loss: 1.18529 Time: 0.00299\n",
      "Iteration: 0032 Loss: 1.15712 Time: 0.00301\n",
      "Iteration: 0033 Loss: 1.24373 Time: 0.00199\n",
      "Iteration: 0034 Loss: 1.16689 Time: 0.00200\n",
      "Iteration: 0035 Loss: 1.18905 Time: 0.00207\n",
      "Iteration: 0036 Loss: 1.18138 Time: 0.00203\n",
      "Iteration: 0037 Loss: 1.13854 Time: 0.00202\n",
      "Iteration: 0038 Loss: 1.14592 Time: 0.00199\n",
      "Iteration: 0039 Loss: 1.12675 Time: 0.00200\n",
      "Iteration: 0040 Loss: 1.14424 Time: 0.00194\n",
      "Iteration: 0041 Loss: 1.09709 Time: 0.00200\n",
      "Iteration: 0042 Loss: 1.12300 Time: 0.00202\n",
      "Iteration: 0043 Loss: 1.10064 Time: 0.00207\n",
      "Iteration: 0044 Loss: 1.12979 Time: 0.00295\n",
      "Iteration: 0045 Loss: 1.15637 Time: 0.00307\n",
      "Iteration: 0046 Loss: 1.02046 Time: 0.00197\n",
      "Iteration: 0047 Loss: 1.09169 Time: 0.00200\n",
      "Iteration: 0048 Loss: 1.04215 Time: 0.00208\n",
      "Iteration: 0049 Loss: 1.00633 Time: 0.00197\n",
      "Iteration: 0050 Loss: 1.05485 Time: 0.00194\n",
      "Iteration: 0051 Loss: 1.03621 Time: 0.00195\n",
      "Iteration: 0052 Loss: 0.99901 Time: 0.00305\n",
      "Iteration: 0053 Loss: 1.05074 Time: 0.00296\n",
      "Iteration: 0054 Loss: 1.07810 Time: 0.00308\n",
      "Iteration: 0055 Loss: 0.97934 Time: 0.00290\n",
      "Iteration: 0056 Loss: 1.02209 Time: 0.00244\n",
      "Iteration: 0057 Loss: 1.00226 Time: 0.00301\n",
      "Iteration: 0058 Loss: 0.92595 Time: 0.00195\n",
      "Iteration: 0059 Loss: 0.94353 Time: 0.00210\n",
      "Iteration: 0060 Loss: 0.95922 Time: 0.00203\n",
      "Iteration: 0061 Loss: 0.97963 Time: 0.00297\n",
      "Iteration: 0062 Loss: 0.96165 Time: 0.00201\n",
      "Iteration: 0063 Loss: 0.93228 Time: 0.00201\n",
      "Iteration: 0064 Loss: 0.98484 Time: 0.00196\n",
      "Iteration: 0065 Loss: 0.93142 Time: 0.00198\n",
      "Iteration: 0066 Loss: 0.93635 Time: 0.00202\n",
      "Iteration: 0067 Loss: 0.92735 Time: 0.00201\n",
      "Iteration: 0068 Loss: 0.89152 Time: 0.00199\n",
      "Iteration: 0069 Loss: 0.87508 Time: 0.00307\n",
      "Iteration: 0070 Loss: 0.90296 Time: 0.00294\n",
      "Iteration: 0071 Loss: 0.89402 Time: 0.00306\n",
      "Iteration: 0072 Loss: 0.83041 Time: 0.00304\n",
      "Iteration: 0073 Loss: 0.85385 Time: 0.00208\n",
      "Iteration: 0074 Loss: 0.88377 Time: 0.00299\n",
      "Iteration: 0075 Loss: 0.86123 Time: 0.00300\n",
      "Iteration: 0076 Loss: 0.85271 Time: 0.00402\n",
      "Iteration: 0077 Loss: 0.87112 Time: 0.00202\n",
      "Iteration: 0078 Loss: 0.87233 Time: 0.00200\n",
      "Iteration: 0079 Loss: 0.78628 Time: 0.00195\n",
      "Iteration: 0080 Loss: 0.82414 Time: 0.00310\n",
      "Iteration: 0081 Loss: 0.79815 Time: 0.00292\n",
      "Iteration: 0082 Loss: 0.76647 Time: 0.00298\n",
      "Iteration: 0083 Loss: 0.76926 Time: 0.00300\n",
      "Iteration: 0084 Loss: 0.78406 Time: 0.00200\n",
      "Iteration: 0085 Loss: 0.79707 Time: 0.00301\n",
      "Iteration: 0086 Loss: 0.78763 Time: 0.00199\n",
      "Iteration: 0087 Loss: 0.78440 Time: 0.00200\n",
      "Iteration: 0088 Loss: 0.76926 Time: 0.00399\n",
      "Iteration: 0089 Loss: 0.78628 Time: 0.00305\n",
      "Iteration: 0090 Loss: 0.76891 Time: 0.00196\n",
      "Iteration: 0091 Loss: 0.70897 Time: 0.00301\n",
      "Iteration: 0092 Loss: 0.75300 Time: 0.00199\n",
      "Iteration: 0093 Loss: 0.77696 Time: 0.00300\n",
      "Iteration: 0094 Loss: 0.74381 Time: 0.00300\n",
      "Iteration: 0095 Loss: 0.76090 Time: 0.00200\n",
      "Iteration: 0096 Loss: 0.73304 Time: 0.00200\n",
      "Iteration: 0097 Loss: 0.75926 Time: 0.00301\n",
      "Iteration: 0098 Loss: 0.75066 Time: 0.00206\n",
      "Iteration: 0099 Loss: 0.72685 Time: 0.00195\n",
      "Iteration: 0100 Loss: 0.74524 Time: 0.00299\n",
      "Iteration: 0101 Loss: 0.72424 Time: 0.00200\n",
      "Iteration: 0102 Loss: 0.73497 Time: 0.00308\n",
      "Iteration: 0103 Loss: 0.74619 Time: 0.00194\n",
      "Iteration: 0104 Loss: 0.73382 Time: 0.00304\n",
      "Iteration: 0105 Loss: 0.71596 Time: 0.00194\n",
      "Iteration: 0106 Loss: 0.72538 Time: 0.00196\n",
      "Iteration: 0107 Loss: 0.73705 Time: 0.00200\n",
      "Iteration: 0108 Loss: 0.78076 Time: 0.00299\n",
      "Iteration: 0109 Loss: 0.74596 Time: 0.00201\n",
      "Iteration: 0110 Loss: 0.70778 Time: 0.00191\n",
      "Iteration: 0111 Loss: 0.74453 Time: 0.00203\n",
      "Iteration: 0112 Loss: 0.73033 Time: 0.00194\n",
      "Iteration: 0113 Loss: 0.73065 Time: 0.00300\n",
      "Iteration: 0114 Loss: 0.70370 Time: 0.00199\n",
      "Iteration: 0115 Loss: 0.72223 Time: 0.00197\n",
      "Iteration: 0116 Loss: 0.70863 Time: 0.00212\n",
      "Iteration: 0117 Loss: 0.75952 Time: 0.00294\n",
      "Iteration: 0118 Loss: 0.72765 Time: 0.00191\n",
      "Iteration: 0119 Loss: 0.69901 Time: 0.00204\n",
      "Iteration: 0120 Loss: 0.70777 Time: 0.00208\n",
      "Iteration: 0121 Loss: 0.70196 Time: 0.00104\n",
      "Iteration: 0122 Loss: 0.72919 Time: 0.00201\n",
      "Iteration: 0123 Loss: 0.73059 Time: 0.00196\n",
      "Iteration: 0124 Loss: 0.70083 Time: 0.00194\n",
      "Iteration: 0125 Loss: 0.70874 Time: 0.00201\n",
      "Iteration: 0126 Loss: 0.67533 Time: 0.00239\n",
      "Iteration: 0127 Loss: 0.69494 Time: 0.00274\n",
      "Iteration: 0128 Loss: 0.69143 Time: 0.00123\n",
      "Iteration: 0129 Loss: 0.66695 Time: 0.00199\n",
      "Iteration: 0130 Loss: 0.71310 Time: 0.00205\n",
      "Iteration: 0131 Loss: 0.69235 Time: 0.00307\n",
      "Iteration: 0132 Loss: 0.69396 Time: 0.00297\n",
      "Iteration: 0133 Loss: 0.69675 Time: 0.00292\n",
      "Iteration: 0134 Loss: 0.68028 Time: 0.00305\n",
      "Iteration: 0135 Loss: 0.70878 Time: 0.00197\n",
      "Iteration: 0136 Loss: 0.67375 Time: 0.00300\n",
      "Iteration: 0137 Loss: 0.68885 Time: 0.00205\n",
      "Iteration: 0138 Loss: 0.69458 Time: 0.00305\n",
      "Iteration: 0139 Loss: 0.70814 Time: 0.00296\n",
      "Iteration: 0140 Loss: 0.68918 Time: 0.00201\n",
      "Iteration: 0141 Loss: 0.65626 Time: 0.00203\n",
      "Iteration: 0142 Loss: 0.70390 Time: 0.00296\n",
      "Iteration: 0143 Loss: 0.69022 Time: 0.00297\n",
      "Iteration: 0144 Loss: 0.67581 Time: 0.00101\n",
      "Iteration: 0145 Loss: 0.67552 Time: 0.00296\n",
      "Iteration: 0146 Loss: 0.69951 Time: 0.00200\n",
      "Iteration: 0147 Loss: 0.70245 Time: 0.00206\n",
      "Iteration: 0148 Loss: 0.67418 Time: 0.00197\n",
      "Iteration: 0149 Loss: 0.67250 Time: 0.00300\n",
      "Iteration: 0150 Loss: 0.71526 Time: 0.00197\n",
      "Iteration: 0151 Loss: 0.67517 Time: 0.00301\n",
      "Iteration: 0152 Loss: 0.66035 Time: 0.00198\n",
      "Iteration: 0153 Loss: 0.66939 Time: 0.00300\n",
      "Iteration: 0154 Loss: 0.67563 Time: 0.00300\n",
      "Iteration: 0155 Loss: 0.66077 Time: 0.00308\n",
      "Iteration: 0156 Loss: 0.66946 Time: 0.00305\n",
      "Iteration: 0157 Loss: 0.67777 Time: 0.00204\n",
      "Iteration: 0158 Loss: 0.66219 Time: 0.00199\n",
      "Iteration: 0159 Loss: 0.64974 Time: 0.00205\n",
      "Iteration: 0160 Loss: 0.67860 Time: 0.00395\n",
      "Iteration: 0161 Loss: 0.67108 Time: 0.00302\n",
      "Iteration: 0162 Loss: 0.69437 Time: 0.00298\n",
      "Iteration: 0163 Loss: 0.65897 Time: 0.00300\n",
      "Iteration: 0164 Loss: 0.66055 Time: 0.00212\n",
      "Iteration: 0165 Loss: 0.65616 Time: 0.00385\n",
      "Iteration: 0166 Loss: 0.65761 Time: 0.00300\n",
      "Iteration: 0167 Loss: 0.66601 Time: 0.00204\n",
      "Iteration: 0168 Loss: 0.65968 Time: 0.00201\n",
      "Iteration: 0169 Loss: 0.64934 Time: 0.00307\n",
      "Iteration: 0170 Loss: 0.65383 Time: 0.00105\n",
      "Iteration: 0171 Loss: 0.64519 Time: 0.00413\n",
      "Iteration: 0172 Loss: 0.65406 Time: 0.00197\n",
      "Iteration: 0173 Loss: 0.63936 Time: 0.00205\n",
      "Iteration: 0174 Loss: 0.65007 Time: 0.00205\n",
      "Iteration: 0175 Loss: 0.66249 Time: 0.00305\n",
      "Iteration: 0176 Loss: 0.65606 Time: 0.00298\n",
      "Iteration: 0177 Loss: 0.65518 Time: 0.00294\n",
      "Iteration: 0178 Loss: 0.63762 Time: 0.00305\n",
      "Iteration: 0179 Loss: 0.64952 Time: 0.00304\n",
      "Iteration: 0180 Loss: 0.63781 Time: 0.00290\n",
      "Iteration: 0181 Loss: 0.65768 Time: 0.00208\n",
      "Iteration: 0182 Loss: 0.65753 Time: 0.00299\n",
      "Iteration: 0183 Loss: 0.63933 Time: 0.00293\n",
      "Iteration: 0184 Loss: 0.65408 Time: 0.00313\n",
      "Iteration: 0185 Loss: 0.64594 Time: 0.00293\n",
      "Iteration: 0186 Loss: 0.63603 Time: 0.00308\n",
      "Iteration: 0187 Loss: 0.65640 Time: 0.00189\n",
      "Iteration: 0188 Loss: 0.64297 Time: 0.00206\n",
      "Iteration: 0189 Loss: 0.65767 Time: 0.00197\n",
      "Iteration: 0190 Loss: 0.64763 Time: 0.00211\n",
      "Iteration: 0191 Loss: 0.65147 Time: 0.00203\n",
      "Iteration: 0192 Loss: 0.63460 Time: 0.00196\n",
      "Iteration: 0193 Loss: 0.63575 Time: 0.00401\n",
      "Iteration: 0194 Loss: 0.62365 Time: 0.00350\n",
      "Iteration: 0195 Loss: 0.62977 Time: 0.00202\n",
      "Iteration: 0196 Loss: 0.63603 Time: 0.00325\n",
      "Iteration: 0197 Loss: 0.62348 Time: 0.00193\n",
      "Iteration: 0198 Loss: 0.62912 Time: 0.00295\n",
      "Iteration: 0199 Loss: 0.65093 Time: 0.00196\n",
      "Iteration: 0200 Loss: 0.64456 Time: 0.00301\n",
      "Done! \n",
      "\n",
      "STEP 3/3 - MODEL EVALUATION \n",
      "\n",
      "Computing the final embedding vectors, for evaluation\n",
      "Done! \n",
      "\n",
      "Testing: link prediction\n",
      "Done! \n",
      "\n",
      "EXPERIMENTS ON MODEL 3 / 10 \n",
      "\n",
      "STEP 1/3 - PREPROCESSING STEPS \n",
      "\n",
      "Masking some edges from the training graph, for link prediction\n",
      "(validation set: 1.0 % of edges - test set: 10.0 % of edges)\n",
      "Done! \n",
      "\n",
      "Preprocessing node features\n",
      "Done! \n",
      "\n",
      "Running the Louvain algorithm for community detection\n",
      "as a preprocessing step for the encoder\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Louvain has found 5 communities \n",
      "\n",
      "Setting up the model and the optimizer\n",
      "Done! \n",
      "\n",
      "Preprocessing on message passing matrices\n",
      "Done! \n",
      "\n",
      "Initializing TF session\n",
      "Done! \n",
      "\n",
      "STEP 2/3 - MODEL TRAINING \n",
      "\n",
      "Starting training\n",
      "Iteration: 0001 Loss: 1.74391 Time: 0.08900\n",
      "Iteration: 0002 Loss: 1.68294 Time: 0.00300\n",
      "Iteration: 0003 Loss: 1.50520 Time: 0.00200\n",
      "Iteration: 0004 Loss: 1.63834 Time: 0.00300\n",
      "Iteration: 0005 Loss: 1.64966 Time: 0.00300\n",
      "Iteration: 0006 Loss: 1.56134 Time: 0.00199\n",
      "Iteration: 0007 Loss: 1.57867 Time: 0.00200\n",
      "Iteration: 0008 Loss: 1.52213 Time: 0.00200\n",
      "Iteration: 0009 Loss: 1.40850 Time: 0.00301\n",
      "Iteration: 0010 Loss: 1.57122 Time: 0.00300\n",
      "Iteration: 0011 Loss: 1.57856 Time: 0.00301\n",
      "Iteration: 0012 Loss: 1.56102 Time: 0.00199\n",
      "Iteration: 0013 Loss: 1.45907 Time: 0.00299\n",
      "Iteration: 0014 Loss: 1.30615 Time: 0.00200\n",
      "Iteration: 0015 Loss: 1.35434 Time: 0.00200\n",
      "Iteration: 0016 Loss: 1.37045 Time: 0.00300\n",
      "Iteration: 0017 Loss: 1.44922 Time: 0.00300\n",
      "Iteration: 0018 Loss: 1.52146 Time: 0.00201\n",
      "Iteration: 0019 Loss: 1.35348 Time: 0.00300\n",
      "Iteration: 0020 Loss: 1.31802 Time: 0.00300\n",
      "Iteration: 0021 Loss: 1.35784 Time: 0.00199\n",
      "Iteration: 0022 Loss: 1.37657 Time: 0.00302\n",
      "Iteration: 0023 Loss: 1.41908 Time: 0.00194\n",
      "Iteration: 0024 Loss: 1.20778 Time: 0.00214\n",
      "Iteration: 0025 Loss: 1.29380 Time: 0.00213\n",
      "Iteration: 0026 Loss: 1.28508 Time: 0.00204\n",
      "Iteration: 0027 Loss: 1.24680 Time: 0.00345\n",
      "Iteration: 0028 Loss: 1.22573 Time: 0.00152\n",
      "Iteration: 0029 Loss: 1.14158 Time: 0.00201\n",
      "Iteration: 0030 Loss: 1.20770 Time: 0.00300\n",
      "Iteration: 0031 Loss: 1.21640 Time: 0.00194\n",
      "Iteration: 0032 Loss: 1.20945 Time: 0.00204\n",
      "Iteration: 0033 Loss: 1.13420 Time: 0.00201\n",
      "Iteration: 0034 Loss: 1.18322 Time: 0.00197\n",
      "Iteration: 0035 Loss: 1.13985 Time: 0.00399\n",
      "Iteration: 0036 Loss: 1.17750 Time: 0.00200\n",
      "Iteration: 0037 Loss: 1.13033 Time: 0.00301\n",
      "Iteration: 0038 Loss: 1.08627 Time: 0.00110\n",
      "Iteration: 0039 Loss: 1.14280 Time: 0.00199\n",
      "Iteration: 0040 Loss: 1.06219 Time: 0.00204\n",
      "Iteration: 0041 Loss: 1.10975 Time: 0.00196\n",
      "Iteration: 0042 Loss: 1.12298 Time: 0.00199\n",
      "Iteration: 0043 Loss: 1.09116 Time: 0.00104\n",
      "Iteration: 0044 Loss: 1.10101 Time: 0.00317\n",
      "Iteration: 0045 Loss: 1.02897 Time: 0.00283\n",
      "Iteration: 0046 Loss: 1.08390 Time: 0.00204\n",
      "Iteration: 0047 Loss: 1.05017 Time: 0.00196\n",
      "Iteration: 0048 Loss: 1.07776 Time: 0.00194\n",
      "Iteration: 0049 Loss: 1.06478 Time: 0.00194\n",
      "Iteration: 0050 Loss: 1.01431 Time: 0.00296\n",
      "Iteration: 0051 Loss: 1.01665 Time: 0.00198\n",
      "Iteration: 0052 Loss: 1.05198 Time: 0.00200\n",
      "Iteration: 0053 Loss: 0.95322 Time: 0.00396\n",
      "Iteration: 0054 Loss: 0.97116 Time: 0.00199\n",
      "Iteration: 0055 Loss: 0.98973 Time: 0.00197\n",
      "Iteration: 0056 Loss: 0.98422 Time: 0.00300\n",
      "Iteration: 0057 Loss: 1.01602 Time: 0.00200\n",
      "Iteration: 0058 Loss: 0.93375 Time: 0.00205\n",
      "Iteration: 0059 Loss: 0.98121 Time: 0.00299\n",
      "Iteration: 0060 Loss: 0.92009 Time: 0.00199\n",
      "Iteration: 0061 Loss: 0.94962 Time: 0.00302\n",
      "Iteration: 0062 Loss: 0.95076 Time: 0.00201\n",
      "Iteration: 0063 Loss: 0.97088 Time: 0.00308\n",
      "Iteration: 0064 Loss: 1.03715 Time: 0.00295\n",
      "Iteration: 0065 Loss: 0.87855 Time: 0.00296\n",
      "Iteration: 0066 Loss: 0.93974 Time: 0.00303\n",
      "Iteration: 0067 Loss: 0.97219 Time: 0.00305\n",
      "Iteration: 0068 Loss: 0.90622 Time: 0.00288\n",
      "Iteration: 0069 Loss: 0.84333 Time: 0.00305\n",
      "Iteration: 0070 Loss: 0.88318 Time: 0.00196\n",
      "Iteration: 0071 Loss: 0.88194 Time: 0.00400\n",
      "Iteration: 0072 Loss: 0.80167 Time: 0.00200\n",
      "Iteration: 0073 Loss: 0.84048 Time: 0.00191\n",
      "Iteration: 0074 Loss: 0.79897 Time: 0.00205\n",
      "Iteration: 0075 Loss: 0.82221 Time: 0.00309\n",
      "Iteration: 0076 Loss: 0.88516 Time: 0.00287\n",
      "Iteration: 0077 Loss: 0.78476 Time: 0.00297\n",
      "Iteration: 0078 Loss: 0.87412 Time: 0.00299\n",
      "Iteration: 0079 Loss: 0.77219 Time: 0.00197\n",
      "Iteration: 0080 Loss: 0.83141 Time: 0.00203\n",
      "Iteration: 0081 Loss: 0.83412 Time: 0.00315\n",
      "Iteration: 0082 Loss: 0.75025 Time: 0.00190\n",
      "Iteration: 0083 Loss: 0.78509 Time: 0.00300\n",
      "Iteration: 0084 Loss: 0.78902 Time: 0.00197\n",
      "Iteration: 0085 Loss: 0.76215 Time: 0.00205\n",
      "Iteration: 0086 Loss: 0.80728 Time: 0.00200\n",
      "Iteration: 0087 Loss: 0.78047 Time: 0.00202\n",
      "Iteration: 0088 Loss: 0.71355 Time: 0.00299\n",
      "Iteration: 0089 Loss: 0.73544 Time: 0.00295\n",
      "Iteration: 0090 Loss: 0.74762 Time: 0.00209\n",
      "Iteration: 0091 Loss: 0.77153 Time: 0.00305\n",
      "Iteration: 0092 Loss: 0.73490 Time: 0.00311\n",
      "Iteration: 0093 Loss: 0.77578 Time: 0.00196\n",
      "Iteration: 0094 Loss: 0.73075 Time: 0.00306\n",
      "Iteration: 0095 Loss: 0.75331 Time: 0.00195\n",
      "Iteration: 0096 Loss: 0.75588 Time: 0.00199\n",
      "Iteration: 0097 Loss: 0.75124 Time: 0.00198\n",
      "Iteration: 0098 Loss: 0.75248 Time: 0.00293\n",
      "Iteration: 0099 Loss: 0.75857 Time: 0.00197\n",
      "Iteration: 0100 Loss: 0.70928 Time: 0.00204\n",
      "Iteration: 0101 Loss: 0.72882 Time: 0.00308\n",
      "Iteration: 0102 Loss: 0.76558 Time: 0.00303\n",
      "Iteration: 0103 Loss: 0.75786 Time: 0.00099\n",
      "Iteration: 0104 Loss: 0.74895 Time: 0.00202\n",
      "Iteration: 0105 Loss: 0.73871 Time: 0.00201\n",
      "Iteration: 0106 Loss: 0.73804 Time: 0.00305\n",
      "Iteration: 0107 Loss: 0.73579 Time: 0.00292\n",
      "Iteration: 0108 Loss: 0.70394 Time: 0.00300\n",
      "Iteration: 0109 Loss: 0.72941 Time: 0.00330\n",
      "Iteration: 0110 Loss: 0.70375 Time: 0.00198\n",
      "Iteration: 0111 Loss: 0.71953 Time: 0.00343\n",
      "Iteration: 0112 Loss: 0.73256 Time: 0.00297\n",
      "Iteration: 0113 Loss: 0.69825 Time: 0.00295\n",
      "Iteration: 0114 Loss: 0.71626 Time: 0.00196\n",
      "Iteration: 0115 Loss: 0.69973 Time: 0.00201\n",
      "Iteration: 0116 Loss: 0.69911 Time: 0.00304\n",
      "Iteration: 0117 Loss: 0.71701 Time: 0.00298\n",
      "Iteration: 0118 Loss: 0.69263 Time: 0.00315\n",
      "Iteration: 0119 Loss: 0.71518 Time: 0.00282\n",
      "Iteration: 0120 Loss: 0.74122 Time: 0.00195\n",
      "Iteration: 0121 Loss: 0.67300 Time: 0.00199\n",
      "Iteration: 0122 Loss: 0.68412 Time: 0.00201\n",
      "Iteration: 0123 Loss: 0.73654 Time: 0.00303\n",
      "Iteration: 0124 Loss: 0.69935 Time: 0.00300\n",
      "Iteration: 0125 Loss: 0.69430 Time: 0.00213\n",
      "Iteration: 0126 Loss: 0.68278 Time: 0.00285\n",
      "Iteration: 0127 Loss: 0.70537 Time: 0.00300\n",
      "Iteration: 0128 Loss: 0.70447 Time: 0.00200\n",
      "Iteration: 0129 Loss: 0.69616 Time: 0.00307\n",
      "Iteration: 0130 Loss: 0.66765 Time: 0.00193\n",
      "Iteration: 0131 Loss: 0.67288 Time: 0.00200\n",
      "Iteration: 0132 Loss: 0.68761 Time: 0.00304\n",
      "Iteration: 0133 Loss: 0.69146 Time: 0.00205\n",
      "Iteration: 0134 Loss: 0.68233 Time: 0.00195\n",
      "Iteration: 0135 Loss: 0.70149 Time: 0.00308\n",
      "Iteration: 0136 Loss: 0.68076 Time: 0.00298\n",
      "Iteration: 0137 Loss: 0.69617 Time: 0.00295\n",
      "Iteration: 0138 Loss: 0.68310 Time: 0.00299\n",
      "Iteration: 0139 Loss: 0.67802 Time: 0.00300\n",
      "Iteration: 0140 Loss: 0.67639 Time: 0.00200\n",
      "Iteration: 0141 Loss: 0.70248 Time: 0.00300\n",
      "Iteration: 0142 Loss: 0.67597 Time: 0.00300\n",
      "Iteration: 0143 Loss: 0.67147 Time: 0.00200\n",
      "Iteration: 0144 Loss: 0.67556 Time: 0.00201\n",
      "Iteration: 0145 Loss: 0.65071 Time: 0.00299\n",
      "Iteration: 0146 Loss: 0.66636 Time: 0.00301\n",
      "Iteration: 0147 Loss: 0.66178 Time: 0.00202\n",
      "Iteration: 0148 Loss: 0.68998 Time: 0.00302\n",
      "Iteration: 0149 Loss: 0.68876 Time: 0.00242\n",
      "Iteration: 0150 Loss: 0.67819 Time: 0.00200\n",
      "Iteration: 0151 Loss: 0.63422 Time: 0.00300\n",
      "Iteration: 0152 Loss: 0.66174 Time: 0.00304\n",
      "Iteration: 0153 Loss: 0.67400 Time: 0.00296\n",
      "Iteration: 0154 Loss: 0.67453 Time: 0.00200\n",
      "Iteration: 0155 Loss: 0.68456 Time: 0.00204\n",
      "Iteration: 0156 Loss: 0.65685 Time: 0.00200\n",
      "Iteration: 0157 Loss: 0.65714 Time: 0.00301\n",
      "Iteration: 0158 Loss: 0.66058 Time: 0.00196\n",
      "Iteration: 0159 Loss: 0.65181 Time: 0.00300\n",
      "Iteration: 0160 Loss: 0.65613 Time: 0.00318\n",
      "Iteration: 0161 Loss: 0.66406 Time: 0.00281\n",
      "Iteration: 0162 Loss: 0.66683 Time: 0.00200\n",
      "Iteration: 0163 Loss: 0.64955 Time: 0.00295\n",
      "Iteration: 0164 Loss: 0.66684 Time: 0.00405\n",
      "Iteration: 0165 Loss: 0.66749 Time: 0.00195\n",
      "Iteration: 0166 Loss: 0.64976 Time: 0.00317\n",
      "Iteration: 0167 Loss: 0.67312 Time: 0.00197\n",
      "Iteration: 0168 Loss: 0.66473 Time: 0.00300\n",
      "Iteration: 0169 Loss: 0.64484 Time: 0.00200\n",
      "Iteration: 0170 Loss: 0.66721 Time: 0.00214\n",
      "Iteration: 0171 Loss: 0.64660 Time: 0.00217\n",
      "Iteration: 0172 Loss: 0.63726 Time: 0.00200\n",
      "Iteration: 0173 Loss: 0.64334 Time: 0.00301\n",
      "Iteration: 0174 Loss: 0.62799 Time: 0.00312\n",
      "Iteration: 0175 Loss: 0.61551 Time: 0.00287\n",
      "Iteration: 0176 Loss: 0.65821 Time: 0.00204\n",
      "Iteration: 0177 Loss: 0.62718 Time: 0.00297\n",
      "Iteration: 0178 Loss: 0.64134 Time: 0.00299\n",
      "Iteration: 0179 Loss: 0.63941 Time: 0.00406\n",
      "Iteration: 0180 Loss: 0.61774 Time: 0.00306\n",
      "Iteration: 0181 Loss: 0.64911 Time: 0.00332\n",
      "Iteration: 0182 Loss: 0.64783 Time: 0.00362\n",
      "Iteration: 0183 Loss: 0.63130 Time: 0.00295\n",
      "Iteration: 0184 Loss: 0.65882 Time: 0.00200\n",
      "Iteration: 0185 Loss: 0.61330 Time: 0.00297\n",
      "Iteration: 0186 Loss: 0.66340 Time: 0.00213\n",
      "Iteration: 0187 Loss: 0.61829 Time: 0.00214\n",
      "Iteration: 0188 Loss: 0.64876 Time: 0.00291\n",
      "Iteration: 0189 Loss: 0.61792 Time: 0.00200\n",
      "Iteration: 0190 Loss: 0.61130 Time: 0.00319\n",
      "Iteration: 0191 Loss: 0.63287 Time: 0.00098\n",
      "Iteration: 0192 Loss: 0.63181 Time: 0.00204\n",
      "Iteration: 0193 Loss: 0.62004 Time: 0.00310\n",
      "Iteration: 0194 Loss: 0.62851 Time: 0.00301\n",
      "Iteration: 0195 Loss: 0.63416 Time: 0.00285\n",
      "Iteration: 0196 Loss: 0.62829 Time: 0.00301\n",
      "Iteration: 0197 Loss: 0.63364 Time: 0.00300\n",
      "Iteration: 0198 Loss: 0.62976 Time: 0.00210\n",
      "Iteration: 0199 Loss: 0.62732 Time: 0.00196\n",
      "Iteration: 0200 Loss: 0.62551 Time: 0.00327\n",
      "Done! \n",
      "\n",
      "STEP 3/3 - MODEL EVALUATION \n",
      "\n",
      "Computing the final embedding vectors, for evaluation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! \n",
      "\n",
      "Testing: link prediction\n",
      "Done! \n",
      "\n",
      "EXPERIMENTS ON MODEL 4 / 10 \n",
      "\n",
      "STEP 1/3 - PREPROCESSING STEPS \n",
      "\n",
      "Masking some edges from the training graph, for link prediction\n",
      "(validation set: 1.0 % of edges - test set: 10.0 % of edges)\n",
      "Done! \n",
      "\n",
      "Preprocessing node features\n",
      "Done! \n",
      "\n",
      "Running the Louvain algorithm for community detection\n",
      "as a preprocessing step for the encoder\n",
      "Done! Louvain has found 5 communities \n",
      "\n",
      "Setting up the model and the optimizer\n",
      "Done! \n",
      "\n",
      "Preprocessing on message passing matrices\n",
      "Done! \n",
      "\n",
      "Initializing TF session\n",
      "Done! \n",
      "\n",
      "STEP 2/3 - MODEL TRAINING \n",
      "\n",
      "Starting training\n",
      "Iteration: 0001 Loss: 1.65408 Time: 0.09643\n",
      "Iteration: 0002 Loss: 1.64918 Time: 0.00300\n",
      "Iteration: 0003 Loss: 1.76869 Time: 0.00199\n",
      "Iteration: 0004 Loss: 1.59679 Time: 0.00200\n",
      "Iteration: 0005 Loss: 1.63460 Time: 0.00400\n",
      "Iteration: 0006 Loss: 1.71425 Time: 0.00526\n",
      "Iteration: 0007 Loss: 1.60837 Time: 0.00300\n",
      "Iteration: 0008 Loss: 1.50189 Time: 0.00199\n",
      "Iteration: 0009 Loss: 1.50343 Time: 0.00301\n",
      "Iteration: 0010 Loss: 1.54499 Time: 0.00199\n",
      "Iteration: 0011 Loss: 1.44231 Time: 0.00210\n",
      "Iteration: 0012 Loss: 1.52884 Time: 0.00221\n",
      "Iteration: 0013 Loss: 1.42623 Time: 0.00199\n",
      "Iteration: 0014 Loss: 1.44874 Time: 0.00200\n",
      "Iteration: 0015 Loss: 1.50097 Time: 0.00199\n",
      "Iteration: 0016 Loss: 1.47890 Time: 0.00300\n",
      "Iteration: 0017 Loss: 1.35310 Time: 0.00106\n",
      "Iteration: 0018 Loss: 1.40594 Time: 0.00206\n",
      "Iteration: 0019 Loss: 1.39409 Time: 0.00294\n",
      "Iteration: 0020 Loss: 1.24872 Time: 0.00307\n",
      "Iteration: 0021 Loss: 1.38727 Time: 0.00198\n",
      "Iteration: 0022 Loss: 1.33866 Time: 0.00294\n",
      "Iteration: 0023 Loss: 1.24579 Time: 0.00200\n",
      "Iteration: 0024 Loss: 1.30878 Time: 0.00318\n",
      "Iteration: 0025 Loss: 1.36969 Time: 0.00197\n",
      "Iteration: 0026 Loss: 1.26163 Time: 0.00212\n",
      "Iteration: 0027 Loss: 1.26359 Time: 0.00288\n",
      "Iteration: 0028 Loss: 1.21297 Time: 0.00300\n",
      "Iteration: 0029 Loss: 1.33289 Time: 0.00204\n",
      "Iteration: 0030 Loss: 1.24360 Time: 0.00302\n",
      "Iteration: 0031 Loss: 1.21429 Time: 0.00301\n",
      "Iteration: 0032 Loss: 1.13207 Time: 0.00197\n",
      "Iteration: 0033 Loss: 1.23611 Time: 0.00195\n",
      "Iteration: 0034 Loss: 1.13831 Time: 0.00196\n",
      "Iteration: 0035 Loss: 1.14513 Time: 0.00297\n",
      "Iteration: 0036 Loss: 1.21877 Time: 0.00208\n",
      "Iteration: 0037 Loss: 1.11579 Time: 0.00291\n",
      "Iteration: 0038 Loss: 1.13147 Time: 0.00226\n",
      "Iteration: 0039 Loss: 1.13783 Time: 0.00190\n",
      "Iteration: 0040 Loss: 1.16630 Time: 0.00196\n",
      "Iteration: 0041 Loss: 1.08821 Time: 0.00206\n",
      "Iteration: 0042 Loss: 1.13291 Time: 0.00190\n",
      "Iteration: 0043 Loss: 1.06003 Time: 0.00306\n",
      "Iteration: 0044 Loss: 1.14424 Time: 0.00203\n",
      "Iteration: 0045 Loss: 1.03297 Time: 0.00196\n",
      "Iteration: 0046 Loss: 1.08116 Time: 0.00309\n",
      "Iteration: 0047 Loss: 1.14412 Time: 0.00292\n",
      "Iteration: 0048 Loss: 1.00664 Time: 0.00196\n",
      "Iteration: 0049 Loss: 0.97765 Time: 0.00203\n",
      "Iteration: 0050 Loss: 0.99967 Time: 0.00300\n",
      "Iteration: 0051 Loss: 1.01786 Time: 0.00200\n",
      "Iteration: 0052 Loss: 1.00485 Time: 0.00086\n",
      "Iteration: 0053 Loss: 0.96557 Time: 0.00403\n",
      "Iteration: 0054 Loss: 1.01992 Time: 0.00295\n",
      "Iteration: 0055 Loss: 0.96166 Time: 0.00203\n",
      "Iteration: 0056 Loss: 0.91290 Time: 0.00349\n",
      "Iteration: 0057 Loss: 0.94408 Time: 0.00205\n",
      "Iteration: 0058 Loss: 0.91802 Time: 0.00301\n",
      "Iteration: 0059 Loss: 0.99941 Time: 0.00335\n",
      "Iteration: 0060 Loss: 0.90733 Time: 0.00260\n",
      "Iteration: 0061 Loss: 0.93523 Time: 0.00201\n",
      "Iteration: 0062 Loss: 0.89871 Time: 0.00299\n",
      "Iteration: 0063 Loss: 0.89995 Time: 0.00198\n",
      "Iteration: 0064 Loss: 0.90369 Time: 0.00302\n",
      "Iteration: 0065 Loss: 0.92012 Time: 0.00297\n",
      "Iteration: 0066 Loss: 0.89842 Time: 0.00392\n",
      "Iteration: 0067 Loss: 0.86388 Time: 0.00200\n",
      "Iteration: 0068 Loss: 0.86189 Time: 0.00305\n",
      "Iteration: 0069 Loss: 0.90322 Time: 0.00200\n",
      "Iteration: 0070 Loss: 0.91126 Time: 0.00104\n",
      "Iteration: 0071 Loss: 0.86627 Time: 0.00395\n",
      "Iteration: 0072 Loss: 0.90256 Time: 0.00298\n",
      "Iteration: 0073 Loss: 0.86661 Time: 0.00311\n",
      "Iteration: 0074 Loss: 0.84725 Time: 0.00198\n",
      "Iteration: 0075 Loss: 0.85437 Time: 0.00195\n",
      "Iteration: 0076 Loss: 0.81447 Time: 0.00315\n",
      "Iteration: 0077 Loss: 0.78810 Time: 0.00280\n",
      "Iteration: 0078 Loss: 0.81498 Time: 0.00301\n",
      "Iteration: 0079 Loss: 0.83794 Time: 0.00207\n",
      "Iteration: 0080 Loss: 0.75697 Time: 0.00209\n",
      "Iteration: 0081 Loss: 0.81551 Time: 0.00191\n",
      "Iteration: 0082 Loss: 0.78260 Time: 0.00298\n",
      "Iteration: 0083 Loss: 0.79002 Time: 0.00201\n",
      "Iteration: 0084 Loss: 0.78316 Time: 0.00296\n",
      "Iteration: 0085 Loss: 0.79157 Time: 0.00299\n",
      "Iteration: 0086 Loss: 0.82420 Time: 0.00303\n",
      "Iteration: 0087 Loss: 0.76023 Time: 0.00197\n",
      "Iteration: 0088 Loss: 0.75627 Time: 0.00300\n",
      "Iteration: 0089 Loss: 0.76781 Time: 0.00300\n",
      "Iteration: 0090 Loss: 0.73082 Time: 0.00200\n",
      "Iteration: 0091 Loss: 0.78386 Time: 0.00299\n",
      "Iteration: 0092 Loss: 0.76303 Time: 0.00298\n",
      "Iteration: 0093 Loss: 0.75026 Time: 0.00212\n",
      "Iteration: 0094 Loss: 0.77081 Time: 0.00312\n",
      "Iteration: 0095 Loss: 0.75365 Time: 0.00192\n",
      "Iteration: 0096 Loss: 0.75550 Time: 0.00303\n",
      "Iteration: 0097 Loss: 0.75155 Time: 0.00194\n",
      "Iteration: 0098 Loss: 0.76094 Time: 0.00304\n",
      "Iteration: 0099 Loss: 0.74621 Time: 0.00195\n",
      "Iteration: 0100 Loss: 0.74945 Time: 0.00195\n",
      "Iteration: 0101 Loss: 0.74273 Time: 0.00199\n",
      "Iteration: 0102 Loss: 0.73055 Time: 0.00301\n",
      "Iteration: 0103 Loss: 0.72733 Time: 0.00198\n",
      "Iteration: 0104 Loss: 0.72717 Time: 0.00305\n",
      "Iteration: 0105 Loss: 0.75194 Time: 0.00195\n",
      "Iteration: 0106 Loss: 0.73236 Time: 0.00201\n",
      "Iteration: 0107 Loss: 0.75156 Time: 0.00300\n",
      "Iteration: 0108 Loss: 0.74804 Time: 0.00201\n",
      "Iteration: 0109 Loss: 0.72996 Time: 0.00202\n",
      "Iteration: 0110 Loss: 0.74345 Time: 0.00213\n",
      "Iteration: 0111 Loss: 0.74731 Time: 0.00284\n",
      "Iteration: 0112 Loss: 0.74894 Time: 0.00299\n",
      "Iteration: 0113 Loss: 0.71821 Time: 0.00200\n",
      "Iteration: 0114 Loss: 0.73010 Time: 0.00289\n",
      "Iteration: 0115 Loss: 0.73655 Time: 0.00199\n",
      "Iteration: 0116 Loss: 0.73842 Time: 0.00211\n",
      "Iteration: 0117 Loss: 0.70424 Time: 0.00301\n",
      "Iteration: 0118 Loss: 0.66907 Time: 0.00199\n",
      "Iteration: 0119 Loss: 0.71830 Time: 0.00195\n",
      "Iteration: 0120 Loss: 0.72124 Time: 0.00200\n",
      "Iteration: 0121 Loss: 0.70716 Time: 0.00193\n",
      "Iteration: 0122 Loss: 0.71092 Time: 0.00301\n",
      "Iteration: 0123 Loss: 0.70619 Time: 0.00300\n",
      "Iteration: 0124 Loss: 0.69669 Time: 0.00196\n",
      "Iteration: 0125 Loss: 0.70012 Time: 0.00301\n",
      "Iteration: 0126 Loss: 0.69390 Time: 0.00305\n",
      "Iteration: 0127 Loss: 0.68317 Time: 0.00311\n",
      "Iteration: 0128 Loss: 0.72274 Time: 0.00301\n",
      "Iteration: 0129 Loss: 0.67677 Time: 0.00199\n",
      "Iteration: 0130 Loss: 0.67120 Time: 0.00191\n",
      "Iteration: 0131 Loss: 0.69389 Time: 0.00298\n",
      "Iteration: 0132 Loss: 0.67766 Time: 0.00214\n",
      "Iteration: 0133 Loss: 0.69466 Time: 0.00303\n",
      "Iteration: 0134 Loss: 0.69150 Time: 0.00292\n",
      "Iteration: 0135 Loss: 0.67044 Time: 0.00301\n",
      "Iteration: 0136 Loss: 0.70418 Time: 0.00203\n",
      "Iteration: 0137 Loss: 0.69854 Time: 0.00305\n",
      "Iteration: 0138 Loss: 0.69545 Time: 0.00496\n",
      "Iteration: 0139 Loss: 0.67277 Time: 0.00310\n",
      "Iteration: 0140 Loss: 0.66630 Time: 0.00194\n",
      "Iteration: 0141 Loss: 0.67004 Time: 0.00205\n",
      "Iteration: 0142 Loss: 0.69336 Time: 0.00301\n",
      "Iteration: 0143 Loss: 0.68926 Time: 0.00299\n",
      "Iteration: 0144 Loss: 0.70542 Time: 0.00297\n",
      "Iteration: 0145 Loss: 0.68789 Time: 0.00204\n",
      "Iteration: 0146 Loss: 0.69551 Time: 0.00308\n",
      "Iteration: 0147 Loss: 0.67006 Time: 0.00205\n",
      "Iteration: 0148 Loss: 0.67947 Time: 0.00200\n",
      "Iteration: 0149 Loss: 0.70054 Time: 0.00196\n",
      "Iteration: 0150 Loss: 0.70334 Time: 0.00202\n",
      "Iteration: 0151 Loss: 0.66552 Time: 0.00296\n",
      "Iteration: 0152 Loss: 0.68152 Time: 0.00199\n",
      "Iteration: 0153 Loss: 0.67022 Time: 0.00200\n",
      "Iteration: 0154 Loss: 0.66243 Time: 0.00202\n",
      "Iteration: 0155 Loss: 0.67927 Time: 0.00201\n",
      "Iteration: 0156 Loss: 0.65736 Time: 0.00304\n",
      "Iteration: 0157 Loss: 0.69524 Time: 0.00291\n",
      "Iteration: 0158 Loss: 0.67483 Time: 0.00319\n",
      "Iteration: 0159 Loss: 0.68125 Time: 0.00184\n",
      "Iteration: 0160 Loss: 0.66195 Time: 0.00203\n",
      "Iteration: 0161 Loss: 0.66975 Time: 0.00197\n",
      "Iteration: 0162 Loss: 0.66866 Time: 0.00198\n",
      "Iteration: 0163 Loss: 0.64905 Time: 0.00299\n",
      "Iteration: 0164 Loss: 0.68668 Time: 0.00205\n",
      "Iteration: 0165 Loss: 0.66447 Time: 0.00206\n",
      "Iteration: 0166 Loss: 0.63743 Time: 0.00402\n",
      "Iteration: 0167 Loss: 0.64716 Time: 0.00189\n",
      "Iteration: 0168 Loss: 0.63770 Time: 0.00314\n",
      "Iteration: 0169 Loss: 0.66691 Time: 0.00201\n",
      "Iteration: 0170 Loss: 0.64403 Time: 0.00314\n",
      "Iteration: 0171 Loss: 0.64722 Time: 0.00293\n",
      "Iteration: 0172 Loss: 0.63675 Time: 0.00198\n",
      "Iteration: 0173 Loss: 0.63699 Time: 0.00203\n",
      "Iteration: 0174 Loss: 0.66804 Time: 0.00298\n",
      "Iteration: 0175 Loss: 0.65558 Time: 0.00297\n",
      "Iteration: 0176 Loss: 0.65156 Time: 0.00301\n",
      "Iteration: 0177 Loss: 0.65358 Time: 0.00295\n",
      "Iteration: 0178 Loss: 0.64112 Time: 0.00200\n",
      "Iteration: 0179 Loss: 0.64103 Time: 0.00313\n",
      "Iteration: 0180 Loss: 0.64306 Time: 0.00199\n",
      "Iteration: 0181 Loss: 0.63013 Time: 0.00296\n",
      "Iteration: 0182 Loss: 0.63803 Time: 0.00300\n",
      "Iteration: 0183 Loss: 0.63315 Time: 0.00203\n",
      "Iteration: 0184 Loss: 0.63770 Time: 0.00498\n",
      "Iteration: 0185 Loss: 0.65292 Time: 0.00303\n",
      "Iteration: 0186 Loss: 0.63623 Time: 0.00303\n",
      "Iteration: 0187 Loss: 0.64305 Time: 0.00198\n",
      "Iteration: 0188 Loss: 0.63752 Time: 0.00295\n",
      "Iteration: 0189 Loss: 0.64963 Time: 0.00316\n",
      "Iteration: 0190 Loss: 0.61632 Time: 0.00284\n",
      "Iteration: 0191 Loss: 0.65752 Time: 0.00300\n",
      "Iteration: 0192 Loss: 0.62482 Time: 0.00200\n",
      "Iteration: 0193 Loss: 0.63120 Time: 0.00098\n",
      "Iteration: 0194 Loss: 0.64694 Time: 0.00409\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0195 Loss: 0.63246 Time: 0.00302\n",
      "Iteration: 0196 Loss: 0.64983 Time: 0.00292\n",
      "Iteration: 0197 Loss: 0.62667 Time: 0.00196\n",
      "Iteration: 0198 Loss: 0.62705 Time: 0.00305\n",
      "Iteration: 0199 Loss: 0.64307 Time: 0.00195\n",
      "Iteration: 0200 Loss: 0.63015 Time: 0.00203\n",
      "Done! \n",
      "\n",
      "STEP 3/3 - MODEL EVALUATION \n",
      "\n",
      "Computing the final embedding vectors, for evaluation\n",
      "Done! \n",
      "\n",
      "Testing: link prediction\n",
      "Done! \n",
      "\n",
      "EXPERIMENTS ON MODEL 5 / 10 \n",
      "\n",
      "STEP 1/3 - PREPROCESSING STEPS \n",
      "\n",
      "Masking some edges from the training graph, for link prediction\n",
      "(validation set: 1.0 % of edges - test set: 10.0 % of edges)\n",
      "Done! \n",
      "\n",
      "Preprocessing node features\n",
      "Done! \n",
      "\n",
      "Running the Louvain algorithm for community detection\n",
      "as a preprocessing step for the encoder\n",
      "Done! Louvain has found 4 communities \n",
      "\n",
      "Setting up the model and the optimizer\n",
      "Done! \n",
      "\n",
      "Preprocessing on message passing matrices\n",
      "Done! \n",
      "\n",
      "Initializing TF session\n",
      "Done! \n",
      "\n",
      "STEP 2/3 - MODEL TRAINING \n",
      "\n",
      "Starting training\n",
      "Iteration: 0001 Loss: 1.79725 Time: 0.11100\n",
      "Iteration: 0002 Loss: 1.71701 Time: 0.00300\n",
      "Iteration: 0003 Loss: 1.71126 Time: 0.00300\n",
      "Iteration: 0004 Loss: 1.61818 Time: 0.00300\n",
      "Iteration: 0005 Loss: 1.66860 Time: 0.00205\n",
      "Iteration: 0006 Loss: 1.56783 Time: 0.00296\n",
      "Iteration: 0007 Loss: 1.62098 Time: 0.00500\n",
      "Iteration: 0008 Loss: 1.57927 Time: 0.00299\n",
      "Iteration: 0009 Loss: 1.57345 Time: 0.00200\n",
      "Iteration: 0010 Loss: 1.55710 Time: 0.00300\n",
      "Iteration: 0011 Loss: 1.47375 Time: 0.00306\n",
      "Iteration: 0012 Loss: 1.43450 Time: 0.00294\n",
      "Iteration: 0013 Loss: 1.48255 Time: 0.00200\n",
      "Iteration: 0014 Loss: 1.44587 Time: 0.00196\n",
      "Iteration: 0015 Loss: 1.50543 Time: 0.00306\n",
      "Iteration: 0016 Loss: 1.48345 Time: 0.00195\n",
      "Iteration: 0017 Loss: 1.33355 Time: 0.00400\n",
      "Iteration: 0018 Loss: 1.36487 Time: 0.00196\n",
      "Iteration: 0019 Loss: 1.37221 Time: 0.00302\n",
      "Iteration: 0020 Loss: 1.39541 Time: 0.00199\n",
      "Iteration: 0021 Loss: 1.25518 Time: 0.00300\n",
      "Iteration: 0022 Loss: 1.33003 Time: 0.00300\n",
      "Iteration: 0023 Loss: 1.22117 Time: 0.00199\n",
      "Iteration: 0024 Loss: 1.31891 Time: 0.00200\n",
      "Iteration: 0025 Loss: 1.21941 Time: 0.00199\n",
      "Iteration: 0026 Loss: 1.31257 Time: 0.00301\n",
      "Iteration: 0027 Loss: 1.29036 Time: 0.00204\n",
      "Iteration: 0028 Loss: 1.27294 Time: 0.00295\n",
      "Iteration: 0029 Loss: 1.18043 Time: 0.00300\n",
      "Iteration: 0030 Loss: 1.20055 Time: 0.00301\n",
      "Iteration: 0031 Loss: 1.25370 Time: 0.00099\n",
      "Iteration: 0032 Loss: 1.23879 Time: 0.00301\n",
      "Iteration: 0033 Loss: 1.17754 Time: 0.00300\n",
      "Iteration: 0034 Loss: 1.18646 Time: 0.00199\n",
      "Iteration: 0035 Loss: 1.18993 Time: 0.00304\n",
      "Iteration: 0036 Loss: 1.15584 Time: 0.00196\n",
      "Iteration: 0037 Loss: 1.19676 Time: 0.00196\n",
      "Iteration: 0038 Loss: 1.21000 Time: 0.00300\n",
      "Iteration: 0039 Loss: 1.13816 Time: 0.00213\n",
      "Iteration: 0040 Loss: 1.23898 Time: 0.00391\n",
      "Iteration: 0041 Loss: 1.10459 Time: 0.00313\n",
      "Iteration: 0042 Loss: 1.09136 Time: 0.00201\n",
      "Iteration: 0043 Loss: 1.09980 Time: 0.00196\n",
      "Iteration: 0044 Loss: 1.05164 Time: 0.00301\n",
      "Iteration: 0045 Loss: 1.06605 Time: 0.00196\n",
      "Iteration: 0046 Loss: 1.07115 Time: 0.00403\n",
      "Iteration: 0047 Loss: 1.09938 Time: 0.00300\n",
      "Iteration: 0048 Loss: 1.02072 Time: 0.00213\n",
      "Iteration: 0049 Loss: 1.05142 Time: 0.00201\n",
      "Iteration: 0050 Loss: 1.00929 Time: 0.00295\n",
      "Iteration: 0051 Loss: 1.00535 Time: 0.00401\n",
      "Iteration: 0052 Loss: 1.00227 Time: 0.00200\n",
      "Iteration: 0053 Loss: 1.04455 Time: 0.00197\n",
      "Iteration: 0054 Loss: 1.02962 Time: 0.00196\n",
      "Iteration: 0055 Loss: 1.00656 Time: 0.00209\n",
      "Iteration: 0056 Loss: 1.03677 Time: 0.00193\n",
      "Iteration: 0057 Loss: 0.97621 Time: 0.00315\n",
      "Iteration: 0058 Loss: 0.97589 Time: 0.00099\n",
      "Iteration: 0059 Loss: 0.93477 Time: 0.00206\n",
      "Iteration: 0060 Loss: 0.99374 Time: 0.00311\n",
      "Iteration: 0061 Loss: 0.96964 Time: 0.00209\n",
      "Iteration: 0062 Loss: 0.92079 Time: 0.00296\n",
      "Iteration: 0063 Loss: 0.91794 Time: 0.00308\n",
      "Iteration: 0064 Loss: 0.90459 Time: 0.00196\n",
      "Iteration: 0065 Loss: 0.89375 Time: 0.00202\n",
      "Iteration: 0066 Loss: 0.91523 Time: 0.00303\n",
      "Iteration: 0067 Loss: 0.90774 Time: 0.00395\n",
      "Iteration: 0068 Loss: 0.83996 Time: 0.00300\n",
      "Iteration: 0069 Loss: 0.89623 Time: 0.00300\n",
      "Iteration: 0070 Loss: 0.82520 Time: 0.00200\n",
      "Iteration: 0071 Loss: 0.92341 Time: 0.00404\n",
      "Iteration: 0072 Loss: 0.85381 Time: 0.00205\n",
      "Iteration: 0073 Loss: 0.81017 Time: 0.00498\n",
      "Iteration: 0074 Loss: 0.83968 Time: 0.00200\n",
      "Iteration: 0075 Loss: 0.79515 Time: 0.00200\n",
      "Iteration: 0076 Loss: 0.86358 Time: 0.00193\n",
      "Iteration: 0077 Loss: 0.78421 Time: 0.00200\n",
      "Iteration: 0078 Loss: 0.84550 Time: 0.00316\n",
      "Iteration: 0079 Loss: 0.83754 Time: 0.00205\n",
      "Iteration: 0080 Loss: 0.79363 Time: 0.00290\n",
      "Iteration: 0081 Loss: 0.76423 Time: 0.00319\n",
      "Iteration: 0082 Loss: 0.77571 Time: 0.00284\n",
      "Iteration: 0083 Loss: 0.78742 Time: 0.00195\n",
      "Iteration: 0084 Loss: 0.78198 Time: 0.00303\n",
      "Iteration: 0085 Loss: 0.77792 Time: 0.00397\n",
      "Iteration: 0086 Loss: 0.78795 Time: 0.00200\n",
      "Iteration: 0087 Loss: 0.72685 Time: 0.00305\n",
      "Iteration: 0088 Loss: 0.75158 Time: 0.00104\n",
      "Iteration: 0089 Loss: 0.78120 Time: 0.00195\n",
      "Iteration: 0090 Loss: 0.77732 Time: 0.00309\n",
      "Iteration: 0091 Loss: 0.77783 Time: 0.00191\n",
      "Iteration: 0092 Loss: 0.72720 Time: 0.00413\n",
      "Iteration: 0093 Loss: 0.76527 Time: 0.00100\n",
      "Iteration: 0094 Loss: 0.78798 Time: 0.00300\n",
      "Iteration: 0095 Loss: 0.75422 Time: 0.00300\n",
      "Iteration: 0096 Loss: 0.72851 Time: 0.00094\n",
      "Iteration: 0097 Loss: 0.74240 Time: 0.00308\n",
      "Iteration: 0098 Loss: 0.73354 Time: 0.00241\n",
      "Iteration: 0099 Loss: 0.76040 Time: 0.00164\n",
      "Iteration: 0100 Loss: 0.74794 Time: 0.00206\n",
      "Iteration: 0101 Loss: 0.75812 Time: 0.00295\n",
      "Iteration: 0102 Loss: 0.74534 Time: 0.00199\n",
      "Iteration: 0103 Loss: 0.76791 Time: 0.00201\n",
      "Iteration: 0104 Loss: 0.72593 Time: 0.00343\n",
      "Iteration: 0105 Loss: 0.69062 Time: 0.00376\n",
      "Iteration: 0106 Loss: 0.71866 Time: 0.00381\n",
      "Iteration: 0107 Loss: 0.73485 Time: 0.00400\n",
      "Iteration: 0108 Loss: 0.73474 Time: 0.00308\n",
      "Iteration: 0109 Loss: 0.69562 Time: 0.00193\n",
      "Iteration: 0110 Loss: 0.75909 Time: 0.00312\n",
      "Iteration: 0111 Loss: 0.71797 Time: 0.00205\n",
      "Iteration: 0112 Loss: 0.71551 Time: 0.00301\n",
      "Iteration: 0113 Loss: 0.71227 Time: 0.00201\n",
      "Iteration: 0114 Loss: 0.72122 Time: 0.00201\n",
      "Iteration: 0115 Loss: 0.68329 Time: 0.00302\n",
      "Iteration: 0116 Loss: 0.70317 Time: 0.00300\n",
      "Iteration: 0117 Loss: 0.72867 Time: 0.00355\n",
      "Iteration: 0118 Loss: 0.72298 Time: 0.00201\n",
      "Iteration: 0119 Loss: 0.71973 Time: 0.00193\n",
      "Iteration: 0120 Loss: 0.68956 Time: 0.00307\n",
      "Iteration: 0121 Loss: 0.72618 Time: 0.00329\n",
      "Iteration: 0122 Loss: 0.71632 Time: 0.00164\n",
      "Iteration: 0123 Loss: 0.68369 Time: 0.00304\n",
      "Iteration: 0124 Loss: 0.72203 Time: 0.00307\n",
      "Iteration: 0125 Loss: 0.72321 Time: 0.00394\n",
      "Iteration: 0126 Loss: 0.68486 Time: 0.00299\n",
      "Iteration: 0127 Loss: 0.69997 Time: 0.00300\n",
      "Iteration: 0128 Loss: 0.66633 Time: 0.00300\n",
      "Iteration: 0129 Loss: 0.70044 Time: 0.00299\n",
      "Iteration: 0130 Loss: 0.71899 Time: 0.00200\n",
      "Iteration: 0131 Loss: 0.67377 Time: 0.00304\n",
      "Iteration: 0132 Loss: 0.65555 Time: 0.00296\n",
      "Iteration: 0133 Loss: 0.67298 Time: 0.00206\n",
      "Iteration: 0134 Loss: 0.68472 Time: 0.00202\n",
      "Iteration: 0135 Loss: 0.68424 Time: 0.00292\n",
      "Iteration: 0136 Loss: 0.68187 Time: 0.00095\n",
      "Iteration: 0137 Loss: 0.68670 Time: 0.00304\n",
      "Iteration: 0138 Loss: 0.70892 Time: 0.00296\n",
      "Iteration: 0139 Loss: 0.70211 Time: 0.00301\n",
      "Iteration: 0140 Loss: 0.71418 Time: 0.00099\n",
      "Iteration: 0141 Loss: 0.67077 Time: 0.00200\n",
      "Iteration: 0142 Loss: 0.72095 Time: 0.00339\n",
      "Iteration: 0143 Loss: 0.66722 Time: 0.00163\n",
      "Iteration: 0144 Loss: 0.67914 Time: 0.00306\n",
      "Iteration: 0145 Loss: 0.68744 Time: 0.00432\n",
      "Iteration: 0146 Loss: 0.68870 Time: 0.00196\n",
      "Iteration: 0147 Loss: 0.66557 Time: 0.00200\n",
      "Iteration: 0148 Loss: 0.66625 Time: 0.00309\n",
      "Iteration: 0149 Loss: 0.65852 Time: 0.00283\n",
      "Iteration: 0150 Loss: 0.67207 Time: 0.00315\n",
      "Iteration: 0151 Loss: 0.66214 Time: 0.00283\n",
      "Iteration: 0152 Loss: 0.66658 Time: 0.00299\n",
      "Iteration: 0153 Loss: 0.67022 Time: 0.00195\n",
      "Iteration: 0154 Loss: 0.66400 Time: 0.00294\n",
      "Iteration: 0155 Loss: 0.66560 Time: 0.00401\n",
      "Iteration: 0156 Loss: 0.67267 Time: 0.00200\n",
      "Iteration: 0157 Loss: 0.65308 Time: 0.00397\n",
      "Iteration: 0158 Loss: 0.65466 Time: 0.00199\n",
      "Iteration: 0159 Loss: 0.66437 Time: 0.00200\n",
      "Iteration: 0160 Loss: 0.67782 Time: 0.00298\n",
      "Iteration: 0161 Loss: 0.67512 Time: 0.00196\n",
      "Iteration: 0162 Loss: 0.65934 Time: 0.00299\n",
      "Iteration: 0163 Loss: 0.68935 Time: 0.00304\n",
      "Iteration: 0164 Loss: 0.65457 Time: 0.00305\n",
      "Iteration: 0165 Loss: 0.68188 Time: 0.00195\n",
      "Iteration: 0166 Loss: 0.62584 Time: 0.00301\n",
      "Iteration: 0167 Loss: 0.66535 Time: 0.00399\n",
      "Iteration: 0168 Loss: 0.64523 Time: 0.00311\n",
      "Iteration: 0169 Loss: 0.65003 Time: 0.00213\n",
      "Iteration: 0170 Loss: 0.66401 Time: 0.00281\n",
      "Iteration: 0171 Loss: 0.64636 Time: 0.00301\n",
      "Iteration: 0172 Loss: 0.64105 Time: 0.00095\n",
      "Iteration: 0173 Loss: 0.66361 Time: 0.00200\n",
      "Iteration: 0174 Loss: 0.65330 Time: 0.00195\n",
      "Iteration: 0175 Loss: 0.67837 Time: 0.00308\n",
      "Iteration: 0176 Loss: 0.65262 Time: 0.00329\n",
      "Iteration: 0177 Loss: 0.64059 Time: 0.00263\n",
      "Iteration: 0178 Loss: 0.68018 Time: 0.00200\n",
      "Iteration: 0179 Loss: 0.63781 Time: 0.00305\n",
      "Iteration: 0180 Loss: 0.64927 Time: 0.00395\n",
      "Iteration: 0181 Loss: 0.66241 Time: 0.00202\n",
      "Iteration: 0182 Loss: 0.64948 Time: 0.00399\n",
      "Iteration: 0183 Loss: 0.64656 Time: 0.00400\n",
      "Iteration: 0184 Loss: 0.64420 Time: 0.00301\n",
      "Iteration: 0185 Loss: 0.63987 Time: 0.00199\n",
      "Iteration: 0186 Loss: 0.64146 Time: 0.00344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0187 Loss: 0.63237 Time: 0.00249\n",
      "Iteration: 0188 Loss: 0.64401 Time: 0.00200\n",
      "Iteration: 0189 Loss: 0.63546 Time: 0.00200\n",
      "Iteration: 0190 Loss: 0.65133 Time: 0.00100\n",
      "Iteration: 0191 Loss: 0.62095 Time: 0.00206\n",
      "Iteration: 0192 Loss: 0.62854 Time: 0.00200\n",
      "Iteration: 0193 Loss: 0.62921 Time: 0.00297\n",
      "Iteration: 0194 Loss: 0.64889 Time: 0.00199\n",
      "Iteration: 0195 Loss: 0.64195 Time: 0.00313\n",
      "Iteration: 0196 Loss: 0.62384 Time: 0.00093\n",
      "Iteration: 0197 Loss: 0.62754 Time: 0.00199\n",
      "Iteration: 0198 Loss: 0.64947 Time: 0.00202\n",
      "Iteration: 0199 Loss: 0.64429 Time: 0.00293\n",
      "Iteration: 0200 Loss: 0.62384 Time: 0.00319\n",
      "Done! \n",
      "\n",
      "STEP 3/3 - MODEL EVALUATION \n",
      "\n",
      "Computing the final embedding vectors, for evaluation\n",
      "Done! \n",
      "\n",
      "Testing: link prediction\n",
      "Done! \n",
      "\n",
      "EXPERIMENTS ON MODEL 6 / 10 \n",
      "\n",
      "STEP 1/3 - PREPROCESSING STEPS \n",
      "\n",
      "Masking some edges from the training graph, for link prediction\n",
      "(validation set: 1.0 % of edges - test set: 10.0 % of edges)\n",
      "Done! \n",
      "\n",
      "Preprocessing node features\n",
      "Done! \n",
      "\n",
      "Running the Louvain algorithm for community detection\n",
      "as a preprocessing step for the encoder\n",
      "Done! Louvain has found 6 communities \n",
      "\n",
      "Setting up the model and the optimizer\n",
      "Done! \n",
      "\n",
      "Preprocessing on message passing matrices\n",
      "Done! \n",
      "\n",
      "Initializing TF session\n",
      "Done! \n",
      "\n",
      "STEP 2/3 - MODEL TRAINING \n",
      "\n",
      "Starting training\n",
      "Iteration: 0001 Loss: 1.63419 Time: 0.13196\n",
      "Iteration: 0002 Loss: 1.68939 Time: 0.00405\n",
      "Iteration: 0003 Loss: 1.76475 Time: 0.00395\n",
      "Iteration: 0004 Loss: 1.63094 Time: 0.00200\n",
      "Iteration: 0005 Loss: 1.61532 Time: 0.00295\n",
      "Iteration: 0006 Loss: 1.55432 Time: 0.00301\n",
      "Iteration: 0007 Loss: 1.52951 Time: 0.00301\n",
      "Iteration: 0008 Loss: 1.79505 Time: 0.00297\n",
      "Iteration: 0009 Loss: 1.54388 Time: 0.00301\n",
      "Iteration: 0010 Loss: 1.49110 Time: 0.00201\n",
      "Iteration: 0011 Loss: 1.50827 Time: 0.00204\n",
      "Iteration: 0012 Loss: 1.51700 Time: 0.00194\n",
      "Iteration: 0013 Loss: 1.35351 Time: 0.00197\n",
      "Iteration: 0014 Loss: 1.50493 Time: 0.00200\n",
      "Iteration: 0015 Loss: 1.40136 Time: 0.00194\n",
      "Iteration: 0016 Loss: 1.41082 Time: 0.00200\n",
      "Iteration: 0017 Loss: 1.39344 Time: 0.00404\n",
      "Iteration: 0018 Loss: 1.39980 Time: 0.00296\n",
      "Iteration: 0019 Loss: 1.25934 Time: 0.00301\n",
      "Iteration: 0020 Loss: 1.36275 Time: 0.00399\n",
      "Iteration: 0021 Loss: 1.33159 Time: 0.00306\n",
      "Iteration: 0022 Loss: 1.28139 Time: 0.00294\n",
      "Iteration: 0023 Loss: 1.33616 Time: 0.00205\n",
      "Iteration: 0024 Loss: 1.38606 Time: 0.00296\n",
      "Iteration: 0025 Loss: 1.23928 Time: 0.00400\n",
      "Iteration: 0026 Loss: 1.30886 Time: 0.00301\n",
      "Iteration: 0027 Loss: 1.24421 Time: 0.00200\n",
      "Iteration: 0028 Loss: 1.35988 Time: 0.00302\n",
      "Iteration: 0029 Loss: 1.17871 Time: 0.00198\n",
      "Iteration: 0030 Loss: 1.21769 Time: 0.00303\n",
      "Iteration: 0031 Loss: 1.17646 Time: 0.00202\n",
      "Iteration: 0032 Loss: 1.21386 Time: 0.00196\n",
      "Iteration: 0033 Loss: 1.23343 Time: 0.00304\n",
      "Iteration: 0034 Loss: 1.16527 Time: 0.00200\n",
      "Iteration: 0035 Loss: 1.19350 Time: 0.00196\n",
      "Iteration: 0036 Loss: 1.11607 Time: 0.00400\n",
      "Iteration: 0037 Loss: 1.13986 Time: 0.00206\n",
      "Iteration: 0038 Loss: 1.22909 Time: 0.00200\n",
      "Iteration: 0039 Loss: 1.17884 Time: 0.00306\n",
      "Iteration: 0040 Loss: 1.10359 Time: 0.00106\n",
      "Iteration: 0041 Loss: 1.04372 Time: 0.00391\n",
      "Iteration: 0042 Loss: 1.09207 Time: 0.00300\n",
      "Iteration: 0043 Loss: 1.08638 Time: 0.00200\n",
      "Iteration: 0044 Loss: 1.06028 Time: 0.00296\n",
      "Iteration: 0045 Loss: 1.04063 Time: 0.00200\n",
      "Iteration: 0046 Loss: 1.07723 Time: 0.00300\n",
      "Iteration: 0047 Loss: 1.09684 Time: 0.00300\n",
      "Iteration: 0048 Loss: 1.03633 Time: 0.00201\n",
      "Iteration: 0049 Loss: 1.07352 Time: 0.00200\n",
      "Iteration: 0050 Loss: 1.01459 Time: 0.00302\n",
      "Iteration: 0051 Loss: 0.99741 Time: 0.00199\n",
      "Iteration: 0052 Loss: 1.03739 Time: 0.00300\n",
      "Iteration: 0053 Loss: 0.98838 Time: 0.00300\n",
      "Iteration: 0054 Loss: 0.95153 Time: 0.00197\n",
      "Iteration: 0055 Loss: 1.00957 Time: 0.00198\n",
      "Iteration: 0056 Loss: 0.99219 Time: 0.00205\n",
      "Iteration: 0057 Loss: 1.00555 Time: 0.00295\n",
      "Iteration: 0058 Loss: 0.96128 Time: 0.00301\n",
      "Iteration: 0059 Loss: 0.93309 Time: 0.00399\n",
      "Iteration: 0060 Loss: 0.94554 Time: 0.00305\n",
      "Iteration: 0061 Loss: 0.89764 Time: 0.00195\n",
      "Iteration: 0062 Loss: 0.90362 Time: 0.00305\n",
      "Iteration: 0063 Loss: 0.90460 Time: 0.00295\n",
      "Iteration: 0064 Loss: 0.87740 Time: 0.00303\n",
      "Iteration: 0065 Loss: 0.91540 Time: 0.00199\n",
      "Iteration: 0066 Loss: 0.86636 Time: 0.00195\n",
      "Iteration: 0067 Loss: 0.93337 Time: 0.00300\n",
      "Iteration: 0068 Loss: 0.87467 Time: 0.00300\n",
      "Iteration: 0069 Loss: 0.90571 Time: 0.00300\n",
      "Iteration: 0070 Loss: 0.85462 Time: 0.00305\n",
      "Iteration: 0071 Loss: 0.87177 Time: 0.00196\n",
      "Iteration: 0072 Loss: 0.92038 Time: 0.00304\n",
      "Iteration: 0073 Loss: 0.83612 Time: 0.00198\n",
      "Iteration: 0074 Loss: 0.81631 Time: 0.00295\n",
      "Iteration: 0075 Loss: 0.80590 Time: 0.00300\n",
      "Iteration: 0076 Loss: 0.85032 Time: 0.00299\n",
      "Iteration: 0077 Loss: 0.79373 Time: 0.00200\n",
      "Iteration: 0078 Loss: 0.83452 Time: 0.00299\n",
      "Iteration: 0079 Loss: 0.76754 Time: 0.00399\n",
      "Iteration: 0080 Loss: 0.79036 Time: 0.00300\n",
      "Iteration: 0081 Loss: 0.78054 Time: 0.00400\n",
      "Iteration: 0082 Loss: 0.81409 Time: 0.00300\n",
      "Iteration: 0083 Loss: 0.78253 Time: 0.00405\n",
      "Iteration: 0084 Loss: 0.79771 Time: 0.00495\n",
      "Iteration: 0085 Loss: 0.77923 Time: 0.00301\n",
      "Iteration: 0086 Loss: 0.77856 Time: 0.00300\n",
      "Iteration: 0087 Loss: 0.75602 Time: 0.00299\n",
      "Iteration: 0088 Loss: 0.76341 Time: 0.00500\n",
      "Iteration: 0089 Loss: 0.73739 Time: 0.00300\n",
      "Iteration: 0090 Loss: 0.75343 Time: 0.00300\n",
      "Iteration: 0091 Loss: 0.72288 Time: 0.00300\n",
      "Iteration: 0092 Loss: 0.75488 Time: 0.00450\n",
      "Iteration: 0093 Loss: 0.71759 Time: 0.00300\n",
      "Iteration: 0094 Loss: 0.76688 Time: 0.00300\n",
      "Iteration: 0095 Loss: 0.76160 Time: 0.00300\n",
      "Iteration: 0096 Loss: 0.74075 Time: 0.00300\n",
      "Iteration: 0097 Loss: 0.75478 Time: 0.00399\n",
      "Iteration: 0098 Loss: 0.73767 Time: 0.00400\n",
      "Iteration: 0099 Loss: 0.76704 Time: 0.00300\n",
      "Iteration: 0100 Loss: 0.76133 Time: 0.00200\n",
      "Iteration: 0101 Loss: 0.73492 Time: 0.00301\n",
      "Iteration: 0102 Loss: 0.75290 Time: 0.00200\n",
      "Iteration: 0103 Loss: 0.74682 Time: 0.00299\n",
      "Iteration: 0104 Loss: 0.74377 Time: 0.00206\n",
      "Iteration: 0105 Loss: 0.70129 Time: 0.00204\n",
      "Iteration: 0106 Loss: 0.76553 Time: 0.00196\n",
      "Iteration: 0107 Loss: 0.70084 Time: 0.00200\n",
      "Iteration: 0108 Loss: 0.74843 Time: 0.00196\n",
      "Iteration: 0109 Loss: 0.72253 Time: 0.00201\n",
      "Iteration: 0110 Loss: 0.70729 Time: 0.00301\n",
      "Iteration: 0111 Loss: 0.72065 Time: 0.00195\n",
      "Iteration: 0112 Loss: 0.73218 Time: 0.00205\n",
      "Iteration: 0113 Loss: 0.73798 Time: 0.00297\n",
      "Iteration: 0114 Loss: 0.72376 Time: 0.00198\n",
      "Iteration: 0115 Loss: 0.70848 Time: 0.00200\n",
      "Iteration: 0116 Loss: 0.71997 Time: 0.00306\n",
      "Iteration: 0117 Loss: 0.69338 Time: 0.00190\n",
      "Iteration: 0118 Loss: 0.70852 Time: 0.00201\n",
      "Iteration: 0119 Loss: 0.71297 Time: 0.00401\n",
      "Iteration: 0120 Loss: 0.70085 Time: 0.00294\n",
      "Iteration: 0121 Loss: 0.69867 Time: 0.00200\n",
      "Iteration: 0122 Loss: 0.68763 Time: 0.00401\n",
      "Iteration: 0123 Loss: 0.69363 Time: 0.00300\n",
      "Iteration: 0124 Loss: 0.68765 Time: 0.00199\n",
      "Iteration: 0125 Loss: 0.70317 Time: 0.00199\n",
      "Iteration: 0126 Loss: 0.69601 Time: 0.00303\n",
      "Iteration: 0127 Loss: 0.73233 Time: 0.00299\n",
      "Iteration: 0128 Loss: 0.70583 Time: 0.00303\n",
      "Iteration: 0129 Loss: 0.72430 Time: 0.00196\n",
      "Iteration: 0130 Loss: 0.68424 Time: 0.00414\n",
      "Iteration: 0131 Loss: 0.68222 Time: 0.00293\n",
      "Iteration: 0132 Loss: 0.68021 Time: 0.00194\n",
      "Iteration: 0133 Loss: 0.70542 Time: 0.00192\n",
      "Iteration: 0134 Loss: 0.67650 Time: 0.00208\n",
      "Iteration: 0135 Loss: 0.69228 Time: 0.00196\n",
      "Iteration: 0136 Loss: 0.67611 Time: 0.00300\n",
      "Iteration: 0137 Loss: 0.71524 Time: 0.00300\n",
      "Iteration: 0138 Loss: 0.71449 Time: 0.00301\n",
      "Iteration: 0139 Loss: 0.71222 Time: 0.00199\n",
      "Iteration: 0140 Loss: 0.66558 Time: 0.00324\n",
      "Iteration: 0141 Loss: 0.70526 Time: 0.00197\n",
      "Iteration: 0142 Loss: 0.69937 Time: 0.00199\n",
      "Iteration: 0143 Loss: 0.67822 Time: 0.00105\n",
      "Iteration: 0144 Loss: 0.70647 Time: 0.00300\n",
      "Iteration: 0145 Loss: 0.70319 Time: 0.00201\n",
      "Iteration: 0146 Loss: 0.67436 Time: 0.00399\n",
      "Iteration: 0147 Loss: 0.68272 Time: 0.00312\n",
      "Iteration: 0148 Loss: 0.68035 Time: 0.00428\n",
      "Iteration: 0149 Loss: 0.68662 Time: 0.00260\n",
      "Iteration: 0150 Loss: 0.66167 Time: 0.00315\n",
      "Iteration: 0151 Loss: 0.68108 Time: 0.00207\n",
      "Iteration: 0152 Loss: 0.66820 Time: 0.00288\n",
      "Iteration: 0153 Loss: 0.67608 Time: 0.00399\n",
      "Iteration: 0154 Loss: 0.66640 Time: 0.00300\n",
      "Iteration: 0155 Loss: 0.66531 Time: 0.00401\n",
      "Iteration: 0156 Loss: 0.67266 Time: 0.00199\n",
      "Iteration: 0157 Loss: 0.65157 Time: 0.00310\n",
      "Iteration: 0158 Loss: 0.66476 Time: 0.00290\n",
      "Iteration: 0159 Loss: 0.66518 Time: 0.00300\n",
      "Iteration: 0160 Loss: 0.66908 Time: 0.00407\n",
      "Iteration: 0161 Loss: 0.65243 Time: 0.00296\n",
      "Iteration: 0162 Loss: 0.65167 Time: 0.00200\n",
      "Iteration: 0163 Loss: 0.66012 Time: 0.00203\n",
      "Iteration: 0164 Loss: 0.66051 Time: 0.00336\n",
      "Iteration: 0165 Loss: 0.65858 Time: 0.00261\n",
      "Iteration: 0166 Loss: 0.69367 Time: 0.00200\n",
      "Iteration: 0167 Loss: 0.65224 Time: 0.00300\n",
      "Iteration: 0168 Loss: 0.64975 Time: 0.00300\n",
      "Iteration: 0169 Loss: 0.64180 Time: 0.00356\n",
      "Iteration: 0170 Loss: 0.65131 Time: 0.00197\n",
      "Iteration: 0171 Loss: 0.64314 Time: 0.00402\n",
      "Iteration: 0172 Loss: 0.65962 Time: 0.00218\n",
      "Iteration: 0173 Loss: 0.65139 Time: 0.00204\n",
      "Iteration: 0174 Loss: 0.63616 Time: 0.00195\n",
      "Iteration: 0175 Loss: 0.64620 Time: 0.00300\n",
      "Iteration: 0176 Loss: 0.66799 Time: 0.00196\n",
      "Iteration: 0177 Loss: 0.65508 Time: 0.00306\n",
      "Iteration: 0178 Loss: 0.64105 Time: 0.00294\n",
      "Iteration: 0179 Loss: 0.66792 Time: 0.00200\n",
      "Iteration: 0180 Loss: 0.62668 Time: 0.00200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0181 Loss: 0.65316 Time: 0.00200\n",
      "Iteration: 0182 Loss: 0.66373 Time: 0.00200\n",
      "Iteration: 0183 Loss: 0.64443 Time: 0.00300\n",
      "Iteration: 0184 Loss: 0.66220 Time: 0.00300\n",
      "Iteration: 0185 Loss: 0.64706 Time: 0.00301\n",
      "Iteration: 0186 Loss: 0.65633 Time: 0.00299\n",
      "Iteration: 0187 Loss: 0.63658 Time: 0.00300\n",
      "Iteration: 0188 Loss: 0.64448 Time: 0.00201\n",
      "Iteration: 0189 Loss: 0.64228 Time: 0.00200\n",
      "Iteration: 0190 Loss: 0.63991 Time: 0.00299\n",
      "Iteration: 0191 Loss: 0.65628 Time: 0.00300\n",
      "Iteration: 0192 Loss: 0.65479 Time: 0.00300\n",
      "Iteration: 0193 Loss: 0.63147 Time: 0.00300\n",
      "Iteration: 0194 Loss: 0.65192 Time: 0.00301\n",
      "Iteration: 0195 Loss: 0.64009 Time: 0.00300\n",
      "Iteration: 0196 Loss: 0.63505 Time: 0.00201\n",
      "Iteration: 0197 Loss: 0.62261 Time: 0.00200\n",
      "Iteration: 0198 Loss: 0.63775 Time: 0.00306\n",
      "Iteration: 0199 Loss: 0.62805 Time: 0.00194\n",
      "Iteration: 0200 Loss: 0.63330 Time: 0.00308\n",
      "Done! \n",
      "\n",
      "STEP 3/3 - MODEL EVALUATION \n",
      "\n",
      "Computing the final embedding vectors, for evaluation\n",
      "Done! \n",
      "\n",
      "Testing: link prediction\n",
      "Done! \n",
      "\n",
      "EXPERIMENTS ON MODEL 7 / 10 \n",
      "\n",
      "STEP 1/3 - PREPROCESSING STEPS \n",
      "\n",
      "Masking some edges from the training graph, for link prediction\n",
      "(validation set: 1.0 % of edges - test set: 10.0 % of edges)\n",
      "Done! \n",
      "\n",
      "Preprocessing node features\n",
      "Done! \n",
      "\n",
      "Running the Louvain algorithm for community detection\n",
      "as a preprocessing step for the encoder\n",
      "Done! Louvain has found 6 communities \n",
      "\n",
      "Setting up the model and the optimizer\n",
      "Done! \n",
      "\n",
      "Preprocessing on message passing matrices\n",
      "Done! \n",
      "\n",
      "Initializing TF session\n",
      "Done! \n",
      "\n",
      "STEP 2/3 - MODEL TRAINING \n",
      "\n",
      "Starting training\n",
      "Iteration: 0001 Loss: 1.58960 Time: 0.13614\n",
      "Iteration: 0002 Loss: 1.64881 Time: 0.00199\n",
      "Iteration: 0003 Loss: 1.79154 Time: 0.00319\n",
      "Iteration: 0004 Loss: 1.68499 Time: 0.00200\n",
      "Iteration: 0005 Loss: 1.53405 Time: 0.00301\n",
      "Iteration: 0006 Loss: 1.62477 Time: 0.00200\n",
      "Iteration: 0007 Loss: 1.68715 Time: 0.00199\n",
      "Iteration: 0008 Loss: 1.60829 Time: 0.00200\n",
      "Iteration: 0009 Loss: 1.45881 Time: 0.00200\n",
      "Iteration: 0010 Loss: 1.55172 Time: 0.00412\n",
      "Iteration: 0011 Loss: 1.50243 Time: 0.00199\n",
      "Iteration: 0012 Loss: 1.60266 Time: 0.00301\n",
      "Iteration: 0013 Loss: 1.52301 Time: 0.00399\n",
      "Iteration: 0014 Loss: 1.36531 Time: 0.00218\n",
      "Iteration: 0015 Loss: 1.52733 Time: 0.00200\n",
      "Iteration: 0016 Loss: 1.47282 Time: 0.00200\n",
      "Iteration: 0017 Loss: 1.43205 Time: 0.00305\n",
      "Iteration: 0018 Loss: 1.36093 Time: 0.00295\n",
      "Iteration: 0019 Loss: 1.34535 Time: 0.00306\n",
      "Iteration: 0020 Loss: 1.40879 Time: 0.00200\n",
      "Iteration: 0021 Loss: 1.38138 Time: 0.00300\n",
      "Iteration: 0022 Loss: 1.31326 Time: 0.00206\n",
      "Iteration: 0023 Loss: 1.40713 Time: 0.00193\n",
      "Iteration: 0024 Loss: 1.28761 Time: 0.00311\n",
      "Iteration: 0025 Loss: 1.33224 Time: 0.00189\n",
      "Iteration: 0026 Loss: 1.26662 Time: 0.00196\n",
      "Iteration: 0027 Loss: 1.21030 Time: 0.00208\n",
      "Iteration: 0028 Loss: 1.27680 Time: 0.00100\n",
      "Iteration: 0029 Loss: 1.20683 Time: 0.00199\n",
      "Iteration: 0030 Loss: 1.19897 Time: 0.00201\n",
      "Iteration: 0031 Loss: 1.22712 Time: 0.00200\n",
      "Iteration: 0032 Loss: 1.15615 Time: 0.00307\n",
      "Iteration: 0033 Loss: 1.15767 Time: 0.00203\n",
      "Iteration: 0034 Loss: 1.19830 Time: 0.00291\n",
      "Iteration: 0035 Loss: 1.19369 Time: 0.00207\n",
      "Iteration: 0036 Loss: 1.20818 Time: 0.00109\n",
      "Iteration: 0037 Loss: 1.19441 Time: 0.00291\n",
      "Iteration: 0038 Loss: 1.14209 Time: 0.00205\n",
      "Iteration: 0039 Loss: 1.09901 Time: 0.00295\n",
      "Iteration: 0040 Loss: 1.02968 Time: 0.00201\n",
      "Iteration: 0041 Loss: 1.07623 Time: 0.00304\n",
      "Iteration: 0042 Loss: 1.04915 Time: 0.00204\n",
      "Iteration: 0043 Loss: 1.15176 Time: 0.00294\n",
      "Iteration: 0044 Loss: 1.04002 Time: 0.00297\n",
      "Iteration: 0045 Loss: 1.04008 Time: 0.00202\n",
      "Iteration: 0046 Loss: 1.02530 Time: 0.00302\n",
      "Iteration: 0047 Loss: 1.02515 Time: 0.00196\n",
      "Iteration: 0048 Loss: 1.03297 Time: 0.00299\n",
      "Iteration: 0049 Loss: 1.05143 Time: 0.00355\n",
      "Iteration: 0050 Loss: 1.02895 Time: 0.00252\n",
      "Iteration: 0051 Loss: 0.98678 Time: 0.00201\n",
      "Iteration: 0052 Loss: 0.98106 Time: 0.00199\n",
      "Iteration: 0053 Loss: 0.95697 Time: 0.00300\n",
      "Iteration: 0054 Loss: 1.03904 Time: 0.00207\n",
      "Iteration: 0055 Loss: 0.94966 Time: 0.00195\n",
      "Iteration: 0056 Loss: 1.00907 Time: 0.00308\n",
      "Iteration: 0057 Loss: 0.95995 Time: 0.00292\n",
      "Iteration: 0058 Loss: 0.96117 Time: 0.00309\n",
      "Iteration: 0059 Loss: 0.91662 Time: 0.00196\n",
      "Iteration: 0060 Loss: 0.92747 Time: 0.00296\n",
      "Iteration: 0061 Loss: 0.89585 Time: 0.00200\n",
      "Iteration: 0062 Loss: 0.89274 Time: 0.00200\n",
      "Iteration: 0063 Loss: 0.89579 Time: 0.00317\n",
      "Iteration: 0064 Loss: 0.87610 Time: 0.00296\n",
      "Iteration: 0065 Loss: 0.85982 Time: 0.00198\n",
      "Iteration: 0066 Loss: 0.84113 Time: 0.00200\n",
      "Iteration: 0067 Loss: 0.87147 Time: 0.00201\n",
      "Iteration: 0068 Loss: 0.82580 Time: 0.00198\n",
      "Iteration: 0069 Loss: 0.84294 Time: 0.00297\n",
      "Iteration: 0070 Loss: 0.89857 Time: 0.00318\n",
      "Iteration: 0071 Loss: 0.82119 Time: 0.00380\n",
      "Iteration: 0072 Loss: 0.81467 Time: 0.00300\n",
      "Iteration: 0073 Loss: 0.80311 Time: 0.00202\n",
      "Iteration: 0074 Loss: 0.81176 Time: 0.00194\n",
      "Iteration: 0075 Loss: 0.82017 Time: 0.00197\n",
      "Iteration: 0076 Loss: 0.84966 Time: 0.00294\n",
      "Iteration: 0077 Loss: 0.80739 Time: 0.00200\n",
      "Iteration: 0078 Loss: 0.80822 Time: 0.00310\n",
      "Iteration: 0079 Loss: 0.84535 Time: 0.00194\n",
      "Iteration: 0080 Loss: 0.78409 Time: 0.00206\n",
      "Iteration: 0081 Loss: 0.82048 Time: 0.00195\n",
      "Iteration: 0082 Loss: 0.80627 Time: 0.00399\n",
      "Iteration: 0083 Loss: 0.83052 Time: 0.00196\n",
      "Iteration: 0084 Loss: 0.78968 Time: 0.00304\n",
      "Iteration: 0085 Loss: 0.78204 Time: 0.00197\n",
      "Iteration: 0086 Loss: 0.81423 Time: 0.00206\n",
      "Iteration: 0087 Loss: 0.79146 Time: 0.00189\n",
      "Iteration: 0088 Loss: 0.82131 Time: 0.00301\n",
      "Iteration: 0089 Loss: 0.76145 Time: 0.00201\n",
      "Iteration: 0090 Loss: 0.75329 Time: 0.00303\n",
      "Iteration: 0091 Loss: 0.75615 Time: 0.00298\n",
      "Iteration: 0092 Loss: 0.72719 Time: 0.00247\n",
      "Iteration: 0093 Loss: 0.76282 Time: 0.00200\n",
      "Iteration: 0094 Loss: 0.75073 Time: 0.00203\n",
      "Iteration: 0095 Loss: 0.77195 Time: 0.00307\n",
      "Iteration: 0096 Loss: 0.76940 Time: 0.00189\n",
      "Iteration: 0097 Loss: 0.77388 Time: 0.00300\n",
      "Iteration: 0098 Loss: 0.72835 Time: 0.00300\n",
      "Iteration: 0099 Loss: 0.76376 Time: 0.00300\n",
      "Iteration: 0100 Loss: 0.73989 Time: 0.00405\n",
      "Iteration: 0101 Loss: 0.77567 Time: 0.00296\n",
      "Iteration: 0102 Loss: 0.75013 Time: 0.00207\n",
      "Iteration: 0103 Loss: 0.74483 Time: 0.00305\n",
      "Iteration: 0104 Loss: 0.75351 Time: 0.00200\n",
      "Iteration: 0105 Loss: 0.74461 Time: 0.00300\n",
      "Iteration: 0106 Loss: 0.71619 Time: 0.00304\n",
      "Iteration: 0107 Loss: 0.74100 Time: 0.00196\n",
      "Iteration: 0108 Loss: 0.75665 Time: 0.00304\n",
      "Iteration: 0109 Loss: 0.73530 Time: 0.00302\n",
      "Iteration: 0110 Loss: 0.69440 Time: 0.00194\n",
      "Iteration: 0111 Loss: 0.73125 Time: 0.00301\n",
      "Iteration: 0112 Loss: 0.74192 Time: 0.00314\n",
      "Iteration: 0113 Loss: 0.74765 Time: 0.00193\n",
      "Iteration: 0114 Loss: 0.71134 Time: 0.00207\n",
      "Iteration: 0115 Loss: 0.72880 Time: 0.00291\n",
      "Iteration: 0116 Loss: 0.72361 Time: 0.00335\n",
      "Iteration: 0117 Loss: 0.70177 Time: 0.00266\n",
      "Iteration: 0118 Loss: 0.72267 Time: 0.00203\n",
      "Iteration: 0119 Loss: 0.67033 Time: 0.00307\n",
      "Iteration: 0120 Loss: 0.73108 Time: 0.00302\n",
      "Iteration: 0121 Loss: 0.72483 Time: 0.00311\n",
      "Iteration: 0122 Loss: 0.69636 Time: 0.00287\n",
      "Iteration: 0123 Loss: 0.70991 Time: 0.00197\n",
      "Iteration: 0124 Loss: 0.67648 Time: 0.00297\n",
      "Iteration: 0125 Loss: 0.70939 Time: 0.00305\n",
      "Iteration: 0126 Loss: 0.70624 Time: 0.00194\n",
      "Iteration: 0127 Loss: 0.69451 Time: 0.00298\n",
      "Iteration: 0128 Loss: 0.70654 Time: 0.00201\n",
      "Iteration: 0129 Loss: 0.70173 Time: 0.00450\n",
      "Iteration: 0130 Loss: 0.66983 Time: 0.00249\n",
      "Iteration: 0131 Loss: 0.70335 Time: 0.00204\n",
      "Iteration: 0132 Loss: 0.69782 Time: 0.00344\n",
      "Iteration: 0133 Loss: 0.69367 Time: 0.00259\n",
      "Iteration: 0134 Loss: 0.69692 Time: 0.00298\n",
      "Iteration: 0135 Loss: 0.68578 Time: 0.00196\n",
      "Iteration: 0136 Loss: 0.68575 Time: 0.00200\n",
      "Iteration: 0137 Loss: 0.66731 Time: 0.00405\n",
      "Iteration: 0138 Loss: 0.68326 Time: 0.00299\n",
      "Iteration: 0139 Loss: 0.70099 Time: 0.00296\n",
      "Iteration: 0140 Loss: 0.71009 Time: 0.00301\n",
      "Iteration: 0141 Loss: 0.69220 Time: 0.00199\n",
      "Iteration: 0142 Loss: 0.68938 Time: 0.00200\n",
      "Iteration: 0143 Loss: 0.68499 Time: 0.00295\n",
      "Iteration: 0144 Loss: 0.68023 Time: 0.00309\n",
      "Iteration: 0145 Loss: 0.68738 Time: 0.00292\n",
      "Iteration: 0146 Loss: 0.65845 Time: 0.00357\n",
      "Iteration: 0147 Loss: 0.71278 Time: 0.00243\n",
      "Iteration: 0148 Loss: 0.66376 Time: 0.00200\n",
      "Iteration: 0149 Loss: 0.67587 Time: 0.00300\n",
      "Iteration: 0150 Loss: 0.65043 Time: 0.00304\n",
      "Iteration: 0151 Loss: 0.67722 Time: 0.00304\n",
      "Iteration: 0152 Loss: 0.68341 Time: 0.00304\n",
      "Iteration: 0153 Loss: 0.65504 Time: 0.00187\n",
      "Iteration: 0154 Loss: 0.66436 Time: 0.00302\n",
      "Iteration: 0155 Loss: 0.64235 Time: 0.00186\n",
      "Iteration: 0156 Loss: 0.67316 Time: 0.00296\n",
      "Iteration: 0157 Loss: 0.66453 Time: 0.00349\n",
      "Iteration: 0158 Loss: 0.66991 Time: 0.00251\n",
      "Iteration: 0159 Loss: 0.65305 Time: 0.00205\n",
      "Iteration: 0160 Loss: 0.64509 Time: 0.00304\n",
      "Iteration: 0161 Loss: 0.65982 Time: 0.00305\n",
      "Iteration: 0162 Loss: 0.66311 Time: 0.00294\n",
      "Iteration: 0163 Loss: 0.66798 Time: 0.00195\n",
      "Iteration: 0164 Loss: 0.66458 Time: 0.00304\n",
      "Iteration: 0165 Loss: 0.68145 Time: 0.00401\n",
      "Iteration: 0166 Loss: 0.63391 Time: 0.00299\n",
      "Iteration: 0167 Loss: 0.65383 Time: 0.00300\n",
      "Iteration: 0168 Loss: 0.65373 Time: 0.00313\n",
      "Iteration: 0169 Loss: 0.64377 Time: 0.00188\n",
      "Iteration: 0170 Loss: 0.65471 Time: 0.00202\n",
      "Iteration: 0171 Loss: 0.65186 Time: 0.00309\n",
      "Iteration: 0172 Loss: 0.67578 Time: 0.00193\n",
      "Iteration: 0173 Loss: 0.63465 Time: 0.00202\n",
      "Iteration: 0174 Loss: 0.65879 Time: 0.00193\n",
      "Iteration: 0175 Loss: 0.65506 Time: 0.00409\n",
      "Iteration: 0176 Loss: 0.63874 Time: 0.00296\n",
      "Iteration: 0177 Loss: 0.64188 Time: 0.00195\n",
      "Iteration: 0178 Loss: 0.67221 Time: 0.00359\n",
      "Iteration: 0179 Loss: 0.63859 Time: 0.00240\n",
      "Iteration: 0180 Loss: 0.66186 Time: 0.00207\n",
      "Iteration: 0181 Loss: 0.64713 Time: 0.00307\n",
      "Iteration: 0182 Loss: 0.64620 Time: 0.00204\n",
      "Iteration: 0183 Loss: 0.64964 Time: 0.00308\n",
      "Iteration: 0184 Loss: 0.66818 Time: 0.00287\n",
      "Iteration: 0185 Loss: 0.64851 Time: 0.00309\n",
      "Iteration: 0186 Loss: 0.65865 Time: 0.00303\n",
      "Iteration: 0187 Loss: 0.65459 Time: 0.00100\n",
      "Iteration: 0188 Loss: 0.64126 Time: 0.00303\n",
      "Iteration: 0189 Loss: 0.64660 Time: 0.00194\n",
      "Iteration: 0190 Loss: 0.65473 Time: 0.00498\n",
      "Iteration: 0191 Loss: 0.62818 Time: 0.00303\n",
      "Iteration: 0192 Loss: 0.65329 Time: 0.00201\n",
      "Iteration: 0193 Loss: 0.64741 Time: 0.00312\n",
      "Iteration: 0194 Loss: 0.65904 Time: 0.00252\n",
      "Iteration: 0195 Loss: 0.64632 Time: 0.00246\n",
      "Iteration: 0196 Loss: 0.62528 Time: 0.00297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0197 Loss: 0.63045 Time: 0.00339\n",
      "Iteration: 0198 Loss: 0.64740 Time: 0.00170\n",
      "Iteration: 0199 Loss: 0.63397 Time: 0.00503\n",
      "Iteration: 0200 Loss: 0.64924 Time: 0.00288\n",
      "Done! \n",
      "\n",
      "STEP 3/3 - MODEL EVALUATION \n",
      "\n",
      "Computing the final embedding vectors, for evaluation\n",
      "Done! \n",
      "\n",
      "Testing: link prediction\n",
      "Done! \n",
      "\n",
      "EXPERIMENTS ON MODEL 8 / 10 \n",
      "\n",
      "STEP 1/3 - PREPROCESSING STEPS \n",
      "\n",
      "Masking some edges from the training graph, for link prediction\n",
      "(validation set: 1.0 % of edges - test set: 10.0 % of edges)\n",
      "Done! \n",
      "\n",
      "Preprocessing node features\n",
      "Done! \n",
      "\n",
      "Running the Louvain algorithm for community detection\n",
      "as a preprocessing step for the encoder\n",
      "Done! Louvain has found 5 communities \n",
      "\n",
      "Setting up the model and the optimizer\n",
      "Done! \n",
      "\n",
      "Preprocessing on message passing matrices\n",
      "Done! \n",
      "\n",
      "Initializing TF session\n",
      "Done! \n",
      "\n",
      "STEP 2/3 - MODEL TRAINING \n",
      "\n",
      "Starting training\n",
      "Iteration: 0001 Loss: 1.76014 Time: 0.13198\n",
      "Iteration: 0002 Loss: 1.82828 Time: 0.00400\n",
      "Iteration: 0003 Loss: 1.70883 Time: 0.00401\n",
      "Iteration: 0004 Loss: 1.66848 Time: 0.00300\n",
      "Iteration: 0005 Loss: 1.64469 Time: 0.00299\n",
      "Iteration: 0006 Loss: 1.60414 Time: 0.00300\n",
      "Iteration: 0007 Loss: 1.59402 Time: 0.00200\n",
      "Iteration: 0008 Loss: 1.58865 Time: 0.00200\n",
      "Iteration: 0009 Loss: 1.64765 Time: 0.00200\n",
      "Iteration: 0010 Loss: 1.52089 Time: 0.00300\n",
      "Iteration: 0011 Loss: 1.50911 Time: 0.00200\n",
      "Iteration: 0012 Loss: 1.53001 Time: 0.00301\n",
      "Iteration: 0013 Loss: 1.49194 Time: 0.00300\n",
      "Iteration: 0014 Loss: 1.49651 Time: 0.00300\n",
      "Iteration: 0015 Loss: 1.48264 Time: 0.00201\n",
      "Iteration: 0016 Loss: 1.44806 Time: 0.00199\n",
      "Iteration: 0017 Loss: 1.42312 Time: 0.00300\n",
      "Iteration: 0018 Loss: 1.42802 Time: 0.00200\n",
      "Iteration: 0019 Loss: 1.39164 Time: 0.00300\n",
      "Iteration: 0020 Loss: 1.40080 Time: 0.00355\n",
      "Iteration: 0021 Loss: 1.34260 Time: 0.00255\n",
      "Iteration: 0022 Loss: 1.42424 Time: 0.00190\n",
      "Iteration: 0023 Loss: 1.34380 Time: 0.00200\n",
      "Iteration: 0024 Loss: 1.40825 Time: 0.00203\n",
      "Iteration: 0025 Loss: 1.25146 Time: 0.00197\n",
      "Iteration: 0026 Loss: 1.29548 Time: 0.00204\n",
      "Iteration: 0027 Loss: 1.31097 Time: 0.00196\n",
      "Iteration: 0028 Loss: 1.19023 Time: 0.00305\n",
      "Iteration: 0029 Loss: 1.27146 Time: 0.00093\n",
      "Iteration: 0030 Loss: 1.29536 Time: 0.00304\n",
      "Iteration: 0031 Loss: 1.23409 Time: 0.00211\n",
      "Iteration: 0032 Loss: 1.13868 Time: 0.00388\n",
      "Iteration: 0033 Loss: 1.12627 Time: 0.00497\n",
      "Iteration: 0034 Loss: 1.23333 Time: 0.00309\n",
      "Iteration: 0035 Loss: 1.17657 Time: 0.00298\n",
      "Iteration: 0036 Loss: 1.10145 Time: 0.00194\n",
      "Iteration: 0037 Loss: 1.11852 Time: 0.00204\n",
      "Iteration: 0038 Loss: 1.14548 Time: 0.00208\n",
      "Iteration: 0039 Loss: 1.05985 Time: 0.00300\n",
      "Iteration: 0040 Loss: 1.15765 Time: 0.00192\n",
      "Iteration: 0041 Loss: 1.17133 Time: 0.00304\n",
      "Iteration: 0042 Loss: 1.12088 Time: 0.00204\n",
      "Iteration: 0043 Loss: 1.11950 Time: 0.00298\n",
      "Iteration: 0044 Loss: 1.12205 Time: 0.00301\n",
      "Iteration: 0045 Loss: 1.13120 Time: 0.00308\n",
      "Iteration: 0046 Loss: 1.10520 Time: 0.00197\n",
      "Iteration: 0047 Loss: 1.06835 Time: 0.00312\n",
      "Iteration: 0048 Loss: 1.04436 Time: 0.00202\n",
      "Iteration: 0049 Loss: 1.01214 Time: 0.00294\n",
      "Iteration: 0050 Loss: 0.96318 Time: 0.00300\n",
      "Iteration: 0051 Loss: 1.00537 Time: 0.00302\n",
      "Iteration: 0052 Loss: 1.06521 Time: 0.00298\n",
      "Iteration: 0053 Loss: 0.96306 Time: 0.00197\n",
      "Iteration: 0054 Loss: 1.00512 Time: 0.00300\n",
      "Iteration: 0055 Loss: 0.94072 Time: 0.00296\n",
      "Iteration: 0056 Loss: 1.01264 Time: 0.00201\n",
      "Iteration: 0057 Loss: 0.97123 Time: 0.00407\n",
      "Iteration: 0058 Loss: 0.95903 Time: 0.00295\n",
      "Iteration: 0059 Loss: 0.96046 Time: 0.00297\n",
      "Iteration: 0060 Loss: 0.83816 Time: 0.00202\n",
      "Iteration: 0061 Loss: 0.92858 Time: 0.00398\n",
      "Iteration: 0062 Loss: 0.93189 Time: 0.00205\n",
      "Iteration: 0063 Loss: 0.97079 Time: 0.00200\n",
      "Iteration: 0064 Loss: 0.90210 Time: 0.00304\n",
      "Iteration: 0065 Loss: 0.90309 Time: 0.00293\n",
      "Iteration: 0066 Loss: 0.86425 Time: 0.00198\n",
      "Iteration: 0067 Loss: 0.82479 Time: 0.00297\n",
      "Iteration: 0068 Loss: 0.87586 Time: 0.00304\n",
      "Iteration: 0069 Loss: 0.86739 Time: 0.00395\n",
      "Iteration: 0070 Loss: 0.90567 Time: 0.00201\n",
      "Iteration: 0071 Loss: 0.85566 Time: 0.00399\n",
      "Iteration: 0072 Loss: 0.82165 Time: 0.00315\n",
      "Iteration: 0073 Loss: 0.88475 Time: 0.00308\n",
      "Iteration: 0074 Loss: 0.82622 Time: 0.00291\n",
      "Iteration: 0075 Loss: 0.85547 Time: 0.00303\n",
      "Iteration: 0076 Loss: 0.81187 Time: 0.00194\n",
      "Iteration: 0077 Loss: 0.87149 Time: 0.00203\n",
      "Iteration: 0078 Loss: 0.83174 Time: 0.00339\n",
      "Iteration: 0079 Loss: 0.81909 Time: 0.00193\n",
      "Iteration: 0080 Loss: 0.83423 Time: 0.00199\n",
      "Iteration: 0081 Loss: 0.79941 Time: 0.00201\n",
      "Iteration: 0082 Loss: 0.84214 Time: 0.00197\n",
      "Iteration: 0083 Loss: 0.81428 Time: 0.00196\n",
      "Iteration: 0084 Loss: 0.75822 Time: 0.00198\n",
      "Iteration: 0085 Loss: 0.78752 Time: 0.00196\n",
      "Iteration: 0086 Loss: 0.80964 Time: 0.00300\n",
      "Iteration: 0087 Loss: 0.79642 Time: 0.00301\n",
      "Iteration: 0088 Loss: 0.78114 Time: 0.00202\n",
      "Iteration: 0089 Loss: 0.75494 Time: 0.00363\n",
      "Iteration: 0090 Loss: 0.78619 Time: 0.00196\n",
      "Iteration: 0091 Loss: 0.80074 Time: 0.00300\n",
      "Iteration: 0092 Loss: 0.77180 Time: 0.00199\n",
      "Iteration: 0093 Loss: 0.75467 Time: 0.00313\n",
      "Iteration: 0094 Loss: 0.75859 Time: 0.00345\n",
      "Iteration: 0095 Loss: 0.74722 Time: 0.00195\n",
      "Iteration: 0096 Loss: 0.71151 Time: 0.00196\n",
      "Iteration: 0097 Loss: 0.73281 Time: 0.00300\n",
      "Iteration: 0098 Loss: 0.72745 Time: 0.00317\n",
      "Iteration: 0099 Loss: 0.75440 Time: 0.00283\n",
      "Iteration: 0100 Loss: 0.73481 Time: 0.00401\n",
      "Iteration: 0101 Loss: 0.75010 Time: 0.00311\n",
      "Iteration: 0102 Loss: 0.73113 Time: 0.00300\n",
      "Iteration: 0103 Loss: 0.72060 Time: 0.00198\n",
      "Iteration: 0104 Loss: 0.73235 Time: 0.00304\n",
      "Iteration: 0105 Loss: 0.73615 Time: 0.00295\n",
      "Iteration: 0106 Loss: 0.77092 Time: 0.00200\n",
      "Iteration: 0107 Loss: 0.73652 Time: 0.00349\n",
      "Iteration: 0108 Loss: 0.74840 Time: 0.00193\n",
      "Iteration: 0109 Loss: 0.69736 Time: 0.00309\n",
      "Iteration: 0110 Loss: 0.72140 Time: 0.00191\n",
      "Iteration: 0111 Loss: 0.72128 Time: 0.00305\n",
      "Iteration: 0112 Loss: 0.71489 Time: 0.00296\n",
      "Iteration: 0113 Loss: 0.74245 Time: 0.00200\n",
      "Iteration: 0114 Loss: 0.75118 Time: 0.00204\n",
      "Iteration: 0115 Loss: 0.70317 Time: 0.00301\n",
      "Iteration: 0116 Loss: 0.71042 Time: 0.00100\n",
      "Iteration: 0117 Loss: 0.69614 Time: 0.00200\n",
      "Iteration: 0118 Loss: 0.68064 Time: 0.00316\n",
      "Iteration: 0119 Loss: 0.72214 Time: 0.00213\n",
      "Iteration: 0120 Loss: 0.74452 Time: 0.00285\n",
      "Iteration: 0121 Loss: 0.71996 Time: 0.00309\n",
      "Iteration: 0122 Loss: 0.71326 Time: 0.00191\n",
      "Iteration: 0123 Loss: 0.69374 Time: 0.00287\n",
      "Iteration: 0124 Loss: 0.73483 Time: 0.00300\n",
      "Iteration: 0125 Loss: 0.70842 Time: 0.00301\n",
      "Iteration: 0126 Loss: 0.69899 Time: 0.00199\n",
      "Iteration: 0127 Loss: 0.69473 Time: 0.00301\n",
      "Iteration: 0128 Loss: 0.67500 Time: 0.00399\n",
      "Iteration: 0129 Loss: 0.72179 Time: 0.00200\n",
      "Iteration: 0130 Loss: 0.70493 Time: 0.00200\n",
      "Iteration: 0131 Loss: 0.68246 Time: 0.00306\n",
      "Iteration: 0132 Loss: 0.74428 Time: 0.00295\n",
      "Iteration: 0133 Loss: 0.72313 Time: 0.00195\n",
      "Iteration: 0134 Loss: 0.75349 Time: 0.00301\n",
      "Iteration: 0135 Loss: 0.69468 Time: 0.00300\n",
      "Iteration: 0136 Loss: 0.69752 Time: 0.00197\n",
      "Iteration: 0137 Loss: 0.70868 Time: 0.00199\n",
      "Iteration: 0138 Loss: 0.68385 Time: 0.00100\n",
      "Iteration: 0139 Loss: 0.66780 Time: 0.00301\n",
      "Iteration: 0140 Loss: 0.68767 Time: 0.00315\n",
      "Iteration: 0141 Loss: 0.69809 Time: 0.00197\n",
      "Iteration: 0142 Loss: 0.68740 Time: 0.00216\n",
      "Iteration: 0143 Loss: 0.67768 Time: 0.00195\n",
      "Iteration: 0144 Loss: 0.68382 Time: 0.00199\n",
      "Iteration: 0145 Loss: 0.65772 Time: 0.00300\n",
      "Iteration: 0146 Loss: 0.69889 Time: 0.00401\n",
      "Iteration: 0147 Loss: 0.68695 Time: 0.00216\n",
      "Iteration: 0148 Loss: 0.67730 Time: 0.00200\n",
      "Iteration: 0149 Loss: 0.68986 Time: 0.00313\n",
      "Iteration: 0150 Loss: 0.68568 Time: 0.00195\n",
      "Iteration: 0151 Loss: 0.67345 Time: 0.00400\n",
      "Iteration: 0152 Loss: 0.68263 Time: 0.00301\n",
      "Iteration: 0153 Loss: 0.66254 Time: 0.00304\n",
      "Iteration: 0154 Loss: 0.66497 Time: 0.00390\n",
      "Iteration: 0155 Loss: 0.66574 Time: 0.00292\n",
      "Iteration: 0156 Loss: 0.68861 Time: 0.00304\n",
      "Iteration: 0157 Loss: 0.66187 Time: 0.00190\n",
      "Iteration: 0158 Loss: 0.66395 Time: 0.00298\n",
      "Iteration: 0159 Loss: 0.65985 Time: 0.00300\n",
      "Iteration: 0160 Loss: 0.65903 Time: 0.00200\n",
      "Iteration: 0161 Loss: 0.66332 Time: 0.00489\n",
      "Iteration: 0162 Loss: 0.65445 Time: 0.00199\n",
      "Iteration: 0163 Loss: 0.67967 Time: 0.00305\n",
      "Iteration: 0164 Loss: 0.64747 Time: 0.00295\n",
      "Iteration: 0165 Loss: 0.67061 Time: 0.00200\n",
      "Iteration: 0166 Loss: 0.64877 Time: 0.00297\n",
      "Iteration: 0167 Loss: 0.64982 Time: 0.00302\n",
      "Iteration: 0168 Loss: 0.65768 Time: 0.00310\n",
      "Iteration: 0169 Loss: 0.65512 Time: 0.00296\n",
      "Iteration: 0170 Loss: 0.67268 Time: 0.00196\n",
      "Iteration: 0171 Loss: 0.64235 Time: 0.00201\n",
      "Iteration: 0172 Loss: 0.66257 Time: 0.00404\n",
      "Iteration: 0173 Loss: 0.65527 Time: 0.00198\n",
      "Iteration: 0174 Loss: 0.64762 Time: 0.00301\n",
      "Iteration: 0175 Loss: 0.66096 Time: 0.00342\n",
      "Iteration: 0176 Loss: 0.66519 Time: 0.00155\n",
      "Iteration: 0177 Loss: 0.62946 Time: 0.00200\n",
      "Iteration: 0178 Loss: 0.64572 Time: 0.00436\n",
      "Iteration: 0179 Loss: 0.65813 Time: 0.00265\n",
      "Iteration: 0180 Loss: 0.64789 Time: 0.00204\n",
      "Iteration: 0181 Loss: 0.64629 Time: 0.00297\n",
      "Iteration: 0182 Loss: 0.65306 Time: 0.00200\n",
      "Iteration: 0183 Loss: 0.66590 Time: 0.00287\n",
      "Iteration: 0184 Loss: 0.63109 Time: 0.00200\n",
      "Iteration: 0185 Loss: 0.64516 Time: 0.00304\n",
      "Iteration: 0186 Loss: 0.64537 Time: 0.00296\n",
      "Iteration: 0187 Loss: 0.65163 Time: 0.00339\n",
      "Iteration: 0188 Loss: 0.65467 Time: 0.00167\n",
      "Iteration: 0189 Loss: 0.64036 Time: 0.00200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0190 Loss: 0.63930 Time: 0.00318\n",
      "Iteration: 0191 Loss: 0.63544 Time: 0.00319\n",
      "Iteration: 0192 Loss: 0.64114 Time: 0.00295\n",
      "Iteration: 0193 Loss: 0.66998 Time: 0.00286\n",
      "Iteration: 0194 Loss: 0.64091 Time: 0.00200\n",
      "Iteration: 0195 Loss: 0.64218 Time: 0.00199\n",
      "Iteration: 0196 Loss: 0.64334 Time: 0.00200\n",
      "Iteration: 0197 Loss: 0.64013 Time: 0.00300\n",
      "Iteration: 0198 Loss: 0.63367 Time: 0.00207\n",
      "Iteration: 0199 Loss: 0.65500 Time: 0.00219\n",
      "Iteration: 0200 Loss: 0.64724 Time: 0.00213\n",
      "Done! \n",
      "\n",
      "STEP 3/3 - MODEL EVALUATION \n",
      "\n",
      "Computing the final embedding vectors, for evaluation\n",
      "Done! \n",
      "\n",
      "Testing: link prediction\n",
      "Done! \n",
      "\n",
      "EXPERIMENTS ON MODEL 9 / 10 \n",
      "\n",
      "STEP 1/3 - PREPROCESSING STEPS \n",
      "\n",
      "Masking some edges from the training graph, for link prediction\n",
      "(validation set: 1.0 % of edges - test set: 10.0 % of edges)\n",
      "Done! \n",
      "\n",
      "Preprocessing node features\n",
      "Done! \n",
      "\n",
      "Running the Louvain algorithm for community detection\n",
      "as a preprocessing step for the encoder\n",
      "Done! Louvain has found 6 communities \n",
      "\n",
      "Setting up the model and the optimizer\n",
      "Done! \n",
      "\n",
      "Preprocessing on message passing matrices\n",
      "Done! \n",
      "\n",
      "Initializing TF session\n",
      "Done! \n",
      "\n",
      "STEP 2/3 - MODEL TRAINING \n",
      "\n",
      "Starting training\n",
      "Iteration: 0001 Loss: 1.66092 Time: 0.14100\n",
      "Iteration: 0002 Loss: 1.70689 Time: 0.00300\n",
      "Iteration: 0003 Loss: 1.58557 Time: 0.00321\n",
      "Iteration: 0004 Loss: 1.68370 Time: 0.00180\n",
      "Iteration: 0005 Loss: 1.64174 Time: 0.00301\n",
      "Iteration: 0006 Loss: 1.58269 Time: 0.00300\n",
      "Iteration: 0007 Loss: 1.59999 Time: 0.00301\n",
      "Iteration: 0008 Loss: 1.58103 Time: 0.00300\n",
      "Iteration: 0009 Loss: 1.61793 Time: 0.00300\n",
      "Iteration: 0010 Loss: 1.62776 Time: 0.00302\n",
      "Iteration: 0011 Loss: 1.54292 Time: 0.00202\n",
      "Iteration: 0012 Loss: 1.50874 Time: 0.00500\n",
      "Iteration: 0013 Loss: 1.44556 Time: 0.00199\n",
      "Iteration: 0014 Loss: 1.48465 Time: 0.00399\n",
      "Iteration: 0015 Loss: 1.45583 Time: 0.00400\n",
      "Iteration: 0016 Loss: 1.50625 Time: 0.00305\n",
      "Iteration: 0017 Loss: 1.37440 Time: 0.00295\n",
      "Iteration: 0018 Loss: 1.51593 Time: 0.00301\n",
      "Iteration: 0019 Loss: 1.46497 Time: 0.00199\n",
      "Iteration: 0020 Loss: 1.44093 Time: 0.00312\n",
      "Iteration: 0021 Loss: 1.32474 Time: 0.00193\n",
      "Iteration: 0022 Loss: 1.26053 Time: 0.00202\n",
      "Iteration: 0023 Loss: 1.33037 Time: 0.00203\n",
      "Iteration: 0024 Loss: 1.30872 Time: 0.00295\n",
      "Iteration: 0025 Loss: 1.31583 Time: 0.00196\n",
      "Iteration: 0026 Loss: 1.21072 Time: 0.00206\n",
      "Iteration: 0027 Loss: 1.34816 Time: 0.00296\n",
      "Iteration: 0028 Loss: 1.22105 Time: 0.00300\n",
      "Iteration: 0029 Loss: 1.23298 Time: 0.00307\n",
      "Iteration: 0030 Loss: 1.25627 Time: 0.00293\n",
      "Iteration: 0031 Loss: 1.23920 Time: 0.00308\n",
      "Iteration: 0032 Loss: 1.23189 Time: 0.00193\n",
      "Iteration: 0033 Loss: 1.24077 Time: 0.00200\n",
      "Iteration: 0034 Loss: 1.15922 Time: 0.00202\n",
      "Iteration: 0035 Loss: 1.16228 Time: 0.00295\n",
      "Iteration: 0036 Loss: 1.09384 Time: 0.00404\n",
      "Iteration: 0037 Loss: 1.20707 Time: 0.00204\n",
      "Iteration: 0038 Loss: 1.06586 Time: 0.00203\n",
      "Iteration: 0039 Loss: 1.19693 Time: 0.00412\n",
      "Iteration: 0040 Loss: 1.09921 Time: 0.00211\n",
      "Iteration: 0041 Loss: 1.10607 Time: 0.00288\n",
      "Iteration: 0042 Loss: 1.10215 Time: 0.00299\n",
      "Iteration: 0043 Loss: 1.11608 Time: 0.00203\n",
      "Iteration: 0044 Loss: 1.02201 Time: 0.00200\n",
      "Iteration: 0045 Loss: 1.08347 Time: 0.00340\n",
      "Iteration: 0046 Loss: 1.03990 Time: 0.00205\n",
      "Iteration: 0047 Loss: 1.05500 Time: 0.00210\n",
      "Iteration: 0048 Loss: 1.01991 Time: 0.00292\n",
      "Iteration: 0049 Loss: 1.01857 Time: 0.00209\n",
      "Iteration: 0050 Loss: 1.09855 Time: 0.00196\n",
      "Iteration: 0051 Loss: 0.98618 Time: 0.00200\n",
      "Iteration: 0052 Loss: 1.00279 Time: 0.00304\n",
      "Iteration: 0053 Loss: 0.97700 Time: 0.00296\n",
      "Iteration: 0054 Loss: 1.02167 Time: 0.00300\n",
      "Iteration: 0055 Loss: 0.96997 Time: 0.00201\n",
      "Iteration: 0056 Loss: 0.96984 Time: 0.00299\n",
      "Iteration: 0057 Loss: 1.04398 Time: 0.00201\n",
      "Iteration: 0058 Loss: 0.95757 Time: 0.00212\n",
      "Iteration: 0059 Loss: 0.92374 Time: 0.00282\n",
      "Iteration: 0060 Loss: 0.95184 Time: 0.00319\n",
      "Iteration: 0061 Loss: 0.92929 Time: 0.00386\n",
      "Iteration: 0062 Loss: 0.93266 Time: 0.00195\n",
      "Iteration: 0063 Loss: 0.92845 Time: 0.00218\n",
      "Iteration: 0064 Loss: 0.87058 Time: 0.00394\n",
      "Iteration: 0065 Loss: 0.89686 Time: 0.00288\n",
      "Iteration: 0066 Loss: 0.89464 Time: 0.00206\n",
      "Iteration: 0067 Loss: 0.88079 Time: 0.00301\n",
      "Iteration: 0068 Loss: 0.89088 Time: 0.00199\n",
      "Iteration: 0069 Loss: 0.90603 Time: 0.00312\n",
      "Iteration: 0070 Loss: 0.83557 Time: 0.00196\n",
      "Iteration: 0071 Loss: 0.81652 Time: 0.00308\n",
      "Iteration: 0072 Loss: 0.88216 Time: 0.00297\n",
      "Iteration: 0073 Loss: 0.87157 Time: 0.00196\n",
      "Iteration: 0074 Loss: 0.83549 Time: 0.00308\n",
      "Iteration: 0075 Loss: 0.83584 Time: 0.00288\n",
      "Iteration: 0076 Loss: 0.81938 Time: 0.00299\n",
      "Iteration: 0077 Loss: 0.82028 Time: 0.00300\n",
      "Iteration: 0078 Loss: 0.82109 Time: 0.00305\n",
      "Iteration: 0079 Loss: 0.73026 Time: 0.00196\n",
      "Iteration: 0080 Loss: 0.78728 Time: 0.00300\n",
      "Iteration: 0081 Loss: 0.81820 Time: 0.00200\n",
      "Iteration: 0082 Loss: 0.81829 Time: 0.00307\n",
      "Iteration: 0083 Loss: 0.79221 Time: 0.00195\n",
      "Iteration: 0084 Loss: 0.76338 Time: 0.00205\n",
      "Iteration: 0085 Loss: 0.81031 Time: 0.00199\n",
      "Iteration: 0086 Loss: 0.74353 Time: 0.00302\n",
      "Iteration: 0087 Loss: 0.78263 Time: 0.00196\n",
      "Iteration: 0088 Loss: 0.76585 Time: 0.00300\n",
      "Iteration: 0089 Loss: 0.78034 Time: 0.00200\n",
      "Iteration: 0090 Loss: 0.74259 Time: 0.00200\n",
      "Iteration: 0091 Loss: 0.77083 Time: 0.00310\n",
      "Iteration: 0092 Loss: 0.81244 Time: 0.00190\n",
      "Iteration: 0093 Loss: 0.76542 Time: 0.00200\n",
      "Iteration: 0094 Loss: 0.77358 Time: 0.00300\n",
      "Iteration: 0095 Loss: 0.77478 Time: 0.00340\n",
      "Iteration: 0096 Loss: 0.74890 Time: 0.00196\n",
      "Iteration: 0097 Loss: 0.75235 Time: 0.00298\n",
      "Iteration: 0098 Loss: 0.76867 Time: 0.00300\n",
      "Iteration: 0099 Loss: 0.77316 Time: 0.00300\n",
      "Iteration: 0100 Loss: 0.73550 Time: 0.00326\n",
      "Iteration: 0101 Loss: 0.75027 Time: 0.00169\n",
      "Iteration: 0102 Loss: 0.72320 Time: 0.00298\n",
      "Iteration: 0103 Loss: 0.73243 Time: 0.00299\n",
      "Iteration: 0104 Loss: 0.69861 Time: 0.00200\n",
      "Iteration: 0105 Loss: 0.72247 Time: 0.00416\n",
      "Iteration: 0106 Loss: 0.72176 Time: 0.00289\n",
      "Iteration: 0107 Loss: 0.71793 Time: 0.00194\n",
      "Iteration: 0108 Loss: 0.72898 Time: 0.00303\n",
      "Iteration: 0109 Loss: 0.72641 Time: 0.00403\n",
      "Iteration: 0110 Loss: 0.70105 Time: 0.00295\n",
      "Iteration: 0111 Loss: 0.70219 Time: 0.00200\n",
      "Iteration: 0112 Loss: 0.69843 Time: 0.00299\n",
      "Iteration: 0113 Loss: 0.73143 Time: 0.00211\n",
      "Iteration: 0114 Loss: 0.71722 Time: 0.00296\n",
      "Iteration: 0115 Loss: 0.72020 Time: 0.00394\n",
      "Iteration: 0116 Loss: 0.69730 Time: 0.00198\n",
      "Iteration: 0117 Loss: 0.69433 Time: 0.00401\n",
      "Iteration: 0118 Loss: 0.76213 Time: 0.00300\n",
      "Iteration: 0119 Loss: 0.70263 Time: 0.00303\n",
      "Iteration: 0120 Loss: 0.67799 Time: 0.00400\n",
      "Iteration: 0121 Loss: 0.69676 Time: 0.00401\n",
      "Iteration: 0122 Loss: 0.71480 Time: 0.00303\n",
      "Iteration: 0123 Loss: 0.69996 Time: 0.00436\n",
      "Iteration: 0124 Loss: 0.68720 Time: 0.00163\n",
      "Iteration: 0125 Loss: 0.68630 Time: 0.00198\n",
      "Iteration: 0126 Loss: 0.65949 Time: 0.00400\n",
      "Iteration: 0127 Loss: 0.71261 Time: 0.00200\n",
      "Iteration: 0128 Loss: 0.69473 Time: 0.00215\n",
      "Iteration: 0129 Loss: 0.68898 Time: 0.00188\n",
      "Iteration: 0130 Loss: 0.70052 Time: 0.00218\n",
      "Iteration: 0131 Loss: 0.69603 Time: 0.00201\n",
      "Iteration: 0132 Loss: 0.69363 Time: 0.00337\n",
      "Iteration: 0133 Loss: 0.68511 Time: 0.00163\n",
      "Iteration: 0134 Loss: 0.68947 Time: 0.00315\n",
      "Iteration: 0135 Loss: 0.67687 Time: 0.00214\n",
      "Iteration: 0136 Loss: 0.68995 Time: 0.00282\n",
      "Iteration: 0137 Loss: 0.70313 Time: 0.00197\n",
      "Iteration: 0138 Loss: 0.68994 Time: 0.00204\n",
      "Iteration: 0139 Loss: 0.71588 Time: 0.00312\n",
      "Iteration: 0140 Loss: 0.68754 Time: 0.00199\n",
      "Iteration: 0141 Loss: 0.68349 Time: 0.00305\n",
      "Iteration: 0142 Loss: 0.67828 Time: 0.00212\n",
      "Iteration: 0143 Loss: 0.65948 Time: 0.00283\n",
      "Iteration: 0144 Loss: 0.69194 Time: 0.00300\n",
      "Iteration: 0145 Loss: 0.67769 Time: 0.00400\n",
      "Iteration: 0146 Loss: 0.68736 Time: 0.00200\n",
      "Iteration: 0147 Loss: 0.69315 Time: 0.00216\n",
      "Iteration: 0148 Loss: 0.67096 Time: 0.00196\n",
      "Iteration: 0149 Loss: 0.65022 Time: 0.00320\n",
      "Iteration: 0150 Loss: 0.70162 Time: 0.00279\n",
      "Iteration: 0151 Loss: 0.66081 Time: 0.00309\n",
      "Iteration: 0152 Loss: 0.66009 Time: 0.00191\n",
      "Iteration: 0153 Loss: 0.67391 Time: 0.00343\n",
      "Iteration: 0154 Loss: 0.64440 Time: 0.00106\n",
      "Iteration: 0155 Loss: 0.64994 Time: 0.00317\n",
      "Iteration: 0156 Loss: 0.67869 Time: 0.00199\n",
      "Iteration: 0157 Loss: 0.66563 Time: 0.00197\n",
      "Iteration: 0158 Loss: 0.65446 Time: 0.00316\n",
      "Iteration: 0159 Loss: 0.64045 Time: 0.00210\n",
      "Iteration: 0160 Loss: 0.68208 Time: 0.00196\n",
      "Iteration: 0161 Loss: 0.66792 Time: 0.00318\n",
      "Iteration: 0162 Loss: 0.65873 Time: 0.00094\n",
      "Iteration: 0163 Loss: 0.67167 Time: 0.00300\n",
      "Iteration: 0164 Loss: 0.65955 Time: 0.00212\n",
      "Iteration: 0165 Loss: 0.65237 Time: 0.00300\n",
      "Iteration: 0166 Loss: 0.63213 Time: 0.00382\n",
      "Iteration: 0167 Loss: 0.64905 Time: 0.00399\n",
      "Iteration: 0168 Loss: 0.67546 Time: 0.00300\n",
      "Iteration: 0169 Loss: 0.63965 Time: 0.00326\n",
      "Iteration: 0170 Loss: 0.65563 Time: 0.00193\n",
      "Iteration: 0171 Loss: 0.65617 Time: 0.00403\n",
      "Iteration: 0172 Loss: 0.66029 Time: 0.00318\n",
      "Iteration: 0173 Loss: 0.65493 Time: 0.00321\n",
      "Iteration: 0174 Loss: 0.63368 Time: 0.00098\n",
      "Iteration: 0175 Loss: 0.66234 Time: 0.00204\n",
      "Iteration: 0176 Loss: 0.65420 Time: 0.00296\n",
      "Iteration: 0177 Loss: 0.65985 Time: 0.00300\n",
      "Iteration: 0178 Loss: 0.65329 Time: 0.00200\n",
      "Iteration: 0179 Loss: 0.65961 Time: 0.00203\n",
      "Iteration: 0180 Loss: 0.64179 Time: 0.00300\n",
      "Iteration: 0181 Loss: 0.63340 Time: 0.00296\n",
      "Iteration: 0182 Loss: 0.63089 Time: 0.00303\n",
      "Iteration: 0183 Loss: 0.67573 Time: 0.00308\n",
      "Iteration: 0184 Loss: 0.61929 Time: 0.00295\n",
      "Iteration: 0185 Loss: 0.62978 Time: 0.00318\n",
      "Iteration: 0186 Loss: 0.64844 Time: 0.00098\n",
      "Iteration: 0187 Loss: 0.65545 Time: 0.00299\n",
      "Iteration: 0188 Loss: 0.62507 Time: 0.00319\n",
      "Iteration: 0189 Loss: 0.65746 Time: 0.00181\n",
      "Iteration: 0190 Loss: 0.64961 Time: 0.00201\n",
      "Iteration: 0191 Loss: 0.62281 Time: 0.00299\n",
      "Iteration: 0192 Loss: 0.62934 Time: 0.00207\n",
      "Iteration: 0193 Loss: 0.63095 Time: 0.00197\n",
      "Iteration: 0194 Loss: 0.64291 Time: 0.00299\n",
      "Iteration: 0195 Loss: 0.64437 Time: 0.00213\n",
      "Iteration: 0196 Loss: 0.64625 Time: 0.00183\n",
      "Iteration: 0197 Loss: 0.63305 Time: 0.00194\n",
      "Iteration: 0198 Loss: 0.63797 Time: 0.00200\n",
      "Iteration: 0199 Loss: 0.63090 Time: 0.00199\n",
      "Iteration: 0200 Loss: 0.63955 Time: 0.00197\n",
      "Done! \n",
      "\n",
      "STEP 3/3 - MODEL EVALUATION \n",
      "\n",
      "Computing the final embedding vectors, for evaluation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! \n",
      "\n",
      "Testing: link prediction\n",
      "Done! \n",
      "\n",
      "EXPERIMENTS ON MODEL 10 / 10 \n",
      "\n",
      "STEP 1/3 - PREPROCESSING STEPS \n",
      "\n",
      "Masking some edges from the training graph, for link prediction\n",
      "(validation set: 1.0 % of edges - test set: 10.0 % of edges)\n",
      "Done! \n",
      "\n",
      "Preprocessing node features\n",
      "Done! \n",
      "\n",
      "Running the Louvain algorithm for community detection\n",
      "as a preprocessing step for the encoder\n",
      "Done! Louvain has found 6 communities \n",
      "\n",
      "Setting up the model and the optimizer\n",
      "Done! \n",
      "\n",
      "Preprocessing on message passing matrices\n",
      "Done! \n",
      "\n",
      "Initializing TF session\n",
      "Done! \n",
      "\n",
      "STEP 2/3 - MODEL TRAINING \n",
      "\n",
      "Starting training\n",
      "Iteration: 0001 Loss: 1.68022 Time: 0.15521\n",
      "Iteration: 0002 Loss: 1.79284 Time: 0.00300\n",
      "Iteration: 0003 Loss: 1.50659 Time: 0.00314\n",
      "Iteration: 0004 Loss: 1.77755 Time: 0.00287\n",
      "Iteration: 0005 Loss: 1.62214 Time: 0.00299\n",
      "Iteration: 0006 Loss: 1.69396 Time: 0.00296\n",
      "Iteration: 0007 Loss: 1.64251 Time: 0.00200\n",
      "Iteration: 0008 Loss: 1.72222 Time: 0.00400\n",
      "Iteration: 0009 Loss: 1.61493 Time: 0.00405\n",
      "Iteration: 0010 Loss: 1.54385 Time: 0.00401\n",
      "Iteration: 0011 Loss: 1.55496 Time: 0.00309\n",
      "Iteration: 0012 Loss: 1.52168 Time: 0.00192\n",
      "Iteration: 0013 Loss: 1.54615 Time: 0.00204\n",
      "Iteration: 0014 Loss: 1.52208 Time: 0.00296\n",
      "Iteration: 0015 Loss: 1.32852 Time: 0.00299\n",
      "Iteration: 0016 Loss: 1.43480 Time: 0.00300\n",
      "Iteration: 0017 Loss: 1.44002 Time: 0.00311\n",
      "Iteration: 0018 Loss: 1.50106 Time: 0.00194\n",
      "Iteration: 0019 Loss: 1.42946 Time: 0.00324\n",
      "Iteration: 0020 Loss: 1.42141 Time: 0.00272\n",
      "Iteration: 0021 Loss: 1.33101 Time: 0.00196\n",
      "Iteration: 0022 Loss: 1.28646 Time: 0.00310\n",
      "Iteration: 0023 Loss: 1.32673 Time: 0.00202\n",
      "Iteration: 0024 Loss: 1.33497 Time: 0.00197\n",
      "Iteration: 0025 Loss: 1.33158 Time: 0.00300\n",
      "Iteration: 0026 Loss: 1.31580 Time: 0.00296\n",
      "Iteration: 0027 Loss: 1.28854 Time: 0.00303\n",
      "Iteration: 0028 Loss: 1.29514 Time: 0.00210\n",
      "Iteration: 0029 Loss: 1.20270 Time: 0.00199\n",
      "Iteration: 0030 Loss: 1.22074 Time: 0.00400\n",
      "Iteration: 0031 Loss: 1.23122 Time: 0.00230\n",
      "Iteration: 0032 Loss: 1.24515 Time: 0.00207\n",
      "Iteration: 0033 Loss: 1.17230 Time: 0.00208\n",
      "Iteration: 0034 Loss: 1.22789 Time: 0.00200\n",
      "Iteration: 0035 Loss: 1.23320 Time: 0.00300\n",
      "Iteration: 0036 Loss: 1.22432 Time: 0.00192\n",
      "Iteration: 0037 Loss: 1.15616 Time: 0.00308\n",
      "Iteration: 0038 Loss: 1.13217 Time: 0.00192\n",
      "Iteration: 0039 Loss: 1.16728 Time: 0.00204\n",
      "Iteration: 0040 Loss: 1.02984 Time: 0.00305\n",
      "Iteration: 0041 Loss: 1.11439 Time: 0.00195\n",
      "Iteration: 0042 Loss: 1.12151 Time: 0.00303\n",
      "Iteration: 0043 Loss: 1.09342 Time: 0.00200\n",
      "Iteration: 0044 Loss: 1.13095 Time: 0.00208\n",
      "Iteration: 0045 Loss: 1.10152 Time: 0.00203\n",
      "Iteration: 0046 Loss: 1.01159 Time: 0.00310\n",
      "Iteration: 0047 Loss: 1.02837 Time: 0.00291\n",
      "Iteration: 0048 Loss: 1.03115 Time: 0.00202\n",
      "Iteration: 0049 Loss: 1.05208 Time: 0.00319\n",
      "Iteration: 0050 Loss: 1.09618 Time: 0.00383\n",
      "Iteration: 0051 Loss: 0.99757 Time: 0.00198\n",
      "Iteration: 0052 Loss: 0.98592 Time: 0.00200\n",
      "Iteration: 0053 Loss: 0.98140 Time: 0.00295\n",
      "Iteration: 0054 Loss: 1.03825 Time: 0.00200\n",
      "Iteration: 0055 Loss: 0.99922 Time: 0.00196\n",
      "Iteration: 0056 Loss: 0.95893 Time: 0.00229\n",
      "Iteration: 0057 Loss: 0.98486 Time: 0.00257\n",
      "Iteration: 0058 Loss: 0.99980 Time: 0.00210\n",
      "Iteration: 0059 Loss: 0.95314 Time: 0.00298\n",
      "Iteration: 0060 Loss: 1.00722 Time: 0.00345\n",
      "Iteration: 0061 Loss: 0.94910 Time: 0.00247\n",
      "Iteration: 0062 Loss: 0.93063 Time: 0.00201\n",
      "Iteration: 0063 Loss: 0.99649 Time: 0.00304\n",
      "Iteration: 0064 Loss: 0.92551 Time: 0.00195\n",
      "Iteration: 0065 Loss: 0.84289 Time: 0.00213\n",
      "Iteration: 0066 Loss: 0.95507 Time: 0.00290\n",
      "Iteration: 0067 Loss: 0.89222 Time: 0.00292\n",
      "Iteration: 0068 Loss: 0.82890 Time: 0.00303\n",
      "Iteration: 0069 Loss: 0.89575 Time: 0.00197\n",
      "Iteration: 0070 Loss: 0.86880 Time: 0.00304\n",
      "Iteration: 0071 Loss: 0.88475 Time: 0.00313\n",
      "Iteration: 0072 Loss: 0.91202 Time: 0.00287\n",
      "Iteration: 0073 Loss: 0.85932 Time: 0.00303\n",
      "Iteration: 0074 Loss: 0.88286 Time: 0.00393\n",
      "Iteration: 0075 Loss: 0.90970 Time: 0.00197\n",
      "Iteration: 0076 Loss: 0.80479 Time: 0.00308\n",
      "Iteration: 0077 Loss: 0.81591 Time: 0.00300\n",
      "Iteration: 0078 Loss: 0.88884 Time: 0.00303\n",
      "Iteration: 0079 Loss: 0.77738 Time: 0.00297\n",
      "Iteration: 0080 Loss: 0.84315 Time: 0.00299\n",
      "Iteration: 0081 Loss: 0.81804 Time: 0.00196\n",
      "Iteration: 0082 Loss: 0.80820 Time: 0.00197\n",
      "Iteration: 0083 Loss: 0.77929 Time: 0.00399\n",
      "Iteration: 0084 Loss: 0.79823 Time: 0.00304\n",
      "Iteration: 0085 Loss: 0.77567 Time: 0.00199\n",
      "Iteration: 0086 Loss: 0.77888 Time: 0.00298\n",
      "Iteration: 0087 Loss: 0.77249 Time: 0.00196\n",
      "Iteration: 0088 Loss: 0.76708 Time: 0.00308\n",
      "Iteration: 0089 Loss: 0.77628 Time: 0.00196\n",
      "Iteration: 0090 Loss: 0.78238 Time: 0.00295\n",
      "Iteration: 0091 Loss: 0.78198 Time: 0.00190\n",
      "Iteration: 0092 Loss: 0.73531 Time: 0.00304\n",
      "Iteration: 0093 Loss: 0.78527 Time: 0.00300\n",
      "Iteration: 0094 Loss: 0.74240 Time: 0.00399\n",
      "Iteration: 0095 Loss: 0.76112 Time: 0.00243\n",
      "Iteration: 0096 Loss: 0.77944 Time: 0.00191\n",
      "Iteration: 0097 Loss: 0.75541 Time: 0.00282\n",
      "Iteration: 0098 Loss: 0.77132 Time: 0.00400\n",
      "Iteration: 0099 Loss: 0.74972 Time: 0.00200\n",
      "Iteration: 0100 Loss: 0.75957 Time: 0.00309\n",
      "Iteration: 0101 Loss: 0.73463 Time: 0.00212\n",
      "Iteration: 0102 Loss: 0.75692 Time: 0.00205\n",
      "Iteration: 0103 Loss: 0.72047 Time: 0.00197\n",
      "Iteration: 0104 Loss: 0.71887 Time: 0.00300\n",
      "Iteration: 0105 Loss: 0.76118 Time: 0.00399\n",
      "Iteration: 0106 Loss: 0.74231 Time: 0.00200\n",
      "Iteration: 0107 Loss: 0.72869 Time: 0.00300\n",
      "Iteration: 0108 Loss: 0.73519 Time: 0.00200\n",
      "Iteration: 0109 Loss: 0.70981 Time: 0.00200\n",
      "Iteration: 0110 Loss: 0.74028 Time: 0.00206\n",
      "Iteration: 0111 Loss: 0.72336 Time: 0.00194\n",
      "Iteration: 0112 Loss: 0.72316 Time: 0.00313\n",
      "Iteration: 0113 Loss: 0.71228 Time: 0.00206\n",
      "Iteration: 0114 Loss: 0.73180 Time: 0.00190\n",
      "Iteration: 0115 Loss: 0.72982 Time: 0.00200\n",
      "Iteration: 0116 Loss: 0.68280 Time: 0.00314\n",
      "Iteration: 0117 Loss: 0.68029 Time: 0.00291\n",
      "Iteration: 0118 Loss: 0.70446 Time: 0.00294\n",
      "Iteration: 0119 Loss: 0.71491 Time: 0.00302\n",
      "Iteration: 0120 Loss: 0.72233 Time: 0.00299\n",
      "Iteration: 0121 Loss: 0.69645 Time: 0.00199\n",
      "Iteration: 0122 Loss: 0.69051 Time: 0.00202\n",
      "Iteration: 0123 Loss: 0.68501 Time: 0.00407\n",
      "Iteration: 0124 Loss: 0.69414 Time: 0.00292\n",
      "Iteration: 0125 Loss: 0.71595 Time: 0.00200\n",
      "Iteration: 0126 Loss: 0.71568 Time: 0.00200\n",
      "Iteration: 0127 Loss: 0.69453 Time: 0.00202\n",
      "Iteration: 0128 Loss: 0.68219 Time: 0.00399\n",
      "Iteration: 0129 Loss: 0.68702 Time: 0.00308\n",
      "Iteration: 0130 Loss: 0.68540 Time: 0.00295\n",
      "Iteration: 0131 Loss: 0.68762 Time: 0.00303\n",
      "Iteration: 0132 Loss: 0.71581 Time: 0.00194\n",
      "Iteration: 0133 Loss: 0.68461 Time: 0.00318\n",
      "Iteration: 0134 Loss: 0.68547 Time: 0.00283\n",
      "Iteration: 0135 Loss: 0.68755 Time: 0.00305\n",
      "Iteration: 0136 Loss: 0.66841 Time: 0.00194\n",
      "Iteration: 0137 Loss: 0.66284 Time: 0.00197\n",
      "Iteration: 0138 Loss: 0.66939 Time: 0.00314\n",
      "Iteration: 0139 Loss: 0.65899 Time: 0.00200\n",
      "Iteration: 0140 Loss: 0.66328 Time: 0.00194\n",
      "Iteration: 0141 Loss: 0.67981 Time: 0.00300\n",
      "Iteration: 0142 Loss: 0.67339 Time: 0.00200\n",
      "Iteration: 0143 Loss: 0.69999 Time: 0.00196\n",
      "Iteration: 0144 Loss: 0.69212 Time: 0.00197\n",
      "Iteration: 0145 Loss: 0.68223 Time: 0.00301\n",
      "Iteration: 0146 Loss: 0.67347 Time: 0.00300\n",
      "Iteration: 0147 Loss: 0.67979 Time: 0.00200\n",
      "Iteration: 0148 Loss: 0.67041 Time: 0.00300\n",
      "Iteration: 0149 Loss: 0.67766 Time: 0.00199\n",
      "Iteration: 0150 Loss: 0.68119 Time: 0.00301\n",
      "Iteration: 0151 Loss: 0.67302 Time: 0.00316\n",
      "Iteration: 0152 Loss: 0.67009 Time: 0.00183\n",
      "Iteration: 0153 Loss: 0.66201 Time: 0.00300\n",
      "Iteration: 0154 Loss: 0.68712 Time: 0.00335\n",
      "Iteration: 0155 Loss: 0.64212 Time: 0.00165\n",
      "Iteration: 0156 Loss: 0.63662 Time: 0.00196\n",
      "Iteration: 0157 Loss: 0.65522 Time: 0.00401\n",
      "Iteration: 0158 Loss: 0.65064 Time: 0.00199\n",
      "Iteration: 0159 Loss: 0.64516 Time: 0.00198\n",
      "Iteration: 0160 Loss: 0.68177 Time: 0.00218\n",
      "Iteration: 0161 Loss: 0.66819 Time: 0.00283\n",
      "Iteration: 0162 Loss: 0.67928 Time: 0.00299\n",
      "Iteration: 0163 Loss: 0.63028 Time: 0.00201\n",
      "Iteration: 0164 Loss: 0.66811 Time: 0.00303\n",
      "Iteration: 0165 Loss: 0.65284 Time: 0.00300\n",
      "Iteration: 0166 Loss: 0.65545 Time: 0.00300\n",
      "Iteration: 0167 Loss: 0.65155 Time: 0.00197\n",
      "Iteration: 0168 Loss: 0.63990 Time: 0.00200\n",
      "Iteration: 0169 Loss: 0.65489 Time: 0.00312\n",
      "Iteration: 0170 Loss: 0.65759 Time: 0.00209\n",
      "Iteration: 0171 Loss: 0.64737 Time: 0.00391\n",
      "Iteration: 0172 Loss: 0.64379 Time: 0.00199\n",
      "Iteration: 0173 Loss: 0.64852 Time: 0.00297\n",
      "Iteration: 0174 Loss: 0.67041 Time: 0.00298\n",
      "Iteration: 0175 Loss: 0.65265 Time: 0.00201\n",
      "Iteration: 0176 Loss: 0.63330 Time: 0.00307\n",
      "Iteration: 0177 Loss: 0.63951 Time: 0.00289\n",
      "Iteration: 0178 Loss: 0.65287 Time: 0.00300\n",
      "Iteration: 0179 Loss: 0.63545 Time: 0.00314\n",
      "Iteration: 0180 Loss: 0.64444 Time: 0.00185\n",
      "Iteration: 0181 Loss: 0.62992 Time: 0.00302\n",
      "Iteration: 0182 Loss: 0.65836 Time: 0.00309\n",
      "Iteration: 0183 Loss: 0.63313 Time: 0.00290\n",
      "Iteration: 0184 Loss: 0.64995 Time: 0.00300\n",
      "Iteration: 0185 Loss: 0.63640 Time: 0.00201\n",
      "Iteration: 0186 Loss: 0.63767 Time: 0.00316\n",
      "Iteration: 0187 Loss: 0.62780 Time: 0.00300\n",
      "Iteration: 0188 Loss: 0.63980 Time: 0.00301\n",
      "Iteration: 0189 Loss: 0.65367 Time: 0.00213\n",
      "Iteration: 0190 Loss: 0.64172 Time: 0.00295\n",
      "Iteration: 0191 Loss: 0.64585 Time: 0.00300\n",
      "Iteration: 0192 Loss: 0.64968 Time: 0.00200\n",
      "Iteration: 0193 Loss: 0.64789 Time: 0.00200\n",
      "Iteration: 0194 Loss: 0.63650 Time: 0.00200\n",
      "Iteration: 0195 Loss: 0.62832 Time: 0.00197\n",
      "Iteration: 0196 Loss: 0.63022 Time: 0.00200\n",
      "Iteration: 0197 Loss: 0.63437 Time: 0.00194\n",
      "Iteration: 0198 Loss: 0.64185 Time: 0.00408\n",
      "Iteration: 0199 Loss: 0.64219 Time: 0.00292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0200 Loss: 0.63471 Time: 0.00200\n",
      "Done! \n",
      "\n",
      "STEP 3/3 - MODEL EVALUATION \n",
      "\n",
      "Computing the final embedding vectors, for evaluation\n",
      "Done! \n",
      "\n",
      "Testing: link prediction\n",
      "Done! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We repeat the entire training+test process FLAGS.nb_run times\n",
    "for i in range(FLAGS.nb_run):\n",
    "    if FLAGS.verbose:\n",
    "        print(\"EXPERIMENTS ON MODEL\", i + 1, \"/\", FLAGS.nb_run, \"\\n\")\n",
    "        print(\"STEP 1/3 - PREPROCESSING STEPS \\n\")\n",
    "\n",
    "\n",
    "    # Edge masking for Link Prediction:\n",
    "    if FLAGS.task == 'task_2':\n",
    "        # Compute Train/Validation/Test sets\n",
    "        if FLAGS.verbose:\n",
    "            print(\"Masking some edges from the training graph, for link prediction\")\n",
    "            print(\"(validation set:\", FLAGS.prop_val, \"% of edges - test set:\",\n",
    "                  FLAGS.prop_test, \"% of edges)\")\n",
    "        #adj, val_edges, val_edges_false, test_edges, test_edges_false = mask_test_edges(adj_init, FLAGS.prop_test, FLAGS.prop_val)\n",
    "        adj, test_edges, test_edges_false = mask_test_edges(adj_init, FLAGS.prop_test)\n",
    "        if FLAGS.verbose:\n",
    "            print(\"Done! \\n\")\n",
    "    else:\n",
    "        adj = adj_init\n",
    "\n",
    "    # Compute the number of nodes\n",
    "    num_nodes = adj.shape[0]\n",
    "\n",
    "    # Preprocessing on node features\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Preprocessing node features\")\n",
    "    if FLAGS.features:\n",
    "        features = features_init\n",
    "    else:\n",
    "        # If features are not used, replace feature matrix by identity matrix\n",
    "        features = sp.identity(num_nodes)\n",
    "    features = sparse_to_tuple(features)\n",
    "    num_features = features[2][1]\n",
    "    features_nonzero = features[1].shape[0]\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Done! \\n\")\n",
    "\n",
    "\n",
    "    # Community detection using Louvain, as a preprocessing step\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Running the Louvain algorithm for community detection\")\n",
    "        print(\"as a preprocessing step for the encoder\")\n",
    "    # Get binary community matrix (adj_louvain_init[i,j] = 1 if nodes i and j are\n",
    "    # in the same community) as well as number of communities found by Louvain\n",
    "    adj_louvain_init, nb_communities_louvain, partition = louvain_clustering(adj, FLAGS.s_reg)\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Done! Louvain has found\", nb_communities_louvain, \"communities \\n\")\n",
    "\n",
    "    # FastGAE: node sampling for stochastic subgraph decoding - used when facing scalability issues\n",
    "    if FLAGS.fastgae:\n",
    "        if FLAGS.verbose:\n",
    "            print(\"Preprocessing operations related to the FastGAE method\")\n",
    "        # Node-level p_i degree-based, core-based or uniform distribution\n",
    "        node_distribution = get_distribution(FLAGS.measure, FLAGS.alpha, adj)\n",
    "        # Node sampling for initializations\n",
    "        sampled_nodes, adj_label, adj_sampled_sparse = node_sampling(adj, node_distribution, FLAGS.nb_node_samples, FLAGS.replace)\n",
    "        if FLAGS.verbose:\n",
    "            print(\"Done! \\n\")\n",
    "    else:\n",
    "        sampled_nodes = np.array(range(FLAGS.nb_node_samples))\n",
    "\n",
    "\n",
    "    # Placeholders\n",
    "    if FLAGS.verbose:\n",
    "        print('Setting up the model and the optimizer')\n",
    "    placeholders = {\n",
    "        'features': tf.sparse_placeholder(tf.float32),\n",
    "        'adj': tf.sparse_placeholder(tf.float32),\n",
    "        'adj_layer2': tf.sparse_placeholder(tf.float32), # Only used for 2-layer GCN encoders\n",
    "        'degree_matrix': tf.sparse_placeholder(tf.float32),\n",
    "        'adj_orig': tf.sparse_placeholder(tf.float32),\n",
    "        'dropout': tf.placeholder_with_default(0., shape = ()),\n",
    "        'sampled_nodes': tf.placeholder_with_default(sampled_nodes, shape = [FLAGS.nb_node_samples])\n",
    "    }\n",
    "\n",
    "\n",
    "    # Create model\n",
    "    if FLAGS.model == 'linear_ae':\n",
    "        # Linear Graph Autoencoder\n",
    "        model = LinearModelAE(placeholders, num_features, features_nonzero)\n",
    "    elif FLAGS.model == 'linear_vae':\n",
    "        # Linear Graph Variational Autoencoder\n",
    "        model = LinearModelVAE(placeholders, num_features, num_nodes, features_nonzero)\n",
    "    elif FLAGS.model == 'gcn_ae':\n",
    "        # 2-layer GCN Graph Autoencoder\n",
    "        model = GCNModelAE(placeholders, num_features, features_nonzero)\n",
    "    elif FLAGS.model == 'gcn_vae':\n",
    "        # 2-layer GCN Graph Variational Autoencoder\n",
    "        model = GCNModelVAE(placeholders, num_features, num_nodes, features_nonzero)\n",
    "    else:\n",
    "        raise ValueError('Undefined model!')\n",
    "\n",
    "\n",
    "    # Optimizer\n",
    "    if FLAGS.fastgae:\n",
    "        num_sampled = adj_sampled_sparse.shape[0]\n",
    "        sum_sampled = adj_sampled_sparse.sum()\n",
    "        pos_weight = float(num_sampled * num_sampled - sum_sampled) / sum_sampled\n",
    "        norm = num_sampled * num_sampled / float((num_sampled * num_sampled - sum_sampled) * 2)\n",
    "    else:\n",
    "        pos_weight = float(num_nodes * num_nodes - adj.sum()) / adj.sum()\n",
    "        norm = num_nodes * num_nodes / float((num_nodes * num_nodes - adj.sum()) * 2)\n",
    "\n",
    "    if FLAGS.model in ('gcn_ae', 'linear_ae'):\n",
    "        opt = OptimizerAE(preds = model.reconstructions,\n",
    "                          labels = tf.reshape(tf.sparse_tensor_to_dense(placeholders['adj_orig'], validate_indices = False), [-1]),\n",
    "                          degree_matrix = tf.reshape(tf.sparse_tensor_to_dense(placeholders['degree_matrix'], validate_indices = False), [-1]),\n",
    "                          num_nodes = num_nodes,\n",
    "                          pos_weight = pos_weight,\n",
    "                          norm = norm,\n",
    "                          clusters_distance = model.clusters)\n",
    "    else:\n",
    "        opt = OptimizerVAE(preds = model.reconstructions,\n",
    "                           labels = tf.reshape(tf.sparse_tensor_to_dense(placeholders['adj_orig'], validate_indices = False), [-1]),\n",
    "                           degree_matrix = tf.reshape(tf.sparse_tensor_to_dense(placeholders['degree_matrix'], validate_indices = False), [-1]),\n",
    "                           model = model,\n",
    "                           num_nodes = num_nodes,\n",
    "                           pos_weight = pos_weight,\n",
    "                           norm = norm,\n",
    "                           clusters_distance = model.clusters)\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Done! \\n\")\n",
    "\n",
    "\n",
    "    # Symmetrically normalized \"message passing\" matrices\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Preprocessing on message passing matrices\")\n",
    "    adj_norm = preprocess_graph(adj + FLAGS.lamb*adj_louvain_init)\n",
    "    adj_norm_layer2 = preprocess_graph(adj)\n",
    "    if not FLAGS.fastgae:\n",
    "        adj_label = sparse_to_tuple(adj + sp.eye(num_nodes))\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Done! \\n\")\n",
    "\n",
    "\n",
    "    # Degree matrices\n",
    "    deg_matrix, deg_matrix_init = preprocess_degree(adj, FLAGS.simple)\n",
    "\n",
    "\n",
    "    # Initialize TF session\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Initializing TF session\")\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Done! \\n\")\n",
    "\n",
    "\n",
    "    # Model training\n",
    "    if FLAGS.verbose:\n",
    "        print(\"STEP 2/3 - MODEL TRAINING \\n\")\n",
    "        print(\"Starting training\")\n",
    "\n",
    "    for iter in range(FLAGS.iterations):\n",
    "\n",
    "        # Flag to compute running time for each iteration\n",
    "        t = time.time()\n",
    "\n",
    "        # Construct feed dictionary\n",
    "        feed_dict = construct_feed_dict(adj_norm, adj_norm_layer2, adj_label, features, deg_matrix, placeholders)\n",
    "        if FLAGS.fastgae:\n",
    "            # Update sampled subgraph and sampled Louvain matrix\n",
    "            feed_dict.update({placeholders['sampled_nodes']: sampled_nodes})\n",
    "            # New node sampling\n",
    "            sampled_nodes, adj_label, adj_label_sparse, = node_sampling(adj, node_distribution, FLAGS.nb_node_samples, FLAGS.replace)\n",
    "\n",
    "        # Weights update\n",
    "        outs = sess.run([opt.opt_op, opt.cost, opt.cost_adj, opt.cost_mod],\n",
    "                        feed_dict = feed_dict)\n",
    "\n",
    "        # Compute average loss\n",
    "        avg_cost = outs[1]\n",
    "        if FLAGS.verbose:\n",
    "            # Display information on the iteration\n",
    "            print(\"Iteration:\", '%04d' % (iter + 1), \"Loss:\", \"{:.5f}\".format(avg_cost),\n",
    "                  \"Time:\", \"{:.5f}\".format(time.time() - t))\n",
    "\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Done! \\n\")\n",
    "\n",
    "    # Compute embedding vectors, for evaluation\n",
    "    if FLAGS.verbose:\n",
    "        print(\"STEP 3/3 - MODEL EVALUATION \\n\")\n",
    "        print(\"Computing the final embedding vectors, for evaluation\")\n",
    "    emb = sess.run(model.z_mean, feed_dict = feed_dict)\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Done! \\n\")\n",
    "\n",
    "    # Test model: link prediction (classification edges/non-edges)\n",
    "\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Testing: link prediction\")\n",
    "    # Get ROC and AP scores\n",
    "    roc_score, ap_score = link_prediction(test_edges, test_edges_false, emb)\n",
    "    mean_roc.append(roc_score)\n",
    "    mean_ap.append(ap_score)\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Done! \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bef29d76",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-28T09:08:52.489736Z",
     "start_time": "2022-10-28T09:08:52.476716Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL RESULTS \n",
      "\n",
      "Recall: the selected task was \"Task 2\", i.e., joint community detection and link prediction, on polbooks\n",
      "All scores reported below are computed over the 10 run(s) \n",
      "\n",
      "Link prediction:\n",
      "\n",
      "Mean AUC score:  0.8827995867768594\n",
      "Std of AUC scores:  0.0245440392675709 \n",
      "\n",
      "Mean AP score:  0.8733185214250383\n",
      "Std of AP scores:  0.03662545420162568 \n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Report final results\n",
    "print(\"FINAL RESULTS \\n\")\n",
    "\n",
    "print('Recall: the selected task was \"Task 2\", i.e., joint community detection and link prediction, on', FLAGS.dataset)\n",
    "\n",
    "print(\"All scores reported below are computed over the\", FLAGS.nb_run, \"run(s) \\n\")\n",
    "\n",
    "print(\"Link prediction:\\n\")\n",
    "print(\"Mean AUC score: \", np.mean(mean_roc))\n",
    "print(\"Std of AUC scores: \", np.std(mean_roc), \"\\n\")\n",
    "\n",
    "print(\"Mean AP score: \", np.mean(mean_ap))\n",
    "print(\"Std of AP scores: \", np.std(mean_ap), \"\\n \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553e6eb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf1.15",
   "language": "python",
   "name": "tf1.15"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
