{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67dbcca0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-02T06:28:06.602319Z",
     "start_time": "2022-11-02T06:28:04.382176Z"
    }
   },
   "outputs": [],
   "source": [
    "from modularity_aware_gae.evaluation import community_detection, link_prediction\n",
    "from modularity_aware_gae.input_data import load_data, load_labels\n",
    "from modularity_aware_gae.louvain import louvain_clustering\n",
    "from modularity_aware_gae.model import  GCNModelAE, GCNModelVAE, LinearModelAE, LinearModelVAE\n",
    "from modularity_aware_gae.optimizer import OptimizerAE, OptimizerVAE\n",
    "from modularity_aware_gae.preprocessing import *\n",
    "from modularity_aware_gae.sampling import get_distribution, node_sampling\n",
    "import numpy as np\n",
    "import os\n",
    "import scipy.sparse as sp\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import copy\n",
    "\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel')#添加的，不报错\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14ffd620",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-02T06:28:06.696784Z",
     "start_time": "2022-11-02T06:28:06.682671Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " \n",
      " \n",
      "[MODULARITY-AWARE GRAPH AUTOENCODERS]\n",
      " \n",
      " \n",
      " \n",
      "\n",
      "EXPERIMENTAL SETTING \n",
      "\n",
      "- Graph dataset: karate\n",
      "- Mode name: linear_vae\n",
      "- Number of models to train: 1\n",
      "- Number of training iterations for each model: 400\n",
      "- Learning rate: 0.01\n",
      "- Dropout rate: 0.0\n",
      "- Use of node features in the input layer: False\n",
      "- Dimension of the output layer: 16\n",
      "- lambda: 0.0\n",
      "- beta: 0.0\n",
      "- gamma: 1.0\n",
      "- s: 2\n",
      "- FastGAE: no \n",
      "\n",
      "Final embedding vectors will be evaluated on:\n",
      "- Task 2, i.e., joint community detection and link prediction\n",
      "\n",
      " \n",
      " \n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "# Parameters related to the input data\n",
    "flags.DEFINE_string('dataset', 'karate', 'Graph dataset, among: cora, citeseer, wiki, celegans, email, polbooks, texas, wisconsin')\n",
    "\n",
    "flags.DEFINE_boolean('features', False, 'Whether to include node features')\n",
    "\n",
    "\n",
    "# Parameters related to the Modularity-Aware GAE/VGAE model to train\n",
    "\n",
    "# 1/3 - General parameters associated with GAE/VGAE\n",
    "flags.DEFINE_string('model', 'linear_vae', 'Model to train, among: gcn_ae, gcn_vae, \\\n",
    "                                            linear_ae, linear_vae')\n",
    "flags.DEFINE_float('dropout', 0., 'Dropout rate')\n",
    "flags.DEFINE_integer('iterations', 400, 'Number of iterations in training')\n",
    "flags.DEFINE_float('learning_rate', 0.01, 'Initial learning rate (Adam)')\n",
    "flags.DEFINE_integer('hidden', 32, 'Dimension of the GCN hidden layer')\n",
    "flags.DEFINE_integer('dimension', 16, 'Dimension of the output layer, i.e., \\\n",
    "                                       dimension of the embedding space')\n",
    "\n",
    "# 2/3 - Additional parameters, specific to Modularity-Aware models\n",
    "flags.DEFINE_float('beta', 0.0, 'Beta hyperparameter of Mod.-Aware models')\n",
    "flags.DEFINE_float('lamb', 0.0, 'Lambda hyperparameter of Mod.-Aware models')\n",
    "flags.DEFINE_float('gamma', 1.0, 'Gamma hyperparameter of Mod.-Aware models')\n",
    "flags.DEFINE_integer('s_reg', 2, 's hyperparameter of Mod.-Aware models')\n",
    "\n",
    "# 3/3 - Additional parameters, aiming to improve scalability\n",
    "flags.DEFINE_boolean('fastgae', False, 'Whether to use the FastGAE framework')\n",
    "flags.DEFINE_integer('nb_node_samples', 1000, 'In FastGAE, number of nodes to \\\n",
    "                                               sample at each iteration, i.e., \\\n",
    "                                               size of decoded subgraph')\n",
    "flags.DEFINE_string('measure', 'degree', 'In FastGAE, node importance measure used \\\n",
    "                                          for sampling: degree, core, or uniform')\n",
    "flags.DEFINE_float('alpha', 1.0, 'alpha hyperparameter associated with the FastGAE sampling')\n",
    "flags.DEFINE_boolean('replace', False, 'Sample nodes with or without replacement')\n",
    "flags.DEFINE_boolean('simple', True, 'Use simpler (and faster) modularity in optimizers')\n",
    "\n",
    "\n",
    "# Parameters related to the experimental setup\n",
    "flags.DEFINE_string('task', 'task_2', 'task_1: pure community detection \\\n",
    "                                       task_2: joint link prediction and \\\n",
    "                                               community detection')\n",
    "flags.DEFINE_integer('nb_run', 1, 'Number of model run + test')\n",
    "flags.DEFINE_float('prop_val', 1., 'Proportion of edges in validation set \\\n",
    "                                    for the link prediction task')\n",
    "flags.DEFINE_float('prop_test', 10., 'Proportion of edges in test set \\\n",
    "                                      for the link prediction task')\n",
    "flags.DEFINE_boolean('validation', False, 'Whether to compute validation \\\n",
    "                                           results at each iteration, for \\\n",
    "                                           the link prediction task')\n",
    "flags.DEFINE_boolean('verbose', True, 'Whether to print all comments details')\n",
    "\n",
    "\n",
    "# Introductory message\n",
    "if FLAGS.verbose:\n",
    "    introductory_message()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ce16ae4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-02T06:28:06.791340Z",
     "start_time": "2022-11-02T06:28:06.776223Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADING DATA\n",
      "\n",
      "Loading the karate graph\n",
      "- Number of nodes: 34\n",
      "- Use of node features: False\n",
      "Done! \n",
      " \n",
      " \n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize lists to collect final results\n",
    "mean_ami = []\n",
    "mean_ari = []\n",
    "\n",
    "mean_roc = []\n",
    "mean_ap = []\n",
    "\n",
    "# Load data\n",
    "if FLAGS.verbose:\n",
    "    print(\"LOADING DATA\\n\")\n",
    "    print(\"Loading the\", FLAGS.dataset, \"graph\")\n",
    "adj_init, features_init = load_data(FLAGS.dataset)\n",
    "#labels = load_labels(adj_init.shape[0])\n",
    "\n",
    "adj_use_ori = copy.deepcopy(adj_init)\n",
    "\n",
    "if FLAGS.verbose:\n",
    "    print(\"- Number of nodes:\", adj_init.shape[0])\n",
    "    #print(\"- Number of communities:\", len(np.unique(labels)))\n",
    "    print(\"- Use of node features:\", FLAGS.features)\n",
    "    print(\"Done! \\n \\n \\n \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd19b933",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-02T06:28:08.005882Z",
     "start_time": "2022-11-02T06:28:06.870544Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENTS ON MODEL 1 / 1 \n",
      "\n",
      "STEP 1/3 - PREPROCESSING STEPS \n",
      "\n",
      "Masking some edges from the training graph, for link prediction\n",
      "(validation set: 1.0 % of edges - test set: 10.0 % of edges)\n",
      "Done! \n",
      "\n",
      "Preprocessing node features\n",
      "Done! \n",
      "\n",
      "Running the Louvain algorithm for community detection\n",
      "as a preprocessing step for the encoder\n",
      "Done! Louvain has found 4 communities \n",
      "\n",
      "Setting up the model and the optimizer\n",
      "Done! \n",
      "\n",
      "Preprocessing on message passing matrices\n",
      "Done! \n",
      "\n",
      "Initializing TF session\n",
      "Done! \n",
      "\n",
      "STEP 2/3 - MODEL TRAINING \n",
      "\n",
      "Starting training\n",
      "Iteration: 0001 Loss: 1.62429 Time: 0.09366\n",
      "Iteration: 0002 Loss: 1.54402 Time: 0.00200\n",
      "Iteration: 0003 Loss: 1.46029 Time: 0.00100\n",
      "Iteration: 0004 Loss: 1.91775 Time: 0.00099\n",
      "Iteration: 0005 Loss: 1.61973 Time: 0.00202\n",
      "Iteration: 0006 Loss: 1.85123 Time: 0.00099\n",
      "Iteration: 0007 Loss: 1.55166 Time: 0.00200\n",
      "Iteration: 0008 Loss: 1.66886 Time: 0.00105\n",
      "Iteration: 0009 Loss: 1.59480 Time: 0.00102\n",
      "Iteration: 0010 Loss: 1.60926 Time: 0.00198\n",
      "Iteration: 0011 Loss: 1.65425 Time: 0.00200\n",
      "Iteration: 0012 Loss: 1.21456 Time: 0.00106\n",
      "Iteration: 0013 Loss: 1.33162 Time: 0.00194\n",
      "Iteration: 0014 Loss: 1.22139 Time: 0.00100\n",
      "Iteration: 0015 Loss: 1.44702 Time: 0.00098\n",
      "Iteration: 0016 Loss: 1.40879 Time: 0.00101\n",
      "Iteration: 0017 Loss: 1.45002 Time: 0.00199\n",
      "Iteration: 0018 Loss: 1.24310 Time: 0.00101\n",
      "Iteration: 0019 Loss: 1.22585 Time: 0.00199\n",
      "Iteration: 0020 Loss: 1.67203 Time: 0.00102\n",
      "Iteration: 0021 Loss: 1.33494 Time: 0.00198\n",
      "Iteration: 0022 Loss: 1.34002 Time: 0.00101\n",
      "Iteration: 0023 Loss: 1.32168 Time: 0.00199\n",
      "Iteration: 0024 Loss: 1.30687 Time: 0.00101\n",
      "Iteration: 0025 Loss: 1.42192 Time: 0.00199\n",
      "Iteration: 0026 Loss: 1.30014 Time: 0.00101\n",
      "Iteration: 0027 Loss: 1.12001 Time: 0.00099\n",
      "Iteration: 0028 Loss: 1.30510 Time: 0.00099\n",
      "Iteration: 0029 Loss: 1.33182 Time: 0.00100\n",
      "Iteration: 0030 Loss: 1.22423 Time: 0.00101\n",
      "Iteration: 0031 Loss: 1.44382 Time: 0.00099\n",
      "Iteration: 0032 Loss: 1.21932 Time: 0.00101\n",
      "Iteration: 0033 Loss: 1.30483 Time: 0.00100\n",
      "Iteration: 0034 Loss: 1.22372 Time: 0.00100\n",
      "Iteration: 0035 Loss: 1.01468 Time: 0.00101\n",
      "Iteration: 0036 Loss: 1.41064 Time: 0.00100\n",
      "Iteration: 0037 Loss: 1.17916 Time: 0.00099\n",
      "Iteration: 0038 Loss: 1.10859 Time: 0.00107\n",
      "Iteration: 0039 Loss: 1.17551 Time: 0.00094\n",
      "Iteration: 0040 Loss: 1.13407 Time: 0.00100\n",
      "Iteration: 0041 Loss: 1.03059 Time: 0.00101\n",
      "Iteration: 0042 Loss: 1.25806 Time: 0.00099\n",
      "Iteration: 0043 Loss: 1.41567 Time: 0.00100\n",
      "Iteration: 0044 Loss: 1.16848 Time: 0.00199\n",
      "Iteration: 0045 Loss: 1.25998 Time: 0.00200\n",
      "Iteration: 0046 Loss: 1.10519 Time: 0.00101\n",
      "Iteration: 0047 Loss: 1.16277 Time: 0.00000\n",
      "Iteration: 0048 Loss: 1.11353 Time: 0.00101\n",
      "Iteration: 0049 Loss: 1.04678 Time: 0.00200\n",
      "Iteration: 0050 Loss: 1.08594 Time: 0.00101\n",
      "Iteration: 0051 Loss: 1.11186 Time: 0.00199\n",
      "Iteration: 0052 Loss: 1.14507 Time: 0.00100\n",
      "Iteration: 0053 Loss: 1.06506 Time: 0.00101\n",
      "Iteration: 0054 Loss: 1.09620 Time: 0.00098\n",
      "Iteration: 0055 Loss: 1.08489 Time: 0.00101\n",
      "Iteration: 0056 Loss: 1.02700 Time: 0.00100\n",
      "Iteration: 0057 Loss: 1.11221 Time: 0.00101\n",
      "Iteration: 0058 Loss: 1.14751 Time: 0.00199\n",
      "Iteration: 0059 Loss: 1.00492 Time: 0.00101\n",
      "Iteration: 0060 Loss: 0.95486 Time: 0.00199\n",
      "Iteration: 0061 Loss: 1.03597 Time: 0.00113\n",
      "Iteration: 0062 Loss: 1.01597 Time: 0.00099\n",
      "Iteration: 0063 Loss: 0.97447 Time: 0.00101\n",
      "Iteration: 0064 Loss: 1.01740 Time: 0.00099\n",
      "Iteration: 0065 Loss: 0.98688 Time: 0.00100\n",
      "Iteration: 0066 Loss: 0.97157 Time: 0.00101\n",
      "Iteration: 0067 Loss: 1.03384 Time: 0.00200\n",
      "Iteration: 0068 Loss: 1.02900 Time: 0.00100\n",
      "Iteration: 0069 Loss: 1.15195 Time: 0.00101\n",
      "Iteration: 0070 Loss: 0.95305 Time: 0.00101\n",
      "Iteration: 0071 Loss: 1.00908 Time: 0.00100\n",
      "Iteration: 0072 Loss: 0.87344 Time: 0.00100\n",
      "Iteration: 0073 Loss: 1.03871 Time: 0.00101\n",
      "Iteration: 0074 Loss: 1.05532 Time: 0.00100\n",
      "Iteration: 0075 Loss: 1.08017 Time: 0.00200\n",
      "Iteration: 0076 Loss: 0.98629 Time: 0.00099\n",
      "Iteration: 0077 Loss: 1.11303 Time: 0.00101\n",
      "Iteration: 0078 Loss: 0.85445 Time: 0.00100\n",
      "Iteration: 0079 Loss: 0.88282 Time: 0.00101\n",
      "Iteration: 0080 Loss: 0.88913 Time: 0.00101\n",
      "Iteration: 0081 Loss: 0.89916 Time: 0.00199\n",
      "Iteration: 0082 Loss: 0.95967 Time: 0.00101\n",
      "Iteration: 0083 Loss: 0.99592 Time: 0.00199\n",
      "Iteration: 0084 Loss: 0.93900 Time: 0.00101\n",
      "Iteration: 0085 Loss: 0.86738 Time: 0.00099\n",
      "Iteration: 0086 Loss: 0.95568 Time: 0.00101\n",
      "Iteration: 0087 Loss: 0.87600 Time: 0.00100\n",
      "Iteration: 0088 Loss: 0.86945 Time: 0.00100\n",
      "Iteration: 0089 Loss: 0.92523 Time: 0.00101\n",
      "Iteration: 0090 Loss: 0.92849 Time: 0.00099\n",
      "Iteration: 0091 Loss: 0.92432 Time: 0.00102\n",
      "Iteration: 0092 Loss: 0.97520 Time: 0.00101\n",
      "Iteration: 0093 Loss: 0.92973 Time: 0.00100\n",
      "Iteration: 0094 Loss: 1.05087 Time: 0.00099\n",
      "Iteration: 0095 Loss: 0.85599 Time: 0.00101\n",
      "Iteration: 0096 Loss: 0.87350 Time: 0.00099\n",
      "Iteration: 0097 Loss: 0.90219 Time: 0.00101\n",
      "Iteration: 0098 Loss: 0.80508 Time: 0.00100\n",
      "Iteration: 0099 Loss: 0.91855 Time: 0.00199\n",
      "Iteration: 0100 Loss: 0.90070 Time: 0.00101\n",
      "Iteration: 0101 Loss: 0.88520 Time: 0.00100\n",
      "Iteration: 0102 Loss: 0.89253 Time: 0.00099\n",
      "Iteration: 0103 Loss: 0.91754 Time: 0.00100\n",
      "Iteration: 0104 Loss: 0.97038 Time: 0.00100\n",
      "Iteration: 0105 Loss: 0.86444 Time: 0.00200\n",
      "Iteration: 0106 Loss: 0.91753 Time: 0.00101\n",
      "Iteration: 0107 Loss: 0.89778 Time: 0.00100\n",
      "Iteration: 0108 Loss: 0.88522 Time: 0.00100\n",
      "Iteration: 0109 Loss: 0.84386 Time: 0.00100\n",
      "Iteration: 0110 Loss: 0.88140 Time: 0.00101\n",
      "Iteration: 0111 Loss: 0.81854 Time: 0.00199\n",
      "Iteration: 0112 Loss: 0.90510 Time: 0.00101\n",
      "Iteration: 0113 Loss: 0.87677 Time: 0.00199\n",
      "Iteration: 0114 Loss: 0.84575 Time: 0.00101\n",
      "Iteration: 0115 Loss: 0.78733 Time: 0.00101\n",
      "Iteration: 0116 Loss: 0.82740 Time: 0.00200\n",
      "Iteration: 0117 Loss: 0.80090 Time: 0.00099\n",
      "Iteration: 0118 Loss: 0.90509 Time: 0.00100\n",
      "Iteration: 0119 Loss: 0.78000 Time: 0.00199\n",
      "Iteration: 0120 Loss: 0.95358 Time: 0.00101\n",
      "Iteration: 0121 Loss: 0.79231 Time: 0.00100\n",
      "Iteration: 0122 Loss: 0.86502 Time: 0.00206\n",
      "Iteration: 0123 Loss: 0.90871 Time: 0.00094\n",
      "Iteration: 0124 Loss: 0.80457 Time: 0.00100\n",
      "Iteration: 0125 Loss: 0.91980 Time: 0.00100\n",
      "Iteration: 0126 Loss: 0.80180 Time: 0.00101\n",
      "Iteration: 0127 Loss: 0.75004 Time: 0.00100\n",
      "Iteration: 0128 Loss: 0.79865 Time: 0.00116\n",
      "Iteration: 0129 Loss: 0.75990 Time: 0.00093\n",
      "Iteration: 0130 Loss: 0.79716 Time: 0.00100\n",
      "Iteration: 0131 Loss: 0.78808 Time: 0.00101\n",
      "Iteration: 0132 Loss: 0.89402 Time: 0.00100\n",
      "Iteration: 0133 Loss: 0.78744 Time: 0.00101\n",
      "Iteration: 0134 Loss: 0.75735 Time: 0.00099\n",
      "Iteration: 0135 Loss: 0.81807 Time: 0.00100\n",
      "Iteration: 0136 Loss: 0.85160 Time: 0.00101\n",
      "Iteration: 0137 Loss: 0.85423 Time: 0.00100\n",
      "Iteration: 0138 Loss: 0.85194 Time: 0.00100\n",
      "Iteration: 0139 Loss: 0.85683 Time: 0.00201\n",
      "Iteration: 0140 Loss: 0.90213 Time: 0.00099\n",
      "Iteration: 0141 Loss: 0.75357 Time: 0.00099\n",
      "Iteration: 0142 Loss: 0.78640 Time: 0.00100\n",
      "Iteration: 0143 Loss: 0.83398 Time: 0.00100\n",
      "Iteration: 0144 Loss: 0.74410 Time: 0.00101\n",
      "Iteration: 0145 Loss: 0.84611 Time: 0.00101\n",
      "Iteration: 0146 Loss: 0.83887 Time: 0.00100\n",
      "Iteration: 0147 Loss: 0.76616 Time: 0.00100\n",
      "Iteration: 0148 Loss: 0.85842 Time: 0.00101\n",
      "Iteration: 0149 Loss: 0.84449 Time: 0.00199\n",
      "Iteration: 0150 Loss: 0.78373 Time: 0.00201\n",
      "Iteration: 0151 Loss: 0.83361 Time: 0.00099\n",
      "Iteration: 0152 Loss: 0.85192 Time: 0.00199\n",
      "Iteration: 0153 Loss: 0.76263 Time: 0.00124\n",
      "Iteration: 0154 Loss: 0.78101 Time: 0.00176\n",
      "Iteration: 0155 Loss: 0.77697 Time: 0.00111\n",
      "Iteration: 0156 Loss: 0.82147 Time: 0.00090\n",
      "Iteration: 0157 Loss: 0.76717 Time: 0.00101\n",
      "Iteration: 0158 Loss: 0.80101 Time: 0.00100\n",
      "Iteration: 0159 Loss: 0.79943 Time: 0.00101\n",
      "Iteration: 0160 Loss: 0.82537 Time: 0.00100\n",
      "Iteration: 0161 Loss: 0.75148 Time: 0.00199\n",
      "Iteration: 0162 Loss: 0.77510 Time: 0.00101\n",
      "Iteration: 0163 Loss: 0.79578 Time: 0.00100\n",
      "Iteration: 0164 Loss: 0.78092 Time: 0.00101\n",
      "Iteration: 0165 Loss: 0.73540 Time: 0.00099\n",
      "Iteration: 0166 Loss: 0.88049 Time: 0.00101\n",
      "Iteration: 0167 Loss: 0.80006 Time: 0.00100\n",
      "Iteration: 0168 Loss: 0.76419 Time: 0.00101\n",
      "Iteration: 0169 Loss: 0.78025 Time: 0.00119\n",
      "Iteration: 0170 Loss: 0.76816 Time: 0.00100\n",
      "Iteration: 0171 Loss: 0.84292 Time: 0.00101\n",
      "Iteration: 0172 Loss: 0.75966 Time: 0.00099\n",
      "Iteration: 0173 Loss: 0.83437 Time: 0.00110\n",
      "Iteration: 0174 Loss: 0.79642 Time: 0.00090\n",
      "Iteration: 0175 Loss: 0.79871 Time: 0.00100\n",
      "Iteration: 0176 Loss: 0.78280 Time: 0.00101\n",
      "Iteration: 0177 Loss: 0.79007 Time: 0.00100\n",
      "Iteration: 0178 Loss: 0.77193 Time: 0.00200\n",
      "Iteration: 0179 Loss: 0.76539 Time: 0.00100\n",
      "Iteration: 0180 Loss: 0.79151 Time: 0.00101\n",
      "Iteration: 0181 Loss: 0.80623 Time: 0.00100\n",
      "Iteration: 0182 Loss: 0.78672 Time: 0.00200\n",
      "Iteration: 0183 Loss: 0.76413 Time: 0.00099\n",
      "Iteration: 0184 Loss: 0.80131 Time: 0.00100\n",
      "Iteration: 0185 Loss: 0.72793 Time: 0.00100\n",
      "Iteration: 0186 Loss: 0.77186 Time: 0.00101\n",
      "Iteration: 0187 Loss: 0.75211 Time: 0.00100\n",
      "Iteration: 0188 Loss: 0.79874 Time: 0.00100\n",
      "Iteration: 0189 Loss: 0.73904 Time: 0.00100\n",
      "Iteration: 0190 Loss: 0.75442 Time: 0.00101\n",
      "Iteration: 0191 Loss: 0.76764 Time: 0.00099\n",
      "Iteration: 0192 Loss: 0.75698 Time: 0.00100\n",
      "Iteration: 0193 Loss: 0.77466 Time: 0.00101\n",
      "Iteration: 0194 Loss: 0.78442 Time: 0.00101\n",
      "Iteration: 0195 Loss: 0.77970 Time: 0.00199\n",
      "Iteration: 0196 Loss: 0.73282 Time: 0.00100\n",
      "Iteration: 0197 Loss: 0.75900 Time: 0.00101\n",
      "Iteration: 0198 Loss: 0.73865 Time: 0.00099\n",
      "Iteration: 0199 Loss: 0.73950 Time: 0.00100\n",
      "Iteration: 0200 Loss: 0.78884 Time: 0.00099\n",
      "Iteration: 0201 Loss: 0.81061 Time: 0.00101\n",
      "Iteration: 0202 Loss: 0.73785 Time: 0.00100\n",
      "Iteration: 0203 Loss: 0.72191 Time: 0.00099\n",
      "Iteration: 0204 Loss: 0.79863 Time: 0.00101\n",
      "Iteration: 0205 Loss: 0.79913 Time: 0.00199\n",
      "Iteration: 0206 Loss: 0.80381 Time: 0.00200\n",
      "Iteration: 0207 Loss: 0.82195 Time: 0.00100\n",
      "Iteration: 0208 Loss: 0.74673 Time: 0.00200\n",
      "Iteration: 0209 Loss: 0.72711 Time: 0.00101\n",
      "Iteration: 0210 Loss: 0.80541 Time: 0.00100\n",
      "Iteration: 0211 Loss: 0.76167 Time: 0.00101\n",
      "Iteration: 0212 Loss: 0.76486 Time: 0.00099\n",
      "Iteration: 0213 Loss: 0.70910 Time: 0.00100\n",
      "Iteration: 0214 Loss: 0.82214 Time: 0.00101\n",
      "Iteration: 0215 Loss: 0.76468 Time: 0.00099\n",
      "Iteration: 0216 Loss: 0.78140 Time: 0.00101\n",
      "Iteration: 0217 Loss: 0.71788 Time: 0.00100\n",
      "Iteration: 0218 Loss: 0.78408 Time: 0.00200\n",
      "Iteration: 0219 Loss: 0.75160 Time: 0.00100\n",
      "Iteration: 0220 Loss: 0.81982 Time: 0.00101\n",
      "Iteration: 0221 Loss: 0.79192 Time: 0.00100\n",
      "Iteration: 0222 Loss: 0.76972 Time: 0.00101\n",
      "Iteration: 0223 Loss: 0.74927 Time: 0.00199\n",
      "Iteration: 0224 Loss: 0.74296 Time: 0.00101\n",
      "Iteration: 0225 Loss: 0.73131 Time: 0.00099\n",
      "Iteration: 0226 Loss: 0.82003 Time: 0.00201\n",
      "Iteration: 0227 Loss: 0.75338 Time: 0.00099\n",
      "Iteration: 0228 Loss: 0.76020 Time: 0.00100\n",
      "Iteration: 0229 Loss: 0.80642 Time: 0.00100\n",
      "Iteration: 0230 Loss: 0.80367 Time: 0.00201\n",
      "Iteration: 0231 Loss: 0.70114 Time: 0.00100\n",
      "Iteration: 0232 Loss: 0.75709 Time: 0.00101\n",
      "Iteration: 0233 Loss: 0.74655 Time: 0.00099\n",
      "Iteration: 0234 Loss: 0.77470 Time: 0.00101\n",
      "Iteration: 0235 Loss: 0.78803 Time: 0.00100\n",
      "Iteration: 0236 Loss: 0.73861 Time: 0.00100\n",
      "Iteration: 0237 Loss: 0.73801 Time: 0.00100\n",
      "Iteration: 0238 Loss: 0.78253 Time: 0.00101\n",
      "Iteration: 0239 Loss: 0.77085 Time: 0.00100\n",
      "Iteration: 0240 Loss: 0.76430 Time: 0.00200\n",
      "Iteration: 0241 Loss: 0.85164 Time: 0.00100\n",
      "Iteration: 0242 Loss: 0.79103 Time: 0.00100\n",
      "Iteration: 0243 Loss: 0.73060 Time: 0.00100\n",
      "Iteration: 0244 Loss: 0.74095 Time: 0.00101\n",
      "Iteration: 0245 Loss: 0.73949 Time: 0.00100\n",
      "Iteration: 0246 Loss: 0.74384 Time: 0.00101\n",
      "Iteration: 0247 Loss: 0.81390 Time: 0.00099\n",
      "Iteration: 0248 Loss: 0.76805 Time: 0.00099\n",
      "Iteration: 0249 Loss: 0.77474 Time: 0.00116\n",
      "Iteration: 0250 Loss: 0.79850 Time: 0.00084\n",
      "Iteration: 0251 Loss: 0.72113 Time: 0.00199\n",
      "Iteration: 0252 Loss: 0.78767 Time: 0.00100\n",
      "Iteration: 0253 Loss: 0.74269 Time: 0.00099\n",
      "Iteration: 0254 Loss: 0.73253 Time: 0.00100\n",
      "Iteration: 0255 Loss: 0.73387 Time: 0.00199\n",
      "Iteration: 0256 Loss: 0.74301 Time: 0.00101\n",
      "Iteration: 0257 Loss: 0.73169 Time: 0.00199\n",
      "Iteration: 0258 Loss: 0.76104 Time: 0.00101\n",
      "Iteration: 0259 Loss: 0.70013 Time: 0.00199\n",
      "Iteration: 0260 Loss: 0.75977 Time: 0.00100\n",
      "Iteration: 0261 Loss: 0.72561 Time: 0.00101\n",
      "Iteration: 0262 Loss: 0.79108 Time: 0.00100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0263 Loss: 0.73398 Time: 0.00199\n",
      "Iteration: 0264 Loss: 0.73834 Time: 0.00098\n",
      "Iteration: 0265 Loss: 0.76884 Time: 0.00200\n",
      "Iteration: 0266 Loss: 0.76777 Time: 0.00101\n",
      "Iteration: 0267 Loss: 0.73679 Time: 0.00100\n",
      "Iteration: 0268 Loss: 0.82001 Time: 0.00101\n",
      "Iteration: 0269 Loss: 0.75119 Time: 0.00099\n",
      "Iteration: 0270 Loss: 0.74368 Time: 0.00200\n",
      "Iteration: 0271 Loss: 0.77867 Time: 0.00101\n",
      "Iteration: 0272 Loss: 0.82408 Time: 0.00199\n",
      "Iteration: 0273 Loss: 0.75091 Time: 0.00101\n",
      "Iteration: 0274 Loss: 0.76789 Time: 0.00200\n",
      "Iteration: 0275 Loss: 0.77162 Time: 0.00100\n",
      "Iteration: 0276 Loss: 0.78104 Time: 0.00100\n",
      "Iteration: 0277 Loss: 0.75015 Time: 0.00100\n",
      "Iteration: 0278 Loss: 0.79007 Time: 0.00101\n",
      "Iteration: 0279 Loss: 0.76627 Time: 0.00000\n",
      "Iteration: 0280 Loss: 0.70121 Time: 0.00100\n",
      "Iteration: 0281 Loss: 0.72597 Time: 0.00200\n",
      "Iteration: 0282 Loss: 0.75504 Time: 0.00101\n",
      "Iteration: 0283 Loss: 0.74439 Time: 0.00100\n",
      "Iteration: 0284 Loss: 0.77814 Time: 0.00199\n",
      "Iteration: 0285 Loss: 0.71358 Time: 0.00201\n",
      "Iteration: 0286 Loss: 0.81479 Time: 0.00100\n",
      "Iteration: 0287 Loss: 0.73129 Time: 0.00100\n",
      "Iteration: 0288 Loss: 0.77450 Time: 0.00100\n",
      "Iteration: 0289 Loss: 0.77185 Time: 0.00200\n",
      "Iteration: 0290 Loss: 0.73962 Time: 0.00100\n",
      "Iteration: 0291 Loss: 0.75443 Time: 0.00101\n",
      "Iteration: 0292 Loss: 0.76499 Time: 0.00100\n",
      "Iteration: 0293 Loss: 0.82178 Time: 0.00100\n",
      "Iteration: 0294 Loss: 0.79576 Time: 0.00098\n",
      "Iteration: 0295 Loss: 0.77807 Time: 0.00200\n",
      "Iteration: 0296 Loss: 0.75214 Time: 0.00101\n",
      "Iteration: 0297 Loss: 0.73419 Time: 0.00101\n",
      "Iteration: 0298 Loss: 0.73150 Time: 0.00100\n",
      "Iteration: 0299 Loss: 0.71944 Time: 0.00199\n",
      "Iteration: 0300 Loss: 0.79506 Time: 0.00100\n",
      "Iteration: 0301 Loss: 0.70570 Time: 0.00101\n",
      "Iteration: 0302 Loss: 0.77162 Time: 0.00099\n",
      "Iteration: 0303 Loss: 0.74882 Time: 0.00101\n",
      "Iteration: 0304 Loss: 0.70228 Time: 0.00099\n",
      "Iteration: 0305 Loss: 0.74240 Time: 0.00101\n",
      "Iteration: 0306 Loss: 0.74426 Time: 0.00200\n",
      "Iteration: 0307 Loss: 0.74878 Time: 0.00199\n",
      "Iteration: 0308 Loss: 0.77895 Time: 0.00101\n",
      "Iteration: 0309 Loss: 0.72242 Time: 0.00100\n",
      "Iteration: 0310 Loss: 0.77375 Time: 0.00101\n",
      "Iteration: 0311 Loss: 0.80289 Time: 0.00199\n",
      "Iteration: 0312 Loss: 0.77296 Time: 0.00101\n",
      "Iteration: 0313 Loss: 0.74234 Time: 0.00099\n",
      "Iteration: 0314 Loss: 0.72376 Time: 0.00099\n",
      "Iteration: 0315 Loss: 0.72083 Time: 0.00100\n",
      "Iteration: 0316 Loss: 0.75099 Time: 0.00101\n",
      "Iteration: 0317 Loss: 0.71116 Time: 0.00112\n",
      "Iteration: 0318 Loss: 0.72228 Time: 0.00188\n",
      "Iteration: 0319 Loss: 0.71383 Time: 0.00123\n",
      "Iteration: 0320 Loss: 0.74301 Time: 0.00099\n",
      "Iteration: 0321 Loss: 0.74550 Time: 0.00100\n",
      "Iteration: 0322 Loss: 0.68491 Time: 0.00101\n",
      "Iteration: 0323 Loss: 0.76007 Time: 0.00100\n",
      "Iteration: 0324 Loss: 0.72831 Time: 0.00099\n",
      "Iteration: 0325 Loss: 0.75135 Time: 0.00101\n",
      "Iteration: 0326 Loss: 0.73832 Time: 0.00100\n",
      "Iteration: 0327 Loss: 0.76682 Time: 0.00101\n",
      "Iteration: 0328 Loss: 0.74079 Time: 0.00099\n",
      "Iteration: 0329 Loss: 0.73468 Time: 0.00200\n",
      "Iteration: 0330 Loss: 0.72227 Time: 0.00101\n",
      "Iteration: 0331 Loss: 0.73512 Time: 0.00100\n",
      "Iteration: 0332 Loss: 0.75021 Time: 0.00101\n",
      "Iteration: 0333 Loss: 0.75211 Time: 0.00200\n",
      "Iteration: 0334 Loss: 0.76807 Time: 0.00113\n",
      "Iteration: 0335 Loss: 0.80417 Time: 0.00099\n",
      "Iteration: 0336 Loss: 0.72171 Time: 0.00100\n",
      "Iteration: 0337 Loss: 0.75304 Time: 0.00100\n",
      "Iteration: 0338 Loss: 0.73277 Time: 0.00101\n",
      "Iteration: 0339 Loss: 0.71029 Time: 0.00100\n",
      "Iteration: 0340 Loss: 0.75682 Time: 0.00101\n",
      "Iteration: 0341 Loss: 0.71106 Time: 0.00101\n",
      "Iteration: 0342 Loss: 0.73539 Time: 0.00199\n",
      "Iteration: 0343 Loss: 0.74332 Time: 0.00100\n",
      "Iteration: 0344 Loss: 0.69811 Time: 0.00202\n",
      "Iteration: 0345 Loss: 0.70887 Time: 0.00099\n",
      "Iteration: 0346 Loss: 0.78418 Time: 0.00099\n",
      "Iteration: 0347 Loss: 0.75650 Time: 0.00101\n",
      "Iteration: 0348 Loss: 0.70696 Time: 0.00099\n",
      "Iteration: 0349 Loss: 0.73986 Time: 0.00100\n",
      "Iteration: 0350 Loss: 0.73742 Time: 0.00099\n",
      "Iteration: 0351 Loss: 0.75915 Time: 0.00101\n",
      "Iteration: 0352 Loss: 0.72319 Time: 0.00101\n",
      "Iteration: 0353 Loss: 0.69315 Time: 0.00199\n",
      "Iteration: 0354 Loss: 0.70890 Time: 0.00100\n",
      "Iteration: 0355 Loss: 0.71237 Time: 0.00099\n",
      "Iteration: 0356 Loss: 0.72466 Time: 0.00101\n",
      "Iteration: 0357 Loss: 0.72568 Time: 0.00099\n",
      "Iteration: 0358 Loss: 0.75558 Time: 0.00100\n",
      "Iteration: 0359 Loss: 0.72090 Time: 0.00100\n",
      "Iteration: 0360 Loss: 0.71819 Time: 0.00101\n",
      "Iteration: 0361 Loss: 0.71949 Time: 0.00100\n",
      "Iteration: 0362 Loss: 0.74519 Time: 0.00199\n",
      "Iteration: 0363 Loss: 0.77319 Time: 0.00200\n",
      "Iteration: 0364 Loss: 0.76283 Time: 0.00118\n",
      "Iteration: 0365 Loss: 0.71516 Time: 0.00182\n",
      "Iteration: 0366 Loss: 0.73742 Time: 0.00122\n",
      "Iteration: 0367 Loss: 0.71240 Time: 0.00178\n",
      "Iteration: 0368 Loss: 0.70877 Time: 0.00112\n",
      "Iteration: 0369 Loss: 0.70762 Time: 0.00188\n",
      "Iteration: 0370 Loss: 0.72991 Time: 0.00100\n",
      "Iteration: 0371 Loss: 0.77128 Time: 0.00200\n",
      "Iteration: 0372 Loss: 0.73682 Time: 0.00100\n",
      "Iteration: 0373 Loss: 0.72065 Time: 0.00100\n",
      "Iteration: 0374 Loss: 0.72261 Time: 0.00100\n",
      "Iteration: 0375 Loss: 0.73589 Time: 0.00200\n",
      "Iteration: 0376 Loss: 0.75993 Time: 0.00100\n",
      "Iteration: 0377 Loss: 0.73855 Time: 0.00101\n",
      "Iteration: 0378 Loss: 0.73584 Time: 0.00099\n",
      "Iteration: 0379 Loss: 0.74726 Time: 0.00101\n",
      "Iteration: 0380 Loss: 0.72772 Time: 0.00100\n",
      "Iteration: 0381 Loss: 0.72974 Time: 0.00101\n",
      "Iteration: 0382 Loss: 0.76923 Time: 0.00100\n",
      "Iteration: 0383 Loss: 0.79140 Time: 0.00101\n",
      "Iteration: 0384 Loss: 0.79834 Time: 0.00100\n",
      "Iteration: 0385 Loss: 0.70741 Time: 0.00100\n",
      "Iteration: 0386 Loss: 0.70352 Time: 0.00100\n",
      "Iteration: 0387 Loss: 0.74197 Time: 0.00111\n",
      "Iteration: 0388 Loss: 0.76072 Time: 0.00088\n",
      "Iteration: 0389 Loss: 0.74104 Time: 0.00100\n",
      "Iteration: 0390 Loss: 0.73081 Time: 0.00101\n",
      "Iteration: 0391 Loss: 0.78321 Time: 0.00102\n",
      "Iteration: 0392 Loss: 0.75558 Time: 0.00098\n",
      "Iteration: 0393 Loss: 0.74630 Time: 0.00200\n",
      "Iteration: 0394 Loss: 0.72011 Time: 0.00101\n",
      "Iteration: 0395 Loss: 0.75480 Time: 0.00199\n",
      "Iteration: 0396 Loss: 0.71600 Time: 0.00123\n",
      "Iteration: 0397 Loss: 0.71899 Time: 0.00177\n",
      "Iteration: 0398 Loss: 0.73934 Time: 0.00100\n",
      "Iteration: 0399 Loss: 0.74859 Time: 0.00100\n",
      "Iteration: 0400 Loss: 0.69218 Time: 0.00100\n",
      "Done! \n",
      "\n",
      "STEP 3/3 - MODEL EVALUATION \n",
      "\n",
      "Computing the final embedding vectors, for evaluation\n",
      "Done! \n",
      "\n",
      "Testing: link prediction\n",
      "Done! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We repeat the entire training+test process FLAGS.nb_run times\n",
    "for i in range(FLAGS.nb_run):\n",
    "    if FLAGS.verbose:\n",
    "        print(\"EXPERIMENTS ON MODEL\", i + 1, \"/\", FLAGS.nb_run, \"\\n\")\n",
    "        print(\"STEP 1/3 - PREPROCESSING STEPS \\n\")\n",
    "\n",
    "\n",
    "    # Edge masking for Link Prediction:\n",
    "    if FLAGS.task == 'task_2':\n",
    "        # Compute Train/Validation/Test sets\n",
    "        if FLAGS.verbose:\n",
    "            print(\"Masking some edges from the training graph, for link prediction\")\n",
    "            print(\"(validation set:\", FLAGS.prop_val, \"% of edges - test set:\",\n",
    "                  FLAGS.prop_test, \"% of edges)\")\n",
    "        #adj, val_edges, val_edges_false, test_edges, test_edges_false = mask_test_edges(adj_init, FLAGS.prop_test, FLAGS.prop_val)\n",
    "        adj, test_edges, test_edges_false = mask_test_edges(adj_init, FLAGS.prop_test)\n",
    "        if FLAGS.verbose:\n",
    "            print(\"Done! \\n\")\n",
    "    else:\n",
    "        adj = adj_init\n",
    "\n",
    "    # Compute the number of nodes\n",
    "    num_nodes = adj.shape[0]\n",
    "\n",
    "    # Preprocessing on node features\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Preprocessing node features\")\n",
    "    if FLAGS.features:\n",
    "        features = features_init\n",
    "    else:\n",
    "        # If features are not used, replace feature matrix by identity matrix\n",
    "        features = sp.identity(num_nodes)\n",
    "    features = sparse_to_tuple(features)\n",
    "    num_features = features[2][1]\n",
    "    features_nonzero = features[1].shape[0]\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Done! \\n\")\n",
    "\n",
    "\n",
    "    # Community detection using Louvain, as a preprocessing step\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Running the Louvain algorithm for community detection\")\n",
    "        print(\"as a preprocessing step for the encoder\")\n",
    "    # Get binary community matrix (adj_louvain_init[i,j] = 1 if nodes i and j are\n",
    "    # in the same community) as well as number of communities found by Louvain\n",
    "    adj_louvain_init, nb_communities_louvain, partition = louvain_clustering(adj, FLAGS.s_reg)\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Done! Louvain has found\", nb_communities_louvain, \"communities \\n\")\n",
    "\n",
    "    # FastGAE: node sampling for stochastic subgraph decoding - used when facing scalability issues\n",
    "    if FLAGS.fastgae:\n",
    "        if FLAGS.verbose:\n",
    "            print(\"Preprocessing operations related to the FastGAE method\")\n",
    "        # Node-level p_i degree-based, core-based or uniform distribution\n",
    "        node_distribution = get_distribution(FLAGS.measure, FLAGS.alpha, adj)\n",
    "        # Node sampling for initializations\n",
    "        sampled_nodes, adj_label, adj_sampled_sparse = node_sampling(adj, node_distribution, FLAGS.nb_node_samples, FLAGS.replace)\n",
    "        if FLAGS.verbose:\n",
    "            print(\"Done! \\n\")\n",
    "    else:\n",
    "        sampled_nodes = np.array(range(FLAGS.nb_node_samples))\n",
    "\n",
    "\n",
    "    # Placeholders\n",
    "    if FLAGS.verbose:\n",
    "        print('Setting up the model and the optimizer')\n",
    "    placeholders = {\n",
    "        'features': tf.sparse_placeholder(tf.float32),\n",
    "        'adj': tf.sparse_placeholder(tf.float32),\n",
    "        'adj_layer2': tf.sparse_placeholder(tf.float32), # Only used for 2-layer GCN encoders\n",
    "        'degree_matrix': tf.sparse_placeholder(tf.float32),\n",
    "        'adj_orig': tf.sparse_placeholder(tf.float32),\n",
    "        'dropout': tf.placeholder_with_default(0., shape = ()),\n",
    "        'sampled_nodes': tf.placeholder_with_default(sampled_nodes, shape = [FLAGS.nb_node_samples])\n",
    "    }\n",
    "\n",
    "\n",
    "    # Create model\n",
    "    if FLAGS.model == 'linear_ae':\n",
    "        # Linear Graph Autoencoder\n",
    "        model = LinearModelAE(placeholders, num_features, features_nonzero)\n",
    "    elif FLAGS.model == 'linear_vae':\n",
    "        # Linear Graph Variational Autoencoder\n",
    "        model = LinearModelVAE(placeholders, num_features, num_nodes, features_nonzero)\n",
    "    elif FLAGS.model == 'gcn_ae':\n",
    "        # 2-layer GCN Graph Autoencoder\n",
    "        model = GCNModelAE(placeholders, num_features, features_nonzero)\n",
    "    elif FLAGS.model == 'gcn_vae':\n",
    "        # 2-layer GCN Graph Variational Autoencoder\n",
    "        model = GCNModelVAE(placeholders, num_features, num_nodes, features_nonzero)\n",
    "    else:\n",
    "        raise ValueError('Undefined model!')\n",
    "\n",
    "\n",
    "    # Optimizer\n",
    "    if FLAGS.fastgae:\n",
    "        num_sampled = adj_sampled_sparse.shape[0]\n",
    "        sum_sampled = adj_sampled_sparse.sum()\n",
    "        pos_weight = float(num_sampled * num_sampled - sum_sampled) / sum_sampled\n",
    "        norm = num_sampled * num_sampled / float((num_sampled * num_sampled - sum_sampled) * 2)\n",
    "    else:\n",
    "        pos_weight = float(num_nodes * num_nodes - adj.sum()) / adj.sum()\n",
    "        norm = num_nodes * num_nodes / float((num_nodes * num_nodes - adj.sum()) * 2)\n",
    "\n",
    "    if FLAGS.model in ('gcn_ae', 'linear_ae'):\n",
    "        opt = OptimizerAE(preds = model.reconstructions,\n",
    "                          labels = tf.reshape(tf.sparse_tensor_to_dense(placeholders['adj_orig'], validate_indices = False), [-1]),\n",
    "                          degree_matrix = tf.reshape(tf.sparse_tensor_to_dense(placeholders['degree_matrix'], validate_indices = False), [-1]),\n",
    "                          num_nodes = num_nodes,\n",
    "                          pos_weight = pos_weight,\n",
    "                          norm = norm,\n",
    "                          clusters_distance = model.clusters)\n",
    "    else:\n",
    "        opt = OptimizerVAE(preds = model.reconstructions,\n",
    "                           labels = tf.reshape(tf.sparse_tensor_to_dense(placeholders['adj_orig'], validate_indices = False), [-1]),\n",
    "                           degree_matrix = tf.reshape(tf.sparse_tensor_to_dense(placeholders['degree_matrix'], validate_indices = False), [-1]),\n",
    "                           model = model,\n",
    "                           num_nodes = num_nodes,\n",
    "                           pos_weight = pos_weight,\n",
    "                           norm = norm,\n",
    "                           clusters_distance = model.clusters)\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Done! \\n\")\n",
    "\n",
    "\n",
    "    # Symmetrically normalized \"message passing\" matrices\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Preprocessing on message passing matrices\")\n",
    "    adj_norm = preprocess_graph(adj + FLAGS.lamb*adj_louvain_init)\n",
    "    adj_norm_layer2 = preprocess_graph(adj)\n",
    "    if not FLAGS.fastgae:\n",
    "        adj_label = sparse_to_tuple(adj + sp.eye(num_nodes))\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Done! \\n\")\n",
    "\n",
    "\n",
    "    # Degree matrices\n",
    "    deg_matrix, deg_matrix_init = preprocess_degree(adj, FLAGS.simple)\n",
    "\n",
    "\n",
    "    # Initialize TF session\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Initializing TF session\")\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Done! \\n\")\n",
    "\n",
    "\n",
    "    # Model training\n",
    "    if FLAGS.verbose:\n",
    "        print(\"STEP 2/3 - MODEL TRAINING \\n\")\n",
    "        print(\"Starting training\")\n",
    "\n",
    "    for iter in range(FLAGS.iterations):\n",
    "\n",
    "        # Flag to compute running time for each iteration\n",
    "        t = time.time()\n",
    "\n",
    "        # Construct feed dictionary\n",
    "        feed_dict = construct_feed_dict(adj_norm, adj_norm_layer2, adj_label, features, deg_matrix, placeholders)\n",
    "        if FLAGS.fastgae:\n",
    "            # Update sampled subgraph and sampled Louvain matrix\n",
    "            feed_dict.update({placeholders['sampled_nodes']: sampled_nodes})\n",
    "            # New node sampling\n",
    "            sampled_nodes, adj_label, adj_label_sparse, = node_sampling(adj, node_distribution, FLAGS.nb_node_samples, FLAGS.replace)\n",
    "\n",
    "        # Weights update\n",
    "        outs = sess.run([opt.opt_op, opt.cost, opt.cost_adj, opt.cost_mod],\n",
    "                        feed_dict = feed_dict)\n",
    "\n",
    "        # Compute average loss\n",
    "        avg_cost = outs[1]\n",
    "        if FLAGS.verbose:\n",
    "            # Display information on the iteration\n",
    "            print(\"Iteration:\", '%04d' % (iter + 1), \"Loss:\", \"{:.5f}\".format(avg_cost),\n",
    "                  \"Time:\", \"{:.5f}\".format(time.time() - t))\n",
    "\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Done! \\n\")\n",
    "\n",
    "    # Compute embedding vectors, for evaluation\n",
    "    if FLAGS.verbose:\n",
    "        print(\"STEP 3/3 - MODEL EVALUATION \\n\")\n",
    "        print(\"Computing the final embedding vectors, for evaluation\")\n",
    "    emb = sess.run(model.z_mean, feed_dict = feed_dict)\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Done! \\n\")\n",
    "\n",
    "    # Test model: link prediction (classification edges/non-edges)\n",
    "\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Testing: link prediction\")\n",
    "    # Get ROC and AP scores\n",
    "    roc_score, ap_score = link_prediction(test_edges, test_edges_false, emb)\n",
    "    mean_roc.append(roc_score)\n",
    "    mean_ap.append(ap_score)\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Done! \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bef29d76",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-02T06:28:08.225307Z",
     "start_time": "2022-11-02T06:28:08.209990Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL RESULTS \n",
      "\n",
      "Recall: the selected task was \"Task 2\", i.e., joint community detection and link prediction, on karate\n",
      "All scores reported below are computed over the 1 run(s) \n",
      "\n",
      "Link prediction:\n",
      "\n",
      "Mean AUC score:  0.7755102040816326\n",
      "Std of AUC scores:  0.0 \n",
      "\n",
      "Mean AP score:  0.834065934065934\n",
      "Std of AP scores:  0.0 \n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Report final results\n",
    "print(\"FINAL RESULTS \\n\")\n",
    "\n",
    "print('Recall: the selected task was \"Task 2\", i.e., joint community detection and link prediction, on', FLAGS.dataset)\n",
    "\n",
    "print(\"All scores reported below are computed over the\", FLAGS.nb_run, \"run(s) \\n\")\n",
    "\n",
    "print(\"Link prediction:\\n\")\n",
    "print(\"Mean AUC score: \", np.mean(mean_roc))\n",
    "print(\"Std of AUC scores: \", np.std(mean_roc), \"\\n\")\n",
    "\n",
    "print(\"Mean AP score: \", np.mean(mean_ap))\n",
    "print(\"Std of AP scores: \", np.std(mean_ap), \"\\n \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a001883e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-02T06:30:51.996874Z",
     "start_time": "2022-11-02T06:30:51.937906Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gama :  0.8\n",
      "edges：  182.0\n",
      "ACC：  0.9325259515570934\n",
      "AP：  0.8437958345555882\n",
      "RECALL：  0.8906666666666667\n",
      "F1 SCORE：  0.8648585457096094\n",
      "gama :  0.81\n",
      "edges：  176.0\n",
      "ACC：  0.9342560553633218\n",
      "AP：  0.8493506493506493\n",
      "RECALL：  0.8862564102564102\n",
      "F1 SCORE：  0.8663502494827796\n",
      "gama :  0.82\n",
      "edges：  164.0\n",
      "ACC：  0.9411764705882353\n",
      "AP：  0.8690253737214791\n",
      "RECALL：  0.8848461538461538\n",
      "F1 SCORE：  0.8766817269076305\n",
      "gama :  0.83\n",
      "edges：  156.0\n",
      "ACC：  0.9377162629757786\n",
      "AP：  0.8666153846153846\n",
      "RECALL：  0.8666153846153846\n",
      "F1 SCORE：  0.8666153846153846\n",
      "gama :  0.84\n",
      "edges：  140.0\n",
      "ACC：  0.9411764705882353\n",
      "AP：  0.8864735658042744\n",
      "RECALL：  0.8523846153846153\n",
      "F1 SCORE：  0.8682700557700558\n",
      "gama :  0.85\n",
      "edges：  138.0\n",
      "ACC：  0.9429065743944637\n",
      "AP：  0.8924147945673528\n",
      "RECALL：  0.8533846153846154\n",
      "F1 SCORE：  0.8714022774620255\n",
      "gama :  0.86\n",
      "edges：  132.0\n",
      "ACC：  0.9411764705882353\n",
      "AP：  0.8942057291666667\n",
      "RECALL：  0.8415641025641025\n",
      "F1 SCORE：  0.8651460254721124\n",
      "gama :  0.87\n",
      "edges：  122.0\n",
      "ACC：  0.9359861591695502\n",
      "AP：  0.8919206011985921\n",
      "RECALL：  0.8169230769230769\n",
      "F1 SCORE：  0.8487157176913336\n",
      "gama :  0.88\n",
      "edges：  118.0\n",
      "ACC：  0.9325259515570934\n",
      "AP：  0.887315894320891\n",
      "RECALL：  0.8041025641025641\n",
      "F1 SCORE：  0.8385278253332664\n",
      "gama :  0.89\n",
      "edges：  110.0\n",
      "ACC：  0.9359861591695502\n",
      "AP：  0.9076829480271162\n",
      "RECALL：  0.8006923076923077\n",
      "F1 SCORE：  0.8428181891679345\n",
      "gama :  0.9\n",
      "edges：  94.0\n",
      "ACC：  0.9359861591695502\n",
      "AP：  0.9360700404696077\n",
      "RECALL：  0.779051282051282\n",
      "F1 SCORE：  0.8340562560620757\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "adj_rec = np.dot(emb, emb.T)\n",
    "adj_rec = sigmoid(adj_rec)\n",
    "adj_rec = adj_rec/adj_rec.max()\n",
    "\n",
    "for i in range(adj_rec.shape[0]):\n",
    "    adj_rec[i, i] = 0\n",
    "s = adj_rec.reshape(adj_rec.shape[0]*adj_rec.shape[0])\n",
    "\n",
    "adj_true = adj_use_ori.toarray()\n",
    "true_edges = adj_true.reshape(34*34)\n",
    "\n",
    "for gama in [0.80, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, 0.90]:\n",
    "    print(\"gama : \",gama)\n",
    "    predict_edges = copy.deepcopy(s)\n",
    "    predict_edges[predict_edges > gama] = 1\n",
    "    predict_edges[predict_edges <= gama] = 0\n",
    "    print(\"edges： \",predict_edges.sum())\n",
    "    print(\"ACC： \",accuracy_score(true_edges, predict_edges))\n",
    "    print(\"AP： \",precision_score(true_edges, predict_edges, average='macro'))\n",
    "    print(\"RECALL： \",recall_score(true_edges, predict_edges, average='macro'))\n",
    "    print(\"F1 SCORE： \",f1_score(true_edges, predict_edges, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9cec88c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-02T06:31:40.704045Z",
     "start_time": "2022-11-02T06:31:40.690971Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edges：  164.0\n",
      "ACC：  0.9411764705882353\n",
      "AP：  0.8690253737214791\n",
      "RECALL：  0.8848461538461538\n",
      "F1 SCORE：  0.8766817269076305\n"
     ]
    }
   ],
   "source": [
    "gama = 0.82\n",
    "predict_edges = copy.deepcopy(s)\n",
    "predict_edges[predict_edges > gama] = 1\n",
    "predict_edges[predict_edges <= gama] = 0\n",
    "print(\"edges： \",predict_edges.sum())\n",
    "print(\"ACC： \",accuracy_score(true_edges, predict_edges))\n",
    "print(\"AP： \",precision_score(true_edges, predict_edges, average='macro'))\n",
    "print(\"RECALL： \",recall_score(true_edges, predict_edges, average='macro'))\n",
    "print(\"F1 SCORE： \",f1_score(true_edges, predict_edges, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b5bf98e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-02T06:33:15.945609Z",
     "start_time": "2022-11-02T06:33:15.921825Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "predict_graph = predict_edges.reshape(34, 34)\n",
    "pd.DataFrame(predict_graph).to_csv('LMA_recon_karate.txt', header=None, index=False, sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743bb7e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf1.15",
   "language": "python",
   "name": "tf1.15"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
