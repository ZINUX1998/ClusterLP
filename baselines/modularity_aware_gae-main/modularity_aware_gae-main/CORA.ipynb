{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67dbcca0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-28T06:11:49.710906Z",
     "start_time": "2022-10-28T06:11:47.530895Z"
    }
   },
   "outputs": [],
   "source": [
    "from modularity_aware_gae.evaluation import community_detection, link_prediction\n",
    "from modularity_aware_gae.input_data import load_data, load_labels\n",
    "from modularity_aware_gae.louvain import louvain_clustering\n",
    "from modularity_aware_gae.model import  GCNModelAE, GCNModelVAE, LinearModelAE, LinearModelVAE\n",
    "from modularity_aware_gae.optimizer import OptimizerAE, OptimizerVAE\n",
    "from modularity_aware_gae.preprocessing import *\n",
    "from modularity_aware_gae.sampling import get_distribution, node_sampling\n",
    "import numpy as np\n",
    "import os\n",
    "import scipy.sparse as sp\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel')#添加的，不报错\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14ffd620",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-28T06:11:49.750388Z",
     "start_time": "2022-10-28T06:11:49.737003Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " \n",
      " \n",
      "[MODULARITY-AWARE GRAPH AUTOENCODERS]\n",
      " \n",
      " \n",
      " \n",
      "\n",
      "EXPERIMENTAL SETTING \n",
      "\n",
      "- Graph dataset: cora\n",
      "- Mode name: linear_vae\n",
      "- Number of models to train: 10\n",
      "- Number of training iterations for each model: 200\n",
      "- Learning rate: 0.01\n",
      "- Dropout rate: 0.0\n",
      "- Use of node features in the input layer: False\n",
      "- Dimension of the output layer: 16\n",
      "- lambda: 0.0\n",
      "- beta: 0.0\n",
      "- gamma: 1.0\n",
      "- s: 2\n",
      "- FastGAE: no \n",
      "\n",
      "Final embedding vectors will be evaluated on:\n",
      "- Task 2, i.e., joint community detection and link prediction\n",
      "\n",
      " \n",
      " \n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "# Parameters related to the input data\n",
    "flags.DEFINE_string('dataset', 'cora', 'Graph dataset, among: cora, citeseer, wiki, celegans, email, polbooks, texas, wisconsin')\n",
    "\n",
    "flags.DEFINE_boolean('features', False, 'Whether to include node features')\n",
    "\n",
    "\n",
    "# Parameters related to the Modularity-Aware GAE/VGAE model to train\n",
    "\n",
    "# 1/3 - General parameters associated with GAE/VGAE\n",
    "flags.DEFINE_string('model', 'linear_vae', 'Model to train, among: gcn_ae, gcn_vae, \\\n",
    "                                            linear_ae, linear_vae')\n",
    "flags.DEFINE_float('dropout', 0., 'Dropout rate')\n",
    "flags.DEFINE_integer('iterations', 200, 'Number of iterations in training')\n",
    "flags.DEFINE_float('learning_rate', 0.01, 'Initial learning rate (Adam)')\n",
    "flags.DEFINE_integer('hidden', 32, 'Dimension of the GCN hidden layer')\n",
    "flags.DEFINE_integer('dimension', 16, 'Dimension of the output layer, i.e., \\\n",
    "                                       dimension of the embedding space')\n",
    "\n",
    "# 2/3 - Additional parameters, specific to Modularity-Aware models\n",
    "flags.DEFINE_float('beta', 0.0, 'Beta hyperparameter of Mod.-Aware models')\n",
    "flags.DEFINE_float('lamb', 0.0, 'Lambda hyperparameter of Mod.-Aware models')\n",
    "flags.DEFINE_float('gamma', 1.0, 'Gamma hyperparameter of Mod.-Aware models')\n",
    "flags.DEFINE_integer('s_reg', 2, 's hyperparameter of Mod.-Aware models')\n",
    "\n",
    "# 3/3 - Additional parameters, aiming to improve scalability\n",
    "flags.DEFINE_boolean('fastgae', False, 'Whether to use the FastGAE framework')\n",
    "flags.DEFINE_integer('nb_node_samples', 1000, 'In FastGAE, number of nodes to \\\n",
    "                                               sample at each iteration, i.e., \\\n",
    "                                               size of decoded subgraph')\n",
    "flags.DEFINE_string('measure', 'degree', 'In FastGAE, node importance measure used \\\n",
    "                                          for sampling: degree, core, or uniform')\n",
    "flags.DEFINE_float('alpha', 1.0, 'alpha hyperparameter associated with the FastGAE sampling')\n",
    "flags.DEFINE_boolean('replace', False, 'Sample nodes with or without replacement')\n",
    "flags.DEFINE_boolean('simple', True, 'Use simpler (and faster) modularity in optimizers')\n",
    "\n",
    "\n",
    "# Parameters related to the experimental setup\n",
    "flags.DEFINE_string('task', 'task_2', 'task_1: pure community detection \\\n",
    "                                       task_2: joint link prediction and \\\n",
    "                                               community detection')\n",
    "flags.DEFINE_integer('nb_run', 10, 'Number of model run + test')\n",
    "flags.DEFINE_float('prop_val', 1., 'Proportion of edges in validation set \\\n",
    "                                    for the link prediction task')\n",
    "flags.DEFINE_float('prop_test', 10., 'Proportion of edges in test set \\\n",
    "                                      for the link prediction task')\n",
    "flags.DEFINE_boolean('validation', False, 'Whether to compute validation \\\n",
    "                                           results at each iteration, for \\\n",
    "                                           the link prediction task')\n",
    "flags.DEFINE_boolean('verbose', True, 'Whether to print all comments details')\n",
    "\n",
    "\n",
    "# Introductory message\n",
    "if FLAGS.verbose:\n",
    "    introductory_message()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ce16ae4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-28T06:11:49.827269Z",
     "start_time": "2022-10-28T06:11:49.783345Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADING DATA\n",
      "\n",
      "Loading the cora graph\n",
      "- Number of nodes: 2708\n",
      "- Use of node features: False\n",
      "Done! \n",
      " \n",
      " \n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize lists to collect final results\n",
    "mean_ami = []\n",
    "mean_ari = []\n",
    "\n",
    "mean_roc = []\n",
    "mean_ap = []\n",
    "\n",
    "# Load data\n",
    "if FLAGS.verbose:\n",
    "    print(\"LOADING DATA\\n\")\n",
    "    print(\"Loading the\", FLAGS.dataset, \"graph\")\n",
    "adj_init, features_init = load_data(FLAGS.dataset)\n",
    "#labels = load_labels(adj_init.shape[0])\n",
    "if FLAGS.verbose:\n",
    "    print(\"- Number of nodes:\", adj_init.shape[0])\n",
    "    #print(\"- Number of communities:\", len(np.unique(labels)))\n",
    "    print(\"- Use of node features:\", FLAGS.features)\n",
    "    print(\"Done! \\n \\n \\n \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd19b933",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-28T06:20:02.199246Z",
     "start_time": "2022-10-28T06:11:49.976895Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENTS ON MODEL 1 / 10 \n",
      "\n",
      "STEP 1/3 - PREPROCESSING STEPS \n",
      "\n",
      "Masking some edges from the training graph, for link prediction\n",
      "(validation set: 1.0 % of edges - test set: 10.0 % of edges)\n",
      "Done! \n",
      "\n",
      "Preprocessing node features\n",
      "Done! \n",
      "\n",
      "Running the Louvain algorithm for community detection\n",
      "as a preprocessing step for the encoder\n",
      "Done! Louvain has found 175 communities \n",
      "\n",
      "Setting up the model and the optimizer\n",
      "Done! \n",
      "\n",
      "Preprocessing on message passing matrices\n",
      "Done! \n",
      "\n",
      "Initializing TF session\n",
      "Done! \n",
      "\n",
      "STEP 2/3 - MODEL TRAINING \n",
      "\n",
      "Starting training\n",
      "Iteration: 0001 Loss: 1.72529 Time: 0.56861\n",
      "Iteration: 0002 Loss: 1.71316 Time: 0.24700\n",
      "Iteration: 0003 Loss: 1.72080 Time: 0.23828\n",
      "Iteration: 0004 Loss: 1.65609 Time: 0.23560\n",
      "Iteration: 0005 Loss: 1.69115 Time: 0.24001\n",
      "Iteration: 0006 Loss: 1.59875 Time: 0.24299\n",
      "Iteration: 0007 Loss: 1.64439 Time: 0.24211\n",
      "Iteration: 0008 Loss: 1.62926 Time: 0.23911\n",
      "Iteration: 0009 Loss: 1.56795 Time: 0.23053\n",
      "Iteration: 0010 Loss: 1.55173 Time: 0.23693\n",
      "Iteration: 0011 Loss: 1.57596 Time: 0.22847\n",
      "Iteration: 0012 Loss: 1.53126 Time: 0.23599\n",
      "Iteration: 0013 Loss: 1.54210 Time: 0.23520\n",
      "Iteration: 0014 Loss: 1.49388 Time: 0.23399\n",
      "Iteration: 0015 Loss: 1.48097 Time: 0.23838\n",
      "Iteration: 0016 Loss: 1.45100 Time: 0.24180\n",
      "Iteration: 0017 Loss: 1.43653 Time: 0.23701\n",
      "Iteration: 0018 Loss: 1.45312 Time: 0.24701\n",
      "Iteration: 0019 Loss: 1.43513 Time: 0.24435\n",
      "Iteration: 0020 Loss: 1.42548 Time: 0.23306\n",
      "Iteration: 0021 Loss: 1.39127 Time: 0.24781\n",
      "Iteration: 0022 Loss: 1.38616 Time: 0.23800\n",
      "Iteration: 0023 Loss: 1.38354 Time: 0.24706\n",
      "Iteration: 0024 Loss: 1.36904 Time: 0.23738\n",
      "Iteration: 0025 Loss: 1.36919 Time: 0.23585\n",
      "Iteration: 0026 Loss: 1.33079 Time: 0.23915\n",
      "Iteration: 0027 Loss: 1.30157 Time: 0.22710\n",
      "Iteration: 0028 Loss: 1.31289 Time: 0.23717\n",
      "Iteration: 0029 Loss: 1.30308 Time: 0.24780\n",
      "Iteration: 0030 Loss: 1.28651 Time: 0.23040\n",
      "Iteration: 0031 Loss: 1.26900 Time: 0.23044\n",
      "Iteration: 0032 Loss: 1.24651 Time: 0.23422\n",
      "Iteration: 0033 Loss: 1.27694 Time: 0.23117\n",
      "Iteration: 0034 Loss: 1.25430 Time: 0.23418\n",
      "Iteration: 0035 Loss: 1.23137 Time: 0.24200\n",
      "Iteration: 0036 Loss: 1.22459 Time: 0.23700\n",
      "Iteration: 0037 Loss: 1.22241 Time: 0.23204\n",
      "Iteration: 0038 Loss: 1.22436 Time: 0.23524\n",
      "Iteration: 0039 Loss: 1.20539 Time: 0.23662\n",
      "Iteration: 0040 Loss: 1.19539 Time: 0.22765\n",
      "Iteration: 0041 Loss: 1.17437 Time: 0.23405\n",
      "Iteration: 0042 Loss: 1.18895 Time: 0.22580\n",
      "Iteration: 0043 Loss: 1.16484 Time: 0.23216\n",
      "Iteration: 0044 Loss: 1.15108 Time: 0.24999\n",
      "Iteration: 0045 Loss: 1.16073 Time: 0.23400\n",
      "Iteration: 0046 Loss: 1.15013 Time: 0.22922\n",
      "Iteration: 0047 Loss: 1.13846 Time: 0.23662\n",
      "Iteration: 0048 Loss: 1.13481 Time: 0.23677\n",
      "Iteration: 0049 Loss: 1.12031 Time: 0.23258\n",
      "Iteration: 0050 Loss: 1.10112 Time: 0.23146\n",
      "Iteration: 0051 Loss: 1.10824 Time: 0.23403\n",
      "Iteration: 0052 Loss: 1.10579 Time: 0.23723\n",
      "Iteration: 0053 Loss: 1.09377 Time: 0.23601\n",
      "Iteration: 0054 Loss: 1.11411 Time: 0.23611\n",
      "Iteration: 0055 Loss: 1.09136 Time: 0.23899\n",
      "Iteration: 0056 Loss: 1.08583 Time: 0.23104\n",
      "Iteration: 0057 Loss: 1.06789 Time: 0.23300\n",
      "Iteration: 0058 Loss: 1.08062 Time: 0.23698\n",
      "Iteration: 0059 Loss: 1.05959 Time: 0.22872\n",
      "Iteration: 0060 Loss: 1.05779 Time: 0.23295\n",
      "Iteration: 0061 Loss: 1.05939 Time: 0.23401\n",
      "Iteration: 0062 Loss: 1.03311 Time: 0.23781\n",
      "Iteration: 0063 Loss: 1.02142 Time: 0.23295\n",
      "Iteration: 0064 Loss: 1.01606 Time: 0.23745\n",
      "Iteration: 0065 Loss: 1.03117 Time: 0.23200\n",
      "Iteration: 0066 Loss: 1.02565 Time: 0.24174\n",
      "Iteration: 0067 Loss: 1.02231 Time: 0.23365\n",
      "Iteration: 0068 Loss: 1.02701 Time: 0.23938\n",
      "Iteration: 0069 Loss: 1.00215 Time: 0.23597\n",
      "Iteration: 0070 Loss: 1.00508 Time: 0.23110\n",
      "Iteration: 0071 Loss: 0.98278 Time: 0.23434\n",
      "Iteration: 0072 Loss: 0.98915 Time: 0.23618\n",
      "Iteration: 0073 Loss: 0.98406 Time: 0.23615\n",
      "Iteration: 0074 Loss: 0.97402 Time: 0.23452\n",
      "Iteration: 0075 Loss: 0.97858 Time: 0.22939\n",
      "Iteration: 0076 Loss: 0.98412 Time: 0.22251\n",
      "Iteration: 0077 Loss: 0.97039 Time: 0.23452\n",
      "Iteration: 0078 Loss: 0.96399 Time: 0.23149\n",
      "Iteration: 0079 Loss: 0.95454 Time: 0.24571\n",
      "Iteration: 0080 Loss: 0.94840 Time: 0.23436\n",
      "Iteration: 0081 Loss: 0.95177 Time: 0.25097\n",
      "Iteration: 0082 Loss: 0.92932 Time: 0.23077\n",
      "Iteration: 0083 Loss: 0.94248 Time: 0.22900\n",
      "Iteration: 0084 Loss: 0.91802 Time: 0.24240\n",
      "Iteration: 0085 Loss: 0.92447 Time: 0.23616\n",
      "Iteration: 0086 Loss: 0.91626 Time: 0.23132\n",
      "Iteration: 0087 Loss: 0.91311 Time: 0.23184\n",
      "Iteration: 0088 Loss: 0.89409 Time: 0.23983\n",
      "Iteration: 0089 Loss: 0.90279 Time: 0.23689\n",
      "Iteration: 0090 Loss: 0.90075 Time: 0.23649\n",
      "Iteration: 0091 Loss: 0.89331 Time: 0.23230\n",
      "Iteration: 0092 Loss: 0.88645 Time: 0.23516\n",
      "Iteration: 0093 Loss: 0.88050 Time: 0.22996\n",
      "Iteration: 0094 Loss: 0.88394 Time: 0.23459\n",
      "Iteration: 0095 Loss: 0.87194 Time: 0.23532\n",
      "Iteration: 0096 Loss: 0.86381 Time: 0.23316\n",
      "Iteration: 0097 Loss: 0.85739 Time: 0.23852\n",
      "Iteration: 0098 Loss: 0.86143 Time: 0.23728\n",
      "Iteration: 0099 Loss: 0.85095 Time: 0.22810\n",
      "Iteration: 0100 Loss: 0.85377 Time: 0.23727\n",
      "Iteration: 0101 Loss: 0.83153 Time: 0.22794\n",
      "Iteration: 0102 Loss: 0.82889 Time: 0.23511\n",
      "Iteration: 0103 Loss: 0.83439 Time: 0.23696\n",
      "Iteration: 0104 Loss: 0.82344 Time: 0.23724\n",
      "Iteration: 0105 Loss: 0.81115 Time: 0.23769\n",
      "Iteration: 0106 Loss: 0.82588 Time: 0.23792\n",
      "Iteration: 0107 Loss: 0.80620 Time: 0.23300\n",
      "Iteration: 0108 Loss: 0.80996 Time: 0.23210\n",
      "Iteration: 0109 Loss: 0.80385 Time: 0.23597\n",
      "Iteration: 0110 Loss: 0.78332 Time: 0.22508\n",
      "Iteration: 0111 Loss: 0.79194 Time: 0.24457\n",
      "Iteration: 0112 Loss: 0.79415 Time: 0.24013\n",
      "Iteration: 0113 Loss: 0.77249 Time: 0.23268\n",
      "Iteration: 0114 Loss: 0.76886 Time: 0.23490\n",
      "Iteration: 0115 Loss: 0.77114 Time: 0.23257\n",
      "Iteration: 0116 Loss: 0.76399 Time: 0.23732\n",
      "Iteration: 0117 Loss: 0.76298 Time: 0.23531\n",
      "Iteration: 0118 Loss: 0.75473 Time: 0.22979\n",
      "Iteration: 0119 Loss: 0.75611 Time: 0.22996\n",
      "Iteration: 0120 Loss: 0.74004 Time: 0.23431\n",
      "Iteration: 0121 Loss: 0.73946 Time: 0.23931\n",
      "Iteration: 0122 Loss: 0.73757 Time: 0.23143\n",
      "Iteration: 0123 Loss: 0.72545 Time: 0.23845\n",
      "Iteration: 0124 Loss: 0.72352 Time: 0.23415\n",
      "Iteration: 0125 Loss: 0.72820 Time: 0.23596\n",
      "Iteration: 0126 Loss: 0.72649 Time: 0.23201\n",
      "Iteration: 0127 Loss: 0.71671 Time: 0.23417\n",
      "Iteration: 0128 Loss: 0.71408 Time: 0.24856\n",
      "Iteration: 0129 Loss: 0.71052 Time: 0.23300\n",
      "Iteration: 0130 Loss: 0.71549 Time: 0.23699\n",
      "Iteration: 0131 Loss: 0.70280 Time: 0.24723\n",
      "Iteration: 0132 Loss: 0.70472 Time: 0.23696\n",
      "Iteration: 0133 Loss: 0.68954 Time: 0.23900\n",
      "Iteration: 0134 Loss: 0.69422 Time: 0.23277\n",
      "Iteration: 0135 Loss: 0.68686 Time: 0.22859\n",
      "Iteration: 0136 Loss: 0.67773 Time: 0.23226\n",
      "Iteration: 0137 Loss: 0.67818 Time: 0.23041\n",
      "Iteration: 0138 Loss: 0.67882 Time: 0.23837\n",
      "Iteration: 0139 Loss: 0.67332 Time: 0.23564\n",
      "Iteration: 0140 Loss: 0.67453 Time: 0.23519\n",
      "Iteration: 0141 Loss: 0.67076 Time: 0.23239\n",
      "Iteration: 0142 Loss: 0.66500 Time: 0.24104\n",
      "Iteration: 0143 Loss: 0.65902 Time: 0.24029\n",
      "Iteration: 0144 Loss: 0.65832 Time: 0.22897\n",
      "Iteration: 0145 Loss: 0.65257 Time: 0.23299\n",
      "Iteration: 0146 Loss: 0.65799 Time: 0.23700\n",
      "Iteration: 0147 Loss: 0.64218 Time: 0.22763\n",
      "Iteration: 0148 Loss: 0.64380 Time: 0.23932\n",
      "Iteration: 0149 Loss: 0.64507 Time: 0.23006\n",
      "Iteration: 0150 Loss: 0.63938 Time: 0.23440\n",
      "Iteration: 0151 Loss: 0.63716 Time: 0.23690\n",
      "Iteration: 0152 Loss: 0.63560 Time: 0.22858\n",
      "Iteration: 0153 Loss: 0.62953 Time: 0.23099\n",
      "Iteration: 0154 Loss: 0.63011 Time: 0.23618\n",
      "Iteration: 0155 Loss: 0.62868 Time: 0.23394\n",
      "Iteration: 0156 Loss: 0.62605 Time: 0.24367\n",
      "Iteration: 0157 Loss: 0.61619 Time: 0.23265\n",
      "Iteration: 0158 Loss: 0.62116 Time: 0.23899\n",
      "Iteration: 0159 Loss: 0.61998 Time: 0.23300\n",
      "Iteration: 0160 Loss: 0.62121 Time: 0.23687\n",
      "Iteration: 0161 Loss: 0.61360 Time: 0.22573\n",
      "Iteration: 0162 Loss: 0.61355 Time: 0.23660\n",
      "Iteration: 0163 Loss: 0.61005 Time: 0.23721\n",
      "Iteration: 0164 Loss: 0.60573 Time: 0.23740\n",
      "Iteration: 0165 Loss: 0.60848 Time: 0.23996\n",
      "Iteration: 0166 Loss: 0.61037 Time: 0.23410\n",
      "Iteration: 0167 Loss: 0.60607 Time: 0.23690\n",
      "Iteration: 0168 Loss: 0.60351 Time: 0.24100\n",
      "Iteration: 0169 Loss: 0.60288 Time: 0.22972\n",
      "Iteration: 0170 Loss: 0.60005 Time: 0.23534\n",
      "Iteration: 0171 Loss: 0.59890 Time: 0.23501\n",
      "Iteration: 0172 Loss: 0.59387 Time: 0.23700\n",
      "Iteration: 0173 Loss: 0.59943 Time: 0.23501\n",
      "Iteration: 0174 Loss: 0.59387 Time: 0.24703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0175 Loss: 0.59948 Time: 0.22812\n",
      "Iteration: 0176 Loss: 0.59281 Time: 0.25387\n",
      "Iteration: 0177 Loss: 0.59251 Time: 0.23084\n",
      "Iteration: 0178 Loss: 0.58844 Time: 0.23311\n",
      "Iteration: 0179 Loss: 0.58641 Time: 0.23404\n",
      "Iteration: 0180 Loss: 0.58901 Time: 0.23388\n",
      "Iteration: 0181 Loss: 0.58677 Time: 0.23796\n",
      "Iteration: 0182 Loss: 0.58164 Time: 0.23005\n",
      "Iteration: 0183 Loss: 0.58071 Time: 0.23049\n",
      "Iteration: 0184 Loss: 0.58223 Time: 0.24045\n",
      "Iteration: 0185 Loss: 0.58152 Time: 0.23399\n",
      "Iteration: 0186 Loss: 0.58108 Time: 0.23175\n",
      "Iteration: 0187 Loss: 0.58035 Time: 0.23130\n",
      "Iteration: 0188 Loss: 0.57490 Time: 0.23517\n",
      "Iteration: 0189 Loss: 0.57942 Time: 0.23000\n",
      "Iteration: 0190 Loss: 0.57529 Time: 0.23209\n",
      "Iteration: 0191 Loss: 0.57349 Time: 0.24904\n",
      "Iteration: 0192 Loss: 0.57141 Time: 0.23026\n",
      "Iteration: 0193 Loss: 0.57245 Time: 0.22896\n",
      "Iteration: 0194 Loss: 0.57089 Time: 0.24192\n",
      "Iteration: 0195 Loss: 0.57113 Time: 0.22880\n",
      "Iteration: 0196 Loss: 0.57119 Time: 0.23205\n",
      "Iteration: 0197 Loss: 0.57066 Time: 0.24174\n",
      "Iteration: 0198 Loss: 0.56995 Time: 0.23055\n",
      "Iteration: 0199 Loss: 0.56905 Time: 0.23255\n",
      "Iteration: 0200 Loss: 0.56627 Time: 0.23011\n",
      "Done! \n",
      "\n",
      "STEP 3/3 - MODEL EVALUATION \n",
      "\n",
      "Computing the final embedding vectors, for evaluation\n",
      "Done! \n",
      "\n",
      "Testing: link prediction\n",
      "Done! \n",
      "\n",
      "EXPERIMENTS ON MODEL 2 / 10 \n",
      "\n",
      "STEP 1/3 - PREPROCESSING STEPS \n",
      "\n",
      "Masking some edges from the training graph, for link prediction\n",
      "(validation set: 1.0 % of edges - test set: 10.0 % of edges)\n",
      "Done! \n",
      "\n",
      "Preprocessing node features\n",
      "Done! \n",
      "\n",
      "Running the Louvain algorithm for community detection\n",
      "as a preprocessing step for the encoder\n",
      "Done! Louvain has found 171 communities \n",
      "\n",
      "Setting up the model and the optimizer\n",
      "Done! \n",
      "\n",
      "Preprocessing on message passing matrices\n",
      "Done! \n",
      "\n",
      "Initializing TF session\n",
      "Done! \n",
      "\n",
      "STEP 2/3 - MODEL TRAINING \n",
      "\n",
      "Starting training\n",
      "Iteration: 0001 Loss: 1.73407 Time: 0.46299\n",
      "Iteration: 0002 Loss: 1.68146 Time: 0.22999\n",
      "Iteration: 0003 Loss: 1.67632 Time: 0.23406\n",
      "Iteration: 0004 Loss: 1.68479 Time: 0.24754\n",
      "Iteration: 0005 Loss: 1.66004 Time: 0.22859\n",
      "Iteration: 0006 Loss: 1.60026 Time: 0.23798\n",
      "Iteration: 0007 Loss: 1.61373 Time: 0.23846\n",
      "Iteration: 0008 Loss: 1.59911 Time: 0.22867\n",
      "Iteration: 0009 Loss: 1.59809 Time: 0.23247\n",
      "Iteration: 0010 Loss: 1.55158 Time: 0.24104\n",
      "Iteration: 0011 Loss: 1.54277 Time: 0.23327\n",
      "Iteration: 0012 Loss: 1.55863 Time: 0.25299\n",
      "Iteration: 0013 Loss: 1.55002 Time: 0.21942\n",
      "Iteration: 0014 Loss: 1.49184 Time: 0.22918\n",
      "Iteration: 0015 Loss: 1.45941 Time: 0.23331\n",
      "Iteration: 0016 Loss: 1.45530 Time: 0.23918\n",
      "Iteration: 0017 Loss: 1.42720 Time: 0.23246\n",
      "Iteration: 0018 Loss: 1.41392 Time: 0.23121\n",
      "Iteration: 0019 Loss: 1.41097 Time: 0.22461\n",
      "Iteration: 0020 Loss: 1.39252 Time: 0.23781\n",
      "Iteration: 0021 Loss: 1.38008 Time: 0.23455\n",
      "Iteration: 0022 Loss: 1.40191 Time: 0.23849\n",
      "Iteration: 0023 Loss: 1.38062 Time: 0.23929\n",
      "Iteration: 0024 Loss: 1.34848 Time: 0.23798\n",
      "Iteration: 0025 Loss: 1.32420 Time: 0.23599\n",
      "Iteration: 0026 Loss: 1.32061 Time: 0.23828\n",
      "Iteration: 0027 Loss: 1.33708 Time: 0.25227\n",
      "Iteration: 0028 Loss: 1.29497 Time: 0.24806\n",
      "Iteration: 0029 Loss: 1.29327 Time: 0.23482\n",
      "Iteration: 0030 Loss: 1.27858 Time: 0.23071\n",
      "Iteration: 0031 Loss: 1.28275 Time: 0.23230\n",
      "Iteration: 0032 Loss: 1.26093 Time: 0.24117\n",
      "Iteration: 0033 Loss: 1.26030 Time: 0.23606\n",
      "Iteration: 0034 Loss: 1.24810 Time: 0.23487\n",
      "Iteration: 0035 Loss: 1.22451 Time: 0.23531\n",
      "Iteration: 0036 Loss: 1.23178 Time: 0.24300\n",
      "Iteration: 0037 Loss: 1.22540 Time: 0.23641\n",
      "Iteration: 0038 Loss: 1.21708 Time: 0.24651\n",
      "Iteration: 0039 Loss: 1.19381 Time: 0.22886\n",
      "Iteration: 0040 Loss: 1.19015 Time: 0.24022\n",
      "Iteration: 0041 Loss: 1.17624 Time: 0.23703\n",
      "Iteration: 0042 Loss: 1.19630 Time: 0.24137\n",
      "Iteration: 0043 Loss: 1.15641 Time: 0.23250\n",
      "Iteration: 0044 Loss: 1.16823 Time: 0.23019\n",
      "Iteration: 0045 Loss: 1.15155 Time: 0.24604\n",
      "Iteration: 0046 Loss: 1.14770 Time: 0.23226\n",
      "Iteration: 0047 Loss: 1.15268 Time: 0.23561\n",
      "Iteration: 0048 Loss: 1.13171 Time: 0.23469\n",
      "Iteration: 0049 Loss: 1.10082 Time: 0.23869\n",
      "Iteration: 0050 Loss: 1.12325 Time: 0.22999\n",
      "Iteration: 0051 Loss: 1.13089 Time: 0.24000\n",
      "Iteration: 0052 Loss: 1.09474 Time: 0.22955\n",
      "Iteration: 0053 Loss: 1.10246 Time: 0.24307\n",
      "Iteration: 0054 Loss: 1.09141 Time: 0.23539\n",
      "Iteration: 0055 Loss: 1.07959 Time: 0.22308\n",
      "Iteration: 0056 Loss: 1.06598 Time: 0.22794\n",
      "Iteration: 0057 Loss: 1.07024 Time: 0.24600\n",
      "Iteration: 0058 Loss: 1.05305 Time: 0.23078\n",
      "Iteration: 0059 Loss: 1.06458 Time: 0.23900\n",
      "Iteration: 0060 Loss: 1.06569 Time: 0.23442\n",
      "Iteration: 0061 Loss: 1.05184 Time: 0.23462\n",
      "Iteration: 0062 Loss: 1.04489 Time: 0.23123\n",
      "Iteration: 0063 Loss: 1.01296 Time: 0.23824\n",
      "Iteration: 0064 Loss: 1.03195 Time: 0.22867\n",
      "Iteration: 0065 Loss: 1.01991 Time: 0.23529\n",
      "Iteration: 0066 Loss: 1.01657 Time: 0.23799\n",
      "Iteration: 0067 Loss: 1.02398 Time: 0.24029\n",
      "Iteration: 0068 Loss: 0.99511 Time: 0.23899\n",
      "Iteration: 0069 Loss: 1.00108 Time: 0.22698\n",
      "Iteration: 0070 Loss: 0.99773 Time: 0.24300\n",
      "Iteration: 0071 Loss: 0.99262 Time: 0.23316\n",
      "Iteration: 0072 Loss: 0.97967 Time: 0.22456\n",
      "Iteration: 0073 Loss: 0.98537 Time: 0.23756\n",
      "Iteration: 0074 Loss: 0.97677 Time: 0.23746\n",
      "Iteration: 0075 Loss: 0.97436 Time: 0.23199\n",
      "Iteration: 0076 Loss: 0.96128 Time: 0.23400\n",
      "Iteration: 0077 Loss: 0.95799 Time: 0.23599\n",
      "Iteration: 0078 Loss: 0.94029 Time: 0.23750\n",
      "Iteration: 0079 Loss: 0.94233 Time: 0.23618\n",
      "Iteration: 0080 Loss: 0.94579 Time: 0.23672\n",
      "Iteration: 0081 Loss: 0.93530 Time: 0.22759\n",
      "Iteration: 0082 Loss: 0.93038 Time: 0.23786\n",
      "Iteration: 0083 Loss: 0.92090 Time: 0.23244\n",
      "Iteration: 0084 Loss: 0.91068 Time: 0.23310\n",
      "Iteration: 0085 Loss: 0.91149 Time: 0.23994\n",
      "Iteration: 0086 Loss: 0.90281 Time: 0.23705\n",
      "Iteration: 0087 Loss: 0.90071 Time: 0.23093\n",
      "Iteration: 0088 Loss: 0.88673 Time: 0.23601\n",
      "Iteration: 0089 Loss: 0.89498 Time: 0.22910\n",
      "Iteration: 0090 Loss: 0.89895 Time: 0.22580\n",
      "Iteration: 0091 Loss: 0.88035 Time: 0.24758\n",
      "Iteration: 0092 Loss: 0.87116 Time: 0.23709\n",
      "Iteration: 0093 Loss: 0.86851 Time: 0.23499\n",
      "Iteration: 0094 Loss: 0.86723 Time: 0.23500\n",
      "Iteration: 0095 Loss: 0.86503 Time: 0.23200\n",
      "Iteration: 0096 Loss: 0.84634 Time: 0.24239\n",
      "Iteration: 0097 Loss: 0.84824 Time: 0.23797\n",
      "Iteration: 0098 Loss: 0.85054 Time: 0.23098\n",
      "Iteration: 0099 Loss: 0.83778 Time: 0.23609\n",
      "Iteration: 0100 Loss: 0.82284 Time: 0.23453\n",
      "Iteration: 0101 Loss: 0.82907 Time: 0.23311\n",
      "Iteration: 0102 Loss: 0.81635 Time: 0.24922\n",
      "Iteration: 0103 Loss: 0.81930 Time: 0.23815\n",
      "Iteration: 0104 Loss: 0.81319 Time: 0.23257\n",
      "Iteration: 0105 Loss: 0.80584 Time: 0.23029\n",
      "Iteration: 0106 Loss: 0.80825 Time: 0.22874\n",
      "Iteration: 0107 Loss: 0.78879 Time: 0.23931\n",
      "Iteration: 0108 Loss: 0.77817 Time: 0.23315\n",
      "Iteration: 0109 Loss: 0.78079 Time: 0.23318\n",
      "Iteration: 0110 Loss: 0.79183 Time: 0.23471\n",
      "Iteration: 0111 Loss: 0.76542 Time: 0.23346\n",
      "Iteration: 0112 Loss: 0.78034 Time: 0.23300\n",
      "Iteration: 0113 Loss: 0.76607 Time: 0.24382\n",
      "Iteration: 0114 Loss: 0.75697 Time: 0.23839\n",
      "Iteration: 0115 Loss: 0.75712 Time: 0.22824\n",
      "Iteration: 0116 Loss: 0.76250 Time: 0.22824\n",
      "Iteration: 0117 Loss: 0.75538 Time: 0.23100\n",
      "Iteration: 0118 Loss: 0.74529 Time: 0.23715\n",
      "Iteration: 0119 Loss: 0.74092 Time: 0.24687\n",
      "Iteration: 0120 Loss: 0.74534 Time: 0.23142\n",
      "Iteration: 0121 Loss: 0.73361 Time: 0.24024\n",
      "Iteration: 0122 Loss: 0.73264 Time: 0.23879\n",
      "Iteration: 0123 Loss: 0.72118 Time: 0.23519\n",
      "Iteration: 0124 Loss: 0.71817 Time: 0.22581\n",
      "Iteration: 0125 Loss: 0.71964 Time: 0.24813\n",
      "Iteration: 0126 Loss: 0.71046 Time: 0.23069\n",
      "Iteration: 0127 Loss: 0.70950 Time: 0.23809\n",
      "Iteration: 0128 Loss: 0.70135 Time: 0.23429\n",
      "Iteration: 0129 Loss: 0.70413 Time: 0.23401\n",
      "Iteration: 0130 Loss: 0.69052 Time: 0.23276\n",
      "Iteration: 0131 Loss: 0.69778 Time: 0.23116\n",
      "Iteration: 0132 Loss: 0.68883 Time: 0.23161\n",
      "Iteration: 0133 Loss: 0.68165 Time: 0.23590\n",
      "Iteration: 0134 Loss: 0.68491 Time: 0.23415\n",
      "Iteration: 0135 Loss: 0.68084 Time: 0.23508\n",
      "Iteration: 0136 Loss: 0.68192 Time: 0.23557\n",
      "Iteration: 0137 Loss: 0.67309 Time: 0.23721\n",
      "Iteration: 0138 Loss: 0.66982 Time: 0.23431\n",
      "Iteration: 0139 Loss: 0.66563 Time: 0.23699\n",
      "Iteration: 0140 Loss: 0.66126 Time: 0.23205\n",
      "Iteration: 0141 Loss: 0.65570 Time: 0.23303\n",
      "Iteration: 0142 Loss: 0.65776 Time: 0.24211\n",
      "Iteration: 0143 Loss: 0.65341 Time: 0.23630\n",
      "Iteration: 0144 Loss: 0.64617 Time: 0.24231\n",
      "Iteration: 0145 Loss: 0.65688 Time: 0.22866\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0146 Loss: 0.64796 Time: 0.23249\n",
      "Iteration: 0147 Loss: 0.64524 Time: 0.23532\n",
      "Iteration: 0148 Loss: 0.63940 Time: 0.23692\n",
      "Iteration: 0149 Loss: 0.63937 Time: 0.22368\n",
      "Iteration: 0150 Loss: 0.64346 Time: 0.23774\n",
      "Iteration: 0151 Loss: 0.63764 Time: 0.23745\n",
      "Iteration: 0152 Loss: 0.63636 Time: 0.23834\n",
      "Iteration: 0153 Loss: 0.62850 Time: 0.23779\n",
      "Iteration: 0154 Loss: 0.62983 Time: 0.23395\n",
      "Iteration: 0155 Loss: 0.62752 Time: 0.22899\n",
      "Iteration: 0156 Loss: 0.62009 Time: 0.23614\n",
      "Iteration: 0157 Loss: 0.62015 Time: 0.23207\n",
      "Iteration: 0158 Loss: 0.61901 Time: 0.23769\n",
      "Iteration: 0159 Loss: 0.62227 Time: 0.22918\n",
      "Iteration: 0160 Loss: 0.61906 Time: 0.23142\n",
      "Iteration: 0161 Loss: 0.61212 Time: 0.23933\n",
      "Iteration: 0162 Loss: 0.60972 Time: 0.23455\n",
      "Iteration: 0163 Loss: 0.61150 Time: 0.23259\n",
      "Iteration: 0164 Loss: 0.61259 Time: 0.23115\n",
      "Iteration: 0165 Loss: 0.60565 Time: 0.23732\n",
      "Iteration: 0166 Loss: 0.60694 Time: 0.22744\n",
      "Iteration: 0167 Loss: 0.60136 Time: 0.23647\n",
      "Iteration: 0168 Loss: 0.60712 Time: 0.23841\n",
      "Iteration: 0169 Loss: 0.60357 Time: 0.23153\n",
      "Iteration: 0170 Loss: 0.60126 Time: 0.23692\n",
      "Iteration: 0171 Loss: 0.59582 Time: 0.23398\n",
      "Iteration: 0172 Loss: 0.59724 Time: 0.22873\n",
      "Iteration: 0173 Loss: 0.59762 Time: 0.23856\n",
      "Iteration: 0174 Loss: 0.59260 Time: 0.23145\n",
      "Iteration: 0175 Loss: 0.59476 Time: 0.23560\n",
      "Iteration: 0176 Loss: 0.59315 Time: 0.23858\n",
      "Iteration: 0177 Loss: 0.58911 Time: 0.23668\n",
      "Iteration: 0178 Loss: 0.58878 Time: 0.24299\n",
      "Iteration: 0179 Loss: 0.58593 Time: 0.24648\n",
      "Iteration: 0180 Loss: 0.59050 Time: 0.23276\n",
      "Iteration: 0181 Loss: 0.58632 Time: 0.23541\n",
      "Iteration: 0182 Loss: 0.58393 Time: 0.23838\n",
      "Iteration: 0183 Loss: 0.58314 Time: 0.23571\n",
      "Iteration: 0184 Loss: 0.57956 Time: 0.23082\n",
      "Iteration: 0185 Loss: 0.57925 Time: 0.25037\n",
      "Iteration: 0186 Loss: 0.57941 Time: 0.23362\n",
      "Iteration: 0187 Loss: 0.57653 Time: 0.24600\n",
      "Iteration: 0188 Loss: 0.57564 Time: 0.24152\n",
      "Iteration: 0189 Loss: 0.57894 Time: 0.23798\n",
      "Iteration: 0190 Loss: 0.57185 Time: 0.22999\n",
      "Iteration: 0191 Loss: 0.57308 Time: 0.23165\n",
      "Iteration: 0192 Loss: 0.57423 Time: 0.23569\n",
      "Iteration: 0193 Loss: 0.57256 Time: 0.23245\n",
      "Iteration: 0194 Loss: 0.57372 Time: 0.23985\n",
      "Iteration: 0195 Loss: 0.56759 Time: 0.23799\n",
      "Iteration: 0196 Loss: 0.56959 Time: 0.23401\n",
      "Iteration: 0197 Loss: 0.56766 Time: 0.23101\n",
      "Iteration: 0198 Loss: 0.56409 Time: 0.23534\n",
      "Iteration: 0199 Loss: 0.56840 Time: 0.22794\n",
      "Iteration: 0200 Loss: 0.56540 Time: 0.22978\n",
      "Done! \n",
      "\n",
      "STEP 3/3 - MODEL EVALUATION \n",
      "\n",
      "Computing the final embedding vectors, for evaluation\n",
      "Done! \n",
      "\n",
      "Testing: link prediction\n",
      "Done! \n",
      "\n",
      "EXPERIMENTS ON MODEL 3 / 10 \n",
      "\n",
      "STEP 1/3 - PREPROCESSING STEPS \n",
      "\n",
      "Masking some edges from the training graph, for link prediction\n",
      "(validation set: 1.0 % of edges - test set: 10.0 % of edges)\n",
      "Done! \n",
      "\n",
      "Preprocessing node features\n",
      "Done! \n",
      "\n",
      "Running the Louvain algorithm for community detection\n",
      "as a preprocessing step for the encoder\n",
      "Done! Louvain has found 165 communities \n",
      "\n",
      "Setting up the model and the optimizer\n",
      "Done! \n",
      "\n",
      "Preprocessing on message passing matrices\n",
      "Done! \n",
      "\n",
      "Initializing TF session\n",
      "Done! \n",
      "\n",
      "STEP 2/3 - MODEL TRAINING \n",
      "\n",
      "Starting training\n",
      "Iteration: 0001 Loss: 1.70086 Time: 0.45791\n",
      "Iteration: 0002 Loss: 1.64268 Time: 0.23962\n",
      "Iteration: 0003 Loss: 1.68041 Time: 0.23705\n",
      "Iteration: 0004 Loss: 1.64889 Time: 0.22924\n",
      "Iteration: 0005 Loss: 1.61126 Time: 0.24045\n",
      "Iteration: 0006 Loss: 1.65141 Time: 0.23143\n",
      "Iteration: 0007 Loss: 1.62455 Time: 0.23700\n",
      "Iteration: 0008 Loss: 1.57648 Time: 0.23434\n",
      "Iteration: 0009 Loss: 1.55916 Time: 0.23393\n",
      "Iteration: 0010 Loss: 1.56561 Time: 0.23794\n",
      "Iteration: 0011 Loss: 1.54011 Time: 0.23343\n",
      "Iteration: 0012 Loss: 1.53628 Time: 0.23895\n",
      "Iteration: 0013 Loss: 1.48554 Time: 0.23599\n",
      "Iteration: 0014 Loss: 1.48657 Time: 0.23917\n",
      "Iteration: 0015 Loss: 1.50006 Time: 0.23064\n",
      "Iteration: 0016 Loss: 1.48292 Time: 0.23070\n",
      "Iteration: 0017 Loss: 1.44339 Time: 0.23584\n",
      "Iteration: 0018 Loss: 1.43446 Time: 0.23491\n",
      "Iteration: 0019 Loss: 1.40975 Time: 0.23512\n",
      "Iteration: 0020 Loss: 1.40919 Time: 0.23500\n",
      "Iteration: 0021 Loss: 1.39826 Time: 0.23499\n",
      "Iteration: 0022 Loss: 1.39030 Time: 0.23800\n",
      "Iteration: 0023 Loss: 1.36801 Time: 0.23261\n",
      "Iteration: 0024 Loss: 1.35961 Time: 0.23904\n",
      "Iteration: 0025 Loss: 1.34314 Time: 0.22941\n",
      "Iteration: 0026 Loss: 1.34721 Time: 0.23324\n",
      "Iteration: 0027 Loss: 1.32209 Time: 0.23176\n",
      "Iteration: 0028 Loss: 1.32397 Time: 0.24555\n",
      "Iteration: 0029 Loss: 1.28081 Time: 0.23600\n",
      "Iteration: 0030 Loss: 1.28851 Time: 0.23400\n",
      "Iteration: 0031 Loss: 1.28254 Time: 0.23578\n",
      "Iteration: 0032 Loss: 1.27034 Time: 0.23003\n",
      "Iteration: 0033 Loss: 1.26743 Time: 0.23475\n",
      "Iteration: 0034 Loss: 1.24707 Time: 0.23764\n",
      "Iteration: 0035 Loss: 1.25872 Time: 0.22580\n",
      "Iteration: 0036 Loss: 1.22248 Time: 0.23271\n",
      "Iteration: 0037 Loss: 1.21556 Time: 0.23800\n",
      "Iteration: 0038 Loss: 1.21574 Time: 0.24030\n",
      "Iteration: 0039 Loss: 1.19874 Time: 0.23967\n",
      "Iteration: 0040 Loss: 1.17160 Time: 0.23566\n",
      "Iteration: 0041 Loss: 1.20066 Time: 0.22578\n",
      "Iteration: 0042 Loss: 1.16397 Time: 0.24040\n",
      "Iteration: 0043 Loss: 1.15000 Time: 0.22564\n",
      "Iteration: 0044 Loss: 1.16110 Time: 0.24167\n",
      "Iteration: 0045 Loss: 1.13628 Time: 0.22871\n",
      "Iteration: 0046 Loss: 1.16355 Time: 0.23552\n",
      "Iteration: 0047 Loss: 1.14813 Time: 0.23400\n",
      "Iteration: 0048 Loss: 1.14719 Time: 0.23553\n",
      "Iteration: 0049 Loss: 1.14452 Time: 0.23649\n",
      "Iteration: 0050 Loss: 1.11389 Time: 0.23584\n",
      "Iteration: 0051 Loss: 1.10046 Time: 0.22802\n",
      "Iteration: 0052 Loss: 1.11777 Time: 0.23487\n",
      "Iteration: 0053 Loss: 1.09877 Time: 0.24047\n",
      "Iteration: 0054 Loss: 1.08320 Time: 0.23134\n",
      "Iteration: 0055 Loss: 1.08531 Time: 0.24154\n",
      "Iteration: 0056 Loss: 1.08924 Time: 0.23386\n",
      "Iteration: 0057 Loss: 1.07099 Time: 0.24432\n",
      "Iteration: 0058 Loss: 1.06690 Time: 0.23730\n",
      "Iteration: 0059 Loss: 1.06748 Time: 0.23941\n",
      "Iteration: 0060 Loss: 1.05712 Time: 0.24326\n",
      "Iteration: 0061 Loss: 1.04711 Time: 0.23998\n",
      "Iteration: 0062 Loss: 1.03766 Time: 0.24800\n",
      "Iteration: 0063 Loss: 1.03398 Time: 0.25400\n",
      "Iteration: 0064 Loss: 1.03394 Time: 0.24876\n",
      "Iteration: 0065 Loss: 1.02116 Time: 0.24341\n",
      "Iteration: 0066 Loss: 1.02353 Time: 0.25638\n",
      "Iteration: 0067 Loss: 1.01872 Time: 0.24681\n",
      "Iteration: 0068 Loss: 1.00460 Time: 0.23661\n",
      "Iteration: 0069 Loss: 1.01118 Time: 0.25449\n",
      "Iteration: 0070 Loss: 1.01053 Time: 0.26059\n",
      "Iteration: 0071 Loss: 0.98889 Time: 0.24501\n",
      "Iteration: 0072 Loss: 0.99227 Time: 0.23624\n",
      "Iteration: 0073 Loss: 0.97972 Time: 0.23436\n",
      "Iteration: 0074 Loss: 0.97824 Time: 0.23743\n",
      "Iteration: 0075 Loss: 0.97167 Time: 0.24003\n",
      "Iteration: 0076 Loss: 0.96739 Time: 0.22776\n",
      "Iteration: 0077 Loss: 0.94573 Time: 0.23212\n",
      "Iteration: 0078 Loss: 0.94366 Time: 0.23265\n",
      "Iteration: 0079 Loss: 0.94034 Time: 0.22972\n",
      "Iteration: 0080 Loss: 0.94248 Time: 0.23326\n",
      "Iteration: 0081 Loss: 0.93321 Time: 0.23371\n",
      "Iteration: 0082 Loss: 0.93608 Time: 0.23415\n",
      "Iteration: 0083 Loss: 0.92526 Time: 0.23764\n",
      "Iteration: 0084 Loss: 0.90847 Time: 0.22881\n",
      "Iteration: 0085 Loss: 0.90809 Time: 0.23008\n",
      "Iteration: 0086 Loss: 0.90492 Time: 0.23559\n",
      "Iteration: 0087 Loss: 0.89713 Time: 0.23871\n",
      "Iteration: 0088 Loss: 0.88960 Time: 0.23788\n",
      "Iteration: 0089 Loss: 0.88274 Time: 0.23600\n",
      "Iteration: 0090 Loss: 0.88919 Time: 0.23582\n",
      "Iteration: 0091 Loss: 0.88235 Time: 0.24200\n",
      "Iteration: 0092 Loss: 0.87631 Time: 0.23293\n",
      "Iteration: 0093 Loss: 0.87162 Time: 0.22354\n",
      "Iteration: 0094 Loss: 0.87581 Time: 0.24451\n",
      "Iteration: 0095 Loss: 0.86197 Time: 0.23526\n",
      "Iteration: 0096 Loss: 0.85600 Time: 0.23100\n",
      "Iteration: 0097 Loss: 0.84876 Time: 0.23474\n",
      "Iteration: 0098 Loss: 0.85576 Time: 0.23811\n",
      "Iteration: 0099 Loss: 0.84277 Time: 0.24112\n",
      "Iteration: 0100 Loss: 0.83720 Time: 0.23397\n",
      "Iteration: 0101 Loss: 0.81768 Time: 0.22711\n",
      "Iteration: 0102 Loss: 0.82662 Time: 0.23344\n",
      "Iteration: 0103 Loss: 0.81982 Time: 0.24569\n",
      "Iteration: 0104 Loss: 0.80960 Time: 0.23545\n",
      "Iteration: 0105 Loss: 0.81814 Time: 0.23634\n",
      "Iteration: 0106 Loss: 0.80195 Time: 0.23933\n",
      "Iteration: 0107 Loss: 0.79829 Time: 0.23064\n",
      "Iteration: 0108 Loss: 0.79178 Time: 0.23989\n",
      "Iteration: 0109 Loss: 0.79353 Time: 0.23900\n",
      "Iteration: 0110 Loss: 0.79640 Time: 0.22361\n",
      "Iteration: 0111 Loss: 0.77760 Time: 0.24535\n",
      "Iteration: 0112 Loss: 0.77503 Time: 0.23061\n",
      "Iteration: 0113 Loss: 0.75999 Time: 0.22803\n",
      "Iteration: 0114 Loss: 0.76861 Time: 0.24009\n",
      "Iteration: 0115 Loss: 0.75883 Time: 0.23300\n",
      "Iteration: 0116 Loss: 0.75141 Time: 0.23433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0117 Loss: 0.75658 Time: 0.24301\n",
      "Iteration: 0118 Loss: 0.74887 Time: 0.23641\n",
      "Iteration: 0119 Loss: 0.73830 Time: 0.22597\n",
      "Iteration: 0120 Loss: 0.73931 Time: 0.22894\n",
      "Iteration: 0121 Loss: 0.73170 Time: 0.23999\n",
      "Iteration: 0122 Loss: 0.72929 Time: 0.23378\n",
      "Iteration: 0123 Loss: 0.72690 Time: 0.23200\n",
      "Iteration: 0124 Loss: 0.72706 Time: 0.24317\n",
      "Iteration: 0125 Loss: 0.71988 Time: 0.23416\n",
      "Iteration: 0126 Loss: 0.71168 Time: 0.23020\n",
      "Iteration: 0127 Loss: 0.71883 Time: 0.23735\n",
      "Iteration: 0128 Loss: 0.70414 Time: 0.22933\n",
      "Iteration: 0129 Loss: 0.70737 Time: 0.22942\n",
      "Iteration: 0130 Loss: 0.69450 Time: 0.24700\n",
      "Iteration: 0131 Loss: 0.69482 Time: 0.22701\n",
      "Iteration: 0132 Loss: 0.68979 Time: 0.23819\n",
      "Iteration: 0133 Loss: 0.68636 Time: 0.23500\n",
      "Iteration: 0134 Loss: 0.68303 Time: 0.23201\n",
      "Iteration: 0135 Loss: 0.68838 Time: 0.23806\n",
      "Iteration: 0136 Loss: 0.67195 Time: 0.22977\n",
      "Iteration: 0137 Loss: 0.67050 Time: 0.23546\n",
      "Iteration: 0138 Loss: 0.67196 Time: 0.23155\n",
      "Iteration: 0139 Loss: 0.67077 Time: 0.23123\n",
      "Iteration: 0140 Loss: 0.66419 Time: 0.23200\n",
      "Iteration: 0141 Loss: 0.65937 Time: 0.23567\n",
      "Iteration: 0142 Loss: 0.64678 Time: 0.23616\n",
      "Iteration: 0143 Loss: 0.65750 Time: 0.23778\n",
      "Iteration: 0144 Loss: 0.65206 Time: 0.22701\n",
      "Iteration: 0145 Loss: 0.64638 Time: 0.23647\n",
      "Iteration: 0146 Loss: 0.64417 Time: 0.23305\n",
      "Iteration: 0147 Loss: 0.64624 Time: 0.23899\n",
      "Iteration: 0148 Loss: 0.63772 Time: 0.23050\n",
      "Iteration: 0149 Loss: 0.63592 Time: 0.23524\n",
      "Iteration: 0150 Loss: 0.64124 Time: 0.24018\n",
      "Iteration: 0151 Loss: 0.64009 Time: 0.23900\n",
      "Iteration: 0152 Loss: 0.63971 Time: 0.23578\n",
      "Iteration: 0153 Loss: 0.62650 Time: 0.22869\n",
      "Iteration: 0154 Loss: 0.62991 Time: 0.23276\n",
      "Iteration: 0155 Loss: 0.62640 Time: 0.23917\n",
      "Iteration: 0156 Loss: 0.62376 Time: 0.23798\n",
      "Iteration: 0157 Loss: 0.62241 Time: 0.23663\n",
      "Iteration: 0158 Loss: 0.61778 Time: 0.23616\n",
      "Iteration: 0159 Loss: 0.62159 Time: 0.23137\n",
      "Iteration: 0160 Loss: 0.61772 Time: 0.23237\n",
      "Iteration: 0161 Loss: 0.60990 Time: 0.22799\n",
      "Iteration: 0162 Loss: 0.61565 Time: 0.23833\n",
      "Iteration: 0163 Loss: 0.60996 Time: 0.23407\n",
      "Iteration: 0164 Loss: 0.60484 Time: 0.23877\n",
      "Iteration: 0165 Loss: 0.60363 Time: 0.22832\n",
      "Iteration: 0166 Loss: 0.60333 Time: 0.23774\n",
      "Iteration: 0167 Loss: 0.60146 Time: 0.23487\n",
      "Iteration: 0168 Loss: 0.60117 Time: 0.23023\n",
      "Iteration: 0169 Loss: 0.60022 Time: 0.24538\n",
      "Iteration: 0170 Loss: 0.59800 Time: 0.22681\n",
      "Iteration: 0171 Loss: 0.60016 Time: 0.23578\n",
      "Iteration: 0172 Loss: 0.60020 Time: 0.23924\n",
      "Iteration: 0173 Loss: 0.59255 Time: 0.24434\n",
      "Iteration: 0174 Loss: 0.58940 Time: 0.22922\n",
      "Iteration: 0175 Loss: 0.59347 Time: 0.23846\n",
      "Iteration: 0176 Loss: 0.58938 Time: 0.23446\n",
      "Iteration: 0177 Loss: 0.58917 Time: 0.24346\n",
      "Iteration: 0178 Loss: 0.58856 Time: 0.22479\n",
      "Iteration: 0179 Loss: 0.59114 Time: 0.24461\n",
      "Iteration: 0180 Loss: 0.59008 Time: 0.23229\n",
      "Iteration: 0181 Loss: 0.58430 Time: 0.23280\n",
      "Iteration: 0182 Loss: 0.58133 Time: 0.23790\n",
      "Iteration: 0183 Loss: 0.58231 Time: 0.22854\n",
      "Iteration: 0184 Loss: 0.57869 Time: 0.23136\n",
      "Iteration: 0185 Loss: 0.57883 Time: 0.23657\n",
      "Iteration: 0186 Loss: 0.57870 Time: 0.23372\n",
      "Iteration: 0187 Loss: 0.57762 Time: 0.23026\n",
      "Iteration: 0188 Loss: 0.57546 Time: 0.23689\n",
      "Iteration: 0189 Loss: 0.57702 Time: 0.23706\n",
      "Iteration: 0190 Loss: 0.57403 Time: 0.23631\n",
      "Iteration: 0191 Loss: 0.57677 Time: 0.23035\n",
      "Iteration: 0192 Loss: 0.56980 Time: 0.23627\n",
      "Iteration: 0193 Loss: 0.57215 Time: 0.23400\n",
      "Iteration: 0194 Loss: 0.56930 Time: 0.23277\n",
      "Iteration: 0195 Loss: 0.56888 Time: 0.23221\n",
      "Iteration: 0196 Loss: 0.56783 Time: 0.22932\n",
      "Iteration: 0197 Loss: 0.56696 Time: 0.23710\n",
      "Iteration: 0198 Loss: 0.56640 Time: 0.23632\n",
      "Iteration: 0199 Loss: 0.56818 Time: 0.23707\n",
      "Iteration: 0200 Loss: 0.56478 Time: 0.23264\n",
      "Done! \n",
      "\n",
      "STEP 3/3 - MODEL EVALUATION \n",
      "\n",
      "Computing the final embedding vectors, for evaluation\n",
      "Done! \n",
      "\n",
      "Testing: link prediction\n",
      "Done! \n",
      "\n",
      "EXPERIMENTS ON MODEL 4 / 10 \n",
      "\n",
      "STEP 1/3 - PREPROCESSING STEPS \n",
      "\n",
      "Masking some edges from the training graph, for link prediction\n",
      "(validation set: 1.0 % of edges - test set: 10.0 % of edges)\n",
      "Done! \n",
      "\n",
      "Preprocessing node features\n",
      "Done! \n",
      "\n",
      "Running the Louvain algorithm for community detection\n",
      "as a preprocessing step for the encoder\n",
      "Done! Louvain has found 158 communities \n",
      "\n",
      "Setting up the model and the optimizer\n",
      "Done! \n",
      "\n",
      "Preprocessing on message passing matrices\n",
      "Done! \n",
      "\n",
      "Initializing TF session\n",
      "Done! \n",
      "\n",
      "STEP 2/3 - MODEL TRAINING \n",
      "\n",
      "Starting training\n",
      "Iteration: 0001 Loss: 1.71735 Time: 0.48289\n",
      "Iteration: 0002 Loss: 1.69774 Time: 0.23364\n",
      "Iteration: 0003 Loss: 1.67690 Time: 0.23458\n",
      "Iteration: 0004 Loss: 1.69458 Time: 0.24065\n",
      "Iteration: 0005 Loss: 1.67241 Time: 0.23557\n",
      "Iteration: 0006 Loss: 1.62996 Time: 0.23052\n",
      "Iteration: 0007 Loss: 1.57653 Time: 0.23828\n",
      "Iteration: 0008 Loss: 1.57924 Time: 0.24141\n",
      "Iteration: 0009 Loss: 1.58716 Time: 0.23541\n",
      "Iteration: 0010 Loss: 1.56925 Time: 0.24123\n",
      "Iteration: 0011 Loss: 1.56047 Time: 0.22868\n",
      "Iteration: 0012 Loss: 1.52218 Time: 0.23668\n",
      "Iteration: 0013 Loss: 1.53579 Time: 0.22562\n",
      "Iteration: 0014 Loss: 1.49839 Time: 0.22869\n",
      "Iteration: 0015 Loss: 1.48483 Time: 0.24374\n",
      "Iteration: 0016 Loss: 1.47477 Time: 0.24006\n",
      "Iteration: 0017 Loss: 1.45315 Time: 0.23655\n",
      "Iteration: 0018 Loss: 1.41891 Time: 0.23342\n",
      "Iteration: 0019 Loss: 1.42836 Time: 0.23197\n",
      "Iteration: 0020 Loss: 1.41382 Time: 0.22538\n",
      "Iteration: 0021 Loss: 1.39276 Time: 0.23801\n",
      "Iteration: 0022 Loss: 1.35556 Time: 0.22807\n",
      "Iteration: 0023 Loss: 1.37961 Time: 0.24258\n",
      "Iteration: 0024 Loss: 1.33103 Time: 0.23762\n",
      "Iteration: 0025 Loss: 1.32861 Time: 0.24011\n",
      "Iteration: 0026 Loss: 1.31494 Time: 0.23400\n",
      "Iteration: 0027 Loss: 1.30739 Time: 0.23403\n",
      "Iteration: 0028 Loss: 1.29736 Time: 0.22523\n",
      "Iteration: 0029 Loss: 1.29326 Time: 0.24074\n",
      "Iteration: 0030 Loss: 1.28830 Time: 0.22553\n",
      "Iteration: 0031 Loss: 1.28346 Time: 0.23737\n",
      "Iteration: 0032 Loss: 1.26798 Time: 0.23655\n",
      "Iteration: 0033 Loss: 1.26527 Time: 0.24068\n",
      "Iteration: 0034 Loss: 1.24539 Time: 0.23243\n",
      "Iteration: 0035 Loss: 1.26431 Time: 0.23555\n",
      "Iteration: 0036 Loss: 1.22541 Time: 0.23711\n",
      "Iteration: 0037 Loss: 1.22111 Time: 0.23086\n",
      "Iteration: 0038 Loss: 1.21401 Time: 0.22341\n",
      "Iteration: 0039 Loss: 1.22092 Time: 0.24104\n",
      "Iteration: 0040 Loss: 1.19578 Time: 0.23200\n",
      "Iteration: 0041 Loss: 1.18529 Time: 0.23317\n",
      "Iteration: 0042 Loss: 1.17809 Time: 0.23613\n",
      "Iteration: 0043 Loss: 1.17269 Time: 0.24035\n",
      "Iteration: 0044 Loss: 1.14521 Time: 0.23000\n",
      "Iteration: 0045 Loss: 1.16928 Time: 0.24407\n",
      "Iteration: 0046 Loss: 1.14544 Time: 0.22684\n",
      "Iteration: 0047 Loss: 1.14376 Time: 0.24412\n",
      "Iteration: 0048 Loss: 1.12784 Time: 0.23401\n",
      "Iteration: 0049 Loss: 1.10766 Time: 0.23908\n",
      "Iteration: 0050 Loss: 1.12133 Time: 0.23600\n",
      "Iteration: 0051 Loss: 1.10434 Time: 0.23400\n",
      "Iteration: 0052 Loss: 1.10071 Time: 0.22841\n",
      "Iteration: 0053 Loss: 1.10237 Time: 0.24096\n",
      "Iteration: 0054 Loss: 1.08966 Time: 0.23277\n",
      "Iteration: 0055 Loss: 1.08744 Time: 0.23967\n",
      "Iteration: 0056 Loss: 1.07906 Time: 0.23299\n",
      "Iteration: 0057 Loss: 1.06761 Time: 0.23100\n",
      "Iteration: 0058 Loss: 1.07258 Time: 0.23335\n",
      "Iteration: 0059 Loss: 1.04383 Time: 0.23761\n",
      "Iteration: 0060 Loss: 1.06320 Time: 0.22928\n",
      "Iteration: 0061 Loss: 1.04710 Time: 0.23716\n",
      "Iteration: 0062 Loss: 1.03449 Time: 0.23614\n",
      "Iteration: 0063 Loss: 1.03935 Time: 0.23795\n",
      "Iteration: 0064 Loss: 1.02542 Time: 0.22977\n",
      "Iteration: 0065 Loss: 1.01463 Time: 0.22565\n",
      "Iteration: 0066 Loss: 1.00696 Time: 0.24534\n",
      "Iteration: 0067 Loss: 1.02948 Time: 0.25283\n",
      "Iteration: 0068 Loss: 0.98649 Time: 0.23595\n",
      "Iteration: 0069 Loss: 1.00323 Time: 0.23063\n",
      "Iteration: 0070 Loss: 1.00989 Time: 0.24525\n",
      "Iteration: 0071 Loss: 1.01779 Time: 0.23141\n",
      "Iteration: 0072 Loss: 0.99392 Time: 0.23130\n",
      "Iteration: 0073 Loss: 0.96888 Time: 0.23200\n",
      "Iteration: 0074 Loss: 0.98261 Time: 0.23934\n",
      "Iteration: 0075 Loss: 0.97586 Time: 0.22799\n",
      "Iteration: 0076 Loss: 0.96370 Time: 0.24325\n",
      "Iteration: 0077 Loss: 0.96057 Time: 0.23144\n",
      "Iteration: 0078 Loss: 0.95175 Time: 0.25572\n",
      "Iteration: 0079 Loss: 0.93991 Time: 0.23972\n",
      "Iteration: 0080 Loss: 0.94892 Time: 0.22866\n",
      "Iteration: 0081 Loss: 0.94438 Time: 0.23306\n",
      "Iteration: 0082 Loss: 0.93477 Time: 0.23158\n",
      "Iteration: 0083 Loss: 0.92205 Time: 0.23500\n",
      "Iteration: 0084 Loss: 0.93425 Time: 0.23461\n",
      "Iteration: 0085 Loss: 0.91243 Time: 0.23039\n",
      "Iteration: 0086 Loss: 0.91952 Time: 0.23824\n",
      "Iteration: 0087 Loss: 0.90011 Time: 0.23709\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0088 Loss: 0.89455 Time: 0.24339\n",
      "Iteration: 0089 Loss: 0.88935 Time: 0.22461\n",
      "Iteration: 0090 Loss: 0.88858 Time: 0.23908\n",
      "Iteration: 0091 Loss: 0.88259 Time: 0.22849\n",
      "Iteration: 0092 Loss: 0.87800 Time: 0.22925\n",
      "Iteration: 0093 Loss: 0.87312 Time: 0.23801\n",
      "Iteration: 0094 Loss: 0.87185 Time: 0.23323\n",
      "Iteration: 0095 Loss: 0.86786 Time: 0.23812\n",
      "Iteration: 0096 Loss: 0.86086 Time: 0.23892\n",
      "Iteration: 0097 Loss: 0.85666 Time: 0.23168\n",
      "Iteration: 0098 Loss: 0.85026 Time: 0.23289\n",
      "Iteration: 0099 Loss: 0.83347 Time: 0.23884\n",
      "Iteration: 0100 Loss: 0.84060 Time: 0.22853\n",
      "Iteration: 0101 Loss: 0.82630 Time: 0.22474\n",
      "Iteration: 0102 Loss: 0.83082 Time: 0.24337\n",
      "Iteration: 0103 Loss: 0.81803 Time: 0.23300\n",
      "Iteration: 0104 Loss: 0.80645 Time: 0.23300\n",
      "Iteration: 0105 Loss: 0.81088 Time: 0.23072\n",
      "Iteration: 0106 Loss: 0.79897 Time: 0.23548\n",
      "Iteration: 0107 Loss: 0.80739 Time: 0.23038\n",
      "Iteration: 0108 Loss: 0.80464 Time: 0.24648\n",
      "Iteration: 0109 Loss: 0.77412 Time: 0.22929\n",
      "Iteration: 0110 Loss: 0.78505 Time: 0.23756\n",
      "Iteration: 0111 Loss: 0.78062 Time: 0.25100\n",
      "Iteration: 0112 Loss: 0.78550 Time: 0.23427\n",
      "Iteration: 0113 Loss: 0.77539 Time: 0.23334\n",
      "Iteration: 0114 Loss: 0.76632 Time: 0.23447\n",
      "Iteration: 0115 Loss: 0.76069 Time: 0.23407\n",
      "Iteration: 0116 Loss: 0.75887 Time: 0.23644\n",
      "Iteration: 0117 Loss: 0.75617 Time: 0.23401\n",
      "Iteration: 0118 Loss: 0.75286 Time: 0.23653\n",
      "Iteration: 0119 Loss: 0.74374 Time: 0.23700\n",
      "Iteration: 0120 Loss: 0.73771 Time: 0.22800\n",
      "Iteration: 0121 Loss: 0.73725 Time: 0.23816\n",
      "Iteration: 0122 Loss: 0.73483 Time: 0.23767\n",
      "Iteration: 0123 Loss: 0.71236 Time: 0.23644\n",
      "Iteration: 0124 Loss: 0.72486 Time: 0.23299\n",
      "Iteration: 0125 Loss: 0.72183 Time: 0.23461\n",
      "Iteration: 0126 Loss: 0.71325 Time: 0.23551\n",
      "Iteration: 0127 Loss: 0.71159 Time: 0.22634\n",
      "Iteration: 0128 Loss: 0.70145 Time: 0.24053\n",
      "Iteration: 0129 Loss: 0.70687 Time: 0.23940\n",
      "Iteration: 0130 Loss: 0.69825 Time: 0.23497\n",
      "Iteration: 0131 Loss: 0.69620 Time: 0.22559\n",
      "Iteration: 0132 Loss: 0.68960 Time: 0.23594\n",
      "Iteration: 0133 Loss: 0.68230 Time: 0.23336\n",
      "Iteration: 0134 Loss: 0.67919 Time: 0.23074\n",
      "Iteration: 0135 Loss: 0.68165 Time: 0.23028\n",
      "Iteration: 0136 Loss: 0.68120 Time: 0.24099\n",
      "Iteration: 0137 Loss: 0.66856 Time: 0.22904\n",
      "Iteration: 0138 Loss: 0.66792 Time: 0.24500\n",
      "Iteration: 0139 Loss: 0.67044 Time: 0.23069\n",
      "Iteration: 0140 Loss: 0.66120 Time: 0.23502\n",
      "Iteration: 0141 Loss: 0.65643 Time: 0.23291\n",
      "Iteration: 0142 Loss: 0.65465 Time: 0.23528\n",
      "Iteration: 0143 Loss: 0.65837 Time: 0.23047\n",
      "Iteration: 0144 Loss: 0.65110 Time: 0.23855\n",
      "Iteration: 0145 Loss: 0.65233 Time: 0.23289\n",
      "Iteration: 0146 Loss: 0.64826 Time: 0.24136\n",
      "Iteration: 0147 Loss: 0.64246 Time: 0.23725\n",
      "Iteration: 0148 Loss: 0.64041 Time: 0.23519\n",
      "Iteration: 0149 Loss: 0.63422 Time: 0.23000\n",
      "Iteration: 0150 Loss: 0.63947 Time: 0.23348\n",
      "Iteration: 0151 Loss: 0.63636 Time: 0.23612\n",
      "Iteration: 0152 Loss: 0.63292 Time: 0.23553\n",
      "Iteration: 0153 Loss: 0.63006 Time: 0.23300\n",
      "Iteration: 0154 Loss: 0.62976 Time: 0.23587\n",
      "Iteration: 0155 Loss: 0.62540 Time: 0.23612\n",
      "Iteration: 0156 Loss: 0.61905 Time: 0.23488\n",
      "Iteration: 0157 Loss: 0.61749 Time: 0.22796\n",
      "Iteration: 0158 Loss: 0.61775 Time: 0.23673\n",
      "Iteration: 0159 Loss: 0.61558 Time: 0.24038\n",
      "Iteration: 0160 Loss: 0.61750 Time: 0.23089\n",
      "Iteration: 0161 Loss: 0.61142 Time: 0.23799\n",
      "Iteration: 0162 Loss: 0.60993 Time: 0.23480\n",
      "Iteration: 0163 Loss: 0.60485 Time: 0.23826\n",
      "Iteration: 0164 Loss: 0.61471 Time: 0.23210\n",
      "Iteration: 0165 Loss: 0.60531 Time: 0.23203\n",
      "Iteration: 0166 Loss: 0.60314 Time: 0.22658\n",
      "Iteration: 0167 Loss: 0.59972 Time: 0.24338\n",
      "Iteration: 0168 Loss: 0.60066 Time: 0.25291\n",
      "Iteration: 0169 Loss: 0.59931 Time: 0.23137\n",
      "Iteration: 0170 Loss: 0.60027 Time: 0.24225\n",
      "Iteration: 0171 Loss: 0.59505 Time: 0.23120\n",
      "Iteration: 0172 Loss: 0.59617 Time: 0.24616\n",
      "Iteration: 0173 Loss: 0.59393 Time: 0.23811\n",
      "Iteration: 0174 Loss: 0.59504 Time: 0.23227\n",
      "Iteration: 0175 Loss: 0.59430 Time: 0.24499\n",
      "Iteration: 0176 Loss: 0.59338 Time: 0.23031\n",
      "Iteration: 0177 Loss: 0.58486 Time: 0.23434\n",
      "Iteration: 0178 Loss: 0.59384 Time: 0.24090\n",
      "Iteration: 0179 Loss: 0.58716 Time: 0.22816\n",
      "Iteration: 0180 Loss: 0.58381 Time: 0.22720\n",
      "Iteration: 0181 Loss: 0.58475 Time: 0.24901\n",
      "Iteration: 0182 Loss: 0.58487 Time: 0.23095\n",
      "Iteration: 0183 Loss: 0.58170 Time: 0.24166\n",
      "Iteration: 0184 Loss: 0.57988 Time: 0.23230\n",
      "Iteration: 0185 Loss: 0.58081 Time: 0.23617\n",
      "Iteration: 0186 Loss: 0.58617 Time: 0.23350\n",
      "Iteration: 0187 Loss: 0.57447 Time: 0.23201\n",
      "Iteration: 0188 Loss: 0.57432 Time: 0.24358\n",
      "Iteration: 0189 Loss: 0.57542 Time: 0.23888\n",
      "Iteration: 0190 Loss: 0.57200 Time: 0.24395\n",
      "Iteration: 0191 Loss: 0.57335 Time: 0.23332\n",
      "Iteration: 0192 Loss: 0.57185 Time: 0.23565\n",
      "Iteration: 0193 Loss: 0.56908 Time: 0.23827\n",
      "Iteration: 0194 Loss: 0.57146 Time: 0.23585\n",
      "Iteration: 0195 Loss: 0.56676 Time: 0.23220\n",
      "Iteration: 0196 Loss: 0.56598 Time: 0.23545\n",
      "Iteration: 0197 Loss: 0.56849 Time: 0.23500\n",
      "Iteration: 0198 Loss: 0.56474 Time: 0.24186\n",
      "Iteration: 0199 Loss: 0.57011 Time: 0.22277\n",
      "Iteration: 0200 Loss: 0.56408 Time: 0.24169\n",
      "Done! \n",
      "\n",
      "STEP 3/3 - MODEL EVALUATION \n",
      "\n",
      "Computing the final embedding vectors, for evaluation\n",
      "Done! \n",
      "\n",
      "Testing: link prediction\n",
      "Done! \n",
      "\n",
      "EXPERIMENTS ON MODEL 5 / 10 \n",
      "\n",
      "STEP 1/3 - PREPROCESSING STEPS \n",
      "\n",
      "Masking some edges from the training graph, for link prediction\n",
      "(validation set: 1.0 % of edges - test set: 10.0 % of edges)\n",
      "Done! \n",
      "\n",
      "Preprocessing node features\n",
      "Done! \n",
      "\n",
      "Running the Louvain algorithm for community detection\n",
      "as a preprocessing step for the encoder\n",
      "Done! Louvain has found 151 communities \n",
      "\n",
      "Setting up the model and the optimizer\n",
      "Done! \n",
      "\n",
      "Preprocessing on message passing matrices\n",
      "Done! \n",
      "\n",
      "Initializing TF session\n",
      "Done! \n",
      "\n",
      "STEP 2/3 - MODEL TRAINING \n",
      "\n",
      "Starting training\n",
      "Iteration: 0001 Loss: 1.73958 Time: 0.49911\n",
      "Iteration: 0002 Loss: 1.71110 Time: 0.22718\n",
      "Iteration: 0003 Loss: 1.67333 Time: 0.23569\n",
      "Iteration: 0004 Loss: 1.64213 Time: 0.24201\n",
      "Iteration: 0005 Loss: 1.66989 Time: 0.24599\n",
      "Iteration: 0006 Loss: 1.61716 Time: 0.22949\n",
      "Iteration: 0007 Loss: 1.60582 Time: 0.22760\n",
      "Iteration: 0008 Loss: 1.59005 Time: 0.23548\n",
      "Iteration: 0009 Loss: 1.56962 Time: 0.23095\n",
      "Iteration: 0010 Loss: 1.55927 Time: 0.23793\n",
      "Iteration: 0011 Loss: 1.54932 Time: 0.23324\n",
      "Iteration: 0012 Loss: 1.52439 Time: 0.22979\n",
      "Iteration: 0013 Loss: 1.50920 Time: 0.23228\n",
      "Iteration: 0014 Loss: 1.47237 Time: 0.23000\n",
      "Iteration: 0015 Loss: 1.46620 Time: 0.23514\n",
      "Iteration: 0016 Loss: 1.44469 Time: 0.22721\n",
      "Iteration: 0017 Loss: 1.45217 Time: 0.23944\n",
      "Iteration: 0018 Loss: 1.41101 Time: 0.23899\n",
      "Iteration: 0019 Loss: 1.40794 Time: 0.23273\n",
      "Iteration: 0020 Loss: 1.42223 Time: 0.24810\n",
      "Iteration: 0021 Loss: 1.37494 Time: 0.23963\n",
      "Iteration: 0022 Loss: 1.38472 Time: 0.23555\n",
      "Iteration: 0023 Loss: 1.37608 Time: 0.23800\n",
      "Iteration: 0024 Loss: 1.36857 Time: 0.22804\n",
      "Iteration: 0025 Loss: 1.33776 Time: 0.23851\n",
      "Iteration: 0026 Loss: 1.34314 Time: 0.23634\n",
      "Iteration: 0027 Loss: 1.33078 Time: 0.23499\n",
      "Iteration: 0028 Loss: 1.31761 Time: 0.23307\n",
      "Iteration: 0029 Loss: 1.28887 Time: 0.23108\n",
      "Iteration: 0030 Loss: 1.28043 Time: 0.23960\n",
      "Iteration: 0031 Loss: 1.27678 Time: 0.23804\n",
      "Iteration: 0032 Loss: 1.27354 Time: 0.22974\n",
      "Iteration: 0033 Loss: 1.23494 Time: 0.23509\n",
      "Iteration: 0034 Loss: 1.23989 Time: 0.23499\n",
      "Iteration: 0035 Loss: 1.22776 Time: 0.23267\n",
      "Iteration: 0036 Loss: 1.22895 Time: 0.24600\n",
      "Iteration: 0037 Loss: 1.21925 Time: 0.23121\n",
      "Iteration: 0038 Loss: 1.20974 Time: 0.24041\n",
      "Iteration: 0039 Loss: 1.21118 Time: 0.23500\n",
      "Iteration: 0040 Loss: 1.21194 Time: 0.22997\n",
      "Iteration: 0041 Loss: 1.17902 Time: 0.23898\n",
      "Iteration: 0042 Loss: 1.16430 Time: 0.23474\n",
      "Iteration: 0043 Loss: 1.17900 Time: 0.23246\n",
      "Iteration: 0044 Loss: 1.16586 Time: 0.23612\n",
      "Iteration: 0045 Loss: 1.14375 Time: 0.23548\n",
      "Iteration: 0046 Loss: 1.14432 Time: 0.23700\n",
      "Iteration: 0047 Loss: 1.15100 Time: 0.23365\n",
      "Iteration: 0048 Loss: 1.11002 Time: 0.23501\n",
      "Iteration: 0049 Loss: 1.13005 Time: 0.23438\n",
      "Iteration: 0050 Loss: 1.12248 Time: 0.23631\n",
      "Iteration: 0051 Loss: 1.10656 Time: 0.24120\n",
      "Iteration: 0052 Loss: 1.09897 Time: 0.23402\n",
      "Iteration: 0053 Loss: 1.09560 Time: 0.23824\n",
      "Iteration: 0054 Loss: 1.10962 Time: 0.23343\n",
      "Iteration: 0055 Loss: 1.09305 Time: 0.23623\n",
      "Iteration: 0056 Loss: 1.06553 Time: 0.23487\n",
      "Iteration: 0057 Loss: 1.06901 Time: 0.23100\n",
      "Iteration: 0058 Loss: 1.06277 Time: 0.23538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0059 Loss: 1.06256 Time: 0.23478\n",
      "Iteration: 0060 Loss: 1.04314 Time: 0.23633\n",
      "Iteration: 0061 Loss: 1.04701 Time: 0.23389\n",
      "Iteration: 0062 Loss: 1.04858 Time: 0.23457\n",
      "Iteration: 0063 Loss: 1.03146 Time: 0.23289\n",
      "Iteration: 0064 Loss: 1.02420 Time: 0.23500\n",
      "Iteration: 0065 Loss: 1.02833 Time: 0.23404\n",
      "Iteration: 0066 Loss: 1.00581 Time: 0.22804\n",
      "Iteration: 0067 Loss: 1.00438 Time: 0.22916\n",
      "Iteration: 0068 Loss: 1.00571 Time: 0.23898\n",
      "Iteration: 0069 Loss: 0.99981 Time: 0.23037\n",
      "Iteration: 0070 Loss: 0.98956 Time: 0.24994\n",
      "Iteration: 0071 Loss: 0.99209 Time: 0.23417\n",
      "Iteration: 0072 Loss: 0.98656 Time: 0.22800\n",
      "Iteration: 0073 Loss: 0.98822 Time: 0.23624\n",
      "Iteration: 0074 Loss: 0.97143 Time: 0.23373\n",
      "Iteration: 0075 Loss: 0.98353 Time: 0.23445\n",
      "Iteration: 0076 Loss: 0.96403 Time: 0.23086\n",
      "Iteration: 0077 Loss: 0.95512 Time: 0.22948\n",
      "Iteration: 0078 Loss: 0.95361 Time: 0.24288\n",
      "Iteration: 0079 Loss: 0.94125 Time: 0.23800\n",
      "Iteration: 0080 Loss: 0.93283 Time: 0.23825\n",
      "Iteration: 0081 Loss: 0.93599 Time: 0.23199\n",
      "Iteration: 0082 Loss: 0.93491 Time: 0.23746\n",
      "Iteration: 0083 Loss: 0.92691 Time: 0.24280\n",
      "Iteration: 0084 Loss: 0.90912 Time: 0.22050\n",
      "Iteration: 0085 Loss: 0.91667 Time: 0.23200\n",
      "Iteration: 0086 Loss: 0.91050 Time: 0.23450\n",
      "Iteration: 0087 Loss: 0.90665 Time: 0.23323\n",
      "Iteration: 0088 Loss: 0.89045 Time: 0.23134\n",
      "Iteration: 0089 Loss: 0.89571 Time: 0.23877\n",
      "Iteration: 0090 Loss: 0.88739 Time: 0.22999\n",
      "Iteration: 0091 Loss: 0.87832 Time: 0.24601\n",
      "Iteration: 0092 Loss: 0.87933 Time: 0.23510\n",
      "Iteration: 0093 Loss: 0.87042 Time: 0.23765\n",
      "Iteration: 0094 Loss: 0.86708 Time: 0.22943\n",
      "Iteration: 0095 Loss: 0.86325 Time: 0.23053\n",
      "Iteration: 0096 Loss: 0.85381 Time: 0.24671\n",
      "Iteration: 0097 Loss: 0.84046 Time: 0.23247\n",
      "Iteration: 0098 Loss: 0.84322 Time: 0.23979\n",
      "Iteration: 0099 Loss: 0.83717 Time: 0.23842\n",
      "Iteration: 0100 Loss: 0.83502 Time: 0.23396\n",
      "Iteration: 0101 Loss: 0.82637 Time: 0.22655\n",
      "Iteration: 0102 Loss: 0.82519 Time: 0.24506\n",
      "Iteration: 0103 Loss: 0.82863 Time: 0.23981\n",
      "Iteration: 0104 Loss: 0.80438 Time: 0.23200\n",
      "Iteration: 0105 Loss: 0.80818 Time: 0.23924\n",
      "Iteration: 0106 Loss: 0.80353 Time: 0.23211\n",
      "Iteration: 0107 Loss: 0.79242 Time: 0.22899\n",
      "Iteration: 0108 Loss: 0.79097 Time: 0.23671\n",
      "Iteration: 0109 Loss: 0.78176 Time: 0.22851\n",
      "Iteration: 0110 Loss: 0.78765 Time: 0.23699\n",
      "Iteration: 0111 Loss: 0.77600 Time: 0.23616\n",
      "Iteration: 0112 Loss: 0.76624 Time: 0.23143\n",
      "Iteration: 0113 Loss: 0.77598 Time: 0.23058\n",
      "Iteration: 0114 Loss: 0.76306 Time: 0.24689\n",
      "Iteration: 0115 Loss: 0.75527 Time: 0.23000\n",
      "Iteration: 0116 Loss: 0.75361 Time: 0.23137\n",
      "Iteration: 0117 Loss: 0.74751 Time: 0.24464\n",
      "Iteration: 0118 Loss: 0.74993 Time: 0.23023\n",
      "Iteration: 0119 Loss: 0.73671 Time: 0.22895\n",
      "Iteration: 0120 Loss: 0.73780 Time: 0.23592\n",
      "Iteration: 0121 Loss: 0.73798 Time: 0.23261\n",
      "Iteration: 0122 Loss: 0.72966 Time: 0.23019\n",
      "Iteration: 0123 Loss: 0.72081 Time: 0.24615\n",
      "Iteration: 0124 Loss: 0.71788 Time: 0.23218\n",
      "Iteration: 0125 Loss: 0.71674 Time: 0.23561\n",
      "Iteration: 0126 Loss: 0.70605 Time: 0.22952\n",
      "Iteration: 0127 Loss: 0.70323 Time: 0.23594\n",
      "Iteration: 0128 Loss: 0.70935 Time: 0.24550\n",
      "Iteration: 0129 Loss: 0.69408 Time: 0.23650\n",
      "Iteration: 0130 Loss: 0.68626 Time: 0.23696\n",
      "Iteration: 0131 Loss: 0.69432 Time: 0.23814\n",
      "Iteration: 0132 Loss: 0.68502 Time: 0.23299\n",
      "Iteration: 0133 Loss: 0.68086 Time: 0.24462\n",
      "Iteration: 0134 Loss: 0.68759 Time: 0.23339\n",
      "Iteration: 0135 Loss: 0.67567 Time: 0.22780\n",
      "Iteration: 0136 Loss: 0.67131 Time: 0.23476\n",
      "Iteration: 0137 Loss: 0.67324 Time: 0.23734\n",
      "Iteration: 0138 Loss: 0.66968 Time: 0.23538\n",
      "Iteration: 0139 Loss: 0.66454 Time: 0.22761\n",
      "Iteration: 0140 Loss: 0.65749 Time: 0.22799\n",
      "Iteration: 0141 Loss: 0.65616 Time: 0.23911\n",
      "Iteration: 0142 Loss: 0.65336 Time: 0.23699\n",
      "Iteration: 0143 Loss: 0.64999 Time: 0.23412\n",
      "Iteration: 0144 Loss: 0.64959 Time: 0.23362\n",
      "Iteration: 0145 Loss: 0.64669 Time: 0.23436\n",
      "Iteration: 0146 Loss: 0.64171 Time: 0.23659\n",
      "Iteration: 0147 Loss: 0.64108 Time: 0.22798\n",
      "Iteration: 0148 Loss: 0.64325 Time: 0.23401\n",
      "Iteration: 0149 Loss: 0.63701 Time: 0.23838\n",
      "Iteration: 0150 Loss: 0.63640 Time: 0.24053\n",
      "Iteration: 0151 Loss: 0.64111 Time: 0.23763\n",
      "Iteration: 0152 Loss: 0.63018 Time: 0.23160\n",
      "Iteration: 0153 Loss: 0.62595 Time: 0.22897\n",
      "Iteration: 0154 Loss: 0.61516 Time: 0.24317\n",
      "Iteration: 0155 Loss: 0.62345 Time: 0.23474\n",
      "Iteration: 0156 Loss: 0.61621 Time: 0.23393\n",
      "Iteration: 0157 Loss: 0.61871 Time: 0.23870\n",
      "Iteration: 0158 Loss: 0.61480 Time: 0.23945\n",
      "Iteration: 0159 Loss: 0.61460 Time: 0.23994\n",
      "Iteration: 0160 Loss: 0.61611 Time: 0.23047\n",
      "Iteration: 0161 Loss: 0.61467 Time: 0.22661\n",
      "Iteration: 0162 Loss: 0.61103 Time: 0.24030\n",
      "Iteration: 0163 Loss: 0.60832 Time: 0.23818\n",
      "Iteration: 0164 Loss: 0.60794 Time: 0.23033\n",
      "Iteration: 0165 Loss: 0.60669 Time: 0.23145\n",
      "Iteration: 0166 Loss: 0.60827 Time: 0.23742\n",
      "Iteration: 0167 Loss: 0.60295 Time: 0.23361\n",
      "Iteration: 0168 Loss: 0.60372 Time: 0.24873\n",
      "Iteration: 0169 Loss: 0.60098 Time: 0.22812\n",
      "Iteration: 0170 Loss: 0.60158 Time: 0.23300\n",
      "Iteration: 0171 Loss: 0.59826 Time: 0.22767\n",
      "Iteration: 0172 Loss: 0.59398 Time: 0.23700\n",
      "Iteration: 0173 Loss: 0.59433 Time: 0.23644\n",
      "Iteration: 0174 Loss: 0.58874 Time: 0.23799\n",
      "Iteration: 0175 Loss: 0.59049 Time: 0.23213\n",
      "Iteration: 0176 Loss: 0.59047 Time: 0.23012\n",
      "Iteration: 0177 Loss: 0.58746 Time: 0.23719\n",
      "Iteration: 0178 Loss: 0.58665 Time: 0.23440\n",
      "Iteration: 0179 Loss: 0.58825 Time: 0.23088\n",
      "Iteration: 0180 Loss: 0.58268 Time: 0.23012\n",
      "Iteration: 0181 Loss: 0.58445 Time: 0.23919\n",
      "Iteration: 0182 Loss: 0.58584 Time: 0.23999\n",
      "Iteration: 0183 Loss: 0.58165 Time: 0.23497\n",
      "Iteration: 0184 Loss: 0.57514 Time: 0.23635\n",
      "Iteration: 0185 Loss: 0.57962 Time: 0.22707\n",
      "Iteration: 0186 Loss: 0.57996 Time: 0.23299\n",
      "Iteration: 0187 Loss: 0.57967 Time: 0.23004\n",
      "Iteration: 0188 Loss: 0.57274 Time: 0.25011\n",
      "Iteration: 0189 Loss: 0.57811 Time: 0.23536\n",
      "Iteration: 0190 Loss: 0.56955 Time: 0.23643\n",
      "Iteration: 0191 Loss: 0.57221 Time: 0.22600\n",
      "Iteration: 0192 Loss: 0.57424 Time: 0.24137\n",
      "Iteration: 0193 Loss: 0.57326 Time: 0.24019\n",
      "Iteration: 0194 Loss: 0.57204 Time: 0.23108\n",
      "Iteration: 0195 Loss: 0.56790 Time: 0.23204\n",
      "Iteration: 0196 Loss: 0.57057 Time: 0.22992\n",
      "Iteration: 0197 Loss: 0.57162 Time: 0.22761\n",
      "Iteration: 0198 Loss: 0.57222 Time: 0.25147\n",
      "Iteration: 0199 Loss: 0.56298 Time: 0.22921\n",
      "Iteration: 0200 Loss: 0.56775 Time: 0.24297\n",
      "Done! \n",
      "\n",
      "STEP 3/3 - MODEL EVALUATION \n",
      "\n",
      "Computing the final embedding vectors, for evaluation\n",
      "Done! \n",
      "\n",
      "Testing: link prediction\n",
      "Done! \n",
      "\n",
      "EXPERIMENTS ON MODEL 6 / 10 \n",
      "\n",
      "STEP 1/3 - PREPROCESSING STEPS \n",
      "\n",
      "Masking some edges from the training graph, for link prediction\n",
      "(validation set: 1.0 % of edges - test set: 10.0 % of edges)\n",
      "Done! \n",
      "\n",
      "Preprocessing node features\n",
      "Done! \n",
      "\n",
      "Running the Louvain algorithm for community detection\n",
      "as a preprocessing step for the encoder\n",
      "Done! Louvain has found 168 communities \n",
      "\n",
      "Setting up the model and the optimizer\n",
      "Done! \n",
      "\n",
      "Preprocessing on message passing matrices\n",
      "Done! \n",
      "\n",
      "Initializing TF session\n",
      "Done! \n",
      "\n",
      "STEP 2/3 - MODEL TRAINING \n",
      "\n",
      "Starting training\n",
      "Iteration: 0001 Loss: 1.73441 Time: 0.50494\n",
      "Iteration: 0002 Loss: 1.73663 Time: 0.23128\n",
      "Iteration: 0003 Loss: 1.67486 Time: 0.23354\n",
      "Iteration: 0004 Loss: 1.67699 Time: 0.23800\n",
      "Iteration: 0005 Loss: 1.66440 Time: 0.23417\n",
      "Iteration: 0006 Loss: 1.64459 Time: 0.23905\n",
      "Iteration: 0007 Loss: 1.57829 Time: 0.23556\n",
      "Iteration: 0008 Loss: 1.60240 Time: 0.25025\n",
      "Iteration: 0009 Loss: 1.57816 Time: 0.22899\n",
      "Iteration: 0010 Loss: 1.56041 Time: 0.24180\n",
      "Iteration: 0011 Loss: 1.55023 Time: 0.22427\n",
      "Iteration: 0012 Loss: 1.52541 Time: 0.24094\n",
      "Iteration: 0013 Loss: 1.51931 Time: 0.24038\n",
      "Iteration: 0014 Loss: 1.48077 Time: 0.24199\n",
      "Iteration: 0015 Loss: 1.52575 Time: 0.23299\n",
      "Iteration: 0016 Loss: 1.49054 Time: 0.23200\n",
      "Iteration: 0017 Loss: 1.45301 Time: 0.24952\n",
      "Iteration: 0018 Loss: 1.43965 Time: 0.23802\n",
      "Iteration: 0019 Loss: 1.41033 Time: 0.22831\n",
      "Iteration: 0020 Loss: 1.40003 Time: 0.22987\n",
      "Iteration: 0021 Loss: 1.38483 Time: 0.23340\n",
      "Iteration: 0022 Loss: 1.35053 Time: 0.23387\n",
      "Iteration: 0023 Loss: 1.36577 Time: 0.23691\n",
      "Iteration: 0024 Loss: 1.33596 Time: 0.24191\n",
      "Iteration: 0025 Loss: 1.32110 Time: 0.22865\n",
      "Iteration: 0026 Loss: 1.34481 Time: 0.23233\n",
      "Iteration: 0027 Loss: 1.32119 Time: 0.24606\n",
      "Iteration: 0028 Loss: 1.31213 Time: 0.22961\n",
      "Iteration: 0029 Loss: 1.29150 Time: 0.23997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0030 Loss: 1.28238 Time: 0.23416\n",
      "Iteration: 0031 Loss: 1.28602 Time: 0.22988\n",
      "Iteration: 0032 Loss: 1.27050 Time: 0.24834\n",
      "Iteration: 0033 Loss: 1.24172 Time: 0.24088\n",
      "Iteration: 0034 Loss: 1.23061 Time: 0.23445\n",
      "Iteration: 0035 Loss: 1.25532 Time: 0.24300\n",
      "Iteration: 0036 Loss: 1.24245 Time: 0.22892\n",
      "Iteration: 0037 Loss: 1.22187 Time: 0.23433\n",
      "Iteration: 0038 Loss: 1.21221 Time: 0.23112\n",
      "Iteration: 0039 Loss: 1.21198 Time: 0.24585\n",
      "Iteration: 0040 Loss: 1.18620 Time: 0.23840\n",
      "Iteration: 0041 Loss: 1.18474 Time: 0.22649\n",
      "Iteration: 0042 Loss: 1.19075 Time: 0.24533\n",
      "Iteration: 0043 Loss: 1.16147 Time: 0.23321\n",
      "Iteration: 0044 Loss: 1.16419 Time: 0.23913\n",
      "Iteration: 0045 Loss: 1.16974 Time: 0.22730\n",
      "Iteration: 0046 Loss: 1.13853 Time: 0.24481\n",
      "Iteration: 0047 Loss: 1.12934 Time: 0.23418\n",
      "Iteration: 0048 Loss: 1.13686 Time: 0.23605\n",
      "Iteration: 0049 Loss: 1.12763 Time: 0.23144\n",
      "Iteration: 0050 Loss: 1.11994 Time: 0.24751\n",
      "Iteration: 0051 Loss: 1.11369 Time: 0.23300\n",
      "Iteration: 0052 Loss: 1.10128 Time: 0.22900\n",
      "Iteration: 0053 Loss: 1.10629 Time: 0.24266\n",
      "Iteration: 0054 Loss: 1.09151 Time: 0.23453\n",
      "Iteration: 0055 Loss: 1.08819 Time: 0.23069\n",
      "Iteration: 0056 Loss: 1.07078 Time: 0.23192\n",
      "Iteration: 0057 Loss: 1.08135 Time: 0.23180\n",
      "Iteration: 0058 Loss: 1.06091 Time: 0.24129\n",
      "Iteration: 0059 Loss: 1.04928 Time: 0.23600\n",
      "Iteration: 0060 Loss: 1.04921 Time: 0.23345\n",
      "Iteration: 0061 Loss: 1.05224 Time: 0.23465\n",
      "Iteration: 0062 Loss: 1.03508 Time: 0.23384\n",
      "Iteration: 0063 Loss: 1.04534 Time: 0.23525\n",
      "Iteration: 0064 Loss: 1.03934 Time: 0.23796\n",
      "Iteration: 0065 Loss: 1.02959 Time: 0.24791\n",
      "Iteration: 0066 Loss: 1.01722 Time: 0.25677\n",
      "Iteration: 0067 Loss: 1.00566 Time: 0.24041\n",
      "Iteration: 0068 Loss: 0.99218 Time: 0.23566\n",
      "Iteration: 0069 Loss: 1.01672 Time: 0.23891\n",
      "Iteration: 0070 Loss: 0.98880 Time: 0.23045\n",
      "Iteration: 0071 Loss: 0.98150 Time: 0.24435\n",
      "Iteration: 0072 Loss: 0.96598 Time: 0.23523\n",
      "Iteration: 0073 Loss: 0.97501 Time: 0.25757\n",
      "Iteration: 0074 Loss: 0.97491 Time: 0.23999\n",
      "Iteration: 0075 Loss: 0.96563 Time: 0.24063\n",
      "Iteration: 0076 Loss: 0.95636 Time: 0.23886\n",
      "Iteration: 0077 Loss: 0.96610 Time: 0.24407\n",
      "Iteration: 0078 Loss: 0.94006 Time: 0.22810\n",
      "Iteration: 0079 Loss: 0.93765 Time: 0.23702\n",
      "Iteration: 0080 Loss: 0.92883 Time: 0.25131\n",
      "Iteration: 0081 Loss: 0.93893 Time: 0.23500\n",
      "Iteration: 0082 Loss: 0.92385 Time: 0.23601\n",
      "Iteration: 0083 Loss: 0.92161 Time: 0.23799\n",
      "Iteration: 0084 Loss: 0.91423 Time: 0.23200\n",
      "Iteration: 0085 Loss: 0.91222 Time: 0.24721\n",
      "Iteration: 0086 Loss: 0.90495 Time: 0.23004\n",
      "Iteration: 0087 Loss: 0.91631 Time: 0.24009\n",
      "Iteration: 0088 Loss: 0.89277 Time: 0.23498\n",
      "Iteration: 0089 Loss: 0.88197 Time: 0.24816\n",
      "Iteration: 0090 Loss: 0.88913 Time: 0.23299\n",
      "Iteration: 0091 Loss: 0.89027 Time: 0.24044\n",
      "Iteration: 0092 Loss: 0.88382 Time: 0.24199\n",
      "Iteration: 0093 Loss: 0.87728 Time: 0.23851\n",
      "Iteration: 0094 Loss: 0.87208 Time: 0.25252\n",
      "Iteration: 0095 Loss: 0.85153 Time: 0.22766\n",
      "Iteration: 0096 Loss: 0.85400 Time: 0.23758\n",
      "Iteration: 0097 Loss: 0.83895 Time: 0.23300\n",
      "Iteration: 0098 Loss: 0.84772 Time: 0.23171\n",
      "Iteration: 0099 Loss: 0.84791 Time: 0.25093\n",
      "Iteration: 0100 Loss: 0.82935 Time: 0.23380\n",
      "Iteration: 0101 Loss: 0.83403 Time: 0.24249\n",
      "Iteration: 0102 Loss: 0.83337 Time: 0.23100\n",
      "Iteration: 0103 Loss: 0.83271 Time: 0.23669\n",
      "Iteration: 0104 Loss: 0.81383 Time: 0.23777\n",
      "Iteration: 0105 Loss: 0.81547 Time: 0.22539\n",
      "Iteration: 0106 Loss: 0.80544 Time: 0.24804\n",
      "Iteration: 0107 Loss: 0.80521 Time: 0.24716\n",
      "Iteration: 0108 Loss: 0.80159 Time: 0.24004\n",
      "Iteration: 0109 Loss: 0.79290 Time: 0.23854\n",
      "Iteration: 0110 Loss: 0.79519 Time: 0.23300\n",
      "Iteration: 0111 Loss: 0.77804 Time: 0.22656\n",
      "Iteration: 0112 Loss: 0.77381 Time: 0.22320\n",
      "Iteration: 0113 Loss: 0.78140 Time: 0.25042\n",
      "Iteration: 0114 Loss: 0.76319 Time: 0.24000\n",
      "Iteration: 0115 Loss: 0.76882 Time: 0.23400\n",
      "Iteration: 0116 Loss: 0.75715 Time: 0.23512\n",
      "Iteration: 0117 Loss: 0.76021 Time: 0.23370\n",
      "Iteration: 0118 Loss: 0.73427 Time: 0.23107\n",
      "Iteration: 0119 Loss: 0.75190 Time: 0.23722\n",
      "Iteration: 0120 Loss: 0.73416 Time: 0.23372\n",
      "Iteration: 0121 Loss: 0.74068 Time: 0.22759\n",
      "Iteration: 0122 Loss: 0.72110 Time: 0.23687\n",
      "Iteration: 0123 Loss: 0.72265 Time: 0.23575\n",
      "Iteration: 0124 Loss: 0.72990 Time: 0.25611\n",
      "Iteration: 0125 Loss: 0.72565 Time: 0.22913\n",
      "Iteration: 0126 Loss: 0.71288 Time: 0.23346\n",
      "Iteration: 0127 Loss: 0.70729 Time: 0.23753\n",
      "Iteration: 0128 Loss: 0.70750 Time: 0.22953\n",
      "Iteration: 0129 Loss: 0.70124 Time: 0.23082\n",
      "Iteration: 0130 Loss: 0.69931 Time: 0.23453\n",
      "Iteration: 0131 Loss: 0.70012 Time: 0.23639\n",
      "Iteration: 0132 Loss: 0.68512 Time: 0.23637\n",
      "Iteration: 0133 Loss: 0.68907 Time: 0.23122\n",
      "Iteration: 0134 Loss: 0.69138 Time: 0.23236\n",
      "Iteration: 0135 Loss: 0.68334 Time: 0.24745\n",
      "Iteration: 0136 Loss: 0.67833 Time: 0.22744\n",
      "Iteration: 0137 Loss: 0.67586 Time: 0.23201\n",
      "Iteration: 0138 Loss: 0.67167 Time: 0.22896\n",
      "Iteration: 0139 Loss: 0.67481 Time: 0.23999\n",
      "Iteration: 0140 Loss: 0.66765 Time: 0.22596\n",
      "Iteration: 0141 Loss: 0.66065 Time: 0.23348\n",
      "Iteration: 0142 Loss: 0.66174 Time: 0.23365\n",
      "Iteration: 0143 Loss: 0.65220 Time: 0.23682\n",
      "Iteration: 0144 Loss: 0.65442 Time: 0.23068\n",
      "Iteration: 0145 Loss: 0.64773 Time: 0.23285\n",
      "Iteration: 0146 Loss: 0.65280 Time: 0.22770\n",
      "Iteration: 0147 Loss: 0.64825 Time: 0.24701\n",
      "Iteration: 0148 Loss: 0.64436 Time: 0.23746\n",
      "Iteration: 0149 Loss: 0.63819 Time: 0.23200\n",
      "Iteration: 0150 Loss: 0.64344 Time: 0.23533\n",
      "Iteration: 0151 Loss: 0.63433 Time: 0.23900\n",
      "Iteration: 0152 Loss: 0.64099 Time: 0.23175\n",
      "Iteration: 0153 Loss: 0.63873 Time: 0.23309\n",
      "Iteration: 0154 Loss: 0.63400 Time: 0.23899\n",
      "Iteration: 0155 Loss: 0.62704 Time: 0.23632\n",
      "Iteration: 0156 Loss: 0.62484 Time: 0.22952\n",
      "Iteration: 0157 Loss: 0.62142 Time: 0.22847\n",
      "Iteration: 0158 Loss: 0.61667 Time: 0.23581\n",
      "Iteration: 0159 Loss: 0.62087 Time: 0.23475\n",
      "Iteration: 0160 Loss: 0.61828 Time: 0.23554\n",
      "Iteration: 0161 Loss: 0.61244 Time: 0.24947\n",
      "Iteration: 0162 Loss: 0.61643 Time: 0.23214\n",
      "Iteration: 0163 Loss: 0.61242 Time: 0.22598\n",
      "Iteration: 0164 Loss: 0.61614 Time: 0.24015\n",
      "Iteration: 0165 Loss: 0.60723 Time: 0.23661\n",
      "Iteration: 0166 Loss: 0.60909 Time: 0.23661\n",
      "Iteration: 0167 Loss: 0.60826 Time: 0.23152\n",
      "Iteration: 0168 Loss: 0.60156 Time: 0.23388\n",
      "Iteration: 0169 Loss: 0.59850 Time: 0.23100\n",
      "Iteration: 0170 Loss: 0.60033 Time: 0.23483\n",
      "Iteration: 0171 Loss: 0.60203 Time: 0.23654\n",
      "Iteration: 0172 Loss: 0.59150 Time: 0.23542\n",
      "Iteration: 0173 Loss: 0.59405 Time: 0.23497\n",
      "Iteration: 0174 Loss: 0.59606 Time: 0.23244\n",
      "Iteration: 0175 Loss: 0.59341 Time: 0.23643\n",
      "Iteration: 0176 Loss: 0.58869 Time: 0.23000\n",
      "Iteration: 0177 Loss: 0.59364 Time: 0.23459\n",
      "Iteration: 0178 Loss: 0.58960 Time: 0.23411\n",
      "Iteration: 0179 Loss: 0.58573 Time: 0.23020\n",
      "Iteration: 0180 Loss: 0.58500 Time: 0.22493\n",
      "Iteration: 0181 Loss: 0.58517 Time: 0.23684\n",
      "Iteration: 0182 Loss: 0.58340 Time: 0.23699\n",
      "Iteration: 0183 Loss: 0.58340 Time: 0.23358\n",
      "Iteration: 0184 Loss: 0.58305 Time: 0.23047\n",
      "Iteration: 0185 Loss: 0.58331 Time: 0.24221\n",
      "Iteration: 0186 Loss: 0.57771 Time: 0.22974\n",
      "Iteration: 0187 Loss: 0.57881 Time: 0.23359\n",
      "Iteration: 0188 Loss: 0.58163 Time: 0.23991\n",
      "Iteration: 0189 Loss: 0.57732 Time: 0.22399\n",
      "Iteration: 0190 Loss: 0.57419 Time: 0.23853\n",
      "Iteration: 0191 Loss: 0.57224 Time: 0.23519\n",
      "Iteration: 0192 Loss: 0.57328 Time: 0.23121\n",
      "Iteration: 0193 Loss: 0.56950 Time: 0.23402\n",
      "Iteration: 0194 Loss: 0.57167 Time: 0.23500\n",
      "Iteration: 0195 Loss: 0.57025 Time: 0.22999\n",
      "Iteration: 0196 Loss: 0.56745 Time: 0.25180\n",
      "Iteration: 0197 Loss: 0.57010 Time: 0.22203\n",
      "Iteration: 0198 Loss: 0.56587 Time: 0.23102\n",
      "Iteration: 0199 Loss: 0.56809 Time: 0.24500\n",
      "Iteration: 0200 Loss: 0.56567 Time: 0.22850\n",
      "Done! \n",
      "\n",
      "STEP 3/3 - MODEL EVALUATION \n",
      "\n",
      "Computing the final embedding vectors, for evaluation\n",
      "Done! \n",
      "\n",
      "Testing: link prediction\n",
      "Done! \n",
      "\n",
      "EXPERIMENTS ON MODEL 7 / 10 \n",
      "\n",
      "STEP 1/3 - PREPROCESSING STEPS \n",
      "\n",
      "Masking some edges from the training graph, for link prediction\n",
      "(validation set: 1.0 % of edges - test set: 10.0 % of edges)\n",
      "Done! \n",
      "\n",
      "Preprocessing node features\n",
      "Done! \n",
      "\n",
      "Running the Louvain algorithm for community detection\n",
      "as a preprocessing step for the encoder\n",
      "Done! Louvain has found 162 communities \n",
      "\n",
      "Setting up the model and the optimizer\n",
      "Done! \n",
      "\n",
      "Preprocessing on message passing matrices\n",
      "Done! \n",
      "\n",
      "Initializing TF session\n",
      "Done! \n",
      "\n",
      "STEP 2/3 - MODEL TRAINING \n",
      "\n",
      "Starting training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0001 Loss: 1.73834 Time: 0.50231\n",
      "Iteration: 0002 Loss: 1.73857 Time: 0.22973\n",
      "Iteration: 0003 Loss: 1.69742 Time: 0.23987\n",
      "Iteration: 0004 Loss: 1.68942 Time: 0.22825\n",
      "Iteration: 0005 Loss: 1.64417 Time: 0.23610\n",
      "Iteration: 0006 Loss: 1.62710 Time: 0.23918\n",
      "Iteration: 0007 Loss: 1.63609 Time: 0.23110\n",
      "Iteration: 0008 Loss: 1.56533 Time: 0.23494\n",
      "Iteration: 0009 Loss: 1.58091 Time: 0.22900\n",
      "Iteration: 0010 Loss: 1.53992 Time: 0.23143\n",
      "Iteration: 0011 Loss: 1.51634 Time: 0.24861\n",
      "Iteration: 0012 Loss: 1.56406 Time: 0.23783\n",
      "Iteration: 0013 Loss: 1.51355 Time: 0.23202\n",
      "Iteration: 0014 Loss: 1.52121 Time: 0.22583\n",
      "Iteration: 0015 Loss: 1.49242 Time: 0.25303\n",
      "Iteration: 0016 Loss: 1.47060 Time: 0.23199\n",
      "Iteration: 0017 Loss: 1.44896 Time: 0.23230\n",
      "Iteration: 0018 Loss: 1.42371 Time: 0.23585\n",
      "Iteration: 0019 Loss: 1.41146 Time: 0.23616\n",
      "Iteration: 0020 Loss: 1.42238 Time: 0.23163\n",
      "Iteration: 0021 Loss: 1.38736 Time: 0.24504\n",
      "Iteration: 0022 Loss: 1.34028 Time: 0.23163\n",
      "Iteration: 0023 Loss: 1.38479 Time: 0.22604\n",
      "Iteration: 0024 Loss: 1.36868 Time: 0.23669\n",
      "Iteration: 0025 Loss: 1.34721 Time: 0.24160\n",
      "Iteration: 0026 Loss: 1.33453 Time: 0.23567\n",
      "Iteration: 0027 Loss: 1.35704 Time: 0.23552\n",
      "Iteration: 0028 Loss: 1.33178 Time: 0.23833\n",
      "Iteration: 0029 Loss: 1.27602 Time: 0.23183\n",
      "Iteration: 0030 Loss: 1.29156 Time: 0.24494\n",
      "Iteration: 0031 Loss: 1.29347 Time: 0.23668\n",
      "Iteration: 0032 Loss: 1.25638 Time: 0.23790\n",
      "Iteration: 0033 Loss: 1.26806 Time: 0.23322\n",
      "Iteration: 0034 Loss: 1.22696 Time: 0.22800\n",
      "Iteration: 0035 Loss: 1.23423 Time: 0.22881\n",
      "Iteration: 0036 Loss: 1.22520 Time: 0.23850\n",
      "Iteration: 0037 Loss: 1.21668 Time: 0.23210\n",
      "Iteration: 0038 Loss: 1.21933 Time: 0.23119\n",
      "Iteration: 0039 Loss: 1.19061 Time: 0.23094\n",
      "Iteration: 0040 Loss: 1.18089 Time: 0.22220\n",
      "Iteration: 0041 Loss: 1.19750 Time: 0.23563\n",
      "Iteration: 0042 Loss: 1.17764 Time: 0.23879\n",
      "Iteration: 0043 Loss: 1.16755 Time: 0.23441\n",
      "Iteration: 0044 Loss: 1.14038 Time: 0.23677\n",
      "Iteration: 0045 Loss: 1.16345 Time: 0.24590\n",
      "Iteration: 0046 Loss: 1.16105 Time: 0.23822\n",
      "Iteration: 0047 Loss: 1.14424 Time: 0.23646\n",
      "Iteration: 0048 Loss: 1.13276 Time: 0.22233\n",
      "Iteration: 0049 Loss: 1.13300 Time: 0.24535\n",
      "Iteration: 0050 Loss: 1.13492 Time: 0.24628\n",
      "Iteration: 0051 Loss: 1.10130 Time: 0.23661\n",
      "Iteration: 0052 Loss: 1.09013 Time: 0.23015\n",
      "Iteration: 0053 Loss: 1.10000 Time: 0.23998\n",
      "Iteration: 0054 Loss: 1.08878 Time: 0.23420\n",
      "Iteration: 0055 Loss: 1.07369 Time: 0.22912\n",
      "Iteration: 0056 Loss: 1.08303 Time: 0.24106\n",
      "Iteration: 0057 Loss: 1.06200 Time: 0.22741\n",
      "Iteration: 0058 Loss: 1.06897 Time: 0.24425\n",
      "Iteration: 0059 Loss: 1.04944 Time: 0.23291\n",
      "Iteration: 0060 Loss: 1.04922 Time: 0.24200\n",
      "Iteration: 0061 Loss: 1.05232 Time: 0.23878\n",
      "Iteration: 0062 Loss: 1.03803 Time: 0.23156\n",
      "Iteration: 0063 Loss: 1.04719 Time: 0.24201\n",
      "Iteration: 0064 Loss: 1.03037 Time: 0.22381\n",
      "Iteration: 0065 Loss: 1.02386 Time: 0.23652\n",
      "Iteration: 0066 Loss: 1.03869 Time: 0.23218\n",
      "Iteration: 0067 Loss: 1.01069 Time: 0.23712\n",
      "Iteration: 0068 Loss: 1.02159 Time: 0.23754\n",
      "Iteration: 0069 Loss: 1.01818 Time: 0.23699\n",
      "Iteration: 0070 Loss: 1.00505 Time: 0.22736\n",
      "Iteration: 0071 Loss: 0.97048 Time: 0.23371\n",
      "Iteration: 0072 Loss: 0.97848 Time: 0.23325\n",
      "Iteration: 0073 Loss: 0.99352 Time: 0.23445\n",
      "Iteration: 0074 Loss: 0.96958 Time: 0.23029\n",
      "Iteration: 0075 Loss: 0.96526 Time: 0.23609\n",
      "Iteration: 0076 Loss: 0.97381 Time: 0.23000\n",
      "Iteration: 0077 Loss: 0.95346 Time: 0.23203\n",
      "Iteration: 0078 Loss: 0.96428 Time: 0.23583\n",
      "Iteration: 0079 Loss: 0.93940 Time: 0.23632\n",
      "Iteration: 0080 Loss: 0.93892 Time: 0.22900\n",
      "Iteration: 0081 Loss: 0.94555 Time: 0.23295\n",
      "Iteration: 0082 Loss: 0.94265 Time: 0.24173\n",
      "Iteration: 0083 Loss: 0.92706 Time: 0.23099\n",
      "Iteration: 0084 Loss: 0.92510 Time: 0.23322\n",
      "Iteration: 0085 Loss: 0.91493 Time: 0.23725\n",
      "Iteration: 0086 Loss: 0.90947 Time: 0.23052\n",
      "Iteration: 0087 Loss: 0.90508 Time: 0.23600\n",
      "Iteration: 0088 Loss: 0.89955 Time: 0.23399\n",
      "Iteration: 0089 Loss: 0.88260 Time: 0.23714\n",
      "Iteration: 0090 Loss: 0.89060 Time: 0.22637\n",
      "Iteration: 0091 Loss: 0.88578 Time: 0.23724\n",
      "Iteration: 0092 Loss: 0.87361 Time: 0.23229\n",
      "Iteration: 0093 Loss: 0.87435 Time: 0.23869\n",
      "Iteration: 0094 Loss: 0.85932 Time: 0.24560\n",
      "Iteration: 0095 Loss: 0.86417 Time: 0.23557\n",
      "Iteration: 0096 Loss: 0.85879 Time: 0.24317\n",
      "Iteration: 0097 Loss: 0.84861 Time: 0.23463\n",
      "Iteration: 0098 Loss: 0.85406 Time: 0.23909\n",
      "Iteration: 0099 Loss: 0.85129 Time: 0.22659\n",
      "Iteration: 0100 Loss: 0.83182 Time: 0.23519\n",
      "Iteration: 0101 Loss: 0.82943 Time: 0.23824\n",
      "Iteration: 0102 Loss: 0.84139 Time: 0.23792\n",
      "Iteration: 0103 Loss: 0.82241 Time: 0.24587\n",
      "Iteration: 0104 Loss: 0.81730 Time: 0.22801\n",
      "Iteration: 0105 Loss: 0.79954 Time: 0.22832\n",
      "Iteration: 0106 Loss: 0.81201 Time: 0.23399\n",
      "Iteration: 0107 Loss: 0.79938 Time: 0.23202\n",
      "Iteration: 0108 Loss: 0.79312 Time: 0.23010\n",
      "Iteration: 0109 Loss: 0.78602 Time: 0.23900\n",
      "Iteration: 0110 Loss: 0.79376 Time: 0.24010\n",
      "Iteration: 0111 Loss: 0.78267 Time: 0.23846\n",
      "Iteration: 0112 Loss: 0.78462 Time: 0.23300\n",
      "Iteration: 0113 Loss: 0.76745 Time: 0.23600\n",
      "Iteration: 0114 Loss: 0.76040 Time: 0.23500\n",
      "Iteration: 0115 Loss: 0.76420 Time: 0.22785\n",
      "Iteration: 0116 Loss: 0.76598 Time: 0.23150\n",
      "Iteration: 0117 Loss: 0.74621 Time: 0.23044\n",
      "Iteration: 0118 Loss: 0.74890 Time: 0.23897\n",
      "Iteration: 0119 Loss: 0.74670 Time: 0.23431\n",
      "Iteration: 0120 Loss: 0.73736 Time: 0.22904\n",
      "Iteration: 0121 Loss: 0.73742 Time: 0.24104\n",
      "Iteration: 0122 Loss: 0.73393 Time: 0.23555\n",
      "Iteration: 0123 Loss: 0.72032 Time: 0.23586\n",
      "Iteration: 0124 Loss: 0.71592 Time: 0.23738\n",
      "Iteration: 0125 Loss: 0.71588 Time: 0.23023\n",
      "Iteration: 0126 Loss: 0.72473 Time: 0.23468\n",
      "Iteration: 0127 Loss: 0.71969 Time: 0.23986\n",
      "Iteration: 0128 Loss: 0.69912 Time: 0.23078\n",
      "Iteration: 0129 Loss: 0.70760 Time: 0.22832\n",
      "Iteration: 0130 Loss: 0.70342 Time: 0.23379\n",
      "Iteration: 0131 Loss: 0.69116 Time: 0.23218\n",
      "Iteration: 0132 Loss: 0.69069 Time: 0.23183\n",
      "Iteration: 0133 Loss: 0.69504 Time: 0.23678\n",
      "Iteration: 0134 Loss: 0.69007 Time: 0.23653\n",
      "Iteration: 0135 Loss: 0.68655 Time: 0.23177\n",
      "Iteration: 0136 Loss: 0.67642 Time: 0.23313\n",
      "Iteration: 0137 Loss: 0.67170 Time: 0.23312\n",
      "Iteration: 0138 Loss: 0.67067 Time: 0.23629\n",
      "Iteration: 0139 Loss: 0.66629 Time: 0.23592\n",
      "Iteration: 0140 Loss: 0.65873 Time: 0.23445\n",
      "Iteration: 0141 Loss: 0.66451 Time: 0.22951\n",
      "Iteration: 0142 Loss: 0.66669 Time: 0.22964\n",
      "Iteration: 0143 Loss: 0.65872 Time: 0.23344\n",
      "Iteration: 0144 Loss: 0.65071 Time: 0.23351\n",
      "Iteration: 0145 Loss: 0.64993 Time: 0.23566\n",
      "Iteration: 0146 Loss: 0.64910 Time: 0.23717\n",
      "Iteration: 0147 Loss: 0.64481 Time: 0.24811\n",
      "Iteration: 0148 Loss: 0.64250 Time: 0.23310\n",
      "Iteration: 0149 Loss: 0.63966 Time: 0.23384\n",
      "Iteration: 0150 Loss: 0.63869 Time: 0.22949\n",
      "Iteration: 0151 Loss: 0.63215 Time: 0.23440\n",
      "Iteration: 0152 Loss: 0.63217 Time: 0.23165\n",
      "Iteration: 0153 Loss: 0.63508 Time: 0.23199\n",
      "Iteration: 0154 Loss: 0.62269 Time: 0.23292\n",
      "Iteration: 0155 Loss: 0.62968 Time: 0.23309\n",
      "Iteration: 0156 Loss: 0.61908 Time: 0.24180\n",
      "Iteration: 0157 Loss: 0.62310 Time: 0.23056\n",
      "Iteration: 0158 Loss: 0.62140 Time: 0.22941\n",
      "Iteration: 0159 Loss: 0.61906 Time: 0.22782\n",
      "Iteration: 0160 Loss: 0.61815 Time: 0.22677\n",
      "Iteration: 0161 Loss: 0.61469 Time: 0.23399\n",
      "Iteration: 0162 Loss: 0.61222 Time: 0.25100\n",
      "Iteration: 0163 Loss: 0.61025 Time: 0.23091\n",
      "Iteration: 0164 Loss: 0.60612 Time: 0.23755\n",
      "Iteration: 0165 Loss: 0.60334 Time: 0.23369\n",
      "Iteration: 0166 Loss: 0.60934 Time: 0.23545\n",
      "Iteration: 0167 Loss: 0.60632 Time: 0.23980\n",
      "Iteration: 0168 Loss: 0.59948 Time: 0.22972\n",
      "Iteration: 0169 Loss: 0.60061 Time: 0.23967\n",
      "Iteration: 0170 Loss: 0.59714 Time: 0.23785\n",
      "Iteration: 0171 Loss: 0.60164 Time: 0.24000\n",
      "Iteration: 0172 Loss: 0.59759 Time: 0.23501\n",
      "Iteration: 0173 Loss: 0.59626 Time: 0.24013\n",
      "Iteration: 0174 Loss: 0.59340 Time: 0.23475\n",
      "Iteration: 0175 Loss: 0.58919 Time: 0.22960\n",
      "Iteration: 0176 Loss: 0.58665 Time: 0.22945\n",
      "Iteration: 0177 Loss: 0.58600 Time: 0.23171\n",
      "Iteration: 0178 Loss: 0.59008 Time: 0.23229\n",
      "Iteration: 0179 Loss: 0.59389 Time: 0.22983\n",
      "Iteration: 0180 Loss: 0.58712 Time: 0.24375\n",
      "Iteration: 0181 Loss: 0.58681 Time: 0.23267\n",
      "Iteration: 0182 Loss: 0.58014 Time: 0.23329\n",
      "Iteration: 0183 Loss: 0.58102 Time: 0.23988\n",
      "Iteration: 0184 Loss: 0.58515 Time: 0.23227\n",
      "Iteration: 0185 Loss: 0.58025 Time: 0.23147\n",
      "Iteration: 0186 Loss: 0.58374 Time: 0.23443\n",
      "Iteration: 0187 Loss: 0.57518 Time: 0.25230\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0188 Loss: 0.57997 Time: 0.23421\n",
      "Iteration: 0189 Loss: 0.57409 Time: 0.23095\n",
      "Iteration: 0190 Loss: 0.57636 Time: 0.23411\n",
      "Iteration: 0191 Loss: 0.57561 Time: 0.22548\n",
      "Iteration: 0192 Loss: 0.57458 Time: 0.23492\n",
      "Iteration: 0193 Loss: 0.57155 Time: 0.23081\n",
      "Iteration: 0194 Loss: 0.57049 Time: 0.23884\n",
      "Iteration: 0195 Loss: 0.56673 Time: 0.23919\n",
      "Iteration: 0196 Loss: 0.57028 Time: 0.23395\n",
      "Iteration: 0197 Loss: 0.56904 Time: 0.23393\n",
      "Iteration: 0198 Loss: 0.56636 Time: 0.23115\n",
      "Iteration: 0199 Loss: 0.56889 Time: 0.23453\n",
      "Iteration: 0200 Loss: 0.56462 Time: 0.23448\n",
      "Done! \n",
      "\n",
      "STEP 3/3 - MODEL EVALUATION \n",
      "\n",
      "Computing the final embedding vectors, for evaluation\n",
      "Done! \n",
      "\n",
      "Testing: link prediction\n",
      "Done! \n",
      "\n",
      "EXPERIMENTS ON MODEL 8 / 10 \n",
      "\n",
      "STEP 1/3 - PREPROCESSING STEPS \n",
      "\n",
      "Masking some edges from the training graph, for link prediction\n",
      "(validation set: 1.0 % of edges - test set: 10.0 % of edges)\n",
      "Done! \n",
      "\n",
      "Preprocessing node features\n",
      "Done! \n",
      "\n",
      "Running the Louvain algorithm for community detection\n",
      "as a preprocessing step for the encoder\n",
      "Done! Louvain has found 170 communities \n",
      "\n",
      "Setting up the model and the optimizer\n",
      "Done! \n",
      "\n",
      "Preprocessing on message passing matrices\n",
      "Done! \n",
      "\n",
      "Initializing TF session\n",
      "Done! \n",
      "\n",
      "STEP 2/3 - MODEL TRAINING \n",
      "\n",
      "Starting training\n",
      "Iteration: 0001 Loss: 1.75618 Time: 0.51580\n",
      "Iteration: 0002 Loss: 1.66096 Time: 0.22500\n",
      "Iteration: 0003 Loss: 1.64654 Time: 0.24248\n",
      "Iteration: 0004 Loss: 1.65652 Time: 0.23088\n",
      "Iteration: 0005 Loss: 1.63200 Time: 0.23799\n",
      "Iteration: 0006 Loss: 1.63159 Time: 0.24801\n",
      "Iteration: 0007 Loss: 1.60408 Time: 0.23374\n",
      "Iteration: 0008 Loss: 1.60379 Time: 0.23913\n",
      "Iteration: 0009 Loss: 1.56061 Time: 0.22682\n",
      "Iteration: 0010 Loss: 1.54091 Time: 0.23479\n",
      "Iteration: 0011 Loss: 1.53968 Time: 0.24100\n",
      "Iteration: 0012 Loss: 1.48933 Time: 0.22757\n",
      "Iteration: 0013 Loss: 1.47739 Time: 0.23757\n",
      "Iteration: 0014 Loss: 1.46234 Time: 0.23631\n",
      "Iteration: 0015 Loss: 1.46963 Time: 0.22999\n",
      "Iteration: 0016 Loss: 1.44556 Time: 0.23207\n",
      "Iteration: 0017 Loss: 1.41454 Time: 0.23946\n",
      "Iteration: 0018 Loss: 1.44093 Time: 0.22954\n",
      "Iteration: 0019 Loss: 1.42492 Time: 0.24600\n",
      "Iteration: 0020 Loss: 1.37232 Time: 0.23425\n",
      "Iteration: 0021 Loss: 1.37386 Time: 0.23800\n",
      "Iteration: 0022 Loss: 1.37890 Time: 0.23118\n",
      "Iteration: 0023 Loss: 1.35433 Time: 0.23022\n",
      "Iteration: 0024 Loss: 1.34299 Time: 0.23595\n",
      "Iteration: 0025 Loss: 1.35366 Time: 0.23699\n",
      "Iteration: 0026 Loss: 1.32890 Time: 0.23194\n",
      "Iteration: 0027 Loss: 1.31170 Time: 0.24506\n",
      "Iteration: 0028 Loss: 1.31566 Time: 0.22701\n",
      "Iteration: 0029 Loss: 1.28654 Time: 0.24039\n",
      "Iteration: 0030 Loss: 1.29835 Time: 0.23200\n",
      "Iteration: 0031 Loss: 1.26791 Time: 0.23656\n",
      "Iteration: 0032 Loss: 1.28730 Time: 0.23449\n",
      "Iteration: 0033 Loss: 1.27113 Time: 0.24734\n",
      "Iteration: 0034 Loss: 1.24910 Time: 0.24092\n",
      "Iteration: 0035 Loss: 1.24157 Time: 0.22785\n",
      "Iteration: 0036 Loss: 1.22378 Time: 0.23102\n",
      "Iteration: 0037 Loss: 1.21400 Time: 0.24497\n",
      "Iteration: 0038 Loss: 1.22545 Time: 0.22863\n",
      "Iteration: 0039 Loss: 1.20014 Time: 0.23098\n",
      "Iteration: 0040 Loss: 1.19634 Time: 0.23235\n",
      "Iteration: 0041 Loss: 1.18645 Time: 0.23776\n",
      "Iteration: 0042 Loss: 1.16405 Time: 0.24154\n",
      "Iteration: 0043 Loss: 1.17580 Time: 0.22175\n",
      "Iteration: 0044 Loss: 1.16664 Time: 0.23691\n",
      "Iteration: 0045 Loss: 1.15518 Time: 0.22718\n",
      "Iteration: 0046 Loss: 1.13872 Time: 0.24182\n",
      "Iteration: 0047 Loss: 1.14137 Time: 0.22921\n",
      "Iteration: 0048 Loss: 1.13751 Time: 0.24647\n",
      "Iteration: 0049 Loss: 1.11626 Time: 0.24100\n",
      "Iteration: 0050 Loss: 1.12543 Time: 0.23399\n",
      "Iteration: 0051 Loss: 1.10206 Time: 0.24074\n",
      "Iteration: 0052 Loss: 1.10836 Time: 0.23641\n",
      "Iteration: 0053 Loss: 1.11069 Time: 0.23900\n",
      "Iteration: 0054 Loss: 1.09121 Time: 0.23323\n",
      "Iteration: 0055 Loss: 1.08967 Time: 0.24299\n",
      "Iteration: 0056 Loss: 1.08898 Time: 0.23414\n",
      "Iteration: 0057 Loss: 1.06303 Time: 0.23913\n",
      "Iteration: 0058 Loss: 1.07421 Time: 0.22878\n",
      "Iteration: 0059 Loss: 1.06611 Time: 0.23528\n",
      "Iteration: 0060 Loss: 1.05426 Time: 0.22695\n",
      "Iteration: 0061 Loss: 1.03997 Time: 0.23860\n",
      "Iteration: 0062 Loss: 1.03398 Time: 0.24006\n",
      "Iteration: 0063 Loss: 1.03349 Time: 0.23508\n",
      "Iteration: 0064 Loss: 1.04357 Time: 0.22933\n",
      "Iteration: 0065 Loss: 1.02816 Time: 0.23378\n",
      "Iteration: 0066 Loss: 1.02777 Time: 0.23419\n",
      "Iteration: 0067 Loss: 1.01166 Time: 0.23294\n",
      "Iteration: 0068 Loss: 1.00382 Time: 0.24092\n",
      "Iteration: 0069 Loss: 0.99927 Time: 0.23046\n",
      "Iteration: 0070 Loss: 1.01174 Time: 0.23350\n",
      "Iteration: 0071 Loss: 0.99278 Time: 0.23340\n",
      "Iteration: 0072 Loss: 0.97674 Time: 0.23630\n",
      "Iteration: 0073 Loss: 0.97842 Time: 0.23300\n",
      "Iteration: 0074 Loss: 0.96783 Time: 0.23103\n",
      "Iteration: 0075 Loss: 0.96258 Time: 0.23986\n",
      "Iteration: 0076 Loss: 0.95919 Time: 0.23688\n",
      "Iteration: 0077 Loss: 0.95837 Time: 0.23332\n",
      "Iteration: 0078 Loss: 0.94880 Time: 0.23410\n",
      "Iteration: 0079 Loss: 0.94732 Time: 0.22794\n",
      "Iteration: 0080 Loss: 0.95658 Time: 0.24064\n",
      "Iteration: 0081 Loss: 0.93981 Time: 0.24000\n",
      "Iteration: 0082 Loss: 0.93800 Time: 0.23600\n",
      "Iteration: 0083 Loss: 0.93789 Time: 0.23000\n",
      "Iteration: 0084 Loss: 0.92440 Time: 0.22852\n",
      "Iteration: 0085 Loss: 0.91789 Time: 0.22817\n",
      "Iteration: 0086 Loss: 0.90206 Time: 0.24826\n",
      "Iteration: 0087 Loss: 0.90503 Time: 0.23311\n",
      "Iteration: 0088 Loss: 0.89761 Time: 0.22952\n",
      "Iteration: 0089 Loss: 0.89490 Time: 0.23391\n",
      "Iteration: 0090 Loss: 0.88923 Time: 0.22990\n",
      "Iteration: 0091 Loss: 0.89797 Time: 0.23300\n",
      "Iteration: 0092 Loss: 0.88012 Time: 0.23991\n",
      "Iteration: 0093 Loss: 0.86418 Time: 0.22825\n",
      "Iteration: 0094 Loss: 0.86137 Time: 0.22765\n",
      "Iteration: 0095 Loss: 0.86138 Time: 0.23625\n",
      "Iteration: 0096 Loss: 0.85430 Time: 0.23177\n",
      "Iteration: 0097 Loss: 0.86006 Time: 0.23644\n",
      "Iteration: 0098 Loss: 0.85142 Time: 0.23355\n",
      "Iteration: 0099 Loss: 0.84526 Time: 0.23477\n",
      "Iteration: 0100 Loss: 0.83634 Time: 0.24591\n",
      "Iteration: 0101 Loss: 0.82454 Time: 0.23341\n",
      "Iteration: 0102 Loss: 0.82420 Time: 0.22804\n",
      "Iteration: 0103 Loss: 0.81840 Time: 0.23276\n",
      "Iteration: 0104 Loss: 0.81960 Time: 0.24477\n",
      "Iteration: 0105 Loss: 0.81478 Time: 0.23825\n",
      "Iteration: 0106 Loss: 0.82096 Time: 0.23153\n",
      "Iteration: 0107 Loss: 0.79402 Time: 0.23000\n",
      "Iteration: 0108 Loss: 0.80649 Time: 0.23442\n",
      "Iteration: 0109 Loss: 0.78571 Time: 0.23787\n",
      "Iteration: 0110 Loss: 0.79426 Time: 0.23684\n",
      "Iteration: 0111 Loss: 0.78294 Time: 0.22382\n",
      "Iteration: 0112 Loss: 0.76801 Time: 0.24214\n",
      "Iteration: 0113 Loss: 0.76332 Time: 0.24000\n",
      "Iteration: 0114 Loss: 0.77025 Time: 0.24074\n",
      "Iteration: 0115 Loss: 0.76001 Time: 0.23109\n",
      "Iteration: 0116 Loss: 0.76182 Time: 0.23400\n",
      "Iteration: 0117 Loss: 0.76316 Time: 0.24150\n",
      "Iteration: 0118 Loss: 0.75257 Time: 0.23411\n",
      "Iteration: 0119 Loss: 0.74132 Time: 0.22817\n",
      "Iteration: 0120 Loss: 0.74039 Time: 0.22890\n",
      "Iteration: 0121 Loss: 0.72975 Time: 0.23010\n",
      "Iteration: 0122 Loss: 0.73507 Time: 0.23445\n",
      "Iteration: 0123 Loss: 0.73216 Time: 0.23171\n",
      "Iteration: 0124 Loss: 0.70911 Time: 0.23415\n",
      "Iteration: 0125 Loss: 0.71940 Time: 0.23699\n",
      "Iteration: 0126 Loss: 0.70957 Time: 0.23033\n",
      "Iteration: 0127 Loss: 0.71239 Time: 0.24266\n",
      "Iteration: 0128 Loss: 0.70492 Time: 0.22604\n",
      "Iteration: 0129 Loss: 0.69695 Time: 0.23037\n",
      "Iteration: 0130 Loss: 0.70162 Time: 0.24239\n",
      "Iteration: 0131 Loss: 0.69622 Time: 0.23108\n",
      "Iteration: 0132 Loss: 0.69111 Time: 0.26300\n",
      "Iteration: 0133 Loss: 0.68784 Time: 0.24285\n",
      "Iteration: 0134 Loss: 0.68853 Time: 0.23441\n",
      "Iteration: 0135 Loss: 0.67521 Time: 0.23936\n",
      "Iteration: 0136 Loss: 0.67919 Time: 0.23027\n",
      "Iteration: 0137 Loss: 0.68126 Time: 0.23869\n",
      "Iteration: 0138 Loss: 0.67682 Time: 0.23726\n",
      "Iteration: 0139 Loss: 0.67099 Time: 0.24546\n",
      "Iteration: 0140 Loss: 0.66672 Time: 0.22881\n",
      "Iteration: 0141 Loss: 0.65632 Time: 0.23675\n",
      "Iteration: 0142 Loss: 0.65844 Time: 0.23115\n",
      "Iteration: 0143 Loss: 0.64810 Time: 0.23466\n",
      "Iteration: 0144 Loss: 0.64959 Time: 0.22636\n",
      "Iteration: 0145 Loss: 0.65001 Time: 0.23508\n",
      "Iteration: 0146 Loss: 0.64575 Time: 0.23198\n",
      "Iteration: 0147 Loss: 0.64066 Time: 0.24790\n",
      "Iteration: 0148 Loss: 0.64766 Time: 0.23697\n",
      "Iteration: 0149 Loss: 0.63773 Time: 0.23852\n",
      "Iteration: 0150 Loss: 0.63570 Time: 0.23399\n",
      "Iteration: 0151 Loss: 0.63504 Time: 0.24300\n",
      "Iteration: 0152 Loss: 0.62863 Time: 0.23447\n",
      "Iteration: 0153 Loss: 0.63217 Time: 0.22818\n",
      "Iteration: 0154 Loss: 0.62419 Time: 0.23982\n",
      "Iteration: 0155 Loss: 0.61979 Time: 0.23514\n",
      "Iteration: 0156 Loss: 0.61985 Time: 0.22824\n",
      "Iteration: 0157 Loss: 0.62011 Time: 0.24750\n",
      "Iteration: 0158 Loss: 0.62233 Time: 0.24017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0159 Loss: 0.61368 Time: 0.23128\n",
      "Iteration: 0160 Loss: 0.61050 Time: 0.23119\n",
      "Iteration: 0161 Loss: 0.61746 Time: 0.23758\n",
      "Iteration: 0162 Loss: 0.60931 Time: 0.24099\n",
      "Iteration: 0163 Loss: 0.60733 Time: 0.23600\n",
      "Iteration: 0164 Loss: 0.61303 Time: 0.22609\n",
      "Iteration: 0165 Loss: 0.60981 Time: 0.24840\n",
      "Iteration: 0166 Loss: 0.60210 Time: 0.23582\n",
      "Iteration: 0167 Loss: 0.60016 Time: 0.23249\n",
      "Iteration: 0168 Loss: 0.60279 Time: 0.23691\n",
      "Iteration: 0169 Loss: 0.60204 Time: 0.23421\n",
      "Iteration: 0170 Loss: 0.60041 Time: 0.22519\n",
      "Iteration: 0171 Loss: 0.60039 Time: 0.23605\n",
      "Iteration: 0172 Loss: 0.60171 Time: 0.24196\n",
      "Iteration: 0173 Loss: 0.59748 Time: 0.23599\n",
      "Iteration: 0174 Loss: 0.59677 Time: 0.23334\n",
      "Iteration: 0175 Loss: 0.58642 Time: 0.23290\n",
      "Iteration: 0176 Loss: 0.58956 Time: 0.23499\n",
      "Iteration: 0177 Loss: 0.58966 Time: 0.23133\n",
      "Iteration: 0178 Loss: 0.58912 Time: 0.23399\n",
      "Iteration: 0179 Loss: 0.58703 Time: 0.22617\n",
      "Iteration: 0180 Loss: 0.58788 Time: 0.23478\n",
      "Iteration: 0181 Loss: 0.58454 Time: 0.23026\n",
      "Iteration: 0182 Loss: 0.58326 Time: 0.23836\n",
      "Iteration: 0183 Loss: 0.57939 Time: 0.23525\n",
      "Iteration: 0184 Loss: 0.57930 Time: 0.23274\n",
      "Iteration: 0185 Loss: 0.57909 Time: 0.23438\n",
      "Iteration: 0186 Loss: 0.57753 Time: 0.22724\n",
      "Iteration: 0187 Loss: 0.57455 Time: 0.24148\n",
      "Iteration: 0188 Loss: 0.58494 Time: 0.22821\n",
      "Iteration: 0189 Loss: 0.57246 Time: 0.23586\n",
      "Iteration: 0190 Loss: 0.57299 Time: 0.23050\n",
      "Iteration: 0191 Loss: 0.57454 Time: 0.23177\n",
      "Iteration: 0192 Loss: 0.57166 Time: 0.24192\n",
      "Iteration: 0193 Loss: 0.57221 Time: 0.23737\n",
      "Iteration: 0194 Loss: 0.57143 Time: 0.23799\n",
      "Iteration: 0195 Loss: 0.57214 Time: 0.22726\n",
      "Iteration: 0196 Loss: 0.56732 Time: 0.24487\n",
      "Iteration: 0197 Loss: 0.56801 Time: 0.23583\n",
      "Iteration: 0198 Loss: 0.56751 Time: 0.23223\n",
      "Iteration: 0199 Loss: 0.56531 Time: 0.24195\n",
      "Iteration: 0200 Loss: 0.56632 Time: 0.24790\n",
      "Done! \n",
      "\n",
      "STEP 3/3 - MODEL EVALUATION \n",
      "\n",
      "Computing the final embedding vectors, for evaluation\n",
      "Done! \n",
      "\n",
      "Testing: link prediction\n",
      "Done! \n",
      "\n",
      "EXPERIMENTS ON MODEL 9 / 10 \n",
      "\n",
      "STEP 1/3 - PREPROCESSING STEPS \n",
      "\n",
      "Masking some edges from the training graph, for link prediction\n",
      "(validation set: 1.0 % of edges - test set: 10.0 % of edges)\n",
      "Done! \n",
      "\n",
      "Preprocessing node features\n",
      "Done! \n",
      "\n",
      "Running the Louvain algorithm for community detection\n",
      "as a preprocessing step for the encoder\n",
      "Done! Louvain has found 164 communities \n",
      "\n",
      "Setting up the model and the optimizer\n",
      "Done! \n",
      "\n",
      "Preprocessing on message passing matrices\n",
      "Done! \n",
      "\n",
      "Initializing TF session\n",
      "Done! \n",
      "\n",
      "STEP 2/3 - MODEL TRAINING \n",
      "\n",
      "Starting training\n",
      "Iteration: 0001 Loss: 1.75943 Time: 0.52519\n",
      "Iteration: 0002 Loss: 1.65993 Time: 0.24321\n",
      "Iteration: 0003 Loss: 1.69211 Time: 0.23741\n",
      "Iteration: 0004 Loss: 1.65864 Time: 0.23065\n",
      "Iteration: 0005 Loss: 1.67005 Time: 0.23260\n",
      "Iteration: 0006 Loss: 1.62825 Time: 0.23699\n",
      "Iteration: 0007 Loss: 1.60448 Time: 0.24048\n",
      "Iteration: 0008 Loss: 1.59020 Time: 0.24282\n",
      "Iteration: 0009 Loss: 1.58486 Time: 0.23443\n",
      "Iteration: 0010 Loss: 1.56026 Time: 0.23710\n",
      "Iteration: 0011 Loss: 1.54556 Time: 0.23061\n",
      "Iteration: 0012 Loss: 1.49463 Time: 0.23948\n",
      "Iteration: 0013 Loss: 1.51615 Time: 0.23777\n",
      "Iteration: 0014 Loss: 1.51025 Time: 0.23446\n",
      "Iteration: 0015 Loss: 1.47530 Time: 0.23038\n",
      "Iteration: 0016 Loss: 1.48365 Time: 0.23541\n",
      "Iteration: 0017 Loss: 1.44365 Time: 0.24699\n",
      "Iteration: 0018 Loss: 1.45654 Time: 0.23611\n",
      "Iteration: 0019 Loss: 1.41526 Time: 0.22097\n",
      "Iteration: 0020 Loss: 1.39597 Time: 0.24183\n",
      "Iteration: 0021 Loss: 1.39745 Time: 0.23499\n",
      "Iteration: 0022 Loss: 1.38555 Time: 0.24719\n",
      "Iteration: 0023 Loss: 1.38639 Time: 0.23455\n",
      "Iteration: 0024 Loss: 1.35648 Time: 0.23329\n",
      "Iteration: 0025 Loss: 1.33733 Time: 0.23900\n",
      "Iteration: 0026 Loss: 1.31608 Time: 0.23440\n",
      "Iteration: 0027 Loss: 1.32495 Time: 0.23773\n",
      "Iteration: 0028 Loss: 1.29964 Time: 0.23599\n",
      "Iteration: 0029 Loss: 1.30225 Time: 0.23056\n",
      "Iteration: 0030 Loss: 1.28344 Time: 0.23240\n",
      "Iteration: 0031 Loss: 1.27691 Time: 0.23001\n",
      "Iteration: 0032 Loss: 1.26093 Time: 0.23970\n",
      "Iteration: 0033 Loss: 1.27755 Time: 0.23089\n",
      "Iteration: 0034 Loss: 1.25948 Time: 0.23398\n",
      "Iteration: 0035 Loss: 1.23236 Time: 0.23399\n",
      "Iteration: 0036 Loss: 1.22958 Time: 0.24798\n",
      "Iteration: 0037 Loss: 1.22659 Time: 0.23489\n",
      "Iteration: 0038 Loss: 1.21726 Time: 0.23105\n",
      "Iteration: 0039 Loss: 1.19327 Time: 0.23693\n",
      "Iteration: 0040 Loss: 1.18530 Time: 0.23116\n",
      "Iteration: 0041 Loss: 1.17911 Time: 0.23359\n",
      "Iteration: 0042 Loss: 1.17806 Time: 0.23463\n",
      "Iteration: 0043 Loss: 1.18092 Time: 0.23598\n",
      "Iteration: 0044 Loss: 1.16055 Time: 0.23406\n",
      "Iteration: 0045 Loss: 1.15545 Time: 0.24247\n",
      "Iteration: 0046 Loss: 1.14013 Time: 0.22649\n",
      "Iteration: 0047 Loss: 1.14134 Time: 0.22826\n",
      "Iteration: 0048 Loss: 1.13514 Time: 0.23348\n",
      "Iteration: 0049 Loss: 1.11467 Time: 0.24441\n",
      "Iteration: 0050 Loss: 1.12136 Time: 0.23361\n",
      "Iteration: 0051 Loss: 1.10604 Time: 0.23750\n",
      "Iteration: 0052 Loss: 1.08724 Time: 0.23260\n",
      "Iteration: 0053 Loss: 1.09288 Time: 0.23707\n",
      "Iteration: 0054 Loss: 1.08417 Time: 0.23250\n",
      "Iteration: 0055 Loss: 1.07938 Time: 0.22754\n",
      "Iteration: 0056 Loss: 1.07430 Time: 0.23688\n",
      "Iteration: 0057 Loss: 1.07925 Time: 0.23799\n",
      "Iteration: 0058 Loss: 1.05816 Time: 0.23323\n",
      "Iteration: 0059 Loss: 1.07120 Time: 0.23263\n",
      "Iteration: 0060 Loss: 1.04357 Time: 0.23585\n",
      "Iteration: 0061 Loss: 1.04422 Time: 0.23614\n",
      "Iteration: 0062 Loss: 1.05316 Time: 0.22970\n",
      "Iteration: 0063 Loss: 1.02746 Time: 0.23795\n",
      "Iteration: 0064 Loss: 1.02983 Time: 0.23091\n",
      "Iteration: 0065 Loss: 1.01888 Time: 0.23599\n",
      "Iteration: 0066 Loss: 1.00762 Time: 0.23317\n",
      "Iteration: 0067 Loss: 1.00182 Time: 0.23478\n",
      "Iteration: 0068 Loss: 1.01031 Time: 0.24056\n",
      "Iteration: 0069 Loss: 1.00140 Time: 0.23702\n",
      "Iteration: 0070 Loss: 1.00752 Time: 0.23279\n",
      "Iteration: 0071 Loss: 0.98447 Time: 0.23252\n",
      "Iteration: 0072 Loss: 0.99679 Time: 0.23310\n",
      "Iteration: 0073 Loss: 0.98407 Time: 0.24555\n",
      "Iteration: 0074 Loss: 0.96940 Time: 0.23183\n",
      "Iteration: 0075 Loss: 0.97889 Time: 0.24021\n",
      "Iteration: 0076 Loss: 0.96271 Time: 0.23845\n",
      "Iteration: 0077 Loss: 0.95568 Time: 0.23009\n",
      "Iteration: 0078 Loss: 0.96118 Time: 0.24235\n",
      "Iteration: 0079 Loss: 0.95093 Time: 0.22660\n",
      "Iteration: 0080 Loss: 0.94603 Time: 0.23141\n",
      "Iteration: 0081 Loss: 0.93721 Time: 0.23008\n",
      "Iteration: 0082 Loss: 0.93655 Time: 0.22689\n",
      "Iteration: 0083 Loss: 0.93368 Time: 0.24400\n",
      "Iteration: 0084 Loss: 0.93475 Time: 0.24585\n",
      "Iteration: 0085 Loss: 0.92221 Time: 0.23320\n",
      "Iteration: 0086 Loss: 0.90114 Time: 0.23509\n",
      "Iteration: 0087 Loss: 0.88996 Time: 0.24052\n",
      "Iteration: 0088 Loss: 0.90865 Time: 0.23117\n",
      "Iteration: 0089 Loss: 0.90442 Time: 0.22600\n",
      "Iteration: 0090 Loss: 0.88575 Time: 0.24399\n",
      "Iteration: 0091 Loss: 0.89241 Time: 0.23606\n",
      "Iteration: 0092 Loss: 0.88642 Time: 0.23359\n",
      "Iteration: 0093 Loss: 0.88459 Time: 0.23921\n",
      "Iteration: 0094 Loss: 0.86521 Time: 0.23377\n",
      "Iteration: 0095 Loss: 0.86307 Time: 0.23524\n",
      "Iteration: 0096 Loss: 0.85072 Time: 0.23552\n",
      "Iteration: 0097 Loss: 0.85629 Time: 0.23466\n",
      "Iteration: 0098 Loss: 0.86022 Time: 0.23294\n",
      "Iteration: 0099 Loss: 0.84851 Time: 0.23953\n",
      "Iteration: 0100 Loss: 0.84263 Time: 0.23512\n",
      "Iteration: 0101 Loss: 0.84133 Time: 0.22857\n",
      "Iteration: 0102 Loss: 0.83555 Time: 0.23844\n",
      "Iteration: 0103 Loss: 0.82705 Time: 0.24066\n",
      "Iteration: 0104 Loss: 0.81527 Time: 0.22906\n",
      "Iteration: 0105 Loss: 0.81210 Time: 0.23353\n",
      "Iteration: 0106 Loss: 0.81071 Time: 0.22706\n",
      "Iteration: 0107 Loss: 0.81175 Time: 0.23442\n",
      "Iteration: 0108 Loss: 0.79973 Time: 0.24655\n",
      "Iteration: 0109 Loss: 0.79823 Time: 0.23331\n",
      "Iteration: 0110 Loss: 0.79072 Time: 0.23500\n",
      "Iteration: 0111 Loss: 0.78771 Time: 0.23600\n",
      "Iteration: 0112 Loss: 0.77331 Time: 0.23790\n",
      "Iteration: 0113 Loss: 0.77877 Time: 0.23568\n",
      "Iteration: 0114 Loss: 0.76393 Time: 0.22765\n",
      "Iteration: 0115 Loss: 0.76151 Time: 0.22968\n",
      "Iteration: 0116 Loss: 0.76195 Time: 0.24532\n",
      "Iteration: 0117 Loss: 0.75290 Time: 0.23500\n",
      "Iteration: 0118 Loss: 0.74054 Time: 0.23272\n",
      "Iteration: 0119 Loss: 0.74903 Time: 0.23573\n",
      "Iteration: 0120 Loss: 0.74559 Time: 0.23579\n",
      "Iteration: 0121 Loss: 0.74812 Time: 0.23303\n",
      "Iteration: 0122 Loss: 0.73012 Time: 0.23442\n",
      "Iteration: 0123 Loss: 0.72821 Time: 0.23531\n",
      "Iteration: 0124 Loss: 0.73232 Time: 0.23399\n",
      "Iteration: 0125 Loss: 0.71437 Time: 0.23131\n",
      "Iteration: 0126 Loss: 0.71806 Time: 0.23379\n",
      "Iteration: 0127 Loss: 0.70965 Time: 0.23186\n",
      "Iteration: 0128 Loss: 0.70312 Time: 0.23833\n",
      "Iteration: 0129 Loss: 0.70310 Time: 0.23121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0130 Loss: 0.70531 Time: 0.22986\n",
      "Iteration: 0131 Loss: 0.69868 Time: 0.23172\n",
      "Iteration: 0132 Loss: 0.69374 Time: 0.23069\n",
      "Iteration: 0133 Loss: 0.68665 Time: 0.23000\n",
      "Iteration: 0134 Loss: 0.68380 Time: 0.23594\n",
      "Iteration: 0135 Loss: 0.68410 Time: 0.24639\n",
      "Iteration: 0136 Loss: 0.68187 Time: 0.23200\n",
      "Iteration: 0137 Loss: 0.67341 Time: 0.24641\n",
      "Iteration: 0138 Loss: 0.67598 Time: 0.23716\n",
      "Iteration: 0139 Loss: 0.66772 Time: 0.23231\n",
      "Iteration: 0140 Loss: 0.66603 Time: 0.22681\n",
      "Iteration: 0141 Loss: 0.66304 Time: 0.23547\n",
      "Iteration: 0142 Loss: 0.65664 Time: 0.23844\n",
      "Iteration: 0143 Loss: 0.65012 Time: 0.23515\n",
      "Iteration: 0144 Loss: 0.65698 Time: 0.23177\n",
      "Iteration: 0145 Loss: 0.65234 Time: 0.23897\n",
      "Iteration: 0146 Loss: 0.65202 Time: 0.23735\n",
      "Iteration: 0147 Loss: 0.65218 Time: 0.24231\n",
      "Iteration: 0148 Loss: 0.63930 Time: 0.23953\n",
      "Iteration: 0149 Loss: 0.63894 Time: 0.23671\n",
      "Iteration: 0150 Loss: 0.64059 Time: 0.23849\n",
      "Iteration: 0151 Loss: 0.63430 Time: 0.23498\n",
      "Iteration: 0152 Loss: 0.63591 Time: 0.23800\n",
      "Iteration: 0153 Loss: 0.62752 Time: 0.23606\n",
      "Iteration: 0154 Loss: 0.63008 Time: 0.23988\n",
      "Iteration: 0155 Loss: 0.62427 Time: 0.23117\n",
      "Iteration: 0156 Loss: 0.62290 Time: 0.24044\n",
      "Iteration: 0157 Loss: 0.62331 Time: 0.23372\n",
      "Iteration: 0158 Loss: 0.61990 Time: 0.23298\n",
      "Iteration: 0159 Loss: 0.62541 Time: 0.23303\n",
      "Iteration: 0160 Loss: 0.61680 Time: 0.23761\n",
      "Iteration: 0161 Loss: 0.61251 Time: 0.22299\n",
      "Iteration: 0162 Loss: 0.61414 Time: 0.23929\n",
      "Iteration: 0163 Loss: 0.60984 Time: 0.23907\n",
      "Iteration: 0164 Loss: 0.60560 Time: 0.22861\n",
      "Iteration: 0165 Loss: 0.60394 Time: 0.23091\n",
      "Iteration: 0166 Loss: 0.60636 Time: 0.23375\n",
      "Iteration: 0167 Loss: 0.60611 Time: 0.24093\n",
      "Iteration: 0168 Loss: 0.60234 Time: 0.23434\n",
      "Iteration: 0169 Loss: 0.60126 Time: 0.23024\n",
      "Iteration: 0170 Loss: 0.60425 Time: 0.24137\n",
      "Iteration: 0171 Loss: 0.59811 Time: 0.23233\n",
      "Iteration: 0172 Loss: 0.59644 Time: 0.24162\n",
      "Iteration: 0173 Loss: 0.59949 Time: 0.22803\n",
      "Iteration: 0174 Loss: 0.58849 Time: 0.23189\n",
      "Iteration: 0175 Loss: 0.59337 Time: 0.23300\n",
      "Iteration: 0176 Loss: 0.59083 Time: 0.23564\n",
      "Iteration: 0177 Loss: 0.59448 Time: 0.23831\n",
      "Iteration: 0178 Loss: 0.59178 Time: 0.23924\n",
      "Iteration: 0179 Loss: 0.59304 Time: 0.22850\n",
      "Iteration: 0180 Loss: 0.58274 Time: 0.24128\n",
      "Iteration: 0181 Loss: 0.58713 Time: 0.23812\n",
      "Iteration: 0182 Loss: 0.58527 Time: 0.22844\n",
      "Iteration: 0183 Loss: 0.57808 Time: 0.23023\n",
      "Iteration: 0184 Loss: 0.58721 Time: 0.24066\n",
      "Iteration: 0185 Loss: 0.58565 Time: 0.24100\n",
      "Iteration: 0186 Loss: 0.58068 Time: 0.23361\n",
      "Iteration: 0187 Loss: 0.58029 Time: 0.23864\n",
      "Iteration: 0188 Loss: 0.57541 Time: 0.24163\n",
      "Iteration: 0189 Loss: 0.57445 Time: 0.22295\n",
      "Iteration: 0190 Loss: 0.57347 Time: 0.23101\n",
      "Iteration: 0191 Loss: 0.57277 Time: 0.23012\n",
      "Iteration: 0192 Loss: 0.56915 Time: 0.23419\n",
      "Iteration: 0193 Loss: 0.57112 Time: 0.23527\n",
      "Iteration: 0194 Loss: 0.56846 Time: 0.23529\n",
      "Iteration: 0195 Loss: 0.56860 Time: 0.23639\n",
      "Iteration: 0196 Loss: 0.57001 Time: 0.23304\n",
      "Iteration: 0197 Loss: 0.57090 Time: 0.24326\n",
      "Iteration: 0198 Loss: 0.56354 Time: 0.22829\n",
      "Iteration: 0199 Loss: 0.56658 Time: 0.23067\n",
      "Iteration: 0200 Loss: 0.56611 Time: 0.23271\n",
      "Done! \n",
      "\n",
      "STEP 3/3 - MODEL EVALUATION \n",
      "\n",
      "Computing the final embedding vectors, for evaluation\n",
      "Done! \n",
      "\n",
      "Testing: link prediction\n",
      "Done! \n",
      "\n",
      "EXPERIMENTS ON MODEL 10 / 10 \n",
      "\n",
      "STEP 1/3 - PREPROCESSING STEPS \n",
      "\n",
      "Masking some edges from the training graph, for link prediction\n",
      "(validation set: 1.0 % of edges - test set: 10.0 % of edges)\n",
      "Done! \n",
      "\n",
      "Preprocessing node features\n",
      "Done! \n",
      "\n",
      "Running the Louvain algorithm for community detection\n",
      "as a preprocessing step for the encoder\n",
      "Done! Louvain has found 162 communities \n",
      "\n",
      "Setting up the model and the optimizer\n",
      "Done! \n",
      "\n",
      "Preprocessing on message passing matrices\n",
      "Done! \n",
      "\n",
      "Initializing TF session\n",
      "Done! \n",
      "\n",
      "STEP 2/3 - MODEL TRAINING \n",
      "\n",
      "Starting training\n",
      "Iteration: 0001 Loss: 1.74166 Time: 0.54891\n",
      "Iteration: 0002 Loss: 1.74846 Time: 0.24213\n",
      "Iteration: 0003 Loss: 1.70673 Time: 0.23473\n",
      "Iteration: 0004 Loss: 1.63729 Time: 0.24034\n",
      "Iteration: 0005 Loss: 1.68376 Time: 0.23720\n",
      "Iteration: 0006 Loss: 1.64633 Time: 0.24303\n",
      "Iteration: 0007 Loss: 1.59485 Time: 0.23156\n",
      "Iteration: 0008 Loss: 1.57823 Time: 0.23169\n",
      "Iteration: 0009 Loss: 1.57188 Time: 0.23942\n",
      "Iteration: 0010 Loss: 1.57872 Time: 0.24581\n",
      "Iteration: 0011 Loss: 1.53241 Time: 0.23591\n",
      "Iteration: 0012 Loss: 1.50541 Time: 0.24777\n",
      "Iteration: 0013 Loss: 1.48397 Time: 0.23730\n",
      "Iteration: 0014 Loss: 1.52233 Time: 0.24194\n",
      "Iteration: 0015 Loss: 1.47382 Time: 0.23261\n",
      "Iteration: 0016 Loss: 1.44936 Time: 0.23642\n",
      "Iteration: 0017 Loss: 1.42241 Time: 0.24571\n",
      "Iteration: 0018 Loss: 1.44094 Time: 0.24065\n",
      "Iteration: 0019 Loss: 1.41710 Time: 0.23998\n",
      "Iteration: 0020 Loss: 1.40617 Time: 0.23940\n",
      "Iteration: 0021 Loss: 1.38742 Time: 0.23403\n",
      "Iteration: 0022 Loss: 1.37132 Time: 0.24571\n",
      "Iteration: 0023 Loss: 1.34592 Time: 0.22860\n",
      "Iteration: 0024 Loss: 1.34686 Time: 0.23501\n",
      "Iteration: 0025 Loss: 1.36570 Time: 0.23776\n",
      "Iteration: 0026 Loss: 1.34988 Time: 0.23393\n",
      "Iteration: 0027 Loss: 1.33697 Time: 0.23999\n",
      "Iteration: 0028 Loss: 1.32105 Time: 0.23765\n",
      "Iteration: 0029 Loss: 1.30003 Time: 0.23191\n",
      "Iteration: 0030 Loss: 1.30128 Time: 0.23654\n",
      "Iteration: 0031 Loss: 1.28351 Time: 0.24063\n",
      "Iteration: 0032 Loss: 1.25683 Time: 0.23360\n",
      "Iteration: 0033 Loss: 1.25845 Time: 0.23591\n",
      "Iteration: 0034 Loss: 1.25864 Time: 0.23500\n",
      "Iteration: 0035 Loss: 1.24130 Time: 0.24116\n",
      "Iteration: 0036 Loss: 1.22565 Time: 0.23608\n",
      "Iteration: 0037 Loss: 1.21051 Time: 0.24516\n",
      "Iteration: 0038 Loss: 1.22029 Time: 0.23340\n",
      "Iteration: 0039 Loss: 1.23358 Time: 0.24196\n",
      "Iteration: 0040 Loss: 1.18478 Time: 0.23408\n",
      "Iteration: 0041 Loss: 1.16678 Time: 0.23828\n",
      "Iteration: 0042 Loss: 1.16567 Time: 0.24004\n",
      "Iteration: 0043 Loss: 1.18645 Time: 0.23884\n",
      "Iteration: 0044 Loss: 1.16528 Time: 0.24224\n",
      "Iteration: 0045 Loss: 1.15847 Time: 0.24087\n",
      "Iteration: 0046 Loss: 1.15247 Time: 0.23972\n",
      "Iteration: 0047 Loss: 1.12750 Time: 0.23999\n",
      "Iteration: 0048 Loss: 1.13414 Time: 0.23878\n",
      "Iteration: 0049 Loss: 1.12798 Time: 0.23336\n",
      "Iteration: 0050 Loss: 1.12046 Time: 0.23622\n",
      "Iteration: 0051 Loss: 1.12141 Time: 0.23408\n",
      "Iteration: 0052 Loss: 1.10841 Time: 0.23901\n",
      "Iteration: 0053 Loss: 1.11004 Time: 0.23757\n",
      "Iteration: 0054 Loss: 1.09885 Time: 0.24060\n",
      "Iteration: 0055 Loss: 1.06451 Time: 0.23932\n",
      "Iteration: 0056 Loss: 1.07729 Time: 0.22947\n",
      "Iteration: 0057 Loss: 1.06491 Time: 0.24787\n",
      "Iteration: 0058 Loss: 1.06556 Time: 0.22204\n",
      "Iteration: 0059 Loss: 1.06708 Time: 0.23919\n",
      "Iteration: 0060 Loss: 1.05538 Time: 0.23834\n",
      "Iteration: 0061 Loss: 1.04459 Time: 0.22955\n",
      "Iteration: 0062 Loss: 1.03471 Time: 0.24103\n",
      "Iteration: 0063 Loss: 1.04028 Time: 0.23674\n",
      "Iteration: 0064 Loss: 1.02250 Time: 0.24369\n",
      "Iteration: 0065 Loss: 1.02319 Time: 0.23530\n",
      "Iteration: 0066 Loss: 1.02991 Time: 0.23518\n",
      "Iteration: 0067 Loss: 1.01332 Time: 0.24542\n",
      "Iteration: 0068 Loss: 1.00633 Time: 0.23572\n",
      "Iteration: 0069 Loss: 1.00909 Time: 0.23607\n",
      "Iteration: 0070 Loss: 1.00786 Time: 0.24564\n",
      "Iteration: 0071 Loss: 0.98476 Time: 0.23578\n",
      "Iteration: 0072 Loss: 1.00501 Time: 0.23801\n",
      "Iteration: 0073 Loss: 0.98457 Time: 0.24113\n",
      "Iteration: 0074 Loss: 0.98853 Time: 0.23413\n",
      "Iteration: 0075 Loss: 0.96664 Time: 0.23866\n",
      "Iteration: 0076 Loss: 0.96960 Time: 0.23119\n",
      "Iteration: 0077 Loss: 0.96956 Time: 0.23900\n",
      "Iteration: 0078 Loss: 0.96562 Time: 0.23215\n",
      "Iteration: 0079 Loss: 0.94835 Time: 0.24530\n",
      "Iteration: 0080 Loss: 0.93753 Time: 0.23980\n",
      "Iteration: 0081 Loss: 0.94280 Time: 0.23398\n",
      "Iteration: 0082 Loss: 0.93049 Time: 0.23785\n",
      "Iteration: 0083 Loss: 0.93127 Time: 0.23540\n",
      "Iteration: 0084 Loss: 0.92586 Time: 0.23840\n",
      "Iteration: 0085 Loss: 0.92624 Time: 0.23819\n",
      "Iteration: 0086 Loss: 0.91997 Time: 0.23920\n",
      "Iteration: 0087 Loss: 0.90769 Time: 0.23773\n",
      "Iteration: 0088 Loss: 0.89583 Time: 0.23330\n",
      "Iteration: 0089 Loss: 0.89626 Time: 0.23301\n",
      "Iteration: 0090 Loss: 0.90124 Time: 0.23636\n",
      "Iteration: 0091 Loss: 0.88645 Time: 0.23299\n",
      "Iteration: 0092 Loss: 0.88770 Time: 0.23662\n",
      "Iteration: 0093 Loss: 0.87984 Time: 0.23506\n",
      "Iteration: 0094 Loss: 0.87140 Time: 0.23947\n",
      "Iteration: 0095 Loss: 0.87176 Time: 0.23430\n",
      "Iteration: 0096 Loss: 0.86644 Time: 0.23562\n",
      "Iteration: 0097 Loss: 0.85707 Time: 0.24700\n",
      "Iteration: 0098 Loss: 0.85006 Time: 0.24004\n",
      "Iteration: 0099 Loss: 0.85739 Time: 0.22570\n",
      "Iteration: 0100 Loss: 0.85637 Time: 0.23553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0101 Loss: 0.85076 Time: 0.23800\n",
      "Iteration: 0102 Loss: 0.82708 Time: 0.23783\n",
      "Iteration: 0103 Loss: 0.83239 Time: 0.22700\n",
      "Iteration: 0104 Loss: 0.82354 Time: 0.24801\n",
      "Iteration: 0105 Loss: 0.81746 Time: 0.23720\n",
      "Iteration: 0106 Loss: 0.82467 Time: 0.23400\n",
      "Iteration: 0107 Loss: 0.81408 Time: 0.23115\n",
      "Iteration: 0108 Loss: 0.80351 Time: 0.23874\n",
      "Iteration: 0109 Loss: 0.79628 Time: 0.24700\n",
      "Iteration: 0110 Loss: 0.79636 Time: 0.23810\n",
      "Iteration: 0111 Loss: 0.78785 Time: 0.24023\n",
      "Iteration: 0112 Loss: 0.79513 Time: 0.24207\n",
      "Iteration: 0113 Loss: 0.77980 Time: 0.24280\n",
      "Iteration: 0114 Loss: 0.77517 Time: 0.23259\n",
      "Iteration: 0115 Loss: 0.77550 Time: 0.23700\n",
      "Iteration: 0116 Loss: 0.76964 Time: 0.23752\n",
      "Iteration: 0117 Loss: 0.76386 Time: 0.23842\n",
      "Iteration: 0118 Loss: 0.76083 Time: 0.24018\n",
      "Iteration: 0119 Loss: 0.75179 Time: 0.23968\n",
      "Iteration: 0120 Loss: 0.75672 Time: 0.23785\n",
      "Iteration: 0121 Loss: 0.73426 Time: 0.23199\n",
      "Iteration: 0122 Loss: 0.74046 Time: 0.24883\n",
      "Iteration: 0123 Loss: 0.73736 Time: 0.25400\n",
      "Iteration: 0124 Loss: 0.72841 Time: 0.23675\n",
      "Iteration: 0125 Loss: 0.72325 Time: 0.23250\n",
      "Iteration: 0126 Loss: 0.73522 Time: 0.23915\n",
      "Iteration: 0127 Loss: 0.70657 Time: 0.24328\n",
      "Iteration: 0128 Loss: 0.72263 Time: 0.24431\n",
      "Iteration: 0129 Loss: 0.71168 Time: 0.23232\n",
      "Iteration: 0130 Loss: 0.70654 Time: 0.23928\n",
      "Iteration: 0131 Loss: 0.68971 Time: 0.24023\n",
      "Iteration: 0132 Loss: 0.69235 Time: 0.23833\n",
      "Iteration: 0133 Loss: 0.69827 Time: 0.22930\n",
      "Iteration: 0134 Loss: 0.68143 Time: 0.23299\n",
      "Iteration: 0135 Loss: 0.68107 Time: 0.23409\n",
      "Iteration: 0136 Loss: 0.68268 Time: 0.24001\n",
      "Iteration: 0137 Loss: 0.68262 Time: 0.22965\n",
      "Iteration: 0138 Loss: 0.67065 Time: 0.24299\n",
      "Iteration: 0139 Loss: 0.67541 Time: 0.24000\n",
      "Iteration: 0140 Loss: 0.66888 Time: 0.23100\n",
      "Iteration: 0141 Loss: 0.65984 Time: 0.22861\n",
      "Iteration: 0142 Loss: 0.66263 Time: 0.24765\n",
      "Iteration: 0143 Loss: 0.65544 Time: 0.23652\n",
      "Iteration: 0144 Loss: 0.66187 Time: 0.23818\n",
      "Iteration: 0145 Loss: 0.65943 Time: 0.23702\n",
      "Iteration: 0146 Loss: 0.65612 Time: 0.23239\n",
      "Iteration: 0147 Loss: 0.64926 Time: 0.24658\n",
      "Iteration: 0148 Loss: 0.64212 Time: 0.23983\n",
      "Iteration: 0149 Loss: 0.64629 Time: 0.23531\n",
      "Iteration: 0150 Loss: 0.63886 Time: 0.22985\n",
      "Iteration: 0151 Loss: 0.64185 Time: 0.24140\n",
      "Iteration: 0152 Loss: 0.63590 Time: 0.23715\n",
      "Iteration: 0153 Loss: 0.63101 Time: 0.23561\n",
      "Iteration: 0154 Loss: 0.62794 Time: 0.23500\n",
      "Iteration: 0155 Loss: 0.62939 Time: 0.23411\n",
      "Iteration: 0156 Loss: 0.62766 Time: 0.24117\n",
      "Iteration: 0157 Loss: 0.62190 Time: 0.24046\n",
      "Iteration: 0158 Loss: 0.62050 Time: 0.23429\n",
      "Iteration: 0159 Loss: 0.61958 Time: 0.23945\n",
      "Iteration: 0160 Loss: 0.61858 Time: 0.23357\n",
      "Iteration: 0161 Loss: 0.61276 Time: 0.23365\n",
      "Iteration: 0162 Loss: 0.61631 Time: 0.23501\n",
      "Iteration: 0163 Loss: 0.61161 Time: 0.24200\n",
      "Iteration: 0164 Loss: 0.60763 Time: 0.23140\n",
      "Iteration: 0165 Loss: 0.60975 Time: 0.23549\n",
      "Iteration: 0166 Loss: 0.60578 Time: 0.22971\n",
      "Iteration: 0167 Loss: 0.60514 Time: 0.23999\n",
      "Iteration: 0168 Loss: 0.60198 Time: 0.23600\n",
      "Iteration: 0169 Loss: 0.60797 Time: 0.23541\n",
      "Iteration: 0170 Loss: 0.59819 Time: 0.23535\n",
      "Iteration: 0171 Loss: 0.59733 Time: 0.24132\n",
      "Iteration: 0172 Loss: 0.59590 Time: 0.23800\n",
      "Iteration: 0173 Loss: 0.59580 Time: 0.24419\n",
      "Iteration: 0174 Loss: 0.59085 Time: 0.23197\n",
      "Iteration: 0175 Loss: 0.59814 Time: 0.22489\n",
      "Iteration: 0176 Loss: 0.58966 Time: 0.23583\n",
      "Iteration: 0177 Loss: 0.59741 Time: 0.24707\n",
      "Iteration: 0178 Loss: 0.59371 Time: 0.23960\n",
      "Iteration: 0179 Loss: 0.58504 Time: 0.23400\n",
      "Iteration: 0180 Loss: 0.58732 Time: 0.23622\n",
      "Iteration: 0181 Loss: 0.58580 Time: 0.23948\n",
      "Iteration: 0182 Loss: 0.58719 Time: 0.24384\n",
      "Iteration: 0183 Loss: 0.58163 Time: 0.23671\n",
      "Iteration: 0184 Loss: 0.58079 Time: 0.23055\n",
      "Iteration: 0185 Loss: 0.58348 Time: 0.23798\n",
      "Iteration: 0186 Loss: 0.57761 Time: 0.23970\n",
      "Iteration: 0187 Loss: 0.57619 Time: 0.23700\n",
      "Iteration: 0188 Loss: 0.57806 Time: 0.23713\n",
      "Iteration: 0189 Loss: 0.57347 Time: 0.23901\n",
      "Iteration: 0190 Loss: 0.57201 Time: 0.23966\n",
      "Iteration: 0191 Loss: 0.57787 Time: 0.22768\n",
      "Iteration: 0192 Loss: 0.57657 Time: 0.23839\n",
      "Iteration: 0193 Loss: 0.57321 Time: 0.23507\n",
      "Iteration: 0194 Loss: 0.57282 Time: 0.24458\n",
      "Iteration: 0195 Loss: 0.57067 Time: 0.24387\n",
      "Iteration: 0196 Loss: 0.57012 Time: 0.23793\n",
      "Iteration: 0197 Loss: 0.56613 Time: 0.23400\n",
      "Iteration: 0198 Loss: 0.56784 Time: 0.24374\n",
      "Iteration: 0199 Loss: 0.56940 Time: 0.23747\n",
      "Iteration: 0200 Loss: 0.56696 Time: 0.23854\n",
      "Done! \n",
      "\n",
      "STEP 3/3 - MODEL EVALUATION \n",
      "\n",
      "Computing the final embedding vectors, for evaluation\n",
      "Done! \n",
      "\n",
      "Testing: link prediction\n",
      "Done! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We repeat the entire training+test process FLAGS.nb_run times\n",
    "for i in range(FLAGS.nb_run):\n",
    "    if FLAGS.verbose:\n",
    "        print(\"EXPERIMENTS ON MODEL\", i + 1, \"/\", FLAGS.nb_run, \"\\n\")\n",
    "        print(\"STEP 1/3 - PREPROCESSING STEPS \\n\")\n",
    "\n",
    "\n",
    "    # Edge masking for Link Prediction:\n",
    "    if FLAGS.task == 'task_2':\n",
    "        # Compute Train/Validation/Test sets\n",
    "        if FLAGS.verbose:\n",
    "            print(\"Masking some edges from the training graph, for link prediction\")\n",
    "            print(\"(validation set:\", FLAGS.prop_val, \"% of edges - test set:\",\n",
    "                  FLAGS.prop_test, \"% of edges)\")\n",
    "        #adj, val_edges, val_edges_false, test_edges, test_edges_false = mask_test_edges(adj_init, FLAGS.prop_test, FLAGS.prop_val)\n",
    "        adj, test_edges, test_edges_false = mask_test_edges(adj_init, FLAGS.prop_test)\n",
    "        if FLAGS.verbose:\n",
    "            print(\"Done! \\n\")\n",
    "    else:\n",
    "        adj = adj_init\n",
    "\n",
    "    # Compute the number of nodes\n",
    "    num_nodes = adj.shape[0]\n",
    "\n",
    "    # Preprocessing on node features\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Preprocessing node features\")\n",
    "    if FLAGS.features:\n",
    "        features = features_init\n",
    "    else:\n",
    "        # If features are not used, replace feature matrix by identity matrix\n",
    "        features = sp.identity(num_nodes)\n",
    "    features = sparse_to_tuple(features)\n",
    "    num_features = features[2][1]\n",
    "    features_nonzero = features[1].shape[0]\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Done! \\n\")\n",
    "\n",
    "\n",
    "    # Community detection using Louvain, as a preprocessing step\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Running the Louvain algorithm for community detection\")\n",
    "        print(\"as a preprocessing step for the encoder\")\n",
    "    # Get binary community matrix (adj_louvain_init[i,j] = 1 if nodes i and j are\n",
    "    # in the same community) as well as number of communities found by Louvain\n",
    "    adj_louvain_init, nb_communities_louvain, partition = louvain_clustering(adj, FLAGS.s_reg)\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Done! Louvain has found\", nb_communities_louvain, \"communities \\n\")\n",
    "\n",
    "    # FastGAE: node sampling for stochastic subgraph decoding - used when facing scalability issues\n",
    "    if FLAGS.fastgae:\n",
    "        if FLAGS.verbose:\n",
    "            print(\"Preprocessing operations related to the FastGAE method\")\n",
    "        # Node-level p_i degree-based, core-based or uniform distribution\n",
    "        node_distribution = get_distribution(FLAGS.measure, FLAGS.alpha, adj)\n",
    "        # Node sampling for initializations\n",
    "        sampled_nodes, adj_label, adj_sampled_sparse = node_sampling(adj, node_distribution, FLAGS.nb_node_samples, FLAGS.replace)\n",
    "        if FLAGS.verbose:\n",
    "            print(\"Done! \\n\")\n",
    "    else:\n",
    "        sampled_nodes = np.array(range(FLAGS.nb_node_samples))\n",
    "\n",
    "\n",
    "    # Placeholders\n",
    "    if FLAGS.verbose:\n",
    "        print('Setting up the model and the optimizer')\n",
    "    placeholders = {\n",
    "        'features': tf.sparse_placeholder(tf.float32),\n",
    "        'adj': tf.sparse_placeholder(tf.float32),\n",
    "        'adj_layer2': tf.sparse_placeholder(tf.float32), # Only used for 2-layer GCN encoders\n",
    "        'degree_matrix': tf.sparse_placeholder(tf.float32),\n",
    "        'adj_orig': tf.sparse_placeholder(tf.float32),\n",
    "        'dropout': tf.placeholder_with_default(0., shape = ()),\n",
    "        'sampled_nodes': tf.placeholder_with_default(sampled_nodes, shape = [FLAGS.nb_node_samples])\n",
    "    }\n",
    "\n",
    "\n",
    "    # Create model\n",
    "    if FLAGS.model == 'linear_ae':\n",
    "        # Linear Graph Autoencoder\n",
    "        model = LinearModelAE(placeholders, num_features, features_nonzero)\n",
    "    elif FLAGS.model == 'linear_vae':\n",
    "        # Linear Graph Variational Autoencoder\n",
    "        model = LinearModelVAE(placeholders, num_features, num_nodes, features_nonzero)\n",
    "    elif FLAGS.model == 'gcn_ae':\n",
    "        # 2-layer GCN Graph Autoencoder\n",
    "        model = GCNModelAE(placeholders, num_features, features_nonzero)\n",
    "    elif FLAGS.model == 'gcn_vae':\n",
    "        # 2-layer GCN Graph Variational Autoencoder\n",
    "        model = GCNModelVAE(placeholders, num_features, num_nodes, features_nonzero)\n",
    "    else:\n",
    "        raise ValueError('Undefined model!')\n",
    "\n",
    "\n",
    "    # Optimizer\n",
    "    if FLAGS.fastgae:\n",
    "        num_sampled = adj_sampled_sparse.shape[0]\n",
    "        sum_sampled = adj_sampled_sparse.sum()\n",
    "        pos_weight = float(num_sampled * num_sampled - sum_sampled) / sum_sampled\n",
    "        norm = num_sampled * num_sampled / float((num_sampled * num_sampled - sum_sampled) * 2)\n",
    "    else:\n",
    "        pos_weight = float(num_nodes * num_nodes - adj.sum()) / adj.sum()\n",
    "        norm = num_nodes * num_nodes / float((num_nodes * num_nodes - adj.sum()) * 2)\n",
    "\n",
    "    if FLAGS.model in ('gcn_ae', 'linear_ae'):\n",
    "        opt = OptimizerAE(preds = model.reconstructions,\n",
    "                          labels = tf.reshape(tf.sparse_tensor_to_dense(placeholders['adj_orig'], validate_indices = False), [-1]),\n",
    "                          degree_matrix = tf.reshape(tf.sparse_tensor_to_dense(placeholders['degree_matrix'], validate_indices = False), [-1]),\n",
    "                          num_nodes = num_nodes,\n",
    "                          pos_weight = pos_weight,\n",
    "                          norm = norm,\n",
    "                          clusters_distance = model.clusters)\n",
    "    else:\n",
    "        opt = OptimizerVAE(preds = model.reconstructions,\n",
    "                           labels = tf.reshape(tf.sparse_tensor_to_dense(placeholders['adj_orig'], validate_indices = False), [-1]),\n",
    "                           degree_matrix = tf.reshape(tf.sparse_tensor_to_dense(placeholders['degree_matrix'], validate_indices = False), [-1]),\n",
    "                           model = model,\n",
    "                           num_nodes = num_nodes,\n",
    "                           pos_weight = pos_weight,\n",
    "                           norm = norm,\n",
    "                           clusters_distance = model.clusters)\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Done! \\n\")\n",
    "\n",
    "\n",
    "    # Symmetrically normalized \"message passing\" matrices\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Preprocessing on message passing matrices\")\n",
    "    adj_norm = preprocess_graph(adj + FLAGS.lamb*adj_louvain_init)\n",
    "    adj_norm_layer2 = preprocess_graph(adj)\n",
    "    if not FLAGS.fastgae:\n",
    "        adj_label = sparse_to_tuple(adj + sp.eye(num_nodes))\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Done! \\n\")\n",
    "\n",
    "\n",
    "    # Degree matrices\n",
    "    deg_matrix, deg_matrix_init = preprocess_degree(adj, FLAGS.simple)\n",
    "\n",
    "\n",
    "    # Initialize TF session\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Initializing TF session\")\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Done! \\n\")\n",
    "\n",
    "\n",
    "    # Model training\n",
    "    if FLAGS.verbose:\n",
    "        print(\"STEP 2/3 - MODEL TRAINING \\n\")\n",
    "        print(\"Starting training\")\n",
    "\n",
    "    for iter in range(FLAGS.iterations):\n",
    "\n",
    "        # Flag to compute running time for each iteration\n",
    "        t = time.time()\n",
    "\n",
    "        # Construct feed dictionary\n",
    "        feed_dict = construct_feed_dict(adj_norm, adj_norm_layer2, adj_label, features, deg_matrix, placeholders)\n",
    "        if FLAGS.fastgae:\n",
    "            # Update sampled subgraph and sampled Louvain matrix\n",
    "            feed_dict.update({placeholders['sampled_nodes']: sampled_nodes})\n",
    "            # New node sampling\n",
    "            sampled_nodes, adj_label, adj_label_sparse, = node_sampling(adj, node_distribution, FLAGS.nb_node_samples, FLAGS.replace)\n",
    "\n",
    "        # Weights update\n",
    "        outs = sess.run([opt.opt_op, opt.cost, opt.cost_adj, opt.cost_mod],\n",
    "                        feed_dict = feed_dict)\n",
    "\n",
    "        # Compute average loss\n",
    "        avg_cost = outs[1]\n",
    "        if FLAGS.verbose:\n",
    "            # Display information on the iteration\n",
    "            print(\"Iteration:\", '%04d' % (iter + 1), \"Loss:\", \"{:.5f}\".format(avg_cost),\n",
    "                  \"Time:\", \"{:.5f}\".format(time.time() - t))\n",
    "\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Done! \\n\")\n",
    "\n",
    "    # Compute embedding vectors, for evaluation\n",
    "    if FLAGS.verbose:\n",
    "        print(\"STEP 3/3 - MODEL EVALUATION \\n\")\n",
    "        print(\"Computing the final embedding vectors, for evaluation\")\n",
    "    emb = sess.run(model.z_mean, feed_dict = feed_dict)\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Done! \\n\")\n",
    "\n",
    "    # Test model: link prediction (classification edges/non-edges)\n",
    "\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Testing: link prediction\")\n",
    "    # Get ROC and AP scores\n",
    "    roc_score, ap_score = link_prediction(test_edges, test_edges_false, emb)\n",
    "    mean_roc.append(roc_score)\n",
    "    mean_ap.append(ap_score)\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Done! \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bef29d76",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-28T08:26:18.631201Z",
     "start_time": "2022-10-28T08:26:18.613203Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL RESULTS \n",
      "\n",
      "Recall: the selected task was \"Task 2\", i.e., joint community detection and link prediction, on cora\n",
      "All scores reported below are computed over the 10 run(s) \n",
      "\n",
      "Link prediction:\n",
      "\n",
      "Mean AUC score:  0.8647361276640178\n",
      "Std of AUC scores:  0.010520747321905303 \n",
      "\n",
      "Mean AP score:  0.8935980436036692\n",
      "Std of AP scores:  0.008768829726087371 \n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Report final results\n",
    "print(\"FINAL RESULTS \\n\")\n",
    "\n",
    "print('Recall: the selected task was \"Task 2\", i.e., joint community detection and link prediction, on', FLAGS.dataset)\n",
    "\n",
    "print(\"All scores reported below are computed over the\", FLAGS.nb_run, \"run(s) \\n\")\n",
    "\n",
    "print(\"Link prediction:\\n\")\n",
    "print(\"Mean AUC score: \", np.mean(mean_roc))\n",
    "print(\"Std of AUC scores: \", np.std(mean_roc), \"\\n\")\n",
    "\n",
    "print(\"Mean AP score: \", np.mean(mean_ap))\n",
    "print(\"Std of AP scores: \", np.std(mean_ap), \"\\n \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553e6eb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf1.15",
   "language": "python",
   "name": "tf1.15"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
