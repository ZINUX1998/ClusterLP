{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67dbcca0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-28T08:37:43.863988Z",
     "start_time": "2022-10-28T08:37:39.333650Z"
    }
   },
   "outputs": [],
   "source": [
    "from modularity_aware_gae.evaluation import community_detection, link_prediction\n",
    "from modularity_aware_gae.input_data import load_data, load_labels\n",
    "from modularity_aware_gae.louvain import louvain_clustering\n",
    "from modularity_aware_gae.model import  GCNModelAE, GCNModelVAE, LinearModelAE, LinearModelVAE\n",
    "from modularity_aware_gae.optimizer import OptimizerAE, OptimizerVAE\n",
    "from modularity_aware_gae.preprocessing import *\n",
    "from modularity_aware_gae.sampling import get_distribution, node_sampling\n",
    "import numpy as np\n",
    "import os\n",
    "import scipy.sparse as sp\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel')#添加的，不报错\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14ffd620",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-28T08:37:54.204414Z",
     "start_time": "2022-10-28T08:37:54.193444Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " \n",
      " \n",
      "[MODULARITY-AWARE GRAPH AUTOENCODERS]\n",
      " \n",
      " \n",
      " \n",
      "\n",
      "EXPERIMENTAL SETTING \n",
      "\n",
      "- Graph dataset: citeseer\n",
      "- Mode name: linear_vae\n",
      "- Number of models to train: 10\n",
      "- Number of training iterations for each model: 200\n",
      "- Learning rate: 0.01\n",
      "- Dropout rate: 0.0\n",
      "- Use of node features in the input layer: False\n",
      "- Dimension of the output layer: 16\n",
      "- lambda: 0.0\n",
      "- beta: 0.0\n",
      "- gamma: 1.0\n",
      "- s: 2\n",
      "- FastGAE: no \n",
      "\n",
      "Final embedding vectors will be evaluated on:\n",
      "- Task 2, i.e., joint community detection and link prediction\n",
      "\n",
      " \n",
      " \n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "# Parameters related to the input data\n",
    "flags.DEFINE_string('dataset', 'citeseer', 'Graph dataset, among: cora, citeseer, wiki, celegans, email, polbooks, texas, wisconsin')\n",
    "\n",
    "flags.DEFINE_boolean('features', False, 'Whether to include node features')\n",
    "\n",
    "\n",
    "# Parameters related to the Modularity-Aware GAE/VGAE model to train\n",
    "\n",
    "# 1/3 - General parameters associated with GAE/VGAE\n",
    "flags.DEFINE_string('model', 'linear_vae', 'Model to train, among: gcn_ae, gcn_vae, \\\n",
    "                                            linear_ae, linear_vae')\n",
    "flags.DEFINE_float('dropout', 0., 'Dropout rate')\n",
    "flags.DEFINE_integer('iterations', 200, 'Number of iterations in training')\n",
    "flags.DEFINE_float('learning_rate', 0.01, 'Initial learning rate (Adam)')\n",
    "flags.DEFINE_integer('hidden', 32, 'Dimension of the GCN hidden layer')\n",
    "flags.DEFINE_integer('dimension', 16, 'Dimension of the output layer, i.e., \\\n",
    "                                       dimension of the embedding space')\n",
    "\n",
    "# 2/3 - Additional parameters, specific to Modularity-Aware models\n",
    "flags.DEFINE_float('beta', 0.0, 'Beta hyperparameter of Mod.-Aware models')\n",
    "flags.DEFINE_float('lamb', 0.0, 'Lambda hyperparameter of Mod.-Aware models')\n",
    "flags.DEFINE_float('gamma', 1.0, 'Gamma hyperparameter of Mod.-Aware models')\n",
    "flags.DEFINE_integer('s_reg', 2, 's hyperparameter of Mod.-Aware models')\n",
    "\n",
    "# 3/3 - Additional parameters, aiming to improve scalability\n",
    "flags.DEFINE_boolean('fastgae', False, 'Whether to use the FastGAE framework')\n",
    "flags.DEFINE_integer('nb_node_samples', 1000, 'In FastGAE, number of nodes to \\\n",
    "                                               sample at each iteration, i.e., \\\n",
    "                                               size of decoded subgraph')\n",
    "flags.DEFINE_string('measure', 'degree', 'In FastGAE, node importance measure used \\\n",
    "                                          for sampling: degree, core, or uniform')\n",
    "flags.DEFINE_float('alpha', 1.0, 'alpha hyperparameter associated with the FastGAE sampling')\n",
    "flags.DEFINE_boolean('replace', False, 'Sample nodes with or without replacement')\n",
    "flags.DEFINE_boolean('simple', True, 'Use simpler (and faster) modularity in optimizers')\n",
    "\n",
    "\n",
    "# Parameters related to the experimental setup\n",
    "flags.DEFINE_string('task', 'task_2', 'task_1: pure community detection \\\n",
    "                                       task_2: joint link prediction and \\\n",
    "                                               community detection')\n",
    "flags.DEFINE_integer('nb_run', 10, 'Number of model run + test')\n",
    "flags.DEFINE_float('prop_val', 1., 'Proportion of edges in validation set \\\n",
    "                                    for the link prediction task')\n",
    "flags.DEFINE_float('prop_test', 10., 'Proportion of edges in test set \\\n",
    "                                      for the link prediction task')\n",
    "flags.DEFINE_boolean('validation', False, 'Whether to compute validation \\\n",
    "                                           results at each iteration, for \\\n",
    "                                           the link prediction task')\n",
    "flags.DEFINE_boolean('verbose', True, 'Whether to print all comments details')\n",
    "\n",
    "\n",
    "# Introductory message\n",
    "if FLAGS.verbose:\n",
    "    introductory_message()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ce16ae4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-28T08:37:55.181583Z",
     "start_time": "2022-10-28T08:37:55.137065Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADING DATA\n",
      "\n",
      "Loading the citeseer graph\n",
      "- Number of nodes: 3279\n",
      "- Use of node features: False\n",
      "Done! \n",
      " \n",
      " \n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize lists to collect final results\n",
    "mean_ami = []\n",
    "mean_ari = []\n",
    "\n",
    "mean_roc = []\n",
    "mean_ap = []\n",
    "\n",
    "# Load data\n",
    "if FLAGS.verbose:\n",
    "    print(\"LOADING DATA\\n\")\n",
    "    print(\"Loading the\", FLAGS.dataset, \"graph\")\n",
    "adj_init, features_init = load_data(FLAGS.dataset)\n",
    "#labels = load_labels(adj_init.shape[0])\n",
    "if FLAGS.verbose:\n",
    "    print(\"- Number of nodes:\", adj_init.shape[0])\n",
    "    #print(\"- Number of communities:\", len(np.unique(labels)))\n",
    "    print(\"- Use of node features:\", FLAGS.features)\n",
    "    print(\"Done! \\n \\n \\n \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd19b933",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-28T08:50:09.145671Z",
     "start_time": "2022-10-28T08:37:56.657318Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENTS ON MODEL 1 / 10 \n",
      "\n",
      "STEP 1/3 - PREPROCESSING STEPS \n",
      "\n",
      "Masking some edges from the training graph, for link prediction\n",
      "(validation set: 1.0 % of edges - test set: 10.0 % of edges)\n",
      "Done! \n",
      "\n",
      "Preprocessing node features\n",
      "Done! \n",
      "\n",
      "Running the Louvain algorithm for community detection\n",
      "as a preprocessing step for the encoder\n",
      "Done! Louvain has found 576 communities \n",
      "\n",
      "Setting up the model and the optimizer\n",
      "Done! \n",
      "\n",
      "Preprocessing on message passing matrices\n",
      "Done! \n",
      "\n",
      "Initializing TF session\n",
      "Done! \n",
      "\n",
      "STEP 2/3 - MODEL TRAINING \n",
      "\n",
      "Starting training\n",
      "Iteration: 0001 Loss: 1.74409 Time: 0.67783\n",
      "Iteration: 0002 Loss: 1.68983 Time: 0.37846\n",
      "Iteration: 0003 Loss: 1.71099 Time: 0.35691\n",
      "Iteration: 0004 Loss: 1.67968 Time: 0.37227\n",
      "Iteration: 0005 Loss: 1.64290 Time: 0.35892\n",
      "Iteration: 0006 Loss: 1.67207 Time: 0.35718\n",
      "Iteration: 0007 Loss: 1.61486 Time: 0.36004\n",
      "Iteration: 0008 Loss: 1.63826 Time: 0.36800\n",
      "Iteration: 0009 Loss: 1.59279 Time: 0.36654\n",
      "Iteration: 0010 Loss: 1.58538 Time: 0.37983\n",
      "Iteration: 0011 Loss: 1.56351 Time: 0.38130\n",
      "Iteration: 0012 Loss: 1.57025 Time: 0.36730\n",
      "Iteration: 0013 Loss: 1.54650 Time: 0.37859\n",
      "Iteration: 0014 Loss: 1.52825 Time: 0.38038\n",
      "Iteration: 0015 Loss: 1.49248 Time: 0.36334\n",
      "Iteration: 0016 Loss: 1.48633 Time: 0.37900\n",
      "Iteration: 0017 Loss: 1.50112 Time: 0.39612\n",
      "Iteration: 0018 Loss: 1.47433 Time: 0.39800\n",
      "Iteration: 0019 Loss: 1.47160 Time: 0.38400\n",
      "Iteration: 0020 Loss: 1.46733 Time: 0.38962\n",
      "Iteration: 0021 Loss: 1.44130 Time: 0.39160\n",
      "Iteration: 0022 Loss: 1.43054 Time: 0.38769\n",
      "Iteration: 0023 Loss: 1.39462 Time: 0.38599\n",
      "Iteration: 0024 Loss: 1.42572 Time: 0.38094\n",
      "Iteration: 0025 Loss: 1.37514 Time: 0.37402\n",
      "Iteration: 0026 Loss: 1.39712 Time: 0.36832\n",
      "Iteration: 0027 Loss: 1.38203 Time: 0.38375\n",
      "Iteration: 0028 Loss: 1.35595 Time: 0.36563\n",
      "Iteration: 0029 Loss: 1.34068 Time: 0.36825\n",
      "Iteration: 0030 Loss: 1.33711 Time: 0.36043\n",
      "Iteration: 0031 Loss: 1.31379 Time: 0.36069\n",
      "Iteration: 0032 Loss: 1.31975 Time: 0.36422\n",
      "Iteration: 0033 Loss: 1.31837 Time: 0.35125\n",
      "Iteration: 0034 Loss: 1.29485 Time: 0.35600\n",
      "Iteration: 0035 Loss: 1.31480 Time: 0.35688\n",
      "Iteration: 0036 Loss: 1.26538 Time: 0.36940\n",
      "Iteration: 0037 Loss: 1.28715 Time: 0.36724\n",
      "Iteration: 0038 Loss: 1.26835 Time: 0.33736\n",
      "Iteration: 0039 Loss: 1.25440 Time: 0.36138\n",
      "Iteration: 0040 Loss: 1.23024 Time: 0.35597\n",
      "Iteration: 0041 Loss: 1.22675 Time: 0.35053\n",
      "Iteration: 0042 Loss: 1.23501 Time: 0.37349\n",
      "Iteration: 0043 Loss: 1.22941 Time: 0.35138\n",
      "Iteration: 0044 Loss: 1.20690 Time: 0.34994\n",
      "Iteration: 0045 Loss: 1.20074 Time: 0.36130\n",
      "Iteration: 0046 Loss: 1.18942 Time: 0.35650\n",
      "Iteration: 0047 Loss: 1.17884 Time: 0.35599\n",
      "Iteration: 0048 Loss: 1.17323 Time: 0.36528\n",
      "Iteration: 0049 Loss: 1.16566 Time: 0.34884\n",
      "Iteration: 0050 Loss: 1.14799 Time: 0.34226\n",
      "Iteration: 0051 Loss: 1.16383 Time: 0.35913\n",
      "Iteration: 0052 Loss: 1.15248 Time: 0.36900\n",
      "Iteration: 0053 Loss: 1.14342 Time: 0.35631\n",
      "Iteration: 0054 Loss: 1.15547 Time: 0.36869\n",
      "Iteration: 0055 Loss: 1.13577 Time: 0.34028\n",
      "Iteration: 0056 Loss: 1.13422 Time: 0.36274\n",
      "Iteration: 0057 Loss: 1.11602 Time: 0.35471\n",
      "Iteration: 0058 Loss: 1.11835 Time: 0.34772\n",
      "Iteration: 0059 Loss: 1.11971 Time: 0.37485\n",
      "Iteration: 0060 Loss: 1.09306 Time: 0.34633\n",
      "Iteration: 0061 Loss: 1.08225 Time: 0.36654\n",
      "Iteration: 0062 Loss: 1.06830 Time: 0.35600\n",
      "Iteration: 0063 Loss: 1.07114 Time: 0.37699\n",
      "Iteration: 0064 Loss: 1.07944 Time: 0.37728\n",
      "Iteration: 0065 Loss: 1.05281 Time: 0.35201\n",
      "Iteration: 0066 Loss: 1.05451 Time: 0.34507\n",
      "Iteration: 0067 Loss: 1.08019 Time: 0.36099\n",
      "Iteration: 0068 Loss: 1.04282 Time: 0.37580\n",
      "Iteration: 0069 Loss: 1.03429 Time: 0.37142\n",
      "Iteration: 0070 Loss: 1.05306 Time: 0.35311\n",
      "Iteration: 0071 Loss: 1.02121 Time: 0.35697\n",
      "Iteration: 0072 Loss: 1.03625 Time: 0.35631\n",
      "Iteration: 0073 Loss: 1.02604 Time: 0.37423\n",
      "Iteration: 0074 Loss: 1.00188 Time: 0.39472\n",
      "Iteration: 0075 Loss: 1.00647 Time: 0.36137\n",
      "Iteration: 0076 Loss: 1.00459 Time: 0.34659\n",
      "Iteration: 0077 Loss: 0.98333 Time: 0.36679\n",
      "Iteration: 0078 Loss: 0.99073 Time: 0.37800\n",
      "Iteration: 0079 Loss: 0.97985 Time: 0.36191\n",
      "Iteration: 0080 Loss: 0.96561 Time: 0.35959\n",
      "Iteration: 0081 Loss: 0.94940 Time: 0.37021\n",
      "Iteration: 0082 Loss: 0.96199 Time: 0.36400\n",
      "Iteration: 0083 Loss: 0.94341 Time: 0.35990\n",
      "Iteration: 0084 Loss: 0.93395 Time: 0.36731\n",
      "Iteration: 0085 Loss: 0.94725 Time: 0.36659\n",
      "Iteration: 0086 Loss: 0.95853 Time: 0.36713\n",
      "Iteration: 0087 Loss: 0.92383 Time: 0.36114\n",
      "Iteration: 0088 Loss: 0.93256 Time: 0.35219\n",
      "Iteration: 0089 Loss: 0.90904 Time: 0.37373\n",
      "Iteration: 0090 Loss: 0.91792 Time: 0.34760\n",
      "Iteration: 0091 Loss: 0.89477 Time: 0.35732\n",
      "Iteration: 0092 Loss: 0.90354 Time: 0.35132\n",
      "Iteration: 0093 Loss: 0.89355 Time: 0.34360\n",
      "Iteration: 0094 Loss: 0.86992 Time: 0.36228\n",
      "Iteration: 0095 Loss: 0.89603 Time: 0.35800\n",
      "Iteration: 0096 Loss: 0.88880 Time: 0.35108\n",
      "Iteration: 0097 Loss: 0.86892 Time: 0.34291\n",
      "Iteration: 0098 Loss: 0.85325 Time: 0.35621\n",
      "Iteration: 0099 Loss: 0.85764 Time: 0.34552\n",
      "Iteration: 0100 Loss: 0.85231 Time: 0.35081\n",
      "Iteration: 0101 Loss: 0.85293 Time: 0.35788\n",
      "Iteration: 0102 Loss: 0.84102 Time: 0.35287\n",
      "Iteration: 0103 Loss: 0.82653 Time: 0.35417\n",
      "Iteration: 0104 Loss: 0.82956 Time: 0.35504\n",
      "Iteration: 0105 Loss: 0.82246 Time: 0.34047\n",
      "Iteration: 0106 Loss: 0.82242 Time: 0.34695\n",
      "Iteration: 0107 Loss: 0.81460 Time: 0.35193\n",
      "Iteration: 0108 Loss: 0.81201 Time: 0.34561\n",
      "Iteration: 0109 Loss: 0.80023 Time: 0.35738\n",
      "Iteration: 0110 Loss: 0.80215 Time: 0.34404\n",
      "Iteration: 0111 Loss: 0.78813 Time: 0.34991\n",
      "Iteration: 0112 Loss: 0.79032 Time: 0.35716\n",
      "Iteration: 0113 Loss: 0.77779 Time: 0.36774\n",
      "Iteration: 0114 Loss: 0.77501 Time: 0.35834\n",
      "Iteration: 0115 Loss: 0.78061 Time: 0.37315\n",
      "Iteration: 0116 Loss: 0.77011 Time: 0.36928\n",
      "Iteration: 0117 Loss: 0.75970 Time: 0.37755\n",
      "Iteration: 0118 Loss: 0.76216 Time: 0.36105\n",
      "Iteration: 0119 Loss: 0.76059 Time: 0.36155\n",
      "Iteration: 0120 Loss: 0.75792 Time: 0.38743\n",
      "Iteration: 0121 Loss: 0.75204 Time: 0.35701\n",
      "Iteration: 0122 Loss: 0.74055 Time: 0.35330\n",
      "Iteration: 0123 Loss: 0.74007 Time: 0.35598\n",
      "Iteration: 0124 Loss: 0.73357 Time: 0.35200\n",
      "Iteration: 0125 Loss: 0.73106 Time: 0.36600\n",
      "Iteration: 0126 Loss: 0.72270 Time: 0.36696\n",
      "Iteration: 0127 Loss: 0.72875 Time: 0.35598\n",
      "Iteration: 0128 Loss: 0.72196 Time: 0.36965\n",
      "Iteration: 0129 Loss: 0.71638 Time: 0.37599\n",
      "Iteration: 0130 Loss: 0.71349 Time: 0.37300\n",
      "Iteration: 0131 Loss: 0.71681 Time: 0.37401\n",
      "Iteration: 0132 Loss: 0.70928 Time: 0.38222\n",
      "Iteration: 0133 Loss: 0.70453 Time: 0.38699\n",
      "Iteration: 0134 Loss: 0.70056 Time: 0.37524\n",
      "Iteration: 0135 Loss: 0.69518 Time: 0.37000\n",
      "Iteration: 0136 Loss: 0.68688 Time: 0.35900\n",
      "Iteration: 0137 Loss: 0.68932 Time: 0.35748\n",
      "Iteration: 0138 Loss: 0.68917 Time: 0.37952\n",
      "Iteration: 0139 Loss: 0.67901 Time: 0.38001\n",
      "Iteration: 0140 Loss: 0.67576 Time: 0.38199\n",
      "Iteration: 0141 Loss: 0.67876 Time: 0.38300\n",
      "Iteration: 0142 Loss: 0.67034 Time: 0.36900\n",
      "Iteration: 0143 Loss: 0.66375 Time: 0.35689\n",
      "Iteration: 0144 Loss: 0.65642 Time: 0.36000\n",
      "Iteration: 0145 Loss: 0.66098 Time: 0.37291\n",
      "Iteration: 0146 Loss: 0.65749 Time: 0.35800\n",
      "Iteration: 0147 Loss: 0.65652 Time: 0.36569\n",
      "Iteration: 0148 Loss: 0.65854 Time: 0.36609\n",
      "Iteration: 0149 Loss: 0.65298 Time: 0.35991\n",
      "Iteration: 0150 Loss: 0.65157 Time: 0.36600\n",
      "Iteration: 0151 Loss: 0.64818 Time: 0.37500\n",
      "Iteration: 0152 Loss: 0.64883 Time: 0.38000\n",
      "Iteration: 0153 Loss: 0.64267 Time: 0.37300\n",
      "Iteration: 0154 Loss: 0.64096 Time: 0.36912\n",
      "Iteration: 0155 Loss: 0.63675 Time: 0.37327\n",
      "Iteration: 0156 Loss: 0.64215 Time: 0.37161\n",
      "Iteration: 0157 Loss: 0.63617 Time: 0.38300\n",
      "Iteration: 0158 Loss: 0.63429 Time: 0.35899\n",
      "Iteration: 0159 Loss: 0.62815 Time: 0.37143\n",
      "Iteration: 0160 Loss: 0.62592 Time: 0.36958\n",
      "Iteration: 0161 Loss: 0.62934 Time: 0.36596\n",
      "Iteration: 0162 Loss: 0.62775 Time: 0.37705\n",
      "Iteration: 0163 Loss: 0.61988 Time: 0.39600\n",
      "Iteration: 0164 Loss: 0.62243 Time: 0.37499\n",
      "Iteration: 0165 Loss: 0.61979 Time: 0.39200\n",
      "Iteration: 0166 Loss: 0.61599 Time: 0.36300\n",
      "Iteration: 0167 Loss: 0.61621 Time: 0.37315\n",
      "Iteration: 0168 Loss: 0.61501 Time: 0.36516\n",
      "Iteration: 0169 Loss: 0.61235 Time: 0.36288\n",
      "Iteration: 0170 Loss: 0.60809 Time: 0.35289\n",
      "Iteration: 0171 Loss: 0.60749 Time: 0.37631\n",
      "Iteration: 0172 Loss: 0.60895 Time: 0.37114\n",
      "Iteration: 0173 Loss: 0.60551 Time: 0.36735\n",
      "Iteration: 0174 Loss: 0.60389 Time: 0.37836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0175 Loss: 0.60888 Time: 0.36147\n",
      "Iteration: 0176 Loss: 0.60304 Time: 0.36570\n",
      "Iteration: 0177 Loss: 0.60086 Time: 0.37528\n",
      "Iteration: 0178 Loss: 0.60229 Time: 0.37998\n",
      "Iteration: 0179 Loss: 0.59798 Time: 0.37478\n",
      "Iteration: 0180 Loss: 0.59454 Time: 0.37516\n",
      "Iteration: 0181 Loss: 0.59856 Time: 0.38490\n",
      "Iteration: 0182 Loss: 0.59291 Time: 0.36461\n",
      "Iteration: 0183 Loss: 0.59172 Time: 0.34797\n",
      "Iteration: 0184 Loss: 0.58774 Time: 0.35877\n",
      "Iteration: 0185 Loss: 0.58905 Time: 0.35826\n",
      "Iteration: 0186 Loss: 0.59008 Time: 0.35123\n",
      "Iteration: 0187 Loss: 0.59030 Time: 0.36304\n",
      "Iteration: 0188 Loss: 0.58640 Time: 0.35198\n",
      "Iteration: 0189 Loss: 0.58597 Time: 0.38190\n",
      "Iteration: 0190 Loss: 0.58415 Time: 0.37216\n",
      "Iteration: 0191 Loss: 0.58260 Time: 0.36863\n",
      "Iteration: 0192 Loss: 0.58215 Time: 0.38294\n",
      "Iteration: 0193 Loss: 0.58053 Time: 0.37213\n",
      "Iteration: 0194 Loss: 0.57663 Time: 0.36838\n",
      "Iteration: 0195 Loss: 0.57867 Time: 0.36049\n",
      "Iteration: 0196 Loss: 0.58048 Time: 0.35698\n",
      "Iteration: 0197 Loss: 0.57948 Time: 0.35470\n",
      "Iteration: 0198 Loss: 0.57849 Time: 0.36682\n",
      "Iteration: 0199 Loss: 0.57431 Time: 0.36840\n",
      "Iteration: 0200 Loss: 0.57557 Time: 0.38781\n",
      "Done! \n",
      "\n",
      "STEP 3/3 - MODEL EVALUATION \n",
      "\n",
      "Computing the final embedding vectors, for evaluation\n",
      "Done! \n",
      "\n",
      "Testing: link prediction\n",
      "Done! \n",
      "\n",
      "EXPERIMENTS ON MODEL 2 / 10 \n",
      "\n",
      "STEP 1/3 - PREPROCESSING STEPS \n",
      "\n",
      "Masking some edges from the training graph, for link prediction\n",
      "(validation set: 1.0 % of edges - test set: 10.0 % of edges)\n",
      "Done! \n",
      "\n",
      "Preprocessing node features\n",
      "Done! \n",
      "\n",
      "Running the Louvain algorithm for community detection\n",
      "as a preprocessing step for the encoder\n",
      "Done! Louvain has found 588 communities \n",
      "\n",
      "Setting up the model and the optimizer\n",
      "Done! \n",
      "\n",
      "Preprocessing on message passing matrices\n",
      "Done! \n",
      "\n",
      "Initializing TF session\n",
      "Done! \n",
      "\n",
      "STEP 2/3 - MODEL TRAINING \n",
      "\n",
      "Starting training\n",
      "Iteration: 0001 Loss: 1.72724 Time: 0.64345\n",
      "Iteration: 0002 Loss: 1.74910 Time: 0.38224\n",
      "Iteration: 0003 Loss: 1.69418 Time: 0.37173\n",
      "Iteration: 0004 Loss: 1.66099 Time: 0.37287\n",
      "Iteration: 0005 Loss: 1.63710 Time: 0.33951\n",
      "Iteration: 0006 Loss: 1.61628 Time: 0.35309\n",
      "Iteration: 0007 Loss: 1.62385 Time: 0.34582\n",
      "Iteration: 0008 Loss: 1.61649 Time: 0.34830\n",
      "Iteration: 0009 Loss: 1.60304 Time: 0.35335\n",
      "Iteration: 0010 Loss: 1.55297 Time: 0.35282\n",
      "Iteration: 0011 Loss: 1.54088 Time: 0.36088\n",
      "Iteration: 0012 Loss: 1.57402 Time: 0.37870\n",
      "Iteration: 0013 Loss: 1.56022 Time: 0.36675\n",
      "Iteration: 0014 Loss: 1.51675 Time: 0.36398\n",
      "Iteration: 0015 Loss: 1.51074 Time: 0.35272\n",
      "Iteration: 0016 Loss: 1.50635 Time: 0.37195\n",
      "Iteration: 0017 Loss: 1.49956 Time: 0.35747\n",
      "Iteration: 0018 Loss: 1.45842 Time: 0.36500\n",
      "Iteration: 0019 Loss: 1.48189 Time: 0.35366\n",
      "Iteration: 0020 Loss: 1.43198 Time: 0.36097\n",
      "Iteration: 0021 Loss: 1.43141 Time: 0.35657\n",
      "Iteration: 0022 Loss: 1.42700 Time: 0.35847\n",
      "Iteration: 0023 Loss: 1.38508 Time: 0.35123\n",
      "Iteration: 0024 Loss: 1.40507 Time: 0.36878\n",
      "Iteration: 0025 Loss: 1.37073 Time: 0.36766\n",
      "Iteration: 0026 Loss: 1.38729 Time: 0.35536\n",
      "Iteration: 0027 Loss: 1.35604 Time: 0.35977\n",
      "Iteration: 0028 Loss: 1.34996 Time: 0.35541\n",
      "Iteration: 0029 Loss: 1.33872 Time: 0.36974\n",
      "Iteration: 0030 Loss: 1.35432 Time: 0.37476\n",
      "Iteration: 0031 Loss: 1.34701 Time: 0.36400\n",
      "Iteration: 0032 Loss: 1.31198 Time: 0.35268\n",
      "Iteration: 0033 Loss: 1.29108 Time: 0.36985\n",
      "Iteration: 0034 Loss: 1.28817 Time: 0.35857\n",
      "Iteration: 0035 Loss: 1.27803 Time: 0.37364\n",
      "Iteration: 0036 Loss: 1.28076 Time: 0.36091\n",
      "Iteration: 0037 Loss: 1.27576 Time: 0.36908\n",
      "Iteration: 0038 Loss: 1.25578 Time: 0.36388\n",
      "Iteration: 0039 Loss: 1.24732 Time: 0.35894\n",
      "Iteration: 0040 Loss: 1.23216 Time: 0.37352\n",
      "Iteration: 0041 Loss: 1.25734 Time: 0.38000\n",
      "Iteration: 0042 Loss: 1.23487 Time: 0.38705\n",
      "Iteration: 0043 Loss: 1.22354 Time: 0.36825\n",
      "Iteration: 0044 Loss: 1.19124 Time: 0.36074\n",
      "Iteration: 0045 Loss: 1.21502 Time: 0.36714\n",
      "Iteration: 0046 Loss: 1.18447 Time: 0.37285\n",
      "Iteration: 0047 Loss: 1.19024 Time: 0.36262\n",
      "Iteration: 0048 Loss: 1.18697 Time: 0.35572\n",
      "Iteration: 0049 Loss: 1.18362 Time: 0.36300\n",
      "Iteration: 0050 Loss: 1.15359 Time: 0.36007\n",
      "Iteration: 0051 Loss: 1.16167 Time: 0.36453\n",
      "Iteration: 0052 Loss: 1.15256 Time: 0.36184\n",
      "Iteration: 0053 Loss: 1.14436 Time: 0.39014\n",
      "Iteration: 0054 Loss: 1.11943 Time: 0.38927\n",
      "Iteration: 0055 Loss: 1.13591 Time: 0.37521\n",
      "Iteration: 0056 Loss: 1.13794 Time: 0.35879\n",
      "Iteration: 0057 Loss: 1.10552 Time: 0.37030\n",
      "Iteration: 0058 Loss: 1.12071 Time: 0.36545\n",
      "Iteration: 0059 Loss: 1.08649 Time: 0.35776\n",
      "Iteration: 0060 Loss: 1.09097 Time: 0.35939\n",
      "Iteration: 0061 Loss: 1.08479 Time: 0.37215\n",
      "Iteration: 0062 Loss: 1.06633 Time: 0.35811\n",
      "Iteration: 0063 Loss: 1.07575 Time: 0.37075\n",
      "Iteration: 0064 Loss: 1.08578 Time: 0.37347\n",
      "Iteration: 0065 Loss: 1.07456 Time: 0.35446\n",
      "Iteration: 0066 Loss: 1.05434 Time: 0.35628\n",
      "Iteration: 0067 Loss: 1.05828 Time: 0.36358\n",
      "Iteration: 0068 Loss: 1.04152 Time: 0.37070\n",
      "Iteration: 0069 Loss: 1.02582 Time: 0.37046\n",
      "Iteration: 0070 Loss: 1.03421 Time: 0.37939\n",
      "Iteration: 0071 Loss: 1.02592 Time: 0.37790\n",
      "Iteration: 0072 Loss: 0.99741 Time: 0.37851\n",
      "Iteration: 0073 Loss: 1.00776 Time: 0.37260\n",
      "Iteration: 0074 Loss: 1.01678 Time: 0.36293\n",
      "Iteration: 0075 Loss: 1.00392 Time: 0.35669\n",
      "Iteration: 0076 Loss: 0.97108 Time: 0.36221\n",
      "Iteration: 0077 Loss: 0.97952 Time: 0.37750\n",
      "Iteration: 0078 Loss: 0.97532 Time: 0.36921\n",
      "Iteration: 0079 Loss: 0.98364 Time: 0.37076\n",
      "Iteration: 0080 Loss: 0.97385 Time: 0.35337\n",
      "Iteration: 0081 Loss: 0.94988 Time: 0.38078\n",
      "Iteration: 0082 Loss: 0.95262 Time: 0.36742\n",
      "Iteration: 0083 Loss: 0.94448 Time: 0.35832\n",
      "Iteration: 0084 Loss: 0.95198 Time: 0.36162\n",
      "Iteration: 0085 Loss: 0.93163 Time: 0.37948\n",
      "Iteration: 0086 Loss: 0.92666 Time: 0.36900\n",
      "Iteration: 0087 Loss: 0.93268 Time: 0.35337\n",
      "Iteration: 0088 Loss: 0.92146 Time: 0.36888\n",
      "Iteration: 0089 Loss: 0.91532 Time: 0.36217\n",
      "Iteration: 0090 Loss: 0.92184 Time: 0.36677\n",
      "Iteration: 0091 Loss: 0.89270 Time: 0.35751\n",
      "Iteration: 0092 Loss: 0.88915 Time: 0.37458\n",
      "Iteration: 0093 Loss: 0.88343 Time: 0.37461\n",
      "Iteration: 0094 Loss: 0.88405 Time: 0.38847\n",
      "Iteration: 0095 Loss: 0.87604 Time: 0.35944\n",
      "Iteration: 0096 Loss: 0.87266 Time: 0.36093\n",
      "Iteration: 0097 Loss: 0.87630 Time: 0.36845\n",
      "Iteration: 0098 Loss: 0.85297 Time: 0.37301\n",
      "Iteration: 0099 Loss: 0.85345 Time: 0.38452\n",
      "Iteration: 0100 Loss: 0.84826 Time: 0.36152\n",
      "Iteration: 0101 Loss: 0.83370 Time: 0.38060\n",
      "Iteration: 0102 Loss: 0.83398 Time: 0.37951\n",
      "Iteration: 0103 Loss: 0.83299 Time: 0.38026\n",
      "Iteration: 0104 Loss: 0.81441 Time: 0.38087\n",
      "Iteration: 0105 Loss: 0.83866 Time: 0.35792\n",
      "Iteration: 0106 Loss: 0.81081 Time: 0.36201\n",
      "Iteration: 0107 Loss: 0.82517 Time: 0.35625\n",
      "Iteration: 0108 Loss: 0.80890 Time: 0.36229\n",
      "Iteration: 0109 Loss: 0.80025 Time: 0.36029\n",
      "Iteration: 0110 Loss: 0.80135 Time: 0.35901\n",
      "Iteration: 0111 Loss: 0.79426 Time: 0.36244\n",
      "Iteration: 0112 Loss: 0.78342 Time: 0.36069\n",
      "Iteration: 0113 Loss: 0.78427 Time: 0.36043\n",
      "Iteration: 0114 Loss: 0.78138 Time: 0.35600\n",
      "Iteration: 0115 Loss: 0.78020 Time: 0.34700\n",
      "Iteration: 0116 Loss: 0.77614 Time: 0.35308\n",
      "Iteration: 0117 Loss: 0.75973 Time: 0.34588\n",
      "Iteration: 0118 Loss: 0.76643 Time: 0.36049\n",
      "Iteration: 0119 Loss: 0.76062 Time: 0.35252\n",
      "Iteration: 0120 Loss: 0.76136 Time: 0.34961\n",
      "Iteration: 0121 Loss: 0.73620 Time: 0.34820\n",
      "Iteration: 0122 Loss: 0.74399 Time: 0.35156\n",
      "Iteration: 0123 Loss: 0.74810 Time: 0.34774\n",
      "Iteration: 0124 Loss: 0.73409 Time: 0.34879\n",
      "Iteration: 0125 Loss: 0.73038 Time: 0.35577\n",
      "Iteration: 0126 Loss: 0.72821 Time: 0.34776\n",
      "Iteration: 0127 Loss: 0.71937 Time: 0.35054\n",
      "Iteration: 0128 Loss: 0.71548 Time: 0.35500\n",
      "Iteration: 0129 Loss: 0.71215 Time: 0.34465\n",
      "Iteration: 0130 Loss: 0.70924 Time: 0.36199\n",
      "Iteration: 0131 Loss: 0.70889 Time: 0.35529\n",
      "Iteration: 0132 Loss: 0.69321 Time: 0.35891\n",
      "Iteration: 0133 Loss: 0.70227 Time: 0.34679\n",
      "Iteration: 0134 Loss: 0.69085 Time: 0.35139\n",
      "Iteration: 0135 Loss: 0.69428 Time: 0.35504\n",
      "Iteration: 0136 Loss: 0.69189 Time: 0.35325\n",
      "Iteration: 0137 Loss: 0.68615 Time: 0.34689\n",
      "Iteration: 0138 Loss: 0.68786 Time: 0.36098\n",
      "Iteration: 0139 Loss: 0.68586 Time: 0.34632\n",
      "Iteration: 0140 Loss: 0.68011 Time: 0.34276\n",
      "Iteration: 0141 Loss: 0.67863 Time: 0.35320\n",
      "Iteration: 0142 Loss: 0.66714 Time: 0.34895\n",
      "Iteration: 0143 Loss: 0.67265 Time: 0.35200\n",
      "Iteration: 0144 Loss: 0.65977 Time: 0.34600\n",
      "Iteration: 0145 Loss: 0.65938 Time: 0.35939\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0146 Loss: 0.65486 Time: 0.35846\n",
      "Iteration: 0147 Loss: 0.66143 Time: 0.35035\n",
      "Iteration: 0148 Loss: 0.65077 Time: 0.34765\n",
      "Iteration: 0149 Loss: 0.65498 Time: 0.36535\n",
      "Iteration: 0150 Loss: 0.64618 Time: 0.34820\n",
      "Iteration: 0151 Loss: 0.63906 Time: 0.35402\n",
      "Iteration: 0152 Loss: 0.64497 Time: 0.34995\n",
      "Iteration: 0153 Loss: 0.64650 Time: 0.34545\n",
      "Iteration: 0154 Loss: 0.64174 Time: 0.34634\n",
      "Iteration: 0155 Loss: 0.63881 Time: 0.36574\n",
      "Iteration: 0156 Loss: 0.63211 Time: 0.34245\n",
      "Iteration: 0157 Loss: 0.63448 Time: 0.35620\n",
      "Iteration: 0158 Loss: 0.63190 Time: 0.34486\n",
      "Iteration: 0159 Loss: 0.62969 Time: 0.34698\n",
      "Iteration: 0160 Loss: 0.62628 Time: 0.35483\n",
      "Iteration: 0161 Loss: 0.62314 Time: 0.34424\n",
      "Iteration: 0162 Loss: 0.62205 Time: 0.34928\n",
      "Iteration: 0163 Loss: 0.61956 Time: 0.34876\n",
      "Iteration: 0164 Loss: 0.61954 Time: 0.35632\n",
      "Iteration: 0165 Loss: 0.61819 Time: 0.35243\n",
      "Iteration: 0166 Loss: 0.61376 Time: 0.36554\n",
      "Iteration: 0167 Loss: 0.61313 Time: 0.34021\n",
      "Iteration: 0168 Loss: 0.61909 Time: 0.35781\n",
      "Iteration: 0169 Loss: 0.61404 Time: 0.34027\n",
      "Iteration: 0170 Loss: 0.61347 Time: 0.36292\n",
      "Iteration: 0171 Loss: 0.60802 Time: 0.35573\n",
      "Iteration: 0172 Loss: 0.60726 Time: 0.34856\n",
      "Iteration: 0173 Loss: 0.61279 Time: 0.35713\n",
      "Iteration: 0174 Loss: 0.60575 Time: 0.34684\n",
      "Iteration: 0175 Loss: 0.60001 Time: 0.34246\n",
      "Iteration: 0176 Loss: 0.59694 Time: 0.34949\n",
      "Iteration: 0177 Loss: 0.60083 Time: 0.34742\n",
      "Iteration: 0178 Loss: 0.60161 Time: 0.34825\n",
      "Iteration: 0179 Loss: 0.59187 Time: 0.36033\n",
      "Iteration: 0180 Loss: 0.58782 Time: 0.34336\n",
      "Iteration: 0181 Loss: 0.59346 Time: 0.35202\n",
      "Iteration: 0182 Loss: 0.58759 Time: 0.34644\n",
      "Iteration: 0183 Loss: 0.59591 Time: 0.36006\n",
      "Iteration: 0184 Loss: 0.58890 Time: 0.35463\n",
      "Iteration: 0185 Loss: 0.58930 Time: 0.36031\n",
      "Iteration: 0186 Loss: 0.58633 Time: 0.35040\n",
      "Iteration: 0187 Loss: 0.58591 Time: 0.36339\n",
      "Iteration: 0188 Loss: 0.58341 Time: 0.35416\n",
      "Iteration: 0189 Loss: 0.57902 Time: 0.36940\n",
      "Iteration: 0190 Loss: 0.58282 Time: 0.35540\n",
      "Iteration: 0191 Loss: 0.58287 Time: 0.34845\n",
      "Iteration: 0192 Loss: 0.58310 Time: 0.35174\n",
      "Iteration: 0193 Loss: 0.57874 Time: 0.34961\n",
      "Iteration: 0194 Loss: 0.57790 Time: 0.35668\n",
      "Iteration: 0195 Loss: 0.58014 Time: 0.36337\n",
      "Iteration: 0196 Loss: 0.57657 Time: 0.35509\n",
      "Iteration: 0197 Loss: 0.57265 Time: 0.35529\n",
      "Iteration: 0198 Loss: 0.57364 Time: 0.35525\n",
      "Iteration: 0199 Loss: 0.57592 Time: 0.35150\n",
      "Iteration: 0200 Loss: 0.57451 Time: 0.35992\n",
      "Done! \n",
      "\n",
      "STEP 3/3 - MODEL EVALUATION \n",
      "\n",
      "Computing the final embedding vectors, for evaluation\n",
      "Done! \n",
      "\n",
      "Testing: link prediction\n",
      "Done! \n",
      "\n",
      "EXPERIMENTS ON MODEL 3 / 10 \n",
      "\n",
      "STEP 1/3 - PREPROCESSING STEPS \n",
      "\n",
      "Masking some edges from the training graph, for link prediction\n",
      "(validation set: 1.0 % of edges - test set: 10.0 % of edges)\n",
      "Done! \n",
      "\n",
      "Preprocessing node features\n",
      "Done! \n",
      "\n",
      "Running the Louvain algorithm for community detection\n",
      "as a preprocessing step for the encoder\n",
      "Done! Louvain has found 577 communities \n",
      "\n",
      "Setting up the model and the optimizer\n",
      "Done! \n",
      "\n",
      "Preprocessing on message passing matrices\n",
      "Done! \n",
      "\n",
      "Initializing TF session\n",
      "Done! \n",
      "\n",
      "STEP 2/3 - MODEL TRAINING \n",
      "\n",
      "Starting training\n",
      "Iteration: 0001 Loss: 1.72019 Time: 0.62454\n",
      "Iteration: 0002 Loss: 1.72561 Time: 0.33629\n",
      "Iteration: 0003 Loss: 1.66990 Time: 0.35408\n",
      "Iteration: 0004 Loss: 1.72354 Time: 0.34713\n",
      "Iteration: 0005 Loss: 1.65736 Time: 0.35329\n",
      "Iteration: 0006 Loss: 1.65743 Time: 0.35235\n",
      "Iteration: 0007 Loss: 1.62354 Time: 0.34324\n",
      "Iteration: 0008 Loss: 1.60716 Time: 0.34806\n",
      "Iteration: 0009 Loss: 1.60122 Time: 0.34417\n",
      "Iteration: 0010 Loss: 1.59004 Time: 0.35049\n",
      "Iteration: 0011 Loss: 1.57200 Time: 0.35610\n",
      "Iteration: 0012 Loss: 1.56452 Time: 0.36672\n",
      "Iteration: 0013 Loss: 1.52123 Time: 0.33779\n",
      "Iteration: 0014 Loss: 1.49947 Time: 0.35274\n",
      "Iteration: 0015 Loss: 1.49998 Time: 0.35004\n",
      "Iteration: 0016 Loss: 1.48377 Time: 0.35030\n",
      "Iteration: 0017 Loss: 1.47649 Time: 0.35056\n",
      "Iteration: 0018 Loss: 1.48099 Time: 0.35565\n",
      "Iteration: 0019 Loss: 1.45396 Time: 0.33467\n",
      "Iteration: 0020 Loss: 1.45133 Time: 0.36005\n",
      "Iteration: 0021 Loss: 1.44129 Time: 0.35671\n",
      "Iteration: 0022 Loss: 1.43425 Time: 0.34508\n",
      "Iteration: 0023 Loss: 1.39551 Time: 0.35401\n",
      "Iteration: 0024 Loss: 1.39986 Time: 0.35271\n",
      "Iteration: 0025 Loss: 1.38327 Time: 0.34501\n",
      "Iteration: 0026 Loss: 1.36395 Time: 0.35497\n",
      "Iteration: 0027 Loss: 1.36102 Time: 0.35923\n",
      "Iteration: 0028 Loss: 1.35199 Time: 0.35791\n",
      "Iteration: 0029 Loss: 1.37343 Time: 0.35425\n",
      "Iteration: 0030 Loss: 1.32124 Time: 0.37083\n",
      "Iteration: 0031 Loss: 1.34136 Time: 0.34657\n",
      "Iteration: 0032 Loss: 1.29524 Time: 0.34513\n",
      "Iteration: 0033 Loss: 1.31675 Time: 0.33246\n",
      "Iteration: 0034 Loss: 1.28804 Time: 0.35092\n",
      "Iteration: 0035 Loss: 1.28494 Time: 0.35285\n",
      "Iteration: 0036 Loss: 1.28503 Time: 0.34882\n",
      "Iteration: 0037 Loss: 1.27539 Time: 0.35098\n",
      "Iteration: 0038 Loss: 1.24488 Time: 0.35645\n",
      "Iteration: 0039 Loss: 1.25870 Time: 0.34821\n",
      "Iteration: 0040 Loss: 1.25930 Time: 0.36537\n",
      "Iteration: 0041 Loss: 1.22144 Time: 0.35401\n",
      "Iteration: 0042 Loss: 1.25096 Time: 0.36287\n",
      "Iteration: 0043 Loss: 1.21137 Time: 0.36699\n",
      "Iteration: 0044 Loss: 1.20512 Time: 0.35971\n",
      "Iteration: 0045 Loss: 1.21213 Time: 0.36372\n",
      "Iteration: 0046 Loss: 1.20203 Time: 0.36936\n",
      "Iteration: 0047 Loss: 1.19054 Time: 0.35725\n",
      "Iteration: 0048 Loss: 1.20770 Time: 0.35322\n",
      "Iteration: 0049 Loss: 1.18198 Time: 0.35498\n",
      "Iteration: 0050 Loss: 1.18805 Time: 0.33921\n",
      "Iteration: 0051 Loss: 1.15626 Time: 0.35084\n",
      "Iteration: 0052 Loss: 1.15044 Time: 0.34078\n",
      "Iteration: 0053 Loss: 1.15667 Time: 0.34033\n",
      "Iteration: 0054 Loss: 1.13138 Time: 0.33949\n",
      "Iteration: 0055 Loss: 1.13693 Time: 0.36419\n",
      "Iteration: 0056 Loss: 1.11744 Time: 0.34276\n",
      "Iteration: 0057 Loss: 1.11401 Time: 0.36007\n",
      "Iteration: 0058 Loss: 1.12773 Time: 0.34751\n",
      "Iteration: 0059 Loss: 1.10661 Time: 0.36004\n",
      "Iteration: 0060 Loss: 1.09879 Time: 0.36618\n",
      "Iteration: 0061 Loss: 1.11139 Time: 0.36119\n",
      "Iteration: 0062 Loss: 1.08478 Time: 0.34538\n",
      "Iteration: 0063 Loss: 1.08506 Time: 0.34722\n",
      "Iteration: 0064 Loss: 1.05910 Time: 0.35815\n",
      "Iteration: 0065 Loss: 1.06205 Time: 0.38545\n",
      "Iteration: 0066 Loss: 1.06721 Time: 0.37315\n",
      "Iteration: 0067 Loss: 1.05398 Time: 0.37428\n",
      "Iteration: 0068 Loss: 1.05682 Time: 0.38378\n",
      "Iteration: 0069 Loss: 1.05006 Time: 0.35890\n",
      "Iteration: 0070 Loss: 1.04527 Time: 0.34456\n",
      "Iteration: 0071 Loss: 1.03497 Time: 0.35591\n",
      "Iteration: 0072 Loss: 1.02446 Time: 0.34918\n",
      "Iteration: 0073 Loss: 0.99952 Time: 0.34515\n",
      "Iteration: 0074 Loss: 1.01454 Time: 0.36400\n",
      "Iteration: 0075 Loss: 1.01692 Time: 0.33558\n",
      "Iteration: 0076 Loss: 0.99730 Time: 0.35713\n",
      "Iteration: 0077 Loss: 0.99426 Time: 0.35261\n",
      "Iteration: 0078 Loss: 0.99554 Time: 0.37006\n",
      "Iteration: 0079 Loss: 0.96835 Time: 0.34041\n",
      "Iteration: 0080 Loss: 0.97269 Time: 0.34717\n",
      "Iteration: 0081 Loss: 0.97418 Time: 0.34981\n",
      "Iteration: 0082 Loss: 0.96355 Time: 0.35612\n",
      "Iteration: 0083 Loss: 0.95326 Time: 0.34785\n",
      "Iteration: 0084 Loss: 0.95547 Time: 0.33241\n",
      "Iteration: 0085 Loss: 0.93640 Time: 0.34744\n",
      "Iteration: 0086 Loss: 0.93821 Time: 0.35701\n",
      "Iteration: 0087 Loss: 0.94263 Time: 0.33681\n",
      "Iteration: 0088 Loss: 0.92650 Time: 0.34535\n",
      "Iteration: 0089 Loss: 0.91875 Time: 0.35994\n",
      "Iteration: 0090 Loss: 0.91809 Time: 0.34915\n",
      "Iteration: 0091 Loss: 0.90603 Time: 0.35028\n",
      "Iteration: 0092 Loss: 0.91172 Time: 0.35780\n",
      "Iteration: 0093 Loss: 0.89331 Time: 0.34597\n",
      "Iteration: 0094 Loss: 0.88661 Time: 0.34283\n",
      "Iteration: 0095 Loss: 0.89105 Time: 0.34609\n",
      "Iteration: 0096 Loss: 0.87749 Time: 0.36217\n",
      "Iteration: 0097 Loss: 0.87307 Time: 0.35692\n",
      "Iteration: 0098 Loss: 0.86797 Time: 0.36380\n",
      "Iteration: 0099 Loss: 0.88188 Time: 0.36155\n",
      "Iteration: 0100 Loss: 0.85888 Time: 0.34558\n",
      "Iteration: 0101 Loss: 0.84665 Time: 0.35011\n",
      "Iteration: 0102 Loss: 0.84533 Time: 0.35570\n",
      "Iteration: 0103 Loss: 0.84100 Time: 0.35611\n",
      "Iteration: 0104 Loss: 0.82215 Time: 0.35766\n",
      "Iteration: 0105 Loss: 0.83177 Time: 0.34189\n",
      "Iteration: 0106 Loss: 0.81798 Time: 0.35485\n",
      "Iteration: 0107 Loss: 0.80980 Time: 0.36089\n",
      "Iteration: 0108 Loss: 0.81006 Time: 0.36119\n",
      "Iteration: 0109 Loss: 0.81142 Time: 0.34399\n",
      "Iteration: 0110 Loss: 0.79869 Time: 0.35111\n",
      "Iteration: 0111 Loss: 0.79460 Time: 0.35594\n",
      "Iteration: 0112 Loss: 0.79615 Time: 0.34551\n",
      "Iteration: 0113 Loss: 0.78704 Time: 0.35863\n",
      "Iteration: 0114 Loss: 0.78148 Time: 0.35550\n",
      "Iteration: 0115 Loss: 0.77843 Time: 0.34712\n",
      "Iteration: 0116 Loss: 0.76965 Time: 0.34961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0117 Loss: 0.76325 Time: 0.34649\n",
      "Iteration: 0118 Loss: 0.76869 Time: 0.34067\n",
      "Iteration: 0119 Loss: 0.75288 Time: 0.34720\n",
      "Iteration: 0120 Loss: 0.74674 Time: 0.34164\n",
      "Iteration: 0121 Loss: 0.75004 Time: 0.34409\n",
      "Iteration: 0122 Loss: 0.74960 Time: 0.36207\n",
      "Iteration: 0123 Loss: 0.74855 Time: 0.34571\n",
      "Iteration: 0124 Loss: 0.74241 Time: 0.36299\n",
      "Iteration: 0125 Loss: 0.73081 Time: 0.35845\n",
      "Iteration: 0126 Loss: 0.73548 Time: 0.35261\n",
      "Iteration: 0127 Loss: 0.72367 Time: 0.35815\n",
      "Iteration: 0128 Loss: 0.73004 Time: 0.37167\n",
      "Iteration: 0129 Loss: 0.72003 Time: 0.37263\n",
      "Iteration: 0130 Loss: 0.71741 Time: 0.38428\n",
      "Iteration: 0131 Loss: 0.70476 Time: 0.36445\n",
      "Iteration: 0132 Loss: 0.70465 Time: 0.34140\n",
      "Iteration: 0133 Loss: 0.70127 Time: 0.34530\n",
      "Iteration: 0134 Loss: 0.69388 Time: 0.34429\n",
      "Iteration: 0135 Loss: 0.69578 Time: 0.34268\n",
      "Iteration: 0136 Loss: 0.69370 Time: 0.35022\n",
      "Iteration: 0137 Loss: 0.68843 Time: 0.34423\n",
      "Iteration: 0138 Loss: 0.68449 Time: 0.34372\n",
      "Iteration: 0139 Loss: 0.68746 Time: 0.36412\n",
      "Iteration: 0140 Loss: 0.68413 Time: 0.35881\n",
      "Iteration: 0141 Loss: 0.67627 Time: 0.34504\n",
      "Iteration: 0142 Loss: 0.67360 Time: 0.34686\n",
      "Iteration: 0143 Loss: 0.67572 Time: 0.35694\n",
      "Iteration: 0144 Loss: 0.67310 Time: 0.37257\n",
      "Iteration: 0145 Loss: 0.66350 Time: 0.36496\n",
      "Iteration: 0146 Loss: 0.66459 Time: 0.36043\n",
      "Iteration: 0147 Loss: 0.65875 Time: 0.35962\n",
      "Iteration: 0148 Loss: 0.65599 Time: 0.35073\n",
      "Iteration: 0149 Loss: 0.65484 Time: 0.38406\n",
      "Iteration: 0150 Loss: 0.65469 Time: 0.36569\n",
      "Iteration: 0151 Loss: 0.65094 Time: 0.36600\n",
      "Iteration: 0152 Loss: 0.64504 Time: 0.35299\n",
      "Iteration: 0153 Loss: 0.64888 Time: 0.34891\n",
      "Iteration: 0154 Loss: 0.64730 Time: 0.34868\n",
      "Iteration: 0155 Loss: 0.63527 Time: 0.34985\n",
      "Iteration: 0156 Loss: 0.63838 Time: 0.34694\n",
      "Iteration: 0157 Loss: 0.63105 Time: 0.36305\n",
      "Iteration: 0158 Loss: 0.63311 Time: 0.37173\n",
      "Iteration: 0159 Loss: 0.64001 Time: 0.37504\n",
      "Iteration: 0160 Loss: 0.63065 Time: 0.34727\n",
      "Iteration: 0161 Loss: 0.62606 Time: 0.35062\n",
      "Iteration: 0162 Loss: 0.62689 Time: 0.34942\n",
      "Iteration: 0163 Loss: 0.62146 Time: 0.35469\n",
      "Iteration: 0164 Loss: 0.62462 Time: 0.36922\n",
      "Iteration: 0165 Loss: 0.61759 Time: 0.36782\n",
      "Iteration: 0166 Loss: 0.61963 Time: 0.35658\n",
      "Iteration: 0167 Loss: 0.62576 Time: 0.37226\n",
      "Iteration: 0168 Loss: 0.61135 Time: 0.36061\n",
      "Iteration: 0169 Loss: 0.61590 Time: 0.37039\n",
      "Iteration: 0170 Loss: 0.61198 Time: 0.35388\n",
      "Iteration: 0171 Loss: 0.61455 Time: 0.37341\n",
      "Iteration: 0172 Loss: 0.60968 Time: 0.37013\n",
      "Iteration: 0173 Loss: 0.60877 Time: 0.37733\n",
      "Iteration: 0174 Loss: 0.60977 Time: 0.35352\n",
      "Iteration: 0175 Loss: 0.60351 Time: 0.34564\n",
      "Iteration: 0176 Loss: 0.60330 Time: 0.34769\n",
      "Iteration: 0177 Loss: 0.60237 Time: 0.34633\n",
      "Iteration: 0178 Loss: 0.60230 Time: 0.35061\n",
      "Iteration: 0179 Loss: 0.59977 Time: 0.34842\n",
      "Iteration: 0180 Loss: 0.59798 Time: 0.35500\n",
      "Iteration: 0181 Loss: 0.59844 Time: 0.34330\n",
      "Iteration: 0182 Loss: 0.59983 Time: 0.34330\n",
      "Iteration: 0183 Loss: 0.59326 Time: 0.34792\n",
      "Iteration: 0184 Loss: 0.59585 Time: 0.34121\n",
      "Iteration: 0185 Loss: 0.59144 Time: 0.35313\n",
      "Iteration: 0186 Loss: 0.58752 Time: 0.35826\n",
      "Iteration: 0187 Loss: 0.58885 Time: 0.35478\n",
      "Iteration: 0188 Loss: 0.58651 Time: 0.35077\n",
      "Iteration: 0189 Loss: 0.58393 Time: 0.34544\n",
      "Iteration: 0190 Loss: 0.58416 Time: 0.34693\n",
      "Iteration: 0191 Loss: 0.58395 Time: 0.35141\n",
      "Iteration: 0192 Loss: 0.58331 Time: 0.34000\n",
      "Iteration: 0193 Loss: 0.58533 Time: 0.34363\n",
      "Iteration: 0194 Loss: 0.58075 Time: 0.35173\n",
      "Iteration: 0195 Loss: 0.58385 Time: 0.35648\n",
      "Iteration: 0196 Loss: 0.57976 Time: 0.34321\n",
      "Iteration: 0197 Loss: 0.57700 Time: 0.36229\n",
      "Iteration: 0198 Loss: 0.57630 Time: 0.36351\n",
      "Iteration: 0199 Loss: 0.57333 Time: 0.36549\n",
      "Iteration: 0200 Loss: 0.57535 Time: 0.35865\n",
      "Done! \n",
      "\n",
      "STEP 3/3 - MODEL EVALUATION \n",
      "\n",
      "Computing the final embedding vectors, for evaluation\n",
      "Done! \n",
      "\n",
      "Testing: link prediction\n",
      "Done! \n",
      "\n",
      "EXPERIMENTS ON MODEL 4 / 10 \n",
      "\n",
      "STEP 1/3 - PREPROCESSING STEPS \n",
      "\n",
      "Masking some edges from the training graph, for link prediction\n",
      "(validation set: 1.0 % of edges - test set: 10.0 % of edges)\n",
      "Done! \n",
      "\n",
      "Preprocessing node features\n",
      "Done! \n",
      "\n",
      "Running the Louvain algorithm for community detection\n",
      "as a preprocessing step for the encoder\n",
      "Done! Louvain has found 549 communities \n",
      "\n",
      "Setting up the model and the optimizer\n",
      "Done! \n",
      "\n",
      "Preprocessing on message passing matrices\n",
      "Done! \n",
      "\n",
      "Initializing TF session\n",
      "Done! \n",
      "\n",
      "STEP 2/3 - MODEL TRAINING \n",
      "\n",
      "Starting training\n",
      "Iteration: 0001 Loss: 1.72654 Time: 0.62026\n",
      "Iteration: 0002 Loss: 1.72299 Time: 0.35084\n",
      "Iteration: 0003 Loss: 1.65830 Time: 0.35375\n",
      "Iteration: 0004 Loss: 1.66905 Time: 0.35385\n",
      "Iteration: 0005 Loss: 1.67121 Time: 0.34368\n",
      "Iteration: 0006 Loss: 1.68013 Time: 0.34851\n",
      "Iteration: 0007 Loss: 1.63297 Time: 0.34488\n",
      "Iteration: 0008 Loss: 1.63813 Time: 0.35476\n",
      "Iteration: 0009 Loss: 1.58223 Time: 0.35300\n",
      "Iteration: 0010 Loss: 1.56051 Time: 0.35241\n",
      "Iteration: 0011 Loss: 1.54474 Time: 0.34290\n",
      "Iteration: 0012 Loss: 1.56537 Time: 0.35917\n",
      "Iteration: 0013 Loss: 1.53911 Time: 0.35300\n",
      "Iteration: 0014 Loss: 1.57400 Time: 0.34990\n",
      "Iteration: 0015 Loss: 1.52809 Time: 0.34629\n",
      "Iteration: 0016 Loss: 1.49460 Time: 0.35785\n",
      "Iteration: 0017 Loss: 1.48090 Time: 0.33440\n",
      "Iteration: 0018 Loss: 1.45385 Time: 0.34821\n",
      "Iteration: 0019 Loss: 1.46213 Time: 0.35539\n",
      "Iteration: 0020 Loss: 1.43970 Time: 0.37006\n",
      "Iteration: 0021 Loss: 1.43401 Time: 0.35098\n",
      "Iteration: 0022 Loss: 1.40632 Time: 0.35097\n",
      "Iteration: 0023 Loss: 1.44118 Time: 0.33958\n",
      "Iteration: 0024 Loss: 1.40410 Time: 0.36074\n",
      "Iteration: 0025 Loss: 1.37544 Time: 0.35111\n",
      "Iteration: 0026 Loss: 1.36993 Time: 0.34222\n",
      "Iteration: 0027 Loss: 1.37188 Time: 0.36366\n",
      "Iteration: 0028 Loss: 1.36358 Time: 0.35458\n",
      "Iteration: 0029 Loss: 1.32261 Time: 0.34846\n",
      "Iteration: 0030 Loss: 1.34062 Time: 0.34800\n",
      "Iteration: 0031 Loss: 1.30127 Time: 0.34789\n",
      "Iteration: 0032 Loss: 1.31244 Time: 0.35295\n",
      "Iteration: 0033 Loss: 1.31887 Time: 0.35797\n",
      "Iteration: 0034 Loss: 1.29409 Time: 0.34758\n",
      "Iteration: 0035 Loss: 1.27181 Time: 0.35071\n",
      "Iteration: 0036 Loss: 1.27010 Time: 0.36841\n",
      "Iteration: 0037 Loss: 1.26683 Time: 0.35953\n",
      "Iteration: 0038 Loss: 1.27075 Time: 0.37804\n",
      "Iteration: 0039 Loss: 1.25850 Time: 0.35974\n",
      "Iteration: 0040 Loss: 1.23035 Time: 0.34982\n",
      "Iteration: 0041 Loss: 1.24406 Time: 0.35944\n",
      "Iteration: 0042 Loss: 1.24712 Time: 0.34732\n",
      "Iteration: 0043 Loss: 1.25214 Time: 0.34914\n",
      "Iteration: 0044 Loss: 1.19952 Time: 0.35452\n",
      "Iteration: 0045 Loss: 1.20082 Time: 0.34343\n",
      "Iteration: 0046 Loss: 1.19031 Time: 0.34623\n",
      "Iteration: 0047 Loss: 1.17827 Time: 0.34691\n",
      "Iteration: 0048 Loss: 1.17857 Time: 0.34850\n",
      "Iteration: 0049 Loss: 1.17067 Time: 0.35082\n",
      "Iteration: 0050 Loss: 1.17748 Time: 0.34618\n",
      "Iteration: 0051 Loss: 1.15962 Time: 0.34697\n",
      "Iteration: 0052 Loss: 1.14871 Time: 0.35583\n",
      "Iteration: 0053 Loss: 1.14304 Time: 0.34359\n",
      "Iteration: 0054 Loss: 1.14372 Time: 0.35415\n",
      "Iteration: 0055 Loss: 1.14413 Time: 0.35284\n",
      "Iteration: 0056 Loss: 1.13512 Time: 0.35738\n",
      "Iteration: 0057 Loss: 1.11441 Time: 0.34401\n",
      "Iteration: 0058 Loss: 1.11566 Time: 0.35493\n",
      "Iteration: 0059 Loss: 1.11986 Time: 0.34716\n",
      "Iteration: 0060 Loss: 1.09807 Time: 0.35935\n",
      "Iteration: 0061 Loss: 1.10443 Time: 0.36510\n",
      "Iteration: 0062 Loss: 1.08461 Time: 0.34456\n",
      "Iteration: 0063 Loss: 1.07283 Time: 0.36960\n",
      "Iteration: 0064 Loss: 1.05472 Time: 0.36392\n",
      "Iteration: 0065 Loss: 1.06632 Time: 0.34931\n",
      "Iteration: 0066 Loss: 1.05810 Time: 0.34536\n",
      "Iteration: 0067 Loss: 1.04793 Time: 0.34895\n",
      "Iteration: 0068 Loss: 1.04850 Time: 0.34678\n",
      "Iteration: 0069 Loss: 1.03529 Time: 0.35440\n",
      "Iteration: 0070 Loss: 1.02880 Time: 0.34667\n",
      "Iteration: 0071 Loss: 1.03581 Time: 0.35957\n",
      "Iteration: 0072 Loss: 1.03243 Time: 0.37360\n",
      "Iteration: 0073 Loss: 1.00995 Time: 0.35150\n",
      "Iteration: 0074 Loss: 1.01540 Time: 0.34453\n",
      "Iteration: 0075 Loss: 0.99921 Time: 0.34117\n",
      "Iteration: 0076 Loss: 0.98794 Time: 0.34892\n",
      "Iteration: 0077 Loss: 0.98211 Time: 0.34864\n",
      "Iteration: 0078 Loss: 0.97694 Time: 0.34766\n",
      "Iteration: 0079 Loss: 0.99681 Time: 0.34973\n",
      "Iteration: 0080 Loss: 0.95856 Time: 0.34794\n",
      "Iteration: 0081 Loss: 0.96937 Time: 0.36441\n",
      "Iteration: 0082 Loss: 0.96091 Time: 0.35319\n",
      "Iteration: 0083 Loss: 0.95512 Time: 0.35385\n",
      "Iteration: 0084 Loss: 0.95012 Time: 0.36020\n",
      "Iteration: 0085 Loss: 0.94640 Time: 0.37003\n",
      "Iteration: 0086 Loss: 0.93646 Time: 0.35165\n",
      "Iteration: 0087 Loss: 0.92767 Time: 0.36174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0088 Loss: 0.92596 Time: 0.37075\n",
      "Iteration: 0089 Loss: 0.92527 Time: 0.37098\n",
      "Iteration: 0090 Loss: 0.91830 Time: 0.37070\n",
      "Iteration: 0091 Loss: 0.90122 Time: 0.36478\n",
      "Iteration: 0092 Loss: 0.89794 Time: 0.37021\n",
      "Iteration: 0093 Loss: 0.89154 Time: 0.37059\n",
      "Iteration: 0094 Loss: 0.88468 Time: 0.35805\n",
      "Iteration: 0095 Loss: 0.88615 Time: 0.36841\n",
      "Iteration: 0096 Loss: 0.87813 Time: 0.35918\n",
      "Iteration: 0097 Loss: 0.87660 Time: 0.37640\n",
      "Iteration: 0098 Loss: 0.87312 Time: 0.36623\n",
      "Iteration: 0099 Loss: 0.86962 Time: 0.36835\n",
      "Iteration: 0100 Loss: 0.84728 Time: 0.35300\n",
      "Iteration: 0101 Loss: 0.84130 Time: 0.33759\n",
      "Iteration: 0102 Loss: 0.85490 Time: 0.34069\n",
      "Iteration: 0103 Loss: 0.82786 Time: 0.36593\n",
      "Iteration: 0104 Loss: 0.84087 Time: 0.35293\n",
      "Iteration: 0105 Loss: 0.83721 Time: 0.34635\n",
      "Iteration: 0106 Loss: 0.82522 Time: 0.35140\n",
      "Iteration: 0107 Loss: 0.81089 Time: 0.33771\n",
      "Iteration: 0108 Loss: 0.82170 Time: 0.35902\n",
      "Iteration: 0109 Loss: 0.80516 Time: 0.36424\n",
      "Iteration: 0110 Loss: 0.78974 Time: 0.35529\n",
      "Iteration: 0111 Loss: 0.80307 Time: 0.36609\n",
      "Iteration: 0112 Loss: 0.79218 Time: 0.34917\n",
      "Iteration: 0113 Loss: 0.78640 Time: 0.35653\n",
      "Iteration: 0114 Loss: 0.78162 Time: 0.36880\n",
      "Iteration: 0115 Loss: 0.77022 Time: 0.34592\n",
      "Iteration: 0116 Loss: 0.76831 Time: 0.34920\n",
      "Iteration: 0117 Loss: 0.77963 Time: 0.35418\n",
      "Iteration: 0118 Loss: 0.77165 Time: 0.35686\n",
      "Iteration: 0119 Loss: 0.76692 Time: 0.34282\n",
      "Iteration: 0120 Loss: 0.75724 Time: 0.35068\n",
      "Iteration: 0121 Loss: 0.73432 Time: 0.36712\n",
      "Iteration: 0122 Loss: 0.73905 Time: 0.37499\n",
      "Iteration: 0123 Loss: 0.74140 Time: 0.36368\n",
      "Iteration: 0124 Loss: 0.74604 Time: 0.34115\n",
      "Iteration: 0125 Loss: 0.73560 Time: 0.35235\n",
      "Iteration: 0126 Loss: 0.72355 Time: 0.36404\n",
      "Iteration: 0127 Loss: 0.72109 Time: 0.36878\n",
      "Iteration: 0128 Loss: 0.72285 Time: 0.34543\n",
      "Iteration: 0129 Loss: 0.70602 Time: 0.34645\n",
      "Iteration: 0130 Loss: 0.71960 Time: 0.34386\n",
      "Iteration: 0131 Loss: 0.71163 Time: 0.35436\n",
      "Iteration: 0132 Loss: 0.71213 Time: 0.34600\n",
      "Iteration: 0133 Loss: 0.69912 Time: 0.34579\n",
      "Iteration: 0134 Loss: 0.70126 Time: 0.35248\n",
      "Iteration: 0135 Loss: 0.68851 Time: 0.36283\n",
      "Iteration: 0136 Loss: 0.68733 Time: 0.37377\n",
      "Iteration: 0137 Loss: 0.68921 Time: 0.35388\n",
      "Iteration: 0138 Loss: 0.68991 Time: 0.34535\n",
      "Iteration: 0139 Loss: 0.67644 Time: 0.34757\n",
      "Iteration: 0140 Loss: 0.67683 Time: 0.35020\n",
      "Iteration: 0141 Loss: 0.67306 Time: 0.36551\n",
      "Iteration: 0142 Loss: 0.67687 Time: 0.36961\n",
      "Iteration: 0143 Loss: 0.66715 Time: 0.34896\n",
      "Iteration: 0144 Loss: 0.66928 Time: 0.34834\n",
      "Iteration: 0145 Loss: 0.66654 Time: 0.34793\n",
      "Iteration: 0146 Loss: 0.65544 Time: 0.35102\n",
      "Iteration: 0147 Loss: 0.65724 Time: 0.34376\n",
      "Iteration: 0148 Loss: 0.65285 Time: 0.34877\n",
      "Iteration: 0149 Loss: 0.65411 Time: 0.36622\n",
      "Iteration: 0150 Loss: 0.65380 Time: 0.37345\n",
      "Iteration: 0151 Loss: 0.65170 Time: 0.38302\n",
      "Iteration: 0152 Loss: 0.64789 Time: 0.37052\n",
      "Iteration: 0153 Loss: 0.64840 Time: 0.36459\n",
      "Iteration: 0154 Loss: 0.64352 Time: 0.35566\n",
      "Iteration: 0155 Loss: 0.63967 Time: 0.35412\n",
      "Iteration: 0156 Loss: 0.63966 Time: 0.35428\n",
      "Iteration: 0157 Loss: 0.63790 Time: 0.34823\n",
      "Iteration: 0158 Loss: 0.63040 Time: 0.33809\n",
      "Iteration: 0159 Loss: 0.63092 Time: 0.34200\n",
      "Iteration: 0160 Loss: 0.62918 Time: 0.36960\n",
      "Iteration: 0161 Loss: 0.63066 Time: 0.36966\n",
      "Iteration: 0162 Loss: 0.62586 Time: 0.37075\n",
      "Iteration: 0163 Loss: 0.62378 Time: 0.35573\n",
      "Iteration: 0164 Loss: 0.61974 Time: 0.34543\n",
      "Iteration: 0165 Loss: 0.61817 Time: 0.35328\n",
      "Iteration: 0166 Loss: 0.61306 Time: 0.34606\n",
      "Iteration: 0167 Loss: 0.61623 Time: 0.35505\n",
      "Iteration: 0168 Loss: 0.61919 Time: 0.34348\n",
      "Iteration: 0169 Loss: 0.61244 Time: 0.34634\n",
      "Iteration: 0170 Loss: 0.61649 Time: 0.34624\n",
      "Iteration: 0171 Loss: 0.61411 Time: 0.35147\n",
      "Iteration: 0172 Loss: 0.60281 Time: 0.35501\n",
      "Iteration: 0173 Loss: 0.60192 Time: 0.36298\n",
      "Iteration: 0174 Loss: 0.60317 Time: 0.35887\n",
      "Iteration: 0175 Loss: 0.60325 Time: 0.35810\n",
      "Iteration: 0176 Loss: 0.60647 Time: 0.35174\n",
      "Iteration: 0177 Loss: 0.60342 Time: 0.34216\n",
      "Iteration: 0178 Loss: 0.59998 Time: 0.36123\n",
      "Iteration: 0179 Loss: 0.60002 Time: 0.37272\n",
      "Iteration: 0180 Loss: 0.60322 Time: 0.35731\n",
      "Iteration: 0181 Loss: 0.59494 Time: 0.34504\n",
      "Iteration: 0182 Loss: 0.59935 Time: 0.36801\n",
      "Iteration: 0183 Loss: 0.58970 Time: 0.35231\n",
      "Iteration: 0184 Loss: 0.59389 Time: 0.33999\n",
      "Iteration: 0185 Loss: 0.59208 Time: 0.34246\n",
      "Iteration: 0186 Loss: 0.58647 Time: 0.34206\n",
      "Iteration: 0187 Loss: 0.58829 Time: 0.35286\n",
      "Iteration: 0188 Loss: 0.58427 Time: 0.35137\n",
      "Iteration: 0189 Loss: 0.58684 Time: 0.35930\n",
      "Iteration: 0190 Loss: 0.59279 Time: 0.35117\n",
      "Iteration: 0191 Loss: 0.58353 Time: 0.34798\n",
      "Iteration: 0192 Loss: 0.58408 Time: 0.35972\n",
      "Iteration: 0193 Loss: 0.58398 Time: 0.34770\n",
      "Iteration: 0194 Loss: 0.58352 Time: 0.36232\n",
      "Iteration: 0195 Loss: 0.58154 Time: 0.34977\n",
      "Iteration: 0196 Loss: 0.57686 Time: 0.35750\n",
      "Iteration: 0197 Loss: 0.57926 Time: 0.35592\n",
      "Iteration: 0198 Loss: 0.57829 Time: 0.37063\n",
      "Iteration: 0199 Loss: 0.57691 Time: 0.37590\n",
      "Iteration: 0200 Loss: 0.57658 Time: 0.37900\n",
      "Done! \n",
      "\n",
      "STEP 3/3 - MODEL EVALUATION \n",
      "\n",
      "Computing the final embedding vectors, for evaluation\n",
      "Done! \n",
      "\n",
      "Testing: link prediction\n",
      "Done! \n",
      "\n",
      "EXPERIMENTS ON MODEL 5 / 10 \n",
      "\n",
      "STEP 1/3 - PREPROCESSING STEPS \n",
      "\n",
      "Masking some edges from the training graph, for link prediction\n",
      "(validation set: 1.0 % of edges - test set: 10.0 % of edges)\n",
      "Done! \n",
      "\n",
      "Preprocessing node features\n",
      "Done! \n",
      "\n",
      "Running the Louvain algorithm for community detection\n",
      "as a preprocessing step for the encoder\n",
      "Done! Louvain has found 574 communities \n",
      "\n",
      "Setting up the model and the optimizer\n",
      "Done! \n",
      "\n",
      "Preprocessing on message passing matrices\n",
      "Done! \n",
      "\n",
      "Initializing TF session\n",
      "Done! \n",
      "\n",
      "STEP 2/3 - MODEL TRAINING \n",
      "\n",
      "Starting training\n",
      "Iteration: 0001 Loss: 1.74044 Time: 0.63351\n",
      "Iteration: 0002 Loss: 1.73208 Time: 0.34400\n",
      "Iteration: 0003 Loss: 1.68380 Time: 0.35375\n",
      "Iteration: 0004 Loss: 1.69663 Time: 0.35062\n",
      "Iteration: 0005 Loss: 1.67578 Time: 0.35700\n",
      "Iteration: 0006 Loss: 1.64671 Time: 0.35840\n",
      "Iteration: 0007 Loss: 1.61961 Time: 0.34920\n",
      "Iteration: 0008 Loss: 1.60724 Time: 0.35508\n",
      "Iteration: 0009 Loss: 1.61968 Time: 0.34713\n",
      "Iteration: 0010 Loss: 1.58131 Time: 0.34998\n",
      "Iteration: 0011 Loss: 1.58221 Time: 0.35211\n",
      "Iteration: 0012 Loss: 1.55383 Time: 0.35100\n",
      "Iteration: 0013 Loss: 1.56956 Time: 0.35136\n",
      "Iteration: 0014 Loss: 1.51020 Time: 0.35355\n",
      "Iteration: 0015 Loss: 1.53526 Time: 0.34927\n",
      "Iteration: 0016 Loss: 1.52476 Time: 0.34986\n",
      "Iteration: 0017 Loss: 1.49399 Time: 0.36078\n",
      "Iteration: 0018 Loss: 1.49074 Time: 0.35079\n",
      "Iteration: 0019 Loss: 1.44009 Time: 0.35246\n",
      "Iteration: 0020 Loss: 1.46980 Time: 0.34864\n",
      "Iteration: 0021 Loss: 1.43322 Time: 0.34496\n",
      "Iteration: 0022 Loss: 1.44849 Time: 0.34649\n",
      "Iteration: 0023 Loss: 1.42640 Time: 0.37012\n",
      "Iteration: 0024 Loss: 1.39370 Time: 0.35652\n",
      "Iteration: 0025 Loss: 1.38755 Time: 0.34549\n",
      "Iteration: 0026 Loss: 1.37931 Time: 0.34512\n",
      "Iteration: 0027 Loss: 1.35525 Time: 0.34511\n",
      "Iteration: 0028 Loss: 1.34839 Time: 0.37093\n",
      "Iteration: 0029 Loss: 1.34211 Time: 0.34899\n",
      "Iteration: 0030 Loss: 1.36067 Time: 0.34793\n",
      "Iteration: 0031 Loss: 1.34368 Time: 0.35109\n",
      "Iteration: 0032 Loss: 1.33161 Time: 0.34755\n",
      "Iteration: 0033 Loss: 1.31390 Time: 0.35402\n",
      "Iteration: 0034 Loss: 1.30562 Time: 0.34655\n",
      "Iteration: 0035 Loss: 1.27492 Time: 0.34795\n",
      "Iteration: 0036 Loss: 1.28426 Time: 0.35019\n",
      "Iteration: 0037 Loss: 1.26874 Time: 0.35363\n",
      "Iteration: 0038 Loss: 1.24755 Time: 0.36407\n",
      "Iteration: 0039 Loss: 1.23001 Time: 0.36942\n",
      "Iteration: 0040 Loss: 1.24742 Time: 0.35398\n",
      "Iteration: 0041 Loss: 1.21648 Time: 0.34715\n",
      "Iteration: 0042 Loss: 1.24676 Time: 0.34364\n",
      "Iteration: 0043 Loss: 1.22047 Time: 0.35570\n",
      "Iteration: 0044 Loss: 1.20760 Time: 0.34597\n",
      "Iteration: 0045 Loss: 1.21782 Time: 0.35251\n",
      "Iteration: 0046 Loss: 1.17327 Time: 0.34994\n",
      "Iteration: 0047 Loss: 1.18403 Time: 0.36687\n",
      "Iteration: 0048 Loss: 1.18315 Time: 0.35536\n",
      "Iteration: 0049 Loss: 1.17415 Time: 0.35284\n",
      "Iteration: 0050 Loss: 1.18563 Time: 0.36134\n",
      "Iteration: 0051 Loss: 1.17027 Time: 0.34691\n",
      "Iteration: 0052 Loss: 1.15463 Time: 0.34707\n",
      "Iteration: 0053 Loss: 1.16080 Time: 0.35452\n",
      "Iteration: 0054 Loss: 1.16160 Time: 0.34903\n",
      "Iteration: 0055 Loss: 1.13083 Time: 0.35180\n",
      "Iteration: 0056 Loss: 1.13979 Time: 0.34819\n",
      "Iteration: 0057 Loss: 1.12179 Time: 0.36514\n",
      "Iteration: 0058 Loss: 1.12713 Time: 0.36161\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0059 Loss: 1.10422 Time: 0.36390\n",
      "Iteration: 0060 Loss: 1.09196 Time: 0.34212\n",
      "Iteration: 0061 Loss: 1.09810 Time: 0.34239\n",
      "Iteration: 0062 Loss: 1.08540 Time: 0.34564\n",
      "Iteration: 0063 Loss: 1.06440 Time: 0.37064\n",
      "Iteration: 0064 Loss: 1.07300 Time: 0.35002\n",
      "Iteration: 0065 Loss: 1.06611 Time: 0.36649\n",
      "Iteration: 0066 Loss: 1.05985 Time: 0.35862\n",
      "Iteration: 0067 Loss: 1.04898 Time: 0.35055\n",
      "Iteration: 0068 Loss: 1.04577 Time: 0.34454\n",
      "Iteration: 0069 Loss: 1.03649 Time: 0.34835\n",
      "Iteration: 0070 Loss: 1.02957 Time: 0.34746\n",
      "Iteration: 0071 Loss: 1.01773 Time: 0.34133\n",
      "Iteration: 0072 Loss: 1.02294 Time: 0.35709\n",
      "Iteration: 0073 Loss: 1.02970 Time: 0.36401\n",
      "Iteration: 0074 Loss: 1.00956 Time: 0.37408\n",
      "Iteration: 0075 Loss: 1.00155 Time: 0.35199\n",
      "Iteration: 0076 Loss: 1.00039 Time: 0.34414\n",
      "Iteration: 0077 Loss: 1.00759 Time: 0.35058\n",
      "Iteration: 0078 Loss: 0.99079 Time: 0.34742\n",
      "Iteration: 0079 Loss: 0.98624 Time: 0.34477\n",
      "Iteration: 0080 Loss: 0.96770 Time: 0.35133\n",
      "Iteration: 0081 Loss: 0.97159 Time: 0.36959\n",
      "Iteration: 0082 Loss: 0.96377 Time: 0.34292\n",
      "Iteration: 0083 Loss: 0.95994 Time: 0.36051\n",
      "Iteration: 0084 Loss: 0.93812 Time: 0.36039\n",
      "Iteration: 0085 Loss: 0.93758 Time: 0.36953\n",
      "Iteration: 0086 Loss: 0.95321 Time: 0.35701\n",
      "Iteration: 0087 Loss: 0.93285 Time: 0.34632\n",
      "Iteration: 0088 Loss: 0.92380 Time: 0.34705\n",
      "Iteration: 0089 Loss: 0.91730 Time: 0.35300\n",
      "Iteration: 0090 Loss: 0.90367 Time: 0.36056\n",
      "Iteration: 0091 Loss: 0.91236 Time: 0.36155\n",
      "Iteration: 0092 Loss: 0.89392 Time: 0.34579\n",
      "Iteration: 0093 Loss: 0.91166 Time: 0.33708\n",
      "Iteration: 0094 Loss: 0.89589 Time: 0.34070\n",
      "Iteration: 0095 Loss: 0.89063 Time: 0.36700\n",
      "Iteration: 0096 Loss: 0.88794 Time: 0.34938\n",
      "Iteration: 0097 Loss: 0.87749 Time: 0.34562\n",
      "Iteration: 0098 Loss: 0.85332 Time: 0.35842\n",
      "Iteration: 0099 Loss: 0.86163 Time: 0.33943\n",
      "Iteration: 0100 Loss: 0.86664 Time: 0.36277\n",
      "Iteration: 0101 Loss: 0.84011 Time: 0.37357\n",
      "Iteration: 0102 Loss: 0.83661 Time: 0.36181\n",
      "Iteration: 0103 Loss: 0.84438 Time: 0.35392\n",
      "Iteration: 0104 Loss: 0.82281 Time: 0.34953\n",
      "Iteration: 0105 Loss: 0.83511 Time: 0.34616\n",
      "Iteration: 0106 Loss: 0.81793 Time: 0.36147\n",
      "Iteration: 0107 Loss: 0.80836 Time: 0.36367\n",
      "Iteration: 0108 Loss: 0.82038 Time: 0.34671\n",
      "Iteration: 0109 Loss: 0.80796 Time: 0.35413\n",
      "Iteration: 0110 Loss: 0.79868 Time: 0.34086\n",
      "Iteration: 0111 Loss: 0.79150 Time: 0.34657\n",
      "Iteration: 0112 Loss: 0.79366 Time: 0.35571\n",
      "Iteration: 0113 Loss: 0.78461 Time: 0.36194\n",
      "Iteration: 0114 Loss: 0.78798 Time: 0.35773\n",
      "Iteration: 0115 Loss: 0.78585 Time: 0.37301\n",
      "Iteration: 0116 Loss: 0.76948 Time: 0.34625\n",
      "Iteration: 0117 Loss: 0.78221 Time: 0.34756\n",
      "Iteration: 0118 Loss: 0.77260 Time: 0.34969\n",
      "Iteration: 0119 Loss: 0.76561 Time: 0.35877\n",
      "Iteration: 0120 Loss: 0.75732 Time: 0.34784\n",
      "Iteration: 0121 Loss: 0.76203 Time: 0.34149\n",
      "Iteration: 0122 Loss: 0.75622 Time: 0.35367\n",
      "Iteration: 0123 Loss: 0.73670 Time: 0.35506\n",
      "Iteration: 0124 Loss: 0.73675 Time: 0.35285\n",
      "Iteration: 0125 Loss: 0.73202 Time: 0.36760\n",
      "Iteration: 0126 Loss: 0.73169 Time: 0.34876\n",
      "Iteration: 0127 Loss: 0.72485 Time: 0.34237\n",
      "Iteration: 0128 Loss: 0.73247 Time: 0.34273\n",
      "Iteration: 0129 Loss: 0.71334 Time: 0.35201\n",
      "Iteration: 0130 Loss: 0.72288 Time: 0.36520\n",
      "Iteration: 0131 Loss: 0.71894 Time: 0.36826\n",
      "Iteration: 0132 Loss: 0.70410 Time: 0.35140\n",
      "Iteration: 0133 Loss: 0.70694 Time: 0.34440\n",
      "Iteration: 0134 Loss: 0.69654 Time: 0.34788\n",
      "Iteration: 0135 Loss: 0.69048 Time: 0.36353\n",
      "Iteration: 0136 Loss: 0.69268 Time: 0.36532\n",
      "Iteration: 0137 Loss: 0.69286 Time: 0.36320\n",
      "Iteration: 0138 Loss: 0.68869 Time: 0.35504\n",
      "Iteration: 0139 Loss: 0.68366 Time: 0.34456\n",
      "Iteration: 0140 Loss: 0.68147 Time: 0.34762\n",
      "Iteration: 0141 Loss: 0.67249 Time: 0.35429\n",
      "Iteration: 0142 Loss: 0.67166 Time: 0.35153\n",
      "Iteration: 0143 Loss: 0.67261 Time: 0.35061\n",
      "Iteration: 0144 Loss: 0.67132 Time: 0.34648\n",
      "Iteration: 0145 Loss: 0.66753 Time: 0.36148\n",
      "Iteration: 0146 Loss: 0.66108 Time: 0.36345\n",
      "Iteration: 0147 Loss: 0.65619 Time: 0.34638\n",
      "Iteration: 0148 Loss: 0.65715 Time: 0.36983\n",
      "Iteration: 0149 Loss: 0.65035 Time: 0.35115\n",
      "Iteration: 0150 Loss: 0.65379 Time: 0.35338\n",
      "Iteration: 0151 Loss: 0.65211 Time: 0.34798\n",
      "Iteration: 0152 Loss: 0.64835 Time: 0.34999\n",
      "Iteration: 0153 Loss: 0.64197 Time: 0.36440\n",
      "Iteration: 0154 Loss: 0.64429 Time: 0.34838\n",
      "Iteration: 0155 Loss: 0.64496 Time: 0.36303\n",
      "Iteration: 0156 Loss: 0.63691 Time: 0.36915\n",
      "Iteration: 0157 Loss: 0.63467 Time: 0.35478\n",
      "Iteration: 0158 Loss: 0.63570 Time: 0.35026\n",
      "Iteration: 0159 Loss: 0.63427 Time: 0.36099\n",
      "Iteration: 0160 Loss: 0.63801 Time: 0.36671\n",
      "Iteration: 0161 Loss: 0.62330 Time: 0.35461\n",
      "Iteration: 0162 Loss: 0.62490 Time: 0.34656\n",
      "Iteration: 0163 Loss: 0.62041 Time: 0.35281\n",
      "Iteration: 0164 Loss: 0.62357 Time: 0.34739\n",
      "Iteration: 0165 Loss: 0.62403 Time: 0.33734\n",
      "Iteration: 0166 Loss: 0.62183 Time: 0.34911\n",
      "Iteration: 0167 Loss: 0.61666 Time: 0.34563\n",
      "Iteration: 0168 Loss: 0.61815 Time: 0.35321\n",
      "Iteration: 0169 Loss: 0.61361 Time: 0.34400\n",
      "Iteration: 0170 Loss: 0.61238 Time: 0.35155\n",
      "Iteration: 0171 Loss: 0.61233 Time: 0.34310\n",
      "Iteration: 0172 Loss: 0.60814 Time: 0.34570\n",
      "Iteration: 0173 Loss: 0.61095 Time: 0.35289\n",
      "Iteration: 0174 Loss: 0.60755 Time: 0.34644\n",
      "Iteration: 0175 Loss: 0.60292 Time: 0.36281\n",
      "Iteration: 0176 Loss: 0.60280 Time: 0.36301\n",
      "Iteration: 0177 Loss: 0.59877 Time: 0.35227\n",
      "Iteration: 0178 Loss: 0.60226 Time: 0.36365\n",
      "Iteration: 0179 Loss: 0.59647 Time: 0.37375\n",
      "Iteration: 0180 Loss: 0.59926 Time: 0.37626\n",
      "Iteration: 0181 Loss: 0.59504 Time: 0.35367\n",
      "Iteration: 0182 Loss: 0.59532 Time: 0.34700\n",
      "Iteration: 0183 Loss: 0.59827 Time: 0.34129\n",
      "Iteration: 0184 Loss: 0.59104 Time: 0.34777\n",
      "Iteration: 0185 Loss: 0.58667 Time: 0.35919\n",
      "Iteration: 0186 Loss: 0.58868 Time: 0.36607\n",
      "Iteration: 0187 Loss: 0.58314 Time: 0.35357\n",
      "Iteration: 0188 Loss: 0.58559 Time: 0.34820\n",
      "Iteration: 0189 Loss: 0.58751 Time: 0.35101\n",
      "Iteration: 0190 Loss: 0.58583 Time: 0.35354\n",
      "Iteration: 0191 Loss: 0.58136 Time: 0.35258\n",
      "Iteration: 0192 Loss: 0.58571 Time: 0.36062\n",
      "Iteration: 0193 Loss: 0.57894 Time: 0.36387\n",
      "Iteration: 0194 Loss: 0.57559 Time: 0.35729\n",
      "Iteration: 0195 Loss: 0.58142 Time: 0.34938\n",
      "Iteration: 0196 Loss: 0.58012 Time: 0.34191\n",
      "Iteration: 0197 Loss: 0.57454 Time: 0.34715\n",
      "Iteration: 0198 Loss: 0.57654 Time: 0.36870\n",
      "Iteration: 0199 Loss: 0.57547 Time: 0.36400\n",
      "Iteration: 0200 Loss: 0.57938 Time: 0.36239\n",
      "Done! \n",
      "\n",
      "STEP 3/3 - MODEL EVALUATION \n",
      "\n",
      "Computing the final embedding vectors, for evaluation\n",
      "Done! \n",
      "\n",
      "Testing: link prediction\n",
      "Done! \n",
      "\n",
      "EXPERIMENTS ON MODEL 6 / 10 \n",
      "\n",
      "STEP 1/3 - PREPROCESSING STEPS \n",
      "\n",
      "Masking some edges from the training graph, for link prediction\n",
      "(validation set: 1.0 % of edges - test set: 10.0 % of edges)\n",
      "Done! \n",
      "\n",
      "Preprocessing node features\n",
      "Done! \n",
      "\n",
      "Running the Louvain algorithm for community detection\n",
      "as a preprocessing step for the encoder\n",
      "Done! Louvain has found 575 communities \n",
      "\n",
      "Setting up the model and the optimizer\n",
      "Done! \n",
      "\n",
      "Preprocessing on message passing matrices\n",
      "Done! \n",
      "\n",
      "Initializing TF session\n",
      "Done! \n",
      "\n",
      "STEP 2/3 - MODEL TRAINING \n",
      "\n",
      "Starting training\n",
      "Iteration: 0001 Loss: 1.75176 Time: 0.63249\n",
      "Iteration: 0002 Loss: 1.70997 Time: 0.34979\n",
      "Iteration: 0003 Loss: 1.70473 Time: 0.35700\n",
      "Iteration: 0004 Loss: 1.68047 Time: 0.34484\n",
      "Iteration: 0005 Loss: 1.65101 Time: 0.35579\n",
      "Iteration: 0006 Loss: 1.65758 Time: 0.34215\n",
      "Iteration: 0007 Loss: 1.60278 Time: 0.36065\n",
      "Iteration: 0008 Loss: 1.58969 Time: 0.34987\n",
      "Iteration: 0009 Loss: 1.62831 Time: 0.35129\n",
      "Iteration: 0010 Loss: 1.57566 Time: 0.36239\n",
      "Iteration: 0011 Loss: 1.59054 Time: 0.35267\n",
      "Iteration: 0012 Loss: 1.54403 Time: 0.34272\n",
      "Iteration: 0013 Loss: 1.50887 Time: 0.34660\n",
      "Iteration: 0014 Loss: 1.54389 Time: 0.34599\n",
      "Iteration: 0015 Loss: 1.51721 Time: 0.34338\n",
      "Iteration: 0016 Loss: 1.51850 Time: 0.34860\n",
      "Iteration: 0017 Loss: 1.47152 Time: 0.35524\n",
      "Iteration: 0018 Loss: 1.46836 Time: 0.34390\n",
      "Iteration: 0019 Loss: 1.45789 Time: 0.35199\n",
      "Iteration: 0020 Loss: 1.42782 Time: 0.35657\n",
      "Iteration: 0021 Loss: 1.43503 Time: 0.34901\n",
      "Iteration: 0022 Loss: 1.44109 Time: 0.34552\n",
      "Iteration: 0023 Loss: 1.42416 Time: 0.33410\n",
      "Iteration: 0024 Loss: 1.37988 Time: 0.34699\n",
      "Iteration: 0025 Loss: 1.38817 Time: 0.34677\n",
      "Iteration: 0026 Loss: 1.39038 Time: 0.35418\n",
      "Iteration: 0027 Loss: 1.35892 Time: 0.34919\n",
      "Iteration: 0028 Loss: 1.34622 Time: 0.36093\n",
      "Iteration: 0029 Loss: 1.34009 Time: 0.36066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0030 Loss: 1.32983 Time: 0.36654\n",
      "Iteration: 0031 Loss: 1.33551 Time: 0.37018\n",
      "Iteration: 0032 Loss: 1.32059 Time: 0.36409\n",
      "Iteration: 0033 Loss: 1.29508 Time: 0.36359\n",
      "Iteration: 0034 Loss: 1.29847 Time: 0.35916\n",
      "Iteration: 0035 Loss: 1.29211 Time: 0.36623\n",
      "Iteration: 0036 Loss: 1.29196 Time: 0.37331\n",
      "Iteration: 0037 Loss: 1.26750 Time: 0.34810\n",
      "Iteration: 0038 Loss: 1.27264 Time: 0.34822\n",
      "Iteration: 0039 Loss: 1.23651 Time: 0.34394\n",
      "Iteration: 0040 Loss: 1.25485 Time: 0.33787\n",
      "Iteration: 0041 Loss: 1.23625 Time: 0.35595\n",
      "Iteration: 0042 Loss: 1.22395 Time: 0.35112\n",
      "Iteration: 0043 Loss: 1.20965 Time: 0.34875\n",
      "Iteration: 0044 Loss: 1.21880 Time: 0.35079\n",
      "Iteration: 0045 Loss: 1.18862 Time: 0.35946\n",
      "Iteration: 0046 Loss: 1.18471 Time: 0.33880\n",
      "Iteration: 0047 Loss: 1.20540 Time: 0.35937\n",
      "Iteration: 0048 Loss: 1.18298 Time: 0.37096\n",
      "Iteration: 0049 Loss: 1.17604 Time: 0.37820\n",
      "Iteration: 0050 Loss: 1.16017 Time: 0.35214\n",
      "Iteration: 0051 Loss: 1.15260 Time: 0.34537\n",
      "Iteration: 0052 Loss: 1.15824 Time: 0.34265\n",
      "Iteration: 0053 Loss: 1.14567 Time: 0.35626\n",
      "Iteration: 0054 Loss: 1.13881 Time: 0.36783\n",
      "Iteration: 0055 Loss: 1.11596 Time: 0.36452\n",
      "Iteration: 0056 Loss: 1.13981 Time: 0.36226\n",
      "Iteration: 0057 Loss: 1.12884 Time: 0.34282\n",
      "Iteration: 0058 Loss: 1.13846 Time: 0.36044\n",
      "Iteration: 0059 Loss: 1.09787 Time: 0.36141\n",
      "Iteration: 0060 Loss: 1.09283 Time: 0.34735\n",
      "Iteration: 0061 Loss: 1.09522 Time: 0.34926\n",
      "Iteration: 0062 Loss: 1.08443 Time: 0.34481\n",
      "Iteration: 0063 Loss: 1.07166 Time: 0.34584\n",
      "Iteration: 0064 Loss: 1.05691 Time: 0.33998\n",
      "Iteration: 0065 Loss: 1.07821 Time: 0.35139\n",
      "Iteration: 0066 Loss: 1.06337 Time: 0.34559\n",
      "Iteration: 0067 Loss: 1.05086 Time: 0.34985\n",
      "Iteration: 0068 Loss: 1.05087 Time: 0.34527\n",
      "Iteration: 0069 Loss: 1.03618 Time: 0.34646\n",
      "Iteration: 0070 Loss: 1.02687 Time: 0.35301\n",
      "Iteration: 0071 Loss: 1.02296 Time: 0.35211\n",
      "Iteration: 0072 Loss: 1.00568 Time: 0.36988\n",
      "Iteration: 0073 Loss: 1.01279 Time: 0.36762\n",
      "Iteration: 0074 Loss: 1.00356 Time: 0.35480\n",
      "Iteration: 0075 Loss: 1.01188 Time: 0.34625\n",
      "Iteration: 0076 Loss: 1.00196 Time: 0.35035\n",
      "Iteration: 0077 Loss: 1.00716 Time: 0.34828\n",
      "Iteration: 0078 Loss: 0.98090 Time: 0.36642\n",
      "Iteration: 0079 Loss: 0.96220 Time: 0.36178\n",
      "Iteration: 0080 Loss: 0.96702 Time: 0.33713\n",
      "Iteration: 0081 Loss: 0.96596 Time: 0.35607\n",
      "Iteration: 0082 Loss: 0.98164 Time: 0.35333\n",
      "Iteration: 0083 Loss: 0.93808 Time: 0.34797\n",
      "Iteration: 0084 Loss: 0.93454 Time: 0.34712\n",
      "Iteration: 0085 Loss: 0.94147 Time: 0.34847\n",
      "Iteration: 0086 Loss: 0.93660 Time: 0.34542\n",
      "Iteration: 0087 Loss: 0.93699 Time: 0.34912\n",
      "Iteration: 0088 Loss: 0.91662 Time: 0.35564\n",
      "Iteration: 0089 Loss: 0.91965 Time: 0.35545\n",
      "Iteration: 0090 Loss: 0.91366 Time: 0.35157\n",
      "Iteration: 0091 Loss: 0.89264 Time: 0.33595\n",
      "Iteration: 0092 Loss: 0.89701 Time: 0.34738\n",
      "Iteration: 0093 Loss: 0.89591 Time: 0.35745\n",
      "Iteration: 0094 Loss: 0.89730 Time: 0.37332\n",
      "Iteration: 0095 Loss: 0.87685 Time: 0.36905\n",
      "Iteration: 0096 Loss: 0.87131 Time: 0.37003\n",
      "Iteration: 0097 Loss: 0.85389 Time: 0.35672\n",
      "Iteration: 0098 Loss: 0.85391 Time: 0.34768\n",
      "Iteration: 0099 Loss: 0.85966 Time: 0.35421\n",
      "Iteration: 0100 Loss: 0.86109 Time: 0.34843\n",
      "Iteration: 0101 Loss: 0.84294 Time: 0.34543\n",
      "Iteration: 0102 Loss: 0.83609 Time: 0.34971\n",
      "Iteration: 0103 Loss: 0.83997 Time: 0.35873\n",
      "Iteration: 0104 Loss: 0.82523 Time: 0.35176\n",
      "Iteration: 0105 Loss: 0.82137 Time: 0.37065\n",
      "Iteration: 0106 Loss: 0.81562 Time: 0.35889\n",
      "Iteration: 0107 Loss: 0.81142 Time: 0.34723\n",
      "Iteration: 0108 Loss: 0.81540 Time: 0.33635\n",
      "Iteration: 0109 Loss: 0.80410 Time: 0.36599\n",
      "Iteration: 0110 Loss: 0.80019 Time: 0.34674\n",
      "Iteration: 0111 Loss: 0.79275 Time: 0.34596\n",
      "Iteration: 0112 Loss: 0.79178 Time: 0.37600\n",
      "Iteration: 0113 Loss: 0.78432 Time: 0.37116\n",
      "Iteration: 0114 Loss: 0.77681 Time: 0.37420\n",
      "Iteration: 0115 Loss: 0.76404 Time: 0.37000\n",
      "Iteration: 0116 Loss: 0.76070 Time: 0.36126\n",
      "Iteration: 0117 Loss: 0.76128 Time: 0.35273\n",
      "Iteration: 0118 Loss: 0.75459 Time: 0.36243\n",
      "Iteration: 0119 Loss: 0.75596 Time: 0.34673\n",
      "Iteration: 0120 Loss: 0.75167 Time: 0.34680\n",
      "Iteration: 0121 Loss: 0.75658 Time: 0.34431\n",
      "Iteration: 0122 Loss: 0.74236 Time: 0.34550\n",
      "Iteration: 0123 Loss: 0.74942 Time: 0.35100\n",
      "Iteration: 0124 Loss: 0.72649 Time: 0.34535\n",
      "Iteration: 0125 Loss: 0.73248 Time: 0.34305\n",
      "Iteration: 0126 Loss: 0.72732 Time: 0.35868\n",
      "Iteration: 0127 Loss: 0.71997 Time: 0.35605\n",
      "Iteration: 0128 Loss: 0.71368 Time: 0.34938\n",
      "Iteration: 0129 Loss: 0.71387 Time: 0.36013\n",
      "Iteration: 0130 Loss: 0.70783 Time: 0.35398\n",
      "Iteration: 0131 Loss: 0.70722 Time: 0.36594\n",
      "Iteration: 0132 Loss: 0.70299 Time: 0.36088\n",
      "Iteration: 0133 Loss: 0.69021 Time: 0.35394\n",
      "Iteration: 0134 Loss: 0.69907 Time: 0.35125\n",
      "Iteration: 0135 Loss: 0.69043 Time: 0.34750\n",
      "Iteration: 0136 Loss: 0.69100 Time: 0.35432\n",
      "Iteration: 0137 Loss: 0.68047 Time: 0.34600\n",
      "Iteration: 0138 Loss: 0.68833 Time: 0.34817\n",
      "Iteration: 0139 Loss: 0.67382 Time: 0.36766\n",
      "Iteration: 0140 Loss: 0.67893 Time: 0.35769\n",
      "Iteration: 0141 Loss: 0.67796 Time: 0.34228\n",
      "Iteration: 0142 Loss: 0.67461 Time: 0.36076\n",
      "Iteration: 0143 Loss: 0.66116 Time: 0.35321\n",
      "Iteration: 0144 Loss: 0.67106 Time: 0.35237\n",
      "Iteration: 0145 Loss: 0.65766 Time: 0.34301\n",
      "Iteration: 0146 Loss: 0.65993 Time: 0.34078\n",
      "Iteration: 0147 Loss: 0.65347 Time: 0.34384\n",
      "Iteration: 0148 Loss: 0.65348 Time: 0.33823\n",
      "Iteration: 0149 Loss: 0.64722 Time: 0.34901\n",
      "Iteration: 0150 Loss: 0.65119 Time: 0.35943\n",
      "Iteration: 0151 Loss: 0.65363 Time: 0.34456\n",
      "Iteration: 0152 Loss: 0.64086 Time: 0.33904\n",
      "Iteration: 0153 Loss: 0.64216 Time: 0.35441\n",
      "Iteration: 0154 Loss: 0.64971 Time: 0.36224\n",
      "Iteration: 0155 Loss: 0.64371 Time: 0.36503\n",
      "Iteration: 0156 Loss: 0.63443 Time: 0.34908\n",
      "Iteration: 0157 Loss: 0.63915 Time: 0.35335\n",
      "Iteration: 0158 Loss: 0.63288 Time: 0.34395\n",
      "Iteration: 0159 Loss: 0.62495 Time: 0.33898\n",
      "Iteration: 0160 Loss: 0.62925 Time: 0.36299\n",
      "Iteration: 0161 Loss: 0.62825 Time: 0.33998\n",
      "Iteration: 0162 Loss: 0.61824 Time: 0.36301\n",
      "Iteration: 0163 Loss: 0.61923 Time: 0.37039\n",
      "Iteration: 0164 Loss: 0.62156 Time: 0.35150\n",
      "Iteration: 0165 Loss: 0.61957 Time: 0.34744\n",
      "Iteration: 0166 Loss: 0.61849 Time: 0.34868\n",
      "Iteration: 0167 Loss: 0.61427 Time: 0.36178\n",
      "Iteration: 0168 Loss: 0.61322 Time: 0.36763\n",
      "Iteration: 0169 Loss: 0.61178 Time: 0.34940\n",
      "Iteration: 0170 Loss: 0.61085 Time: 0.33171\n",
      "Iteration: 0171 Loss: 0.60826 Time: 0.36145\n",
      "Iteration: 0172 Loss: 0.60692 Time: 0.36095\n",
      "Iteration: 0173 Loss: 0.60220 Time: 0.34824\n",
      "Iteration: 0174 Loss: 0.60237 Time: 0.35237\n",
      "Iteration: 0175 Loss: 0.60347 Time: 0.36888\n",
      "Iteration: 0176 Loss: 0.60202 Time: 0.34055\n",
      "Iteration: 0177 Loss: 0.60525 Time: 0.34653\n",
      "Iteration: 0178 Loss: 0.59999 Time: 0.35070\n",
      "Iteration: 0179 Loss: 0.59994 Time: 0.34576\n",
      "Iteration: 0180 Loss: 0.59273 Time: 0.34744\n",
      "Iteration: 0181 Loss: 0.59876 Time: 0.34985\n",
      "Iteration: 0182 Loss: 0.59610 Time: 0.35131\n",
      "Iteration: 0183 Loss: 0.59188 Time: 0.34457\n",
      "Iteration: 0184 Loss: 0.59184 Time: 0.36388\n",
      "Iteration: 0185 Loss: 0.59144 Time: 0.35702\n",
      "Iteration: 0186 Loss: 0.58903 Time: 0.36982\n",
      "Iteration: 0187 Loss: 0.58950 Time: 0.36285\n",
      "Iteration: 0188 Loss: 0.58552 Time: 0.35617\n",
      "Iteration: 0189 Loss: 0.58768 Time: 0.35378\n",
      "Iteration: 0190 Loss: 0.58867 Time: 0.35143\n",
      "Iteration: 0191 Loss: 0.58736 Time: 0.35575\n",
      "Iteration: 0192 Loss: 0.58222 Time: 0.34901\n",
      "Iteration: 0193 Loss: 0.58111 Time: 0.34026\n",
      "Iteration: 0194 Loss: 0.57779 Time: 0.34965\n",
      "Iteration: 0195 Loss: 0.57988 Time: 0.34599\n",
      "Iteration: 0196 Loss: 0.58040 Time: 0.35700\n",
      "Iteration: 0197 Loss: 0.57798 Time: 0.34618\n",
      "Iteration: 0198 Loss: 0.57934 Time: 0.35967\n",
      "Iteration: 0199 Loss: 0.57361 Time: 0.36808\n",
      "Iteration: 0200 Loss: 0.57219 Time: 0.37176\n",
      "Done! \n",
      "\n",
      "STEP 3/3 - MODEL EVALUATION \n",
      "\n",
      "Computing the final embedding vectors, for evaluation\n",
      "Done! \n",
      "\n",
      "Testing: link prediction\n",
      "Done! \n",
      "\n",
      "EXPERIMENTS ON MODEL 7 / 10 \n",
      "\n",
      "STEP 1/3 - PREPROCESSING STEPS \n",
      "\n",
      "Masking some edges from the training graph, for link prediction\n",
      "(validation set: 1.0 % of edges - test set: 10.0 % of edges)\n",
      "Done! \n",
      "\n",
      "Preprocessing node features\n",
      "Done! \n",
      "\n",
      "Running the Louvain algorithm for community detection\n",
      "as a preprocessing step for the encoder\n",
      "Done! Louvain has found 562 communities \n",
      "\n",
      "Setting up the model and the optimizer\n",
      "Done! \n",
      "\n",
      "Preprocessing on message passing matrices\n",
      "Done! \n",
      "\n",
      "Initializing TF session\n",
      "Done! \n",
      "\n",
      "STEP 2/3 - MODEL TRAINING \n",
      "\n",
      "Starting training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0001 Loss: 1.73564 Time: 0.65100\n",
      "Iteration: 0002 Loss: 1.72959 Time: 0.35179\n",
      "Iteration: 0003 Loss: 1.68907 Time: 0.34437\n",
      "Iteration: 0004 Loss: 1.68176 Time: 0.35416\n",
      "Iteration: 0005 Loss: 1.68285 Time: 0.35448\n",
      "Iteration: 0006 Loss: 1.65073 Time: 0.35030\n",
      "Iteration: 0007 Loss: 1.65613 Time: 0.35547\n",
      "Iteration: 0008 Loss: 1.64184 Time: 0.35550\n",
      "Iteration: 0009 Loss: 1.61626 Time: 0.35971\n",
      "Iteration: 0010 Loss: 1.58925 Time: 0.35030\n",
      "Iteration: 0011 Loss: 1.57449 Time: 0.35097\n",
      "Iteration: 0012 Loss: 1.55177 Time: 0.35003\n",
      "Iteration: 0013 Loss: 1.52981 Time: 0.35502\n",
      "Iteration: 0014 Loss: 1.51987 Time: 0.35492\n",
      "Iteration: 0015 Loss: 1.49439 Time: 0.34342\n",
      "Iteration: 0016 Loss: 1.52860 Time: 0.35695\n",
      "Iteration: 0017 Loss: 1.47827 Time: 0.34587\n",
      "Iteration: 0018 Loss: 1.47368 Time: 0.34407\n",
      "Iteration: 0019 Loss: 1.46826 Time: 0.36601\n",
      "Iteration: 0020 Loss: 1.44815 Time: 0.35200\n",
      "Iteration: 0021 Loss: 1.44719 Time: 0.33769\n",
      "Iteration: 0022 Loss: 1.43344 Time: 0.34475\n",
      "Iteration: 0023 Loss: 1.44406 Time: 0.34918\n",
      "Iteration: 0024 Loss: 1.41695 Time: 0.35045\n",
      "Iteration: 0025 Loss: 1.36805 Time: 0.35395\n",
      "Iteration: 0026 Loss: 1.36435 Time: 0.34810\n",
      "Iteration: 0027 Loss: 1.35504 Time: 0.35696\n",
      "Iteration: 0028 Loss: 1.36213 Time: 0.37970\n",
      "Iteration: 0029 Loss: 1.36222 Time: 0.36589\n",
      "Iteration: 0030 Loss: 1.35593 Time: 0.35047\n",
      "Iteration: 0031 Loss: 1.32513 Time: 0.34791\n",
      "Iteration: 0032 Loss: 1.33990 Time: 0.34815\n",
      "Iteration: 0033 Loss: 1.31098 Time: 0.34502\n",
      "Iteration: 0034 Loss: 1.30368 Time: 0.36569\n",
      "Iteration: 0035 Loss: 1.27881 Time: 0.36360\n",
      "Iteration: 0036 Loss: 1.25703 Time: 0.35441\n",
      "Iteration: 0037 Loss: 1.26880 Time: 0.35269\n",
      "Iteration: 0038 Loss: 1.25606 Time: 0.35111\n",
      "Iteration: 0039 Loss: 1.26503 Time: 0.34729\n",
      "Iteration: 0040 Loss: 1.26350 Time: 0.34681\n",
      "Iteration: 0041 Loss: 1.25085 Time: 0.34522\n",
      "Iteration: 0042 Loss: 1.23600 Time: 0.34881\n",
      "Iteration: 0043 Loss: 1.21694 Time: 0.35212\n",
      "Iteration: 0044 Loss: 1.21051 Time: 0.36383\n",
      "Iteration: 0045 Loss: 1.20281 Time: 0.36936\n",
      "Iteration: 0046 Loss: 1.19234 Time: 0.36013\n",
      "Iteration: 0047 Loss: 1.18940 Time: 0.35497\n",
      "Iteration: 0048 Loss: 1.18544 Time: 0.36835\n",
      "Iteration: 0049 Loss: 1.16800 Time: 0.34589\n",
      "Iteration: 0050 Loss: 1.15303 Time: 0.34559\n",
      "Iteration: 0051 Loss: 1.17115 Time: 0.35764\n",
      "Iteration: 0052 Loss: 1.16034 Time: 0.35609\n",
      "Iteration: 0053 Loss: 1.14530 Time: 0.34971\n",
      "Iteration: 0054 Loss: 1.13799 Time: 0.34290\n",
      "Iteration: 0055 Loss: 1.12225 Time: 0.34940\n",
      "Iteration: 0056 Loss: 1.12439 Time: 0.34488\n",
      "Iteration: 0057 Loss: 1.10669 Time: 0.35304\n",
      "Iteration: 0058 Loss: 1.11415 Time: 0.35731\n",
      "Iteration: 0059 Loss: 1.10590 Time: 0.37167\n",
      "Iteration: 0060 Loss: 1.11147 Time: 0.36180\n",
      "Iteration: 0061 Loss: 1.09491 Time: 0.35391\n",
      "Iteration: 0062 Loss: 1.09795 Time: 0.34955\n",
      "Iteration: 0063 Loss: 1.07575 Time: 0.34850\n",
      "Iteration: 0064 Loss: 1.06962 Time: 0.36039\n",
      "Iteration: 0065 Loss: 1.05914 Time: 0.36200\n",
      "Iteration: 0066 Loss: 1.06170 Time: 0.34196\n",
      "Iteration: 0067 Loss: 1.05355 Time: 0.34563\n",
      "Iteration: 0068 Loss: 1.06950 Time: 0.34141\n",
      "Iteration: 0069 Loss: 1.03975 Time: 0.34774\n",
      "Iteration: 0070 Loss: 1.02644 Time: 0.35459\n",
      "Iteration: 0071 Loss: 1.01843 Time: 0.35917\n",
      "Iteration: 0072 Loss: 1.01064 Time: 0.33978\n",
      "Iteration: 0073 Loss: 1.01440 Time: 0.36921\n",
      "Iteration: 0074 Loss: 1.01662 Time: 0.35568\n",
      "Iteration: 0075 Loss: 1.00313 Time: 0.35905\n",
      "Iteration: 0076 Loss: 1.00776 Time: 0.36566\n",
      "Iteration: 0077 Loss: 0.98917 Time: 0.34825\n",
      "Iteration: 0078 Loss: 0.97250 Time: 0.34085\n",
      "Iteration: 0079 Loss: 0.97587 Time: 0.36600\n",
      "Iteration: 0080 Loss: 0.97241 Time: 0.35313\n",
      "Iteration: 0081 Loss: 0.96030 Time: 0.34500\n",
      "Iteration: 0082 Loss: 0.96010 Time: 0.35700\n",
      "Iteration: 0083 Loss: 0.94952 Time: 0.35143\n",
      "Iteration: 0084 Loss: 0.92711 Time: 0.37478\n",
      "Iteration: 0085 Loss: 0.96912 Time: 0.38800\n",
      "Iteration: 0086 Loss: 0.93169 Time: 0.36117\n",
      "Iteration: 0087 Loss: 0.93553 Time: 0.37999\n",
      "Iteration: 0088 Loss: 0.92645 Time: 0.37884\n",
      "Iteration: 0089 Loss: 0.90995 Time: 0.35882\n",
      "Iteration: 0090 Loss: 0.91389 Time: 0.35598\n",
      "Iteration: 0091 Loss: 0.90959 Time: 0.35120\n",
      "Iteration: 0092 Loss: 0.91022 Time: 0.37642\n",
      "Iteration: 0093 Loss: 0.89244 Time: 0.38087\n",
      "Iteration: 0094 Loss: 0.87578 Time: 0.34992\n",
      "Iteration: 0095 Loss: 0.88928 Time: 0.36844\n",
      "Iteration: 0096 Loss: 0.87837 Time: 0.35388\n",
      "Iteration: 0097 Loss: 0.87160 Time: 0.36349\n",
      "Iteration: 0098 Loss: 0.86606 Time: 0.36844\n",
      "Iteration: 0099 Loss: 0.85032 Time: 0.35077\n",
      "Iteration: 0100 Loss: 0.85654 Time: 0.35744\n",
      "Iteration: 0101 Loss: 0.85146 Time: 0.35393\n",
      "Iteration: 0102 Loss: 0.84978 Time: 0.34694\n",
      "Iteration: 0103 Loss: 0.82780 Time: 0.36356\n",
      "Iteration: 0104 Loss: 0.82778 Time: 0.36337\n",
      "Iteration: 0105 Loss: 0.82639 Time: 0.35784\n",
      "Iteration: 0106 Loss: 0.82251 Time: 0.36472\n",
      "Iteration: 0107 Loss: 0.81271 Time: 0.36781\n",
      "Iteration: 0108 Loss: 0.79558 Time: 0.34385\n",
      "Iteration: 0109 Loss: 0.80955 Time: 0.35718\n",
      "Iteration: 0110 Loss: 0.80336 Time: 0.34584\n",
      "Iteration: 0111 Loss: 0.81154 Time: 0.34626\n",
      "Iteration: 0112 Loss: 0.79550 Time: 0.34678\n",
      "Iteration: 0113 Loss: 0.79269 Time: 0.34045\n",
      "Iteration: 0114 Loss: 0.78453 Time: 0.35033\n",
      "Iteration: 0115 Loss: 0.77561 Time: 0.35825\n",
      "Iteration: 0116 Loss: 0.76818 Time: 0.35672\n",
      "Iteration: 0117 Loss: 0.77113 Time: 0.34635\n",
      "Iteration: 0118 Loss: 0.76689 Time: 0.34921\n",
      "Iteration: 0119 Loss: 0.76308 Time: 0.34710\n",
      "Iteration: 0120 Loss: 0.75214 Time: 0.35003\n",
      "Iteration: 0121 Loss: 0.75960 Time: 0.35036\n",
      "Iteration: 0122 Loss: 0.74763 Time: 0.34282\n",
      "Iteration: 0123 Loss: 0.74731 Time: 0.36059\n",
      "Iteration: 0124 Loss: 0.74199 Time: 0.35472\n",
      "Iteration: 0125 Loss: 0.74341 Time: 0.34899\n",
      "Iteration: 0126 Loss: 0.72838 Time: 0.35435\n",
      "Iteration: 0127 Loss: 0.72181 Time: 0.35764\n",
      "Iteration: 0128 Loss: 0.71668 Time: 0.34009\n",
      "Iteration: 0129 Loss: 0.72149 Time: 0.36612\n",
      "Iteration: 0130 Loss: 0.71655 Time: 0.35977\n",
      "Iteration: 0131 Loss: 0.71299 Time: 0.36681\n",
      "Iteration: 0132 Loss: 0.71047 Time: 0.36658\n",
      "Iteration: 0133 Loss: 0.70667 Time: 0.36110\n",
      "Iteration: 0134 Loss: 0.70356 Time: 0.36585\n",
      "Iteration: 0135 Loss: 0.69757 Time: 0.37171\n",
      "Iteration: 0136 Loss: 0.70076 Time: 0.35864\n",
      "Iteration: 0137 Loss: 0.69037 Time: 0.34510\n",
      "Iteration: 0138 Loss: 0.69759 Time: 0.36303\n",
      "Iteration: 0139 Loss: 0.68817 Time: 0.34803\n",
      "Iteration: 0140 Loss: 0.68498 Time: 0.35898\n",
      "Iteration: 0141 Loss: 0.67669 Time: 0.34884\n",
      "Iteration: 0142 Loss: 0.67689 Time: 0.36279\n",
      "Iteration: 0143 Loss: 0.66546 Time: 0.36602\n",
      "Iteration: 0144 Loss: 0.66487 Time: 0.34966\n",
      "Iteration: 0145 Loss: 0.66817 Time: 0.34356\n",
      "Iteration: 0146 Loss: 0.66647 Time: 0.36253\n",
      "Iteration: 0147 Loss: 0.66604 Time: 0.35038\n",
      "Iteration: 0148 Loss: 0.66318 Time: 0.34764\n",
      "Iteration: 0149 Loss: 0.65756 Time: 0.34714\n",
      "Iteration: 0150 Loss: 0.65255 Time: 0.34134\n",
      "Iteration: 0151 Loss: 0.65041 Time: 0.34634\n",
      "Iteration: 0152 Loss: 0.64791 Time: 0.35254\n",
      "Iteration: 0153 Loss: 0.65460 Time: 0.35673\n",
      "Iteration: 0154 Loss: 0.64350 Time: 0.35974\n",
      "Iteration: 0155 Loss: 0.63991 Time: 0.35988\n",
      "Iteration: 0156 Loss: 0.63695 Time: 0.34816\n",
      "Iteration: 0157 Loss: 0.64634 Time: 0.36719\n",
      "Iteration: 0158 Loss: 0.63224 Time: 0.34346\n",
      "Iteration: 0159 Loss: 0.63201 Time: 0.34861\n",
      "Iteration: 0160 Loss: 0.62549 Time: 0.34839\n",
      "Iteration: 0161 Loss: 0.62021 Time: 0.33877\n",
      "Iteration: 0162 Loss: 0.62951 Time: 0.34909\n",
      "Iteration: 0163 Loss: 0.62586 Time: 0.35189\n",
      "Iteration: 0164 Loss: 0.62435 Time: 0.35063\n",
      "Iteration: 0165 Loss: 0.62209 Time: 0.35154\n",
      "Iteration: 0166 Loss: 0.61926 Time: 0.35173\n",
      "Iteration: 0167 Loss: 0.62030 Time: 0.34112\n",
      "Iteration: 0168 Loss: 0.61493 Time: 0.35079\n",
      "Iteration: 0169 Loss: 0.61719 Time: 0.36073\n",
      "Iteration: 0170 Loss: 0.61771 Time: 0.35292\n",
      "Iteration: 0171 Loss: 0.60836 Time: 0.34890\n",
      "Iteration: 0172 Loss: 0.60866 Time: 0.37861\n",
      "Iteration: 0173 Loss: 0.60766 Time: 0.37498\n",
      "Iteration: 0174 Loss: 0.60854 Time: 0.35492\n",
      "Iteration: 0175 Loss: 0.60457 Time: 0.36017\n",
      "Iteration: 0176 Loss: 0.60627 Time: 0.34983\n",
      "Iteration: 0177 Loss: 0.59913 Time: 0.36403\n",
      "Iteration: 0178 Loss: 0.59868 Time: 0.36164\n",
      "Iteration: 0179 Loss: 0.59949 Time: 0.36313\n",
      "Iteration: 0180 Loss: 0.60075 Time: 0.34603\n",
      "Iteration: 0181 Loss: 0.60323 Time: 0.35713\n",
      "Iteration: 0182 Loss: 0.59498 Time: 0.34299\n",
      "Iteration: 0183 Loss: 0.59288 Time: 0.34664\n",
      "Iteration: 0184 Loss: 0.59342 Time: 0.34706\n",
      "Iteration: 0185 Loss: 0.59135 Time: 0.35113\n",
      "Iteration: 0186 Loss: 0.58844 Time: 0.34874\n",
      "Iteration: 0187 Loss: 0.58913 Time: 0.35001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0188 Loss: 0.58410 Time: 0.36289\n",
      "Iteration: 0189 Loss: 0.58348 Time: 0.36058\n",
      "Iteration: 0190 Loss: 0.58592 Time: 0.34816\n",
      "Iteration: 0191 Loss: 0.58278 Time: 0.34400\n",
      "Iteration: 0192 Loss: 0.58269 Time: 0.36936\n",
      "Iteration: 0193 Loss: 0.58575 Time: 0.34161\n",
      "Iteration: 0194 Loss: 0.58079 Time: 0.35555\n",
      "Iteration: 0195 Loss: 0.58166 Time: 0.34531\n",
      "Iteration: 0196 Loss: 0.57925 Time: 0.34832\n",
      "Iteration: 0197 Loss: 0.58041 Time: 0.35418\n",
      "Iteration: 0198 Loss: 0.57920 Time: 0.36564\n",
      "Iteration: 0199 Loss: 0.57934 Time: 0.37008\n",
      "Iteration: 0200 Loss: 0.57371 Time: 0.37421\n",
      "Done! \n",
      "\n",
      "STEP 3/3 - MODEL EVALUATION \n",
      "\n",
      "Computing the final embedding vectors, for evaluation\n",
      "Done! \n",
      "\n",
      "Testing: link prediction\n",
      "Done! \n",
      "\n",
      "EXPERIMENTS ON MODEL 8 / 10 \n",
      "\n",
      "STEP 1/3 - PREPROCESSING STEPS \n",
      "\n",
      "Masking some edges from the training graph, for link prediction\n",
      "(validation set: 1.0 % of edges - test set: 10.0 % of edges)\n",
      "Done! \n",
      "\n",
      "Preprocessing node features\n",
      "Done! \n",
      "\n",
      "Running the Louvain algorithm for community detection\n",
      "as a preprocessing step for the encoder\n",
      "Done! Louvain has found 573 communities \n",
      "\n",
      "Setting up the model and the optimizer\n",
      "Done! \n",
      "\n",
      "Preprocessing on message passing matrices\n",
      "Done! \n",
      "\n",
      "Initializing TF session\n",
      "Done! \n",
      "\n",
      "STEP 2/3 - MODEL TRAINING \n",
      "\n",
      "Starting training\n",
      "Iteration: 0001 Loss: 1.76489 Time: 0.65114\n",
      "Iteration: 0002 Loss: 1.71537 Time: 0.36099\n",
      "Iteration: 0003 Loss: 1.71773 Time: 0.35854\n",
      "Iteration: 0004 Loss: 1.67918 Time: 0.35841\n",
      "Iteration: 0005 Loss: 1.64912 Time: 0.35723\n",
      "Iteration: 0006 Loss: 1.63082 Time: 0.35983\n",
      "Iteration: 0007 Loss: 1.61381 Time: 0.35231\n",
      "Iteration: 0008 Loss: 1.60057 Time: 0.36559\n",
      "Iteration: 0009 Loss: 1.56051 Time: 0.35400\n",
      "Iteration: 0010 Loss: 1.54572 Time: 0.35702\n",
      "Iteration: 0011 Loss: 1.57983 Time: 0.34993\n",
      "Iteration: 0012 Loss: 1.55114 Time: 0.34843\n",
      "Iteration: 0013 Loss: 1.53525 Time: 0.35803\n",
      "Iteration: 0014 Loss: 1.53174 Time: 0.35400\n",
      "Iteration: 0015 Loss: 1.52589 Time: 0.33675\n",
      "Iteration: 0016 Loss: 1.52819 Time: 0.35796\n",
      "Iteration: 0017 Loss: 1.48116 Time: 0.35858\n",
      "Iteration: 0018 Loss: 1.49663 Time: 0.34663\n",
      "Iteration: 0019 Loss: 1.45044 Time: 0.35329\n",
      "Iteration: 0020 Loss: 1.42918 Time: 0.36348\n",
      "Iteration: 0021 Loss: 1.46121 Time: 0.34796\n",
      "Iteration: 0022 Loss: 1.40587 Time: 0.35581\n",
      "Iteration: 0023 Loss: 1.41243 Time: 0.36158\n",
      "Iteration: 0024 Loss: 1.41843 Time: 0.37187\n",
      "Iteration: 0025 Loss: 1.38477 Time: 0.36499\n",
      "Iteration: 0026 Loss: 1.38222 Time: 0.37161\n",
      "Iteration: 0027 Loss: 1.38216 Time: 0.35349\n",
      "Iteration: 0028 Loss: 1.34243 Time: 0.34880\n",
      "Iteration: 0029 Loss: 1.35221 Time: 0.34181\n",
      "Iteration: 0030 Loss: 1.31485 Time: 0.37038\n",
      "Iteration: 0031 Loss: 1.34321 Time: 0.34896\n",
      "Iteration: 0032 Loss: 1.29743 Time: 0.34600\n",
      "Iteration: 0033 Loss: 1.32685 Time: 0.36369\n",
      "Iteration: 0034 Loss: 1.27755 Time: 0.34946\n",
      "Iteration: 0035 Loss: 1.28686 Time: 0.35481\n",
      "Iteration: 0036 Loss: 1.26566 Time: 0.35410\n",
      "Iteration: 0037 Loss: 1.26007 Time: 0.34599\n",
      "Iteration: 0038 Loss: 1.26978 Time: 0.36310\n",
      "Iteration: 0039 Loss: 1.26613 Time: 0.36436\n",
      "Iteration: 0040 Loss: 1.21960 Time: 0.37139\n",
      "Iteration: 0041 Loss: 1.24052 Time: 0.35705\n",
      "Iteration: 0042 Loss: 1.24106 Time: 0.36600\n",
      "Iteration: 0043 Loss: 1.22825 Time: 0.35611\n",
      "Iteration: 0044 Loss: 1.20264 Time: 0.35190\n",
      "Iteration: 0045 Loss: 1.20149 Time: 0.34738\n",
      "Iteration: 0046 Loss: 1.17841 Time: 0.35122\n",
      "Iteration: 0047 Loss: 1.17168 Time: 0.36517\n",
      "Iteration: 0048 Loss: 1.16189 Time: 0.35229\n",
      "Iteration: 0049 Loss: 1.17724 Time: 0.35682\n",
      "Iteration: 0050 Loss: 1.15195 Time: 0.34957\n",
      "Iteration: 0051 Loss: 1.18898 Time: 0.34421\n",
      "Iteration: 0052 Loss: 1.14726 Time: 0.34913\n",
      "Iteration: 0053 Loss: 1.14551 Time: 0.35656\n",
      "Iteration: 0054 Loss: 1.13760 Time: 0.35625\n",
      "Iteration: 0055 Loss: 1.13136 Time: 0.35419\n",
      "Iteration: 0056 Loss: 1.13170 Time: 0.34648\n",
      "Iteration: 0057 Loss: 1.10666 Time: 0.35896\n",
      "Iteration: 0058 Loss: 1.10285 Time: 0.36224\n",
      "Iteration: 0059 Loss: 1.09810 Time: 0.35680\n",
      "Iteration: 0060 Loss: 1.09137 Time: 0.35579\n",
      "Iteration: 0061 Loss: 1.07668 Time: 0.34335\n",
      "Iteration: 0062 Loss: 1.07011 Time: 0.34945\n",
      "Iteration: 0063 Loss: 1.08482 Time: 0.35504\n",
      "Iteration: 0064 Loss: 1.07105 Time: 0.36200\n",
      "Iteration: 0065 Loss: 1.06770 Time: 0.36744\n",
      "Iteration: 0066 Loss: 1.04872 Time: 0.36348\n",
      "Iteration: 0067 Loss: 1.04772 Time: 0.37005\n",
      "Iteration: 0068 Loss: 1.04221 Time: 0.36805\n",
      "Iteration: 0069 Loss: 1.03419 Time: 0.35511\n",
      "Iteration: 0070 Loss: 1.03939 Time: 0.35764\n",
      "Iteration: 0071 Loss: 1.01966 Time: 0.34817\n",
      "Iteration: 0072 Loss: 1.02073 Time: 0.36071\n",
      "Iteration: 0073 Loss: 1.01058 Time: 0.34472\n",
      "Iteration: 0074 Loss: 1.00670 Time: 0.34615\n",
      "Iteration: 0075 Loss: 1.00452 Time: 0.35558\n",
      "Iteration: 0076 Loss: 0.98903 Time: 0.36106\n",
      "Iteration: 0077 Loss: 0.97908 Time: 0.35011\n",
      "Iteration: 0078 Loss: 0.98224 Time: 0.35315\n",
      "Iteration: 0079 Loss: 0.95620 Time: 0.37015\n",
      "Iteration: 0080 Loss: 0.96451 Time: 0.34146\n",
      "Iteration: 0081 Loss: 0.95341 Time: 0.35040\n",
      "Iteration: 0082 Loss: 0.96740 Time: 0.34972\n",
      "Iteration: 0083 Loss: 0.95125 Time: 0.35390\n",
      "Iteration: 0084 Loss: 0.95887 Time: 0.36437\n",
      "Iteration: 0085 Loss: 0.92753 Time: 0.34402\n",
      "Iteration: 0086 Loss: 0.92269 Time: 0.35379\n",
      "Iteration: 0087 Loss: 0.92381 Time: 0.34701\n",
      "Iteration: 0088 Loss: 0.91011 Time: 0.35414\n",
      "Iteration: 0089 Loss: 0.93047 Time: 0.35460\n",
      "Iteration: 0090 Loss: 0.91269 Time: 0.34377\n",
      "Iteration: 0091 Loss: 0.90609 Time: 0.36086\n",
      "Iteration: 0092 Loss: 0.89671 Time: 0.34945\n",
      "Iteration: 0093 Loss: 0.89148 Time: 0.36343\n",
      "Iteration: 0094 Loss: 0.89412 Time: 0.35012\n",
      "Iteration: 0095 Loss: 0.89063 Time: 0.34908\n",
      "Iteration: 0096 Loss: 0.87554 Time: 0.34442\n",
      "Iteration: 0097 Loss: 0.86345 Time: 0.35262\n",
      "Iteration: 0098 Loss: 0.87019 Time: 0.34928\n",
      "Iteration: 0099 Loss: 0.86709 Time: 0.36202\n",
      "Iteration: 0100 Loss: 0.84293 Time: 0.35700\n",
      "Iteration: 0101 Loss: 0.84595 Time: 0.34887\n",
      "Iteration: 0102 Loss: 0.84169 Time: 0.35002\n",
      "Iteration: 0103 Loss: 0.82749 Time: 0.34737\n",
      "Iteration: 0104 Loss: 0.82355 Time: 0.36341\n",
      "Iteration: 0105 Loss: 0.82793 Time: 0.35247\n",
      "Iteration: 0106 Loss: 0.82514 Time: 0.34542\n",
      "Iteration: 0107 Loss: 0.81433 Time: 0.35700\n",
      "Iteration: 0108 Loss: 0.81915 Time: 0.34638\n",
      "Iteration: 0109 Loss: 0.81432 Time: 0.35107\n",
      "Iteration: 0110 Loss: 0.79664 Time: 0.35200\n",
      "Iteration: 0111 Loss: 0.79297 Time: 0.34911\n",
      "Iteration: 0112 Loss: 0.78555 Time: 0.35171\n",
      "Iteration: 0113 Loss: 0.78829 Time: 0.34926\n",
      "Iteration: 0114 Loss: 0.77966 Time: 0.34301\n",
      "Iteration: 0115 Loss: 0.78445 Time: 0.34592\n",
      "Iteration: 0116 Loss: 0.76426 Time: 0.36039\n",
      "Iteration: 0117 Loss: 0.76897 Time: 0.35305\n",
      "Iteration: 0118 Loss: 0.74055 Time: 0.36628\n",
      "Iteration: 0119 Loss: 0.75475 Time: 0.35991\n",
      "Iteration: 0120 Loss: 0.75265 Time: 0.34917\n",
      "Iteration: 0121 Loss: 0.74446 Time: 0.35100\n",
      "Iteration: 0122 Loss: 0.73576 Time: 0.34843\n",
      "Iteration: 0123 Loss: 0.73030 Time: 0.35457\n",
      "Iteration: 0124 Loss: 0.72661 Time: 0.34069\n",
      "Iteration: 0125 Loss: 0.73230 Time: 0.34918\n",
      "Iteration: 0126 Loss: 0.71582 Time: 0.35100\n",
      "Iteration: 0127 Loss: 0.72258 Time: 0.34705\n",
      "Iteration: 0128 Loss: 0.72443 Time: 0.35985\n",
      "Iteration: 0129 Loss: 0.70884 Time: 0.36625\n",
      "Iteration: 0130 Loss: 0.70582 Time: 0.37195\n",
      "Iteration: 0131 Loss: 0.71155 Time: 0.35373\n",
      "Iteration: 0132 Loss: 0.69850 Time: 0.35284\n",
      "Iteration: 0133 Loss: 0.68888 Time: 0.35224\n",
      "Iteration: 0134 Loss: 0.68956 Time: 0.34625\n",
      "Iteration: 0135 Loss: 0.68890 Time: 0.35770\n",
      "Iteration: 0136 Loss: 0.68946 Time: 0.35784\n",
      "Iteration: 0137 Loss: 0.68900 Time: 0.37486\n",
      "Iteration: 0138 Loss: 0.67925 Time: 0.35574\n",
      "Iteration: 0139 Loss: 0.68431 Time: 0.35343\n",
      "Iteration: 0140 Loss: 0.67221 Time: 0.34323\n",
      "Iteration: 0141 Loss: 0.67580 Time: 0.36685\n",
      "Iteration: 0142 Loss: 0.66850 Time: 0.34260\n",
      "Iteration: 0143 Loss: 0.66957 Time: 0.35003\n",
      "Iteration: 0144 Loss: 0.66466 Time: 0.34960\n",
      "Iteration: 0145 Loss: 0.66126 Time: 0.35940\n",
      "Iteration: 0146 Loss: 0.65762 Time: 0.35119\n",
      "Iteration: 0147 Loss: 0.65648 Time: 0.34895\n",
      "Iteration: 0148 Loss: 0.65198 Time: 0.34095\n",
      "Iteration: 0149 Loss: 0.65313 Time: 0.35735\n",
      "Iteration: 0150 Loss: 0.64926 Time: 0.35300\n",
      "Iteration: 0151 Loss: 0.64647 Time: 0.34324\n",
      "Iteration: 0152 Loss: 0.64247 Time: 0.35160\n",
      "Iteration: 0153 Loss: 0.64228 Time: 0.34550\n",
      "Iteration: 0154 Loss: 0.63898 Time: 0.36166\n",
      "Iteration: 0155 Loss: 0.64073 Time: 0.35143\n",
      "Iteration: 0156 Loss: 0.63511 Time: 0.35779\n",
      "Iteration: 0157 Loss: 0.63811 Time: 0.35235\n",
      "Iteration: 0158 Loss: 0.63138 Time: 0.35681\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0159 Loss: 0.61970 Time: 0.33782\n",
      "Iteration: 0160 Loss: 0.62578 Time: 0.34717\n",
      "Iteration: 0161 Loss: 0.62798 Time: 0.35100\n",
      "Iteration: 0162 Loss: 0.62673 Time: 0.34923\n",
      "Iteration: 0163 Loss: 0.62216 Time: 0.34722\n",
      "Iteration: 0164 Loss: 0.61913 Time: 0.34620\n",
      "Iteration: 0165 Loss: 0.61383 Time: 0.35558\n",
      "Iteration: 0166 Loss: 0.61689 Time: 0.34400\n",
      "Iteration: 0167 Loss: 0.61081 Time: 0.36401\n",
      "Iteration: 0168 Loss: 0.61215 Time: 0.35199\n",
      "Iteration: 0169 Loss: 0.61630 Time: 0.37094\n",
      "Iteration: 0170 Loss: 0.60848 Time: 0.36012\n",
      "Iteration: 0171 Loss: 0.60802 Time: 0.35425\n",
      "Iteration: 0172 Loss: 0.60645 Time: 0.34557\n",
      "Iteration: 0173 Loss: 0.60784 Time: 0.34995\n",
      "Iteration: 0174 Loss: 0.59909 Time: 0.37219\n",
      "Iteration: 0175 Loss: 0.59667 Time: 0.35585\n",
      "Iteration: 0176 Loss: 0.59562 Time: 0.35487\n",
      "Iteration: 0177 Loss: 0.59674 Time: 0.34594\n",
      "Iteration: 0178 Loss: 0.59547 Time: 0.36918\n",
      "Iteration: 0179 Loss: 0.59377 Time: 0.34609\n",
      "Iteration: 0180 Loss: 0.59733 Time: 0.35751\n",
      "Iteration: 0181 Loss: 0.59683 Time: 0.34769\n",
      "Iteration: 0182 Loss: 0.59489 Time: 0.34186\n",
      "Iteration: 0183 Loss: 0.58894 Time: 0.35573\n",
      "Iteration: 0184 Loss: 0.58872 Time: 0.35700\n",
      "Iteration: 0185 Loss: 0.59149 Time: 0.36001\n",
      "Iteration: 0186 Loss: 0.59153 Time: 0.35120\n",
      "Iteration: 0187 Loss: 0.58427 Time: 0.35294\n",
      "Iteration: 0188 Loss: 0.58960 Time: 0.36017\n",
      "Iteration: 0189 Loss: 0.58545 Time: 0.35677\n",
      "Iteration: 0190 Loss: 0.58339 Time: 0.35968\n",
      "Iteration: 0191 Loss: 0.58057 Time: 0.35928\n",
      "Iteration: 0192 Loss: 0.59027 Time: 0.34793\n",
      "Iteration: 0193 Loss: 0.57961 Time: 0.35483\n",
      "Iteration: 0194 Loss: 0.58169 Time: 0.34929\n",
      "Iteration: 0195 Loss: 0.58284 Time: 0.35106\n",
      "Iteration: 0196 Loss: 0.57873 Time: 0.35446\n",
      "Iteration: 0197 Loss: 0.57313 Time: 0.34900\n",
      "Iteration: 0198 Loss: 0.57387 Time: 0.36075\n",
      "Iteration: 0199 Loss: 0.57740 Time: 0.35325\n",
      "Iteration: 0200 Loss: 0.57036 Time: 0.34600\n",
      "Done! \n",
      "\n",
      "STEP 3/3 - MODEL EVALUATION \n",
      "\n",
      "Computing the final embedding vectors, for evaluation\n",
      "Done! \n",
      "\n",
      "Testing: link prediction\n",
      "Done! \n",
      "\n",
      "EXPERIMENTS ON MODEL 9 / 10 \n",
      "\n",
      "STEP 1/3 - PREPROCESSING STEPS \n",
      "\n",
      "Masking some edges from the training graph, for link prediction\n",
      "(validation set: 1.0 % of edges - test set: 10.0 % of edges)\n",
      "Done! \n",
      "\n",
      "Preprocessing node features\n",
      "Done! \n",
      "\n",
      "Running the Louvain algorithm for community detection\n",
      "as a preprocessing step for the encoder\n",
      "Done! Louvain has found 569 communities \n",
      "\n",
      "Setting up the model and the optimizer\n",
      "Done! \n",
      "\n",
      "Preprocessing on message passing matrices\n",
      "Done! \n",
      "\n",
      "Initializing TF session\n",
      "Done! \n",
      "\n",
      "STEP 2/3 - MODEL TRAINING \n",
      "\n",
      "Starting training\n",
      "Iteration: 0001 Loss: 1.76782 Time: 0.67792\n",
      "Iteration: 0002 Loss: 1.69801 Time: 0.34815\n",
      "Iteration: 0003 Loss: 1.70684 Time: 0.35018\n",
      "Iteration: 0004 Loss: 1.67862 Time: 0.34513\n",
      "Iteration: 0005 Loss: 1.65876 Time: 0.34687\n",
      "Iteration: 0006 Loss: 1.63409 Time: 0.35581\n",
      "Iteration: 0007 Loss: 1.63230 Time: 0.35829\n",
      "Iteration: 0008 Loss: 1.63319 Time: 0.34868\n",
      "Iteration: 0009 Loss: 1.59587 Time: 0.34922\n",
      "Iteration: 0010 Loss: 1.57897 Time: 0.34892\n",
      "Iteration: 0011 Loss: 1.57847 Time: 0.35601\n",
      "Iteration: 0012 Loss: 1.57826 Time: 0.35125\n",
      "Iteration: 0013 Loss: 1.51787 Time: 0.35158\n",
      "Iteration: 0014 Loss: 1.52824 Time: 0.35133\n",
      "Iteration: 0015 Loss: 1.52313 Time: 0.35491\n",
      "Iteration: 0016 Loss: 1.49974 Time: 0.34868\n",
      "Iteration: 0017 Loss: 1.46346 Time: 0.37456\n",
      "Iteration: 0018 Loss: 1.46858 Time: 0.35516\n",
      "Iteration: 0019 Loss: 1.45582 Time: 0.35165\n",
      "Iteration: 0020 Loss: 1.47259 Time: 0.34783\n",
      "Iteration: 0021 Loss: 1.41765 Time: 0.36218\n",
      "Iteration: 0022 Loss: 1.42604 Time: 0.35187\n",
      "Iteration: 0023 Loss: 1.37479 Time: 0.34869\n",
      "Iteration: 0024 Loss: 1.39677 Time: 0.36328\n",
      "Iteration: 0025 Loss: 1.41382 Time: 0.35830\n",
      "Iteration: 0026 Loss: 1.37853 Time: 0.34280\n",
      "Iteration: 0027 Loss: 1.36423 Time: 0.34561\n",
      "Iteration: 0028 Loss: 1.37029 Time: 0.38132\n",
      "Iteration: 0029 Loss: 1.34518 Time: 0.34446\n",
      "Iteration: 0030 Loss: 1.32789 Time: 0.34858\n",
      "Iteration: 0031 Loss: 1.30573 Time: 0.35127\n",
      "Iteration: 0032 Loss: 1.33214 Time: 0.35668\n",
      "Iteration: 0033 Loss: 1.29667 Time: 0.36778\n",
      "Iteration: 0034 Loss: 1.29712 Time: 0.36661\n",
      "Iteration: 0035 Loss: 1.28374 Time: 0.35334\n",
      "Iteration: 0036 Loss: 1.27095 Time: 0.35025\n",
      "Iteration: 0037 Loss: 1.27381 Time: 0.34797\n",
      "Iteration: 0038 Loss: 1.22297 Time: 0.34744\n",
      "Iteration: 0039 Loss: 1.24549 Time: 0.35062\n",
      "Iteration: 0040 Loss: 1.25817 Time: 0.36493\n",
      "Iteration: 0041 Loss: 1.25126 Time: 0.36354\n",
      "Iteration: 0042 Loss: 1.20825 Time: 0.35816\n",
      "Iteration: 0043 Loss: 1.23865 Time: 0.34888\n",
      "Iteration: 0044 Loss: 1.21815 Time: 0.34712\n",
      "Iteration: 0045 Loss: 1.19338 Time: 0.34894\n",
      "Iteration: 0046 Loss: 1.21292 Time: 0.34275\n",
      "Iteration: 0047 Loss: 1.19181 Time: 0.34632\n",
      "Iteration: 0048 Loss: 1.17899 Time: 0.35208\n",
      "Iteration: 0049 Loss: 1.18552 Time: 0.35302\n",
      "Iteration: 0050 Loss: 1.17787 Time: 0.36490\n",
      "Iteration: 0051 Loss: 1.15739 Time: 0.36386\n",
      "Iteration: 0052 Loss: 1.14430 Time: 0.36041\n",
      "Iteration: 0053 Loss: 1.14601 Time: 0.34761\n",
      "Iteration: 0054 Loss: 1.13822 Time: 0.36271\n",
      "Iteration: 0055 Loss: 1.12496 Time: 0.34589\n",
      "Iteration: 0056 Loss: 1.14951 Time: 0.34339\n",
      "Iteration: 0057 Loss: 1.11160 Time: 0.36014\n",
      "Iteration: 0058 Loss: 1.09985 Time: 0.35158\n",
      "Iteration: 0059 Loss: 1.10686 Time: 0.35729\n",
      "Iteration: 0060 Loss: 1.09653 Time: 0.34658\n",
      "Iteration: 0061 Loss: 1.08699 Time: 0.34707\n",
      "Iteration: 0062 Loss: 1.07705 Time: 0.34908\n",
      "Iteration: 0063 Loss: 1.06726 Time: 0.35403\n",
      "Iteration: 0064 Loss: 1.05971 Time: 0.35065\n",
      "Iteration: 0065 Loss: 1.06094 Time: 0.35650\n",
      "Iteration: 0066 Loss: 1.05956 Time: 0.36100\n",
      "Iteration: 0067 Loss: 1.05217 Time: 0.34579\n",
      "Iteration: 0068 Loss: 1.04616 Time: 0.34538\n",
      "Iteration: 0069 Loss: 1.02323 Time: 0.34714\n",
      "Iteration: 0070 Loss: 1.01910 Time: 0.34914\n",
      "Iteration: 0071 Loss: 1.02416 Time: 0.34949\n",
      "Iteration: 0072 Loss: 1.01463 Time: 0.34517\n",
      "Iteration: 0073 Loss: 1.01487 Time: 0.36406\n",
      "Iteration: 0074 Loss: 1.00965 Time: 0.34854\n",
      "Iteration: 0075 Loss: 0.98611 Time: 0.34946\n",
      "Iteration: 0076 Loss: 0.99312 Time: 0.35831\n",
      "Iteration: 0077 Loss: 0.98878 Time: 0.35217\n",
      "Iteration: 0078 Loss: 0.99379 Time: 0.34851\n",
      "Iteration: 0079 Loss: 0.97744 Time: 0.34726\n",
      "Iteration: 0080 Loss: 0.97040 Time: 0.35381\n",
      "Iteration: 0081 Loss: 0.96687 Time: 0.36438\n",
      "Iteration: 0082 Loss: 0.95072 Time: 0.35956\n",
      "Iteration: 0083 Loss: 0.95463 Time: 0.34363\n",
      "Iteration: 0084 Loss: 0.93936 Time: 0.34113\n",
      "Iteration: 0085 Loss: 0.94700 Time: 0.35935\n",
      "Iteration: 0086 Loss: 0.93243 Time: 0.37169\n",
      "Iteration: 0087 Loss: 0.92540 Time: 0.35269\n",
      "Iteration: 0088 Loss: 0.90921 Time: 0.34437\n",
      "Iteration: 0089 Loss: 0.90821 Time: 0.34608\n",
      "Iteration: 0090 Loss: 0.90408 Time: 0.36317\n",
      "Iteration: 0091 Loss: 0.90706 Time: 0.34807\n",
      "Iteration: 0092 Loss: 0.89718 Time: 0.35076\n",
      "Iteration: 0093 Loss: 0.88849 Time: 0.34783\n",
      "Iteration: 0094 Loss: 0.88349 Time: 0.35390\n",
      "Iteration: 0095 Loss: 0.87121 Time: 0.35693\n",
      "Iteration: 0096 Loss: 0.87539 Time: 0.36263\n",
      "Iteration: 0097 Loss: 0.87478 Time: 0.36093\n",
      "Iteration: 0098 Loss: 0.87329 Time: 0.34680\n",
      "Iteration: 0099 Loss: 0.85227 Time: 0.36474\n",
      "Iteration: 0100 Loss: 0.86296 Time: 0.34685\n",
      "Iteration: 0101 Loss: 0.84737 Time: 0.34677\n",
      "Iteration: 0102 Loss: 0.84189 Time: 0.35099\n",
      "Iteration: 0103 Loss: 0.82946 Time: 0.35709\n",
      "Iteration: 0104 Loss: 0.82259 Time: 0.35118\n",
      "Iteration: 0105 Loss: 0.80783 Time: 0.35317\n",
      "Iteration: 0106 Loss: 0.81284 Time: 0.34525\n",
      "Iteration: 0107 Loss: 0.81878 Time: 0.34955\n",
      "Iteration: 0108 Loss: 0.79867 Time: 0.34685\n",
      "Iteration: 0109 Loss: 0.80468 Time: 0.35260\n",
      "Iteration: 0110 Loss: 0.80617 Time: 0.36275\n",
      "Iteration: 0111 Loss: 0.79624 Time: 0.35551\n",
      "Iteration: 0112 Loss: 0.78548 Time: 0.34871\n",
      "Iteration: 0113 Loss: 0.79137 Time: 0.34688\n",
      "Iteration: 0114 Loss: 0.78139 Time: 0.35394\n",
      "Iteration: 0115 Loss: 0.77182 Time: 0.36187\n",
      "Iteration: 0116 Loss: 0.77559 Time: 0.35149\n",
      "Iteration: 0117 Loss: 0.76999 Time: 0.34703\n",
      "Iteration: 0118 Loss: 0.76085 Time: 0.34210\n",
      "Iteration: 0119 Loss: 0.75388 Time: 0.35007\n",
      "Iteration: 0120 Loss: 0.74898 Time: 0.36288\n",
      "Iteration: 0121 Loss: 0.75504 Time: 0.36163\n",
      "Iteration: 0122 Loss: 0.73871 Time: 0.35498\n",
      "Iteration: 0123 Loss: 0.74245 Time: 0.35106\n",
      "Iteration: 0124 Loss: 0.73370 Time: 0.35282\n",
      "Iteration: 0125 Loss: 0.73349 Time: 0.34427\n",
      "Iteration: 0126 Loss: 0.72301 Time: 0.35054\n",
      "Iteration: 0127 Loss: 0.72266 Time: 0.35083\n",
      "Iteration: 0128 Loss: 0.71467 Time: 0.34749\n",
      "Iteration: 0129 Loss: 0.71967 Time: 0.35011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0130 Loss: 0.70858 Time: 0.34554\n",
      "Iteration: 0131 Loss: 0.69163 Time: 0.35579\n",
      "Iteration: 0132 Loss: 0.70166 Time: 0.36141\n",
      "Iteration: 0133 Loss: 0.70375 Time: 0.36267\n",
      "Iteration: 0134 Loss: 0.69909 Time: 0.37371\n",
      "Iteration: 0135 Loss: 0.68902 Time: 0.35315\n",
      "Iteration: 0136 Loss: 0.69729 Time: 0.34764\n",
      "Iteration: 0137 Loss: 0.68745 Time: 0.34899\n",
      "Iteration: 0138 Loss: 0.68726 Time: 0.36005\n",
      "Iteration: 0139 Loss: 0.67834 Time: 0.34639\n",
      "Iteration: 0140 Loss: 0.67448 Time: 0.34751\n",
      "Iteration: 0141 Loss: 0.67519 Time: 0.35449\n",
      "Iteration: 0142 Loss: 0.68023 Time: 0.35147\n",
      "Iteration: 0143 Loss: 0.67336 Time: 0.34575\n",
      "Iteration: 0144 Loss: 0.65573 Time: 0.35010\n",
      "Iteration: 0145 Loss: 0.65791 Time: 0.35282\n",
      "Iteration: 0146 Loss: 0.66535 Time: 0.36534\n",
      "Iteration: 0147 Loss: 0.65339 Time: 0.35686\n",
      "Iteration: 0148 Loss: 0.65316 Time: 0.34357\n",
      "Iteration: 0149 Loss: 0.64910 Time: 0.35417\n",
      "Iteration: 0150 Loss: 0.64483 Time: 0.35942\n",
      "Iteration: 0151 Loss: 0.64861 Time: 0.35772\n",
      "Iteration: 0152 Loss: 0.64334 Time: 0.35105\n",
      "Iteration: 0153 Loss: 0.64827 Time: 0.34873\n",
      "Iteration: 0154 Loss: 0.64173 Time: 0.35234\n",
      "Iteration: 0155 Loss: 0.64053 Time: 0.35698\n",
      "Iteration: 0156 Loss: 0.63976 Time: 0.34388\n",
      "Iteration: 0157 Loss: 0.63549 Time: 0.35189\n",
      "Iteration: 0158 Loss: 0.63551 Time: 0.35247\n",
      "Iteration: 0159 Loss: 0.62515 Time: 0.36091\n",
      "Iteration: 0160 Loss: 0.62423 Time: 0.34706\n",
      "Iteration: 0161 Loss: 0.63773 Time: 0.34692\n",
      "Iteration: 0162 Loss: 0.62420 Time: 0.34926\n",
      "Iteration: 0163 Loss: 0.62205 Time: 0.34683\n",
      "Iteration: 0164 Loss: 0.62646 Time: 0.36604\n",
      "Iteration: 0165 Loss: 0.62125 Time: 0.35528\n",
      "Iteration: 0166 Loss: 0.62015 Time: 0.35885\n",
      "Iteration: 0167 Loss: 0.60855 Time: 0.35235\n",
      "Iteration: 0168 Loss: 0.61653 Time: 0.34080\n",
      "Iteration: 0169 Loss: 0.61559 Time: 0.34718\n",
      "Iteration: 0170 Loss: 0.60972 Time: 0.35227\n",
      "Iteration: 0171 Loss: 0.61456 Time: 0.36201\n",
      "Iteration: 0172 Loss: 0.60796 Time: 0.35933\n",
      "Iteration: 0173 Loss: 0.61084 Time: 0.35191\n",
      "Iteration: 0174 Loss: 0.60194 Time: 0.35193\n",
      "Iteration: 0175 Loss: 0.60485 Time: 0.34241\n",
      "Iteration: 0176 Loss: 0.59797 Time: 0.35154\n",
      "Iteration: 0177 Loss: 0.59921 Time: 0.34919\n",
      "Iteration: 0178 Loss: 0.59870 Time: 0.35284\n",
      "Iteration: 0179 Loss: 0.59544 Time: 0.34243\n",
      "Iteration: 0180 Loss: 0.59899 Time: 0.34976\n",
      "Iteration: 0181 Loss: 0.59775 Time: 0.35508\n",
      "Iteration: 0182 Loss: 0.59550 Time: 0.35875\n",
      "Iteration: 0183 Loss: 0.59123 Time: 0.37049\n",
      "Iteration: 0184 Loss: 0.58990 Time: 0.35352\n",
      "Iteration: 0185 Loss: 0.59199 Time: 0.34192\n",
      "Iteration: 0186 Loss: 0.59246 Time: 0.35963\n",
      "Iteration: 0187 Loss: 0.58698 Time: 0.35147\n",
      "Iteration: 0188 Loss: 0.58699 Time: 0.35498\n",
      "Iteration: 0189 Loss: 0.59011 Time: 0.34813\n",
      "Iteration: 0190 Loss: 0.58428 Time: 0.34705\n",
      "Iteration: 0191 Loss: 0.58775 Time: 0.33938\n",
      "Iteration: 0192 Loss: 0.58585 Time: 0.34964\n",
      "Iteration: 0193 Loss: 0.58004 Time: 0.36370\n",
      "Iteration: 0194 Loss: 0.58236 Time: 0.34849\n",
      "Iteration: 0195 Loss: 0.57795 Time: 0.35422\n",
      "Iteration: 0196 Loss: 0.57934 Time: 0.36373\n",
      "Iteration: 0197 Loss: 0.57924 Time: 0.35671\n",
      "Iteration: 0198 Loss: 0.57758 Time: 0.35768\n",
      "Iteration: 0199 Loss: 0.57434 Time: 0.34334\n",
      "Iteration: 0200 Loss: 0.57918 Time: 0.34584\n",
      "Done! \n",
      "\n",
      "STEP 3/3 - MODEL EVALUATION \n",
      "\n",
      "Computing the final embedding vectors, for evaluation\n",
      "Done! \n",
      "\n",
      "Testing: link prediction\n",
      "Done! \n",
      "\n",
      "EXPERIMENTS ON MODEL 10 / 10 \n",
      "\n",
      "STEP 1/3 - PREPROCESSING STEPS \n",
      "\n",
      "Masking some edges from the training graph, for link prediction\n",
      "(validation set: 1.0 % of edges - test set: 10.0 % of edges)\n",
      "Done! \n",
      "\n",
      "Preprocessing node features\n",
      "Done! \n",
      "\n",
      "Running the Louvain algorithm for community detection\n",
      "as a preprocessing step for the encoder\n",
      "Done! Louvain has found 566 communities \n",
      "\n",
      "Setting up the model and the optimizer\n",
      "Done! \n",
      "\n",
      "Preprocessing on message passing matrices\n",
      "Done! \n",
      "\n",
      "Initializing TF session\n",
      "Done! \n",
      "\n",
      "STEP 2/3 - MODEL TRAINING \n",
      "\n",
      "Starting training\n",
      "Iteration: 0001 Loss: 1.75712 Time: 0.66835\n",
      "Iteration: 0002 Loss: 1.68863 Time: 0.35792\n",
      "Iteration: 0003 Loss: 1.68255 Time: 0.35064\n",
      "Iteration: 0004 Loss: 1.68535 Time: 0.35667\n",
      "Iteration: 0005 Loss: 1.67474 Time: 0.35838\n",
      "Iteration: 0006 Loss: 1.61807 Time: 0.35879\n",
      "Iteration: 0007 Loss: 1.61781 Time: 0.35070\n",
      "Iteration: 0008 Loss: 1.62915 Time: 0.35951\n",
      "Iteration: 0009 Loss: 1.59170 Time: 0.34892\n",
      "Iteration: 0010 Loss: 1.57142 Time: 0.35570\n",
      "Iteration: 0011 Loss: 1.59160 Time: 0.35318\n",
      "Iteration: 0012 Loss: 1.57000 Time: 0.35009\n",
      "Iteration: 0013 Loss: 1.56561 Time: 0.34273\n",
      "Iteration: 0014 Loss: 1.52276 Time: 0.35291\n",
      "Iteration: 0015 Loss: 1.51427 Time: 0.35922\n",
      "Iteration: 0016 Loss: 1.47390 Time: 0.34654\n",
      "Iteration: 0017 Loss: 1.49012 Time: 0.37269\n",
      "Iteration: 0018 Loss: 1.46791 Time: 0.34986\n",
      "Iteration: 0019 Loss: 1.45171 Time: 0.34628\n",
      "Iteration: 0020 Loss: 1.44727 Time: 0.34618\n",
      "Iteration: 0021 Loss: 1.44119 Time: 0.35904\n",
      "Iteration: 0022 Loss: 1.44123 Time: 0.35474\n",
      "Iteration: 0023 Loss: 1.43994 Time: 0.34917\n",
      "Iteration: 0024 Loss: 1.39673 Time: 0.35484\n",
      "Iteration: 0025 Loss: 1.39881 Time: 0.34343\n",
      "Iteration: 0026 Loss: 1.41335 Time: 0.35826\n",
      "Iteration: 0027 Loss: 1.35766 Time: 0.34878\n",
      "Iteration: 0028 Loss: 1.37018 Time: 0.35715\n",
      "Iteration: 0029 Loss: 1.34892 Time: 0.34822\n",
      "Iteration: 0030 Loss: 1.36652 Time: 0.35279\n",
      "Iteration: 0031 Loss: 1.32455 Time: 0.35594\n",
      "Iteration: 0032 Loss: 1.32228 Time: 0.34420\n",
      "Iteration: 0033 Loss: 1.32632 Time: 0.36333\n",
      "Iteration: 0034 Loss: 1.30185 Time: 0.35142\n",
      "Iteration: 0035 Loss: 1.30495 Time: 0.35093\n",
      "Iteration: 0036 Loss: 1.29583 Time: 0.35018\n",
      "Iteration: 0037 Loss: 1.24670 Time: 0.35062\n",
      "Iteration: 0038 Loss: 1.25951 Time: 0.35862\n",
      "Iteration: 0039 Loss: 1.25976 Time: 0.35328\n",
      "Iteration: 0040 Loss: 1.24349 Time: 0.35063\n",
      "Iteration: 0041 Loss: 1.24088 Time: 0.36005\n",
      "Iteration: 0042 Loss: 1.23570 Time: 0.36410\n",
      "Iteration: 0043 Loss: 1.23554 Time: 0.35100\n",
      "Iteration: 0044 Loss: 1.19442 Time: 0.35300\n",
      "Iteration: 0045 Loss: 1.21134 Time: 0.35484\n",
      "Iteration: 0046 Loss: 1.20191 Time: 0.36346\n",
      "Iteration: 0047 Loss: 1.19021 Time: 0.35430\n",
      "Iteration: 0048 Loss: 1.16270 Time: 0.35637\n",
      "Iteration: 0049 Loss: 1.18089 Time: 0.35797\n",
      "Iteration: 0050 Loss: 1.15549 Time: 0.36591\n",
      "Iteration: 0051 Loss: 1.12668 Time: 0.35560\n",
      "Iteration: 0052 Loss: 1.14712 Time: 0.36197\n",
      "Iteration: 0053 Loss: 1.13312 Time: 0.35889\n",
      "Iteration: 0054 Loss: 1.14017 Time: 0.35352\n",
      "Iteration: 0055 Loss: 1.15110 Time: 0.35100\n",
      "Iteration: 0056 Loss: 1.14092 Time: 0.36077\n",
      "Iteration: 0057 Loss: 1.12192 Time: 0.35089\n",
      "Iteration: 0058 Loss: 1.11625 Time: 0.33982\n",
      "Iteration: 0059 Loss: 1.10048 Time: 0.36315\n",
      "Iteration: 0060 Loss: 1.09553 Time: 0.35591\n",
      "Iteration: 0061 Loss: 1.10189 Time: 0.35845\n",
      "Iteration: 0062 Loss: 1.06929 Time: 0.35691\n",
      "Iteration: 0063 Loss: 1.06572 Time: 0.34819\n",
      "Iteration: 0064 Loss: 1.08488 Time: 0.35364\n",
      "Iteration: 0065 Loss: 1.07297 Time: 0.34916\n",
      "Iteration: 0066 Loss: 1.04535 Time: 0.36059\n",
      "Iteration: 0067 Loss: 1.05107 Time: 0.34991\n",
      "Iteration: 0068 Loss: 1.05090 Time: 0.36493\n",
      "Iteration: 0069 Loss: 1.04608 Time: 0.34400\n",
      "Iteration: 0070 Loss: 1.02037 Time: 0.34980\n",
      "Iteration: 0071 Loss: 1.01143 Time: 0.35600\n",
      "Iteration: 0072 Loss: 1.02127 Time: 0.34378\n",
      "Iteration: 0073 Loss: 1.01756 Time: 0.35312\n",
      "Iteration: 0074 Loss: 1.00527 Time: 0.34840\n",
      "Iteration: 0075 Loss: 1.00729 Time: 0.34216\n",
      "Iteration: 0076 Loss: 0.99782 Time: 0.35022\n",
      "Iteration: 0077 Loss: 0.98374 Time: 0.34904\n",
      "Iteration: 0078 Loss: 0.97046 Time: 0.36668\n",
      "Iteration: 0079 Loss: 0.97734 Time: 0.35148\n",
      "Iteration: 0080 Loss: 0.96573 Time: 0.36007\n",
      "Iteration: 0081 Loss: 0.96854 Time: 0.34690\n",
      "Iteration: 0082 Loss: 0.96023 Time: 0.35327\n",
      "Iteration: 0083 Loss: 0.95711 Time: 0.34422\n",
      "Iteration: 0084 Loss: 0.94421 Time: 0.35600\n",
      "Iteration: 0085 Loss: 0.93768 Time: 0.34333\n",
      "Iteration: 0086 Loss: 0.92693 Time: 0.34892\n",
      "Iteration: 0087 Loss: 0.92023 Time: 0.34892\n",
      "Iteration: 0088 Loss: 0.92968 Time: 0.34997\n",
      "Iteration: 0089 Loss: 0.89740 Time: 0.35100\n",
      "Iteration: 0090 Loss: 0.90230 Time: 0.35598\n",
      "Iteration: 0091 Loss: 0.90461 Time: 0.34741\n",
      "Iteration: 0092 Loss: 0.90290 Time: 0.35200\n",
      "Iteration: 0093 Loss: 0.89528 Time: 0.35444\n",
      "Iteration: 0094 Loss: 0.88735 Time: 0.35199\n",
      "Iteration: 0095 Loss: 0.88062 Time: 0.35748\n",
      "Iteration: 0096 Loss: 0.88252 Time: 0.35597\n",
      "Iteration: 0097 Loss: 0.87269 Time: 0.36141\n",
      "Iteration: 0098 Loss: 0.85029 Time: 0.34431\n",
      "Iteration: 0099 Loss: 0.85043 Time: 0.35441\n",
      "Iteration: 0100 Loss: 0.84962 Time: 0.35710\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0101 Loss: 0.83710 Time: 0.34575\n",
      "Iteration: 0102 Loss: 0.86237 Time: 0.36407\n",
      "Iteration: 0103 Loss: 0.84140 Time: 0.35822\n",
      "Iteration: 0104 Loss: 0.84054 Time: 0.35380\n",
      "Iteration: 0105 Loss: 0.82938 Time: 0.35468\n",
      "Iteration: 0106 Loss: 0.82458 Time: 0.35938\n",
      "Iteration: 0107 Loss: 0.81133 Time: 0.35758\n",
      "Iteration: 0108 Loss: 0.80476 Time: 0.34826\n",
      "Iteration: 0109 Loss: 0.80761 Time: 0.34529\n",
      "Iteration: 0110 Loss: 0.80136 Time: 0.34463\n",
      "Iteration: 0111 Loss: 0.79658 Time: 0.36755\n",
      "Iteration: 0112 Loss: 0.79042 Time: 0.35804\n",
      "Iteration: 0113 Loss: 0.78275 Time: 0.34913\n",
      "Iteration: 0114 Loss: 0.77881 Time: 0.34981\n",
      "Iteration: 0115 Loss: 0.78271 Time: 0.35016\n",
      "Iteration: 0116 Loss: 0.76952 Time: 0.35131\n",
      "Iteration: 0117 Loss: 0.76839 Time: 0.35680\n",
      "Iteration: 0118 Loss: 0.75967 Time: 0.36019\n",
      "Iteration: 0119 Loss: 0.76536 Time: 0.34910\n",
      "Iteration: 0120 Loss: 0.75565 Time: 0.36113\n",
      "Iteration: 0121 Loss: 0.75159 Time: 0.35192\n",
      "Iteration: 0122 Loss: 0.75368 Time: 0.34775\n",
      "Iteration: 0123 Loss: 0.74120 Time: 0.35599\n",
      "Iteration: 0124 Loss: 0.74005 Time: 0.35821\n",
      "Iteration: 0125 Loss: 0.73180 Time: 0.35298\n",
      "Iteration: 0126 Loss: 0.72511 Time: 0.34666\n",
      "Iteration: 0127 Loss: 0.73200 Time: 0.36779\n",
      "Iteration: 0128 Loss: 0.72829 Time: 0.34792\n",
      "Iteration: 0129 Loss: 0.72055 Time: 0.36499\n",
      "Iteration: 0130 Loss: 0.71124 Time: 0.35098\n",
      "Iteration: 0131 Loss: 0.71466 Time: 0.36052\n",
      "Iteration: 0132 Loss: 0.71571 Time: 0.34703\n",
      "Iteration: 0133 Loss: 0.69922 Time: 0.35088\n",
      "Iteration: 0134 Loss: 0.69931 Time: 0.35017\n",
      "Iteration: 0135 Loss: 0.70585 Time: 0.35294\n",
      "Iteration: 0136 Loss: 0.69194 Time: 0.35818\n",
      "Iteration: 0137 Loss: 0.68984 Time: 0.34524\n",
      "Iteration: 0138 Loss: 0.68392 Time: 0.34593\n",
      "Iteration: 0139 Loss: 0.68433 Time: 0.36625\n",
      "Iteration: 0140 Loss: 0.67332 Time: 0.34789\n",
      "Iteration: 0141 Loss: 0.67108 Time: 0.35462\n",
      "Iteration: 0142 Loss: 0.66919 Time: 0.35462\n",
      "Iteration: 0143 Loss: 0.67665 Time: 0.34568\n",
      "Iteration: 0144 Loss: 0.67146 Time: 0.35462\n",
      "Iteration: 0145 Loss: 0.66492 Time: 0.36511\n",
      "Iteration: 0146 Loss: 0.66231 Time: 0.35134\n",
      "Iteration: 0147 Loss: 0.66350 Time: 0.34414\n",
      "Iteration: 0148 Loss: 0.65888 Time: 0.34562\n",
      "Iteration: 0149 Loss: 0.65674 Time: 0.34270\n",
      "Iteration: 0150 Loss: 0.65477 Time: 0.37081\n",
      "Iteration: 0151 Loss: 0.64948 Time: 0.36019\n",
      "Iteration: 0152 Loss: 0.64844 Time: 0.34598\n",
      "Iteration: 0153 Loss: 0.65020 Time: 0.36104\n",
      "Iteration: 0154 Loss: 0.64527 Time: 0.34810\n",
      "Iteration: 0155 Loss: 0.63839 Time: 0.34909\n",
      "Iteration: 0156 Loss: 0.63935 Time: 0.35199\n",
      "Iteration: 0157 Loss: 0.63099 Time: 0.35432\n",
      "Iteration: 0158 Loss: 0.63457 Time: 0.35266\n",
      "Iteration: 0159 Loss: 0.63380 Time: 0.34727\n",
      "Iteration: 0160 Loss: 0.62896 Time: 0.34580\n",
      "Iteration: 0161 Loss: 0.62997 Time: 0.36010\n",
      "Iteration: 0162 Loss: 0.63087 Time: 0.35591\n",
      "Iteration: 0163 Loss: 0.62049 Time: 0.34686\n",
      "Iteration: 0164 Loss: 0.62166 Time: 0.35491\n",
      "Iteration: 0165 Loss: 0.62425 Time: 0.35302\n",
      "Iteration: 0166 Loss: 0.61753 Time: 0.35288\n",
      "Iteration: 0167 Loss: 0.61604 Time: 0.34600\n",
      "Iteration: 0168 Loss: 0.61817 Time: 0.35227\n",
      "Iteration: 0169 Loss: 0.61018 Time: 0.35630\n",
      "Iteration: 0170 Loss: 0.61581 Time: 0.35751\n",
      "Iteration: 0171 Loss: 0.61157 Time: 0.34590\n",
      "Iteration: 0172 Loss: 0.60702 Time: 0.35318\n",
      "Iteration: 0173 Loss: 0.61208 Time: 0.35122\n",
      "Iteration: 0174 Loss: 0.60905 Time: 0.35406\n",
      "Iteration: 0175 Loss: 0.60588 Time: 0.35841\n",
      "Iteration: 0176 Loss: 0.60696 Time: 0.35371\n",
      "Iteration: 0177 Loss: 0.60569 Time: 0.34137\n",
      "Iteration: 0178 Loss: 0.59700 Time: 0.35607\n",
      "Iteration: 0179 Loss: 0.60007 Time: 0.33900\n",
      "Iteration: 0180 Loss: 0.59837 Time: 0.35110\n",
      "Iteration: 0181 Loss: 0.59930 Time: 0.35757\n",
      "Iteration: 0182 Loss: 0.60307 Time: 0.34893\n",
      "Iteration: 0183 Loss: 0.59265 Time: 0.34539\n",
      "Iteration: 0184 Loss: 0.59954 Time: 0.36628\n",
      "Iteration: 0185 Loss: 0.59312 Time: 0.34722\n",
      "Iteration: 0186 Loss: 0.59169 Time: 0.35699\n",
      "Iteration: 0187 Loss: 0.59067 Time: 0.35342\n",
      "Iteration: 0188 Loss: 0.58532 Time: 0.34709\n",
      "Iteration: 0189 Loss: 0.58522 Time: 0.34209\n",
      "Iteration: 0190 Loss: 0.58859 Time: 0.36157\n",
      "Iteration: 0191 Loss: 0.58551 Time: 0.34699\n",
      "Iteration: 0192 Loss: 0.58571 Time: 0.36551\n",
      "Iteration: 0193 Loss: 0.58259 Time: 0.35961\n",
      "Iteration: 0194 Loss: 0.58256 Time: 0.33595\n",
      "Iteration: 0195 Loss: 0.58135 Time: 0.35995\n",
      "Iteration: 0196 Loss: 0.58339 Time: 0.35149\n",
      "Iteration: 0197 Loss: 0.58457 Time: 0.34733\n",
      "Iteration: 0198 Loss: 0.58204 Time: 0.35310\n",
      "Iteration: 0199 Loss: 0.58263 Time: 0.34638\n",
      "Iteration: 0200 Loss: 0.57924 Time: 0.35167\n",
      "Done! \n",
      "\n",
      "STEP 3/3 - MODEL EVALUATION \n",
      "\n",
      "Computing the final embedding vectors, for evaluation\n",
      "Done! \n",
      "\n",
      "Testing: link prediction\n",
      "Done! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We repeat the entire training+test process FLAGS.nb_run times\n",
    "for i in range(FLAGS.nb_run):\n",
    "    if FLAGS.verbose:\n",
    "        print(\"EXPERIMENTS ON MODEL\", i + 1, \"/\", FLAGS.nb_run, \"\\n\")\n",
    "        print(\"STEP 1/3 - PREPROCESSING STEPS \\n\")\n",
    "\n",
    "\n",
    "    # Edge masking for Link Prediction:\n",
    "    if FLAGS.task == 'task_2':\n",
    "        # Compute Train/Validation/Test sets\n",
    "        if FLAGS.verbose:\n",
    "            print(\"Masking some edges from the training graph, for link prediction\")\n",
    "            print(\"(validation set:\", FLAGS.prop_val, \"% of edges - test set:\",\n",
    "                  FLAGS.prop_test, \"% of edges)\")\n",
    "        #adj, val_edges, val_edges_false, test_edges, test_edges_false = mask_test_edges(adj_init, FLAGS.prop_test, FLAGS.prop_val)\n",
    "        adj, test_edges, test_edges_false = mask_test_edges(adj_init, FLAGS.prop_test)\n",
    "        if FLAGS.verbose:\n",
    "            print(\"Done! \\n\")\n",
    "    else:\n",
    "        adj = adj_init\n",
    "\n",
    "    # Compute the number of nodes\n",
    "    num_nodes = adj.shape[0]\n",
    "\n",
    "    # Preprocessing on node features\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Preprocessing node features\")\n",
    "    if FLAGS.features:\n",
    "        features = features_init\n",
    "    else:\n",
    "        # If features are not used, replace feature matrix by identity matrix\n",
    "        features = sp.identity(num_nodes)\n",
    "    features = sparse_to_tuple(features)\n",
    "    num_features = features[2][1]\n",
    "    features_nonzero = features[1].shape[0]\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Done! \\n\")\n",
    "\n",
    "\n",
    "    # Community detection using Louvain, as a preprocessing step\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Running the Louvain algorithm for community detection\")\n",
    "        print(\"as a preprocessing step for the encoder\")\n",
    "    # Get binary community matrix (adj_louvain_init[i,j] = 1 if nodes i and j are\n",
    "    # in the same community) as well as number of communities found by Louvain\n",
    "    adj_louvain_init, nb_communities_louvain, partition = louvain_clustering(adj, FLAGS.s_reg)\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Done! Louvain has found\", nb_communities_louvain, \"communities \\n\")\n",
    "\n",
    "    # FastGAE: node sampling for stochastic subgraph decoding - used when facing scalability issues\n",
    "    if FLAGS.fastgae:\n",
    "        if FLAGS.verbose:\n",
    "            print(\"Preprocessing operations related to the FastGAE method\")\n",
    "        # Node-level p_i degree-based, core-based or uniform distribution\n",
    "        node_distribution = get_distribution(FLAGS.measure, FLAGS.alpha, adj)\n",
    "        # Node sampling for initializations\n",
    "        sampled_nodes, adj_label, adj_sampled_sparse = node_sampling(adj, node_distribution, FLAGS.nb_node_samples, FLAGS.replace)\n",
    "        if FLAGS.verbose:\n",
    "            print(\"Done! \\n\")\n",
    "    else:\n",
    "        sampled_nodes = np.array(range(FLAGS.nb_node_samples))\n",
    "\n",
    "\n",
    "    # Placeholders\n",
    "    if FLAGS.verbose:\n",
    "        print('Setting up the model and the optimizer')\n",
    "    placeholders = {\n",
    "        'features': tf.sparse_placeholder(tf.float32),\n",
    "        'adj': tf.sparse_placeholder(tf.float32),\n",
    "        'adj_layer2': tf.sparse_placeholder(tf.float32), # Only used for 2-layer GCN encoders\n",
    "        'degree_matrix': tf.sparse_placeholder(tf.float32),\n",
    "        'adj_orig': tf.sparse_placeholder(tf.float32),\n",
    "        'dropout': tf.placeholder_with_default(0., shape = ()),\n",
    "        'sampled_nodes': tf.placeholder_with_default(sampled_nodes, shape = [FLAGS.nb_node_samples])\n",
    "    }\n",
    "\n",
    "\n",
    "    # Create model\n",
    "    if FLAGS.model == 'linear_ae':\n",
    "        # Linear Graph Autoencoder\n",
    "        model = LinearModelAE(placeholders, num_features, features_nonzero)\n",
    "    elif FLAGS.model == 'linear_vae':\n",
    "        # Linear Graph Variational Autoencoder\n",
    "        model = LinearModelVAE(placeholders, num_features, num_nodes, features_nonzero)\n",
    "    elif FLAGS.model == 'gcn_ae':\n",
    "        # 2-layer GCN Graph Autoencoder\n",
    "        model = GCNModelAE(placeholders, num_features, features_nonzero)\n",
    "    elif FLAGS.model == 'gcn_vae':\n",
    "        # 2-layer GCN Graph Variational Autoencoder\n",
    "        model = GCNModelVAE(placeholders, num_features, num_nodes, features_nonzero)\n",
    "    else:\n",
    "        raise ValueError('Undefined model!')\n",
    "\n",
    "\n",
    "    # Optimizer\n",
    "    if FLAGS.fastgae:\n",
    "        num_sampled = adj_sampled_sparse.shape[0]\n",
    "        sum_sampled = adj_sampled_sparse.sum()\n",
    "        pos_weight = float(num_sampled * num_sampled - sum_sampled) / sum_sampled\n",
    "        norm = num_sampled * num_sampled / float((num_sampled * num_sampled - sum_sampled) * 2)\n",
    "    else:\n",
    "        pos_weight = float(num_nodes * num_nodes - adj.sum()) / adj.sum()\n",
    "        norm = num_nodes * num_nodes / float((num_nodes * num_nodes - adj.sum()) * 2)\n",
    "\n",
    "    if FLAGS.model in ('gcn_ae', 'linear_ae'):\n",
    "        opt = OptimizerAE(preds = model.reconstructions,\n",
    "                          labels = tf.reshape(tf.sparse_tensor_to_dense(placeholders['adj_orig'], validate_indices = False), [-1]),\n",
    "                          degree_matrix = tf.reshape(tf.sparse_tensor_to_dense(placeholders['degree_matrix'], validate_indices = False), [-1]),\n",
    "                          num_nodes = num_nodes,\n",
    "                          pos_weight = pos_weight,\n",
    "                          norm = norm,\n",
    "                          clusters_distance = model.clusters)\n",
    "    else:\n",
    "        opt = OptimizerVAE(preds = model.reconstructions,\n",
    "                           labels = tf.reshape(tf.sparse_tensor_to_dense(placeholders['adj_orig'], validate_indices = False), [-1]),\n",
    "                           degree_matrix = tf.reshape(tf.sparse_tensor_to_dense(placeholders['degree_matrix'], validate_indices = False), [-1]),\n",
    "                           model = model,\n",
    "                           num_nodes = num_nodes,\n",
    "                           pos_weight = pos_weight,\n",
    "                           norm = norm,\n",
    "                           clusters_distance = model.clusters)\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Done! \\n\")\n",
    "\n",
    "\n",
    "    # Symmetrically normalized \"message passing\" matrices\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Preprocessing on message passing matrices\")\n",
    "    adj_norm = preprocess_graph(adj + FLAGS.lamb*adj_louvain_init)\n",
    "    adj_norm_layer2 = preprocess_graph(adj)\n",
    "    if not FLAGS.fastgae:\n",
    "        adj_label = sparse_to_tuple(adj + sp.eye(num_nodes))\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Done! \\n\")\n",
    "\n",
    "\n",
    "    # Degree matrices\n",
    "    deg_matrix, deg_matrix_init = preprocess_degree(adj, FLAGS.simple)\n",
    "\n",
    "\n",
    "    # Initialize TF session\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Initializing TF session\")\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Done! \\n\")\n",
    "\n",
    "\n",
    "    # Model training\n",
    "    if FLAGS.verbose:\n",
    "        print(\"STEP 2/3 - MODEL TRAINING \\n\")\n",
    "        print(\"Starting training\")\n",
    "\n",
    "    for iter in range(FLAGS.iterations):\n",
    "\n",
    "        # Flag to compute running time for each iteration\n",
    "        t = time.time()\n",
    "\n",
    "        # Construct feed dictionary\n",
    "        feed_dict = construct_feed_dict(adj_norm, adj_norm_layer2, adj_label, features, deg_matrix, placeholders)\n",
    "        if FLAGS.fastgae:\n",
    "            # Update sampled subgraph and sampled Louvain matrix\n",
    "            feed_dict.update({placeholders['sampled_nodes']: sampled_nodes})\n",
    "            # New node sampling\n",
    "            sampled_nodes, adj_label, adj_label_sparse, = node_sampling(adj, node_distribution, FLAGS.nb_node_samples, FLAGS.replace)\n",
    "\n",
    "        # Weights update\n",
    "        outs = sess.run([opt.opt_op, opt.cost, opt.cost_adj, opt.cost_mod],\n",
    "                        feed_dict = feed_dict)\n",
    "\n",
    "        # Compute average loss\n",
    "        avg_cost = outs[1]\n",
    "        if FLAGS.verbose:\n",
    "            # Display information on the iteration\n",
    "            print(\"Iteration:\", '%04d' % (iter + 1), \"Loss:\", \"{:.5f}\".format(avg_cost),\n",
    "                  \"Time:\", \"{:.5f}\".format(time.time() - t))\n",
    "\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Done! \\n\")\n",
    "\n",
    "    # Compute embedding vectors, for evaluation\n",
    "    if FLAGS.verbose:\n",
    "        print(\"STEP 3/3 - MODEL EVALUATION \\n\")\n",
    "        print(\"Computing the final embedding vectors, for evaluation\")\n",
    "    emb = sess.run(model.z_mean, feed_dict = feed_dict)\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Done! \\n\")\n",
    "\n",
    "    # Test model: link prediction (classification edges/non-edges)\n",
    "\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Testing: link prediction\")\n",
    "    # Get ROC and AP scores\n",
    "    roc_score, ap_score = link_prediction(test_edges, test_edges_false, emb)\n",
    "    mean_roc.append(roc_score)\n",
    "    mean_ap.append(ap_score)\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Done! \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bef29d76",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-28T08:50:14.447257Z",
     "start_time": "2022-10-28T08:50:14.439217Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL RESULTS \n",
      "\n",
      "Recall: the selected task was \"Task 2\", i.e., joint community detection and link prediction, on citeseer\n",
      "All scores reported below are computed over the 10 run(s) \n",
      "\n",
      "Link prediction:\n",
      "\n",
      "Mean AUC score:  0.7899190918971138\n",
      "Std of AUC scores:  0.013616774985648498 \n",
      "\n",
      "Mean AP score:  0.8346534571022513\n",
      "Std of AP scores:  0.010142090058210514 \n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Report final results\n",
    "print(\"FINAL RESULTS \\n\")\n",
    "\n",
    "print('Recall: the selected task was \"Task 2\", i.e., joint community detection and link prediction, on', FLAGS.dataset)\n",
    "\n",
    "print(\"All scores reported below are computed over the\", FLAGS.nb_run, \"run(s) \\n\")\n",
    "\n",
    "print(\"Link prediction:\\n\")\n",
    "print(\"Mean AUC score: \", np.mean(mean_roc))\n",
    "print(\"Std of AUC scores: \", np.std(mean_roc), \"\\n\")\n",
    "\n",
    "print(\"Mean AP score: \", np.mean(mean_ap))\n",
    "print(\"Std of AP scores: \", np.std(mean_ap), \"\\n \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553e6eb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf1.15",
   "language": "python",
   "name": "tf1.15"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
