{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61fb81d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-11T11:12:42.478689Z",
     "start_time": "2022-10-11T11:12:42.421630Z"
    }
   },
   "outputs": [],
   "source": [
    "\"Implementation based on https://github.com/PetarV-/DGI\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from models import GIC, LogReg\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "import statistics\n",
    "import argparse\n",
    "import pickle as pkl\n",
    "import networkx as nx\n",
    "\n",
    "from scipy.sparse.linalg import eigsh\n",
    "\n",
    "import sys\n",
    "import torch.nn.functional as F\n",
    "import sklearn\n",
    "import sklearn.cluster\n",
    "#from functional import  pairwise_distances\n",
    "from layers import GCN, AvgReadout, Discriminator, Discriminator_cluster\n",
    "\n",
    "def parse_skipgram(fname):\n",
    "    with open(fname) as f:\n",
    "        toks = list(f.read().split())\n",
    "    nb_nodes = int(toks[0])\n",
    "    nb_features = int(toks[1])\n",
    "    ret = np.empty((nb_nodes, nb_features))\n",
    "    it = 2\n",
    "    for i in range(nb_nodes):\n",
    "        cur_nd = int(toks[it]) - 1\n",
    "        it += 1\n",
    "        for j in range(nb_features):\n",
    "            cur_ft = float(toks[it])\n",
    "            ret[cur_nd][j] = cur_ft\n",
    "            it += 1\n",
    "    return ret\n",
    "\n",
    "# Process a (subset of) a TU dataset into standard form\n",
    "def process_tu(data, nb_nodes):\n",
    "    nb_graphs = len(data)\n",
    "    ft_size = data.num_features\n",
    "\n",
    "    features = np.zeros((nb_graphs, nb_nodes, ft_size))\n",
    "    adjacency = np.zeros((nb_graphs, nb_nodes, nb_nodes))\n",
    "    labels = np.zeros(nb_graphs)\n",
    "    sizes = np.zeros(nb_graphs, dtype=np.int32)\n",
    "    masks = np.zeros((nb_graphs, nb_nodes))\n",
    "       \n",
    "    for g in range(nb_graphs):\n",
    "        sizes[g] = data[g].x.shape[0]\n",
    "        features[g, :sizes[g]] = data[g].x\n",
    "        labels[g] = data[g].y[0]\n",
    "        masks[g, :sizes[g]] = 1.0\n",
    "        e_ind = data[g].edge_index\n",
    "        coo = sp.coo_matrix((np.ones(e_ind.shape[1]), (e_ind[0, :], e_ind[1, :])), shape=(nb_nodes, nb_nodes))\n",
    "        adjacency[g] = coo.todense()\n",
    "\n",
    "    return features, adjacency, labels, sizes, masks\n",
    "\n",
    "def micro_f1(logits, labels):\n",
    "    # Compute predictions\n",
    "    preds = torch.round(nn.Sigmoid()(logits))\n",
    "    \n",
    "    # Cast to avoid trouble\n",
    "    preds = preds.long()\n",
    "    labels = labels.long()\n",
    "\n",
    "    # Count true positives, true negatives, false positives, false negatives\n",
    "    tp = torch.nonzero(preds * labels).shape[0] * 1.0\n",
    "    tn = torch.nonzero((preds - 1) * (labels - 1)).shape[0] * 1.0\n",
    "    fp = torch.nonzero(preds * (labels - 1)).shape[0] * 1.0\n",
    "    fn = torch.nonzero((preds - 1) * labels).shape[0] * 1.0\n",
    "\n",
    "    # Compute micro-f1 score\n",
    "    prec = tp / (tp + fp)\n",
    "    rec = tp / (tp + fn)\n",
    "    f1 = (2 * prec * rec) / (prec + rec)\n",
    "    return f1\n",
    "\n",
    "\"\"\"\n",
    " Prepare adjacency matrix by expanding up to a given neighbourhood.\n",
    " This will insert loops on every node.\n",
    " Finally, the matrix is converted to bias vectors.\n",
    " Expected shape: [graph, nodes, nodes]\n",
    "\"\"\"\n",
    "def adj_to_bias(adj, sizes, nhood=1):\n",
    "    nb_graphs = adj.shape[0]\n",
    "    mt = np.empty(adj.shape)\n",
    "    for g in range(nb_graphs):\n",
    "        mt[g] = np.eye(adj.shape[1])\n",
    "        for _ in range(nhood):\n",
    "            mt[g] = np.matmul(mt[g], (adj[g] + np.eye(adj.shape[1])))\n",
    "        for i in range(sizes[g]):\n",
    "            for j in range(sizes[g]):\n",
    "                if mt[g][i][j] > 0.0:\n",
    "                    mt[g][i][j] = 1.0\n",
    "    return -1e9 * (1.0 - mt)\n",
    "\n",
    "\n",
    "###############################################\n",
    "# This section of code adapted from tkipf/gcn #\n",
    "###############################################\n",
    "\n",
    "def parse_index_file(filename):\n",
    "    \"\"\"Parse index file.\"\"\"\n",
    "    index = []\n",
    "    for line in open(filename):\n",
    "        index.append(int(line.strip()))\n",
    "    return index\n",
    "\n",
    "def sample_mask(idx, l):\n",
    "    \"\"\"Create mask.\"\"\"\n",
    "    mask = np.zeros(l)\n",
    "    mask[idx] = 1\n",
    "    return np.array(mask, dtype=np.bool)\n",
    "\n",
    "def load_data(dataset_str): # {'pubmed', 'citeseer', 'cora'}\n",
    "    \"\"\"Load data.\"\"\"\n",
    "    names = ['x', 'y', 'tx', 'ty', 'allx', 'ally', 'graph']\n",
    "    objects = []\n",
    "    for i in range(len(names)):\n",
    "        with open(\"data/ind.{}.{}\".format(dataset_str, names[i]), 'rb') as f:\n",
    "            if sys.version_info > (3, 0):\n",
    "                objects.append(pkl.load(f, encoding='latin1'))\n",
    "            else:\n",
    "                objects.append(pkl.load(f))\n",
    "\n",
    "    x, y, tx, ty, allx, ally, graph = tuple(objects)\n",
    "    test_idx_reorder = parse_index_file(\"data/ind.{}.test.index\".format(dataset_str))\n",
    "    test_idx_range = np.sort(test_idx_reorder)\n",
    "\n",
    "    if dataset_str == 'citeseer'  :\n",
    "        # Fix citeseer dataset (there are some isolated nodes in the graph)\n",
    "        # Find isolated nodes, add them as zero-vecs into the right position\n",
    "        test_idx_range_full = range(min(test_idx_reorder), max(test_idx_reorder)+1)\n",
    "        tx_extended = sp.lil_matrix((len(test_idx_range_full), x.shape[1]))\n",
    "        tx_extended[test_idx_range-min(test_idx_range), :] = tx\n",
    "        tx = tx_extended\n",
    "        ty_extended = np.zeros((len(test_idx_range_full), y.shape[1]))\n",
    "        ty_extended[test_idx_range-min(test_idx_range), :] = ty\n",
    "        ty = ty_extended\n",
    "\n",
    "    features = sp.vstack((allx, tx)).tolil()\n",
    "    features[test_idx_reorder, :] = features[test_idx_range, :]\n",
    "    adj = nx.adjacency_matrix(nx.from_dict_of_lists(graph))\n",
    "\n",
    "    labels = np.vstack((ally, ty))\n",
    "    labels[test_idx_reorder, :] = labels[test_idx_range, :]\n",
    "\n",
    "    idx_test = test_idx_range.tolist()\n",
    "    idx_train = range(len(y))\n",
    "    idx_val = range(len(y), len(y)+500)\n",
    "\n",
    "    return adj, features, labels, idx_train, idx_val, idx_test\n",
    "\n",
    "def sparse_to_tuple(sparse_mx, insert_batch=False):\n",
    "    \"\"\"Convert sparse matrix to tuple representation.\"\"\"\n",
    "    \"\"\"Set insert_batch=True if you want to insert a batch dimension.\"\"\"\n",
    "    def to_tuple(mx):\n",
    "        if not sp.isspmatrix_coo(mx):\n",
    "            mx = mx.tocoo()\n",
    "        if insert_batch:\n",
    "            coords = np.vstack((np.zeros(mx.row.shape[0]), mx.row, mx.col)).transpose()\n",
    "            values = mx.data\n",
    "            shape = (1,) + mx.shape\n",
    "        else:\n",
    "            coords = np.vstack((mx.row, mx.col)).transpose()\n",
    "            values = mx.data\n",
    "            shape = mx.shape\n",
    "        return coords, values, shape\n",
    "\n",
    "    if isinstance(sparse_mx, list):\n",
    "        for i in range(len(sparse_mx)):\n",
    "            sparse_mx[i] = to_tuple(sparse_mx[i])\n",
    "    else:\n",
    "        sparse_mx = to_tuple(sparse_mx)\n",
    "\n",
    "    return sparse_mx\n",
    "\n",
    "def standardize_data(f, train_mask):\n",
    "    \"\"\"Standardize feature matrix and convert to tuple representation\"\"\"\n",
    "    # standardize data\n",
    "    f = f.todense()\n",
    "    mu = f[train_mask == True, :].mean(axis=0)\n",
    "    sigma = f[train_mask == True, :].std(axis=0)\n",
    "    f = f[:, np.squeeze(np.array(sigma > 0))]\n",
    "    mu = f[train_mask == True, :].mean(axis=0)\n",
    "    sigma = f[train_mask == True, :].std(axis=0)\n",
    "    f = (f - mu) / sigma\n",
    "    return f\n",
    "\n",
    "def preprocess_features(features):\n",
    "    \"\"\"Row-normalize feature matrix and convert to tuple representation\"\"\"\n",
    "    rowsum = np.array(features.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    features = r_mat_inv.dot(features)\n",
    "    return features.todense(), sparse_to_tuple(features)\n",
    "\n",
    "def normalize_adj(adj):\n",
    "    \"\"\"Symmetrically normalize adjacency matrix.\"\"\"\n",
    "    adj = sp.coo_matrix(adj)\n",
    "    rowsum = np.array(adj.sum(1))\n",
    "    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
    "    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()\n",
    "\n",
    "\n",
    "def preprocess_adj(adj):\n",
    "    \"\"\"Preprocessing of adjacency matrix for simple GCN model and conversion to tuple representation.\"\"\"\n",
    "    adj_normalized = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
    "    return sparse_to_tuple(adj_normalized)\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n",
    "\n",
    "# Perform train-test split\n",
    "    # Takes in adjacency matrix in sparse format\n",
    "    # Returns: adj_train, train_edges, val_edges, val_edges_false, \n",
    "        # test_edges, test_edges_false\n",
    "def mask_test_edges(adj, test_frac=.1, val_frac=0, prevent_disconnect=True, verbose=False):\n",
    "    # NOTE: Splits are randomized and results might slightly deviate from reported numbers in the paper.\n",
    "    \"from https://github.com/tkipf/gae\"\n",
    "    \n",
    "    if verbose == True:\n",
    "        print('preprocessing...')\n",
    "\n",
    "    # Remove diagonal elements\n",
    "    adj = adj - sp.dia_matrix((adj.diagonal()[np.newaxis, :], [0]), shape=adj.shape)\n",
    "    adj.eliminate_zeros()\n",
    "    # Check that diag is zero:\n",
    "    assert np.diag(adj.todense()).sum() == 0\n",
    "\n",
    "    g = nx.from_scipy_sparse_matrix(adj)\n",
    "    orig_num_cc = nx.number_connected_components(g)\n",
    "\n",
    "    adj_triu = sp.triu(adj) # upper triangular portion of adj matrix\n",
    "    adj_tuple = sparse_to_tuple(adj_triu) # (coords, values, shape), edges only 1 way\n",
    "    edges = adj_tuple[0] # all edges, listed only once (not 2 ways)\n",
    "    # edges_all = sparse_to_tuple(adj)[0] # ALL edges (includes both ways)\n",
    "    num_test = int(np.floor(edges.shape[0] * test_frac)) # controls how large the test set should be\n",
    "    num_val = int(np.floor(edges.shape[0] * val_frac)) # controls how alrge the validation set should be\n",
    "\n",
    "    # Store edges in list of ordered tuples (node1, node2) where node1 < node2\n",
    "    edge_tuples = [(min(edge[0], edge[1]), max(edge[0], edge[1])) for edge in edges]\n",
    "    all_edge_tuples = set(edge_tuples)\n",
    "    train_edges = set(edge_tuples) # initialize train_edges to have all edges\n",
    "    test_edges = set()\n",
    "    val_edges = set()\n",
    "\n",
    "    if verbose == True:\n",
    "        print('generating test/val sets...')\n",
    "\n",
    "    # Iterate over shuffled edges, add to train/val sets\n",
    "    np.random.shuffle(edge_tuples)\n",
    "    for edge in edge_tuples:\n",
    "        # print edge\n",
    "        node1 = edge[0]\n",
    "        node2 = edge[1]\n",
    "\n",
    "        # If removing edge would disconnect a connected component, backtrack and move on\n",
    "        g.remove_edge(node1, node2)\n",
    "        if prevent_disconnect == True:\n",
    "            if nx.number_connected_components(g) > orig_num_cc:\n",
    "                g.add_edge(node1, node2)\n",
    "                continue\n",
    "\n",
    "        # Fill test_edges first\n",
    "        if len(test_edges) < num_test:\n",
    "            test_edges.add(edge)\n",
    "            train_edges.remove(edge)\n",
    "\n",
    "        # Then, fill val_edges\n",
    "        elif len(val_edges) < num_val:\n",
    "            val_edges.add(edge)\n",
    "            train_edges.remove(edge)\n",
    "\n",
    "        # Both edge lists full --> break loop\n",
    "        elif len(test_edges) == num_test and len(val_edges) == num_val:\n",
    "            break\n",
    "\n",
    "    if (len(val_edges) < num_val or len(test_edges) < num_test):\n",
    "        print(\"WARNING: not enough removable edges to perform full train-test split!\")\n",
    "        print(\"Num. (test, val) edges requested: (\", num_test, \", \", num_val, \")\")\n",
    "        print(\"Num. (test, val) edges returned: (\", len(test_edges), \", \", len(val_edges), \")\")\n",
    "\n",
    "    if prevent_disconnect == True:\n",
    "        assert nx.number_connected_components(g) == orig_num_cc\n",
    "\n",
    "    if verbose == True:\n",
    "        print('creating false test edges...')\n",
    "\n",
    "    test_edges_false = set()\n",
    "    while len(test_edges_false) < num_test:  \n",
    "        idx_i = np.random.randint(0, adj.shape[0])\n",
    "        idx_j = np.random.randint(0, adj.shape[0])\n",
    "        if idx_i == idx_j:\n",
    "            continue\n",
    "\n",
    "        false_edge = (min(idx_i, idx_j), max(idx_i, idx_j))\n",
    "\n",
    "        # Make sure false_edge not an actual edge, and not a repeat\n",
    "        if false_edge in all_edge_tuples:\n",
    "            continue\n",
    "        if false_edge in test_edges_false:\n",
    "            continue\n",
    "\n",
    "        test_edges_false.add(false_edge)\n",
    "\n",
    "    if verbose == True:\n",
    "        print('creating false val edges...')\n",
    "\n",
    "    val_edges_false = set()\n",
    "    while len(val_edges_false) < num_val:\n",
    "        idx_i = np.random.randint(0, adj.shape[0])\n",
    "        idx_j = np.random.randint(0, adj.shape[0])\n",
    "        if idx_i == idx_j:\n",
    "            continue\n",
    "\n",
    "        false_edge = (min(idx_i, idx_j), max(idx_i, idx_j))\n",
    "\n",
    "        # Make sure false_edge in not an actual edge, not in test_edges_false, not a repeat\n",
    "        if false_edge in all_edge_tuples or \\\n",
    "            false_edge in test_edges_false or \\\n",
    "            false_edge in val_edges_false:\n",
    "            continue\n",
    "            \n",
    "        val_edges_false.add(false_edge)\n",
    "\n",
    "    if verbose == True:\n",
    "        print('creating false train edges...')\n",
    "\n",
    "    train_edges_false = set()\n",
    "    while len(train_edges_false) < len(train_edges):\n",
    "        idx_i = np.random.randint(0, adj.shape[0])\n",
    "        idx_j = np.random.randint(0, adj.shape[0])\n",
    "        if idx_i == idx_j:\n",
    "            continue\n",
    "\n",
    "        false_edge = (min(idx_i, idx_j), max(idx_i, idx_j))\n",
    "\n",
    "        # Make sure false_edge in not an actual edge, not in test_edges_false, \n",
    "            # not in val_edges_false, not a repeat\n",
    "        if false_edge in all_edge_tuples or \\\n",
    "            false_edge in test_edges_false or \\\n",
    "            false_edge in val_edges_false or \\\n",
    "            false_edge in train_edges_false:\n",
    "            continue\n",
    "\n",
    "        train_edges_false.add(false_edge)\n",
    "\n",
    "    if verbose == True:\n",
    "        print('final checks for disjointness...')\n",
    "\n",
    "    # assert: false_edges are actually false (not in all_edge_tuples)\n",
    "    assert test_edges_false.isdisjoint(all_edge_tuples)\n",
    "    assert val_edges_false.isdisjoint(all_edge_tuples)\n",
    "    assert train_edges_false.isdisjoint(all_edge_tuples)\n",
    "\n",
    "    # assert: test, val, train false edges disjoint\n",
    "    assert test_edges_false.isdisjoint(val_edges_false)\n",
    "    assert test_edges_false.isdisjoint(train_edges_false)\n",
    "    assert val_edges_false.isdisjoint(train_edges_false)\n",
    "\n",
    "    # assert: test, val, train positive edges disjoint\n",
    "    assert val_edges.isdisjoint(train_edges)\n",
    "    assert test_edges.isdisjoint(train_edges)\n",
    "    assert val_edges.isdisjoint(test_edges)\n",
    "\n",
    "    if verbose == True:\n",
    "        print('creating adj_train...')\n",
    "\n",
    "    # Re-build adj matrix using remaining graph\n",
    "    adj_train = nx.adjacency_matrix(g)\n",
    "\n",
    "    # Convert edge-lists to numpy arrays\n",
    "    train_edges = np.array([list(edge_tuple) for edge_tuple in train_edges])\n",
    "    train_edges_false = np.array([list(edge_tuple) for edge_tuple in train_edges_false])\n",
    "    val_edges = np.array([list(edge_tuple) for edge_tuple in val_edges])\n",
    "    val_edges_false = np.array([list(edge_tuple) for edge_tuple in val_edges_false])\n",
    "    test_edges = np.array([list(edge_tuple) for edge_tuple in test_edges])\n",
    "    test_edges_false = np.array([list(edge_tuple) for edge_tuple in test_edges_false])\n",
    "\n",
    "    if verbose == True:\n",
    "        print('Done with train-test split!')\n",
    "        print('')\n",
    "\n",
    "    # NOTE: these edge lists only contain single direction of edge!\n",
    "    return adj_train, train_edges, train_edges_false, \\\n",
    "        val_edges, val_edges_false, test_edges, test_edges_false\n",
    "\n",
    "'''\n",
    "    pytorch (differentiable) implementation of soft k-means clustering. \n",
    "    Modified from https://github.com/bwilder0/clusternet\n",
    "'''\n",
    "def cluster(data, k, temp, num_iter, init, cluster_temp):\n",
    "    cuda0 = False#False\n",
    "    \n",
    "    mu = init\n",
    "    n = data.shape[0]\n",
    "    d = data.shape[1]\n",
    "    \n",
    "    data = data / (data.norm(dim=1)[:, None] + 1e-6) #prevent zero-division loss with 1e-6\n",
    "    for t in range(num_iter):\n",
    "        \n",
    "        mu = mu / (mu.norm(dim=1)[:, None] + 1e-6) #prevent zero-division with 1e-6\n",
    "        dist = torch.mm(data, mu.transpose(0,1))\n",
    "\n",
    "        #cluster responsibilities via softmax\n",
    "        r = F.softmax(cluster_temp*dist, dim=1)\n",
    "        #total responsibility of each cluster\n",
    "        cluster_r = r.sum(dim=0)\n",
    "        #mean of points in each cluster weighted by responsibility\n",
    "        cluster_mean = r.t() @ data\n",
    "        #update cluster means\n",
    "        new_mu = torch.diag(1/cluster_r) @ cluster_mean\n",
    "        mu = new_mu\n",
    "        \n",
    "    r = F.softmax(cluster_temp*dist, dim=1)\n",
    "    return mu, r\n",
    "\n",
    "class Clusterator(nn.Module):\n",
    "    def __init__(self, nout, K):\n",
    "        super(Clusterator, self).__init__()\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.K = K\n",
    "        self.nout = nout\n",
    "        self.init =  torch.rand(self.K, nout)\n",
    "        \n",
    "    def forward(self, embeds, cluster_temp, num_iter=10):\n",
    "        mu_init, _ = cluster(embeds, self.K, 1, num_iter, cluster_temp = torch.tensor(cluster_temp), init = self.init)\n",
    "        #self.init = mu_init.clone().detach()\n",
    "        mu, r = cluster(embeds, self.K, 1, 1, cluster_temp = torch.tensor(cluster_temp), init = mu_init.clone().detach())\n",
    "        return mu, r\n",
    "    \n",
    "class GIC(nn.Module):\n",
    "    def __init__(self,n_nb, n_in, n_h, activation, num_clusters, beta):\n",
    "        super(GIC, self).__init__()\n",
    "        self.gcn = GCN(n_in, n_h, activation)\n",
    "        self.read = AvgReadout()\n",
    "        self.sigm = nn.Sigmoid()\n",
    "        self.disc = Discriminator(n_h)\n",
    "        self.disc_c = Discriminator_cluster(n_h,n_h,n_nb,num_clusters)\n",
    "        self.beta = beta\n",
    "        self.cluster = Clusterator(n_h,num_clusters)\n",
    "    \n",
    "    def forward(self, seq1, seq2, adj, sparse, msk, samp_bias1, samp_bias2, cluster_temp):\n",
    "        h_1 = self.gcn(seq1, adj, sparse)\n",
    "        h_2 = self.gcn(seq2, adj, sparse)\n",
    "        self.beta = cluster_temp\n",
    "        Z, S = self.cluster(h_1[-1,:,:], cluster_temp)\n",
    "        Z_t = S @ Z\n",
    "        c2 = Z_t\n",
    "        c2 = self.sigm(c2)\n",
    "        c = self.read(h_1, msk)\n",
    "        c = self.sigm(c) \n",
    "        c_x = c.unsqueeze(1)\n",
    "        c_x = c_x.expand_as(h_1)\n",
    "        ret = self.disc(c_x, h_1, h_2, samp_bias1, samp_bias2)\n",
    "        ret2 = self.disc_c(c2, c2,h_1[-1,:,:], h_1[-1,:,:] ,h_2[-1,:,:], S , samp_bias1, samp_bias2)\n",
    "        return ret, ret2 \n",
    "\n",
    "    # Detach the return variables\n",
    "    def embed(self, seq, adj, sparse, msk, cluster_temp):\n",
    "        h_1 = self.gcn(seq, adj, sparse)\n",
    "        c = self.read(h_1, msk)\n",
    "        Z, S = self.cluster(h_1[-1,:,:], self.beta)\n",
    "        H = S@Z\n",
    "        return h_1.detach(), H.detach(), c.detach(), Z.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf17ca68",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-11T11:12:42.994488Z",
     "start_time": "2022-10-11T11:12:42.977040Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_network_data(filename_adj, nodes_numbers):\n",
    "    raw_edges = pd.read_csv(filename_adj, header=None, sep='\\t')\n",
    "    \n",
    "    drop_self_loop = raw_edges[raw_edges[0] != raw_edges[1]]\n",
    "    \n",
    "    graph_np = np.zeros((nodes_numbers, nodes_numbers))\n",
    "    for i in range(drop_self_loop.shape[0]):\n",
    "        graph_np[drop_self_loop.iloc[i,0], drop_self_loop.iloc[i,1]]=1\n",
    "        graph_np[drop_self_loop.iloc[i,1], drop_self_loop.iloc[i,0]]=1\n",
    "        \n",
    "    adj = nx.adjacency_matrix(nx.from_numpy_matrix(graph_np))\n",
    "    \n",
    "    features = sp.lil_matrix(np.eye(nodes_numbers))\n",
    "    \n",
    "    return adj, features\n",
    "\n",
    "def get_roc_score(edges_pos, edges_neg, embeddings, adj_sparse):\n",
    "    \"from https://github.com/tkipf/gae\"\n",
    "    \n",
    "    score_matrix = np.dot(embeddings, embeddings.T)\n",
    "    \n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    # Store positive edge predictions, actual values\n",
    "    preds_pos = []\n",
    "    pos = []\n",
    "    for edge in edges_pos:\n",
    "        preds_pos.append(sigmoid(score_matrix[edge[0], edge[1]])) # predicted score\n",
    "        pos.append(adj_sparse[edge[0], edge[1]]) # actual value (1 for positive)\n",
    "        \n",
    "    # Store negative edge predictions, actual values\n",
    "    preds_neg = []\n",
    "    neg = []\n",
    "    for edge in edges_neg:\n",
    "        preds_neg.append(sigmoid(score_matrix[edge[0], edge[1]])) # predicted score\n",
    "        neg.append(adj_sparse[edge[0], edge[1]]) # actual value (0 for negative)\n",
    "    \n",
    "    # Calculate scores\n",
    "    preds_all = np.hstack([preds_pos, preds_neg])\n",
    "    labels_all = np.hstack([np.ones(len(preds_pos)), np.zeros(len(preds_neg))])\n",
    "    \n",
    "    #print(preds_all, labels_all )\n",
    "    \n",
    "    roc_score = roc_auc_score(labels_all, preds_all)\n",
    "    ap_score = average_precision_score(labels_all, preds_all)\n",
    "    \n",
    "    return ap_score, roc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc58185e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-11T11:12:43.734608Z",
     "start_time": "2022-10-11T11:12:43.714578Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(1234)\n",
    "parser = argparse.ArgumentParser(description='Options')\n",
    "parser.add_argument('--d', dest='dataset', type=str, default='wiki',help='')\n",
    "parser.add_argument('--b', dest='beta', type=int, default=100,help='')\n",
    "parser.add_argument('--c', dest='num_clusters', type=float, default=256,help='')\n",
    "parser.add_argument('--a', dest='alpha', type=float, default=0.5,help='')\n",
    "parser.add_argument('--test_rate', dest='test_rate', type=float, default=0.1,help='')\n",
    "args, _ = parser.parse_known_args()\n",
    "#print(args.accumulate(args.integers))\n",
    "\n",
    "cuda0 = False\n",
    "\n",
    "beta = args.beta\n",
    "alpha = args.alpha\n",
    "num_clusters = int(args.num_clusters)\n",
    "\n",
    "dataset = args.dataset\n",
    "\n",
    "# training params\n",
    "batch_size = 1\n",
    "nb_epochs = 2000\n",
    "patience = 50\n",
    "lr = 0.001\n",
    "l2_coef = 0.0\n",
    "drop_prob = 0.0\n",
    "hid_units = 16\n",
    "sparse = True\n",
    "nonlinearity = 'prelu' # special name to separate parameters\n",
    "      \n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ddf0f214",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-11T11:29:46.427895Z",
     "start_time": "2022-10-11T11:13:12.997670Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################# m:  0\n",
      "数据集数量：  10437 10437 0 0 1159 1159\n",
      "ap:  0.8785712135748397     auc:  0.8332746461834865\n",
      "################# m:  1\n",
      "数据集数量：  10437 10437 0 0 1159 1159\n",
      "ap:  0.8829726084188272     auc:  0.8447688905002007\n",
      "################# m:  2\n",
      "数据集数量：  10437 10437 0 0 1159 1159\n",
      "ap:  0.8666525520316688     auc:  0.8188480295634346\n",
      "################# m:  3\n",
      "数据集数量：  10437 10437 0 0 1159 1159\n",
      "ap:  0.8735887889973848     auc:  0.8278409357386876\n",
      "################# m:  4\n",
      "数据集数量：  10437 10437 0 0 1159 1159\n",
      "ap:  0.8870067970895352     auc:  0.8444264453975007\n",
      "################# m:  5\n",
      "数据集数量：  10437 10437 0 0 1159 1159\n",
      "ap:  0.8832334389537413     auc:  0.8464773937843237\n",
      "################# m:  6\n",
      "数据集数量：  10437 10437 0 0 1159 1159\n",
      "ap:  0.8889623419385022     auc:  0.8497559334197387\n",
      "################# m:  7\n",
      "数据集数量：  10437 10437 0 0 1159 1159\n",
      "ap:  0.8873562277611234     auc:  0.8445232233613071\n",
      "################# m:  8\n",
      "数据集数量：  10437 10437 0 0 1159 1159\n",
      "ap:  0.8739298740193978     auc:  0.8260929768231664\n",
      "################# m:  9\n",
      "数据集数量：  10437 10437 0 0 1159 1159\n",
      "ap:  0.8894901966357541     auc:  0.8545873871513108\n"
     ]
    }
   ],
   "source": [
    "roc0=[]\n",
    "ap0=[]\n",
    "roc1=[]\n",
    "ap1=[]\n",
    "roc100 = []\n",
    "ap100 = []\n",
    "adj_name = 'datasets/wiki.txt'\n",
    "nodes_number = 2405\n",
    "\n",
    "for m in range(10):\n",
    "    print(\"################# m: \",m)\n",
    "    adj, features = load_network_data(adj_name, nodes_number)\n",
    "    adj_sparse = adj\n",
    "    #print('Edges init',adj.getnnz())\n",
    "    adj_train, train_edges, train_edges_false, val_edges, val_edges_false, test_edges, test_edges_false = mask_test_edges(adj, test_frac=args.test_rate, val_frac=0)\n",
    "    print(\"数据集数量： \",len(train_edges), len(train_edges_false), len(val_edges), len(val_edges_false), len(test_edges), len(test_edges_false))\n",
    "    \n",
    "    adj = adj_train\n",
    "    #print('Edges new',adj.getnnz())\n",
    "    #ylabels = labels\n",
    "    features, _ = preprocess_features(features)\n",
    "    nb_nodes = features.shape[0]\n",
    "    ft_size = features.shape[1]\n",
    "    #nb_classes = labels.shape[1]\n",
    "\n",
    "    adj = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
    "\n",
    "    if sparse:\n",
    "        sp_adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
    "    else:\n",
    "        adj = (adj + sp.eye(adj.shape[0])).todense()\n",
    "\n",
    "    features = torch.FloatTensor(features[np.newaxis])\n",
    "    if not sparse:\n",
    "        adj = torch.FloatTensor(adj[np.newaxis])\n",
    "    #labels = torch.FloatTensor(labels[np.newaxis])\n",
    "    #idx_train = torch.LongTensor(idx_train)\n",
    "    #idx_val = torch.LongTensor(idx_val)\n",
    "    #idx_test = torch.LongTensor(idx_test)\n",
    "\n",
    "    b_xent = nn.BCEWithLogitsLoss()\n",
    "    b_bce = nn.BCELoss()\n",
    "    #xent = nn.CrossEntropyLoss()\n",
    "    \n",
    "    all_accs = []\n",
    "\n",
    "    for beta in [args.beta]:\n",
    "        #print(\"bata: \",beta)\n",
    "        for K in [int(args.num_clusters)]:\n",
    "            #K = int(Kr * nb_nodes)\n",
    "            for alpha in [args.alpha]:\n",
    "                #print(m, alpha)\n",
    "                model = GIC(nb_nodes,ft_size, hid_units, nonlinearity, num_clusters, beta)\n",
    "                optimiser = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=l2_coef)\n",
    "                cnt_wait = 0\n",
    "                best = 1e9\n",
    "                best_t = 0\n",
    "                val_best = 0\n",
    "\n",
    "                for epoch in range(nb_epochs):\n",
    "                    model.train()\n",
    "                    optimiser.zero_grad()\n",
    "\n",
    "                    idx = np.random.permutation(nb_nodes)\n",
    "                    shuf_fts = features[:, idx, :]\n",
    "\n",
    "                    lbl_1 = torch.ones(batch_size, nb_nodes)\n",
    "                    lbl_2 = torch.zeros(batch_size, nb_nodes)\n",
    "                    lbl = torch.cat((lbl_1, lbl_2), 1)\n",
    "                    \n",
    "                    logits, logits2  = model(features, shuf_fts, sp_adj if sparse else adj, sparse, None, None, None, beta) \n",
    "\n",
    "                    loss = alpha* b_xent(logits, lbl)  + (1-alpha)*b_xent(logits2, lbl) \n",
    "\n",
    "                    if loss < best:\n",
    "                        best = loss\n",
    "                        best_t = epoch\n",
    "                        cnt_wait = 0\n",
    "                        torch.save(model.state_dict(), dataset+'-link.pkl')\n",
    "                        \n",
    "                    else:\n",
    "                        cnt_wait += 1\n",
    "\n",
    "                    if cnt_wait == patience:\n",
    "                        #print('Early stopping!')\n",
    "                        break\n",
    "\n",
    "                    loss.backward()\n",
    "                    optimiser.step()\n",
    "\n",
    "                model.load_state_dict(torch.load(dataset+'-link.pkl'))\n",
    "\n",
    "                embeds, _,_, S= model.embed(features, sp_adj if sparse else adj, sparse, None, beta)\n",
    "                embs = embeds[0, :]\n",
    "                embs = embs / embs.norm(dim=1)[:, None]\n",
    "                ap_score, auc_score = get_roc_score(test_edges, test_edges_false, embs.cpu().detach().numpy(), adj_sparse)\n",
    "                #print(beta, K, alpha, sc_roc, sc_ap,flush=True)\n",
    "                #print('Dataset',args.dataset)\n",
    "                #print('alpha, beta, K:',alpha,beta,K)\n",
    "                print('ap: ', ap_score, '    auc: ', auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d9a7184",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-12T07:40:57.048645Z",
     "start_time": "2022-10-12T07:40:57.034645Z"
    }
   },
   "outputs": [],
   "source": [
    "AP = [0.8786, 0.8830, 0.8667, 0.8736, 0.8870, 0.8832, 0.8890, 0.8874, 0.8739, 0.8895]\n",
    "AUC = [0.8333, 0.8448, 0.8188, 0.8278, 0.8444, 0.8465, 0.8498, 0.8445, 0.8261, 0.8546]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10046d4e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-12T07:41:13.160986Z",
     "start_time": "2022-10-12T07:41:13.144346Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f639b66a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-12T07:41:43.675201Z",
     "start_time": "2022-10-12T07:41:43.658162Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8811899999999999,\n",
       " 0.007328772066315047,\n",
       " 0.8390600000000001,\n",
       " 0.011142908058491753)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(AP), np.std(AP), np.mean(AUC), np.std(AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7cc7fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-env",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
